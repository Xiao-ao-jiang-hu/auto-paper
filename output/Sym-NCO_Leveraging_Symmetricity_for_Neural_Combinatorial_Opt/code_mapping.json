{
  "file_path": "Sym-NCO-AM/problems/op/problem_op.py, Sym-NCO-AM/problems/tsp/problem_tsp.py, Sym-NCO-AM/train.py",
  "function_name": "OP class and OPDataset class, TSP.get_costs, train_batch",
  "code_snippet": "\n\n# ==========================================\n# File: Sym-NCO-AM/problems/op/problem_op.py\n# Function/Context: OP class and OPDataset class\n# ==========================================\nfrom torch.utils.data import Dataset\nimport torch\nimport os\nimport pickle\nfrom problems.op.state_op import StateOP\nfrom utils.beam_search import beam_search\n\n\nclass OP(object):\n\n    NAME = 'op'  # Orienteering problem\n\n    @staticmethod\n    def get_costs(dataset, pi):\n        if pi.size(-1) == 1:  # In case all tours directly return to depot, prevent further problems\n            assert (pi == 0).all(), \"If all length 1 tours, they should be zero\"\n            # Return\n            return torch.zeros(pi.size(0), dtype=torch.float, device=pi.device), None\n\n        # Check that tours are valid, i.e. contain 0 to n -1\n        sorted_pi = pi.data.sort(1)[0]\n        # Make sure each node visited once at most (except for depot)\n        assert ((sorted_pi[:, 1:] == 0) | (sorted_pi[:, 1:] > sorted_pi[:, :-1])).all(), \"Duplicates\"\n\n        prize_with_depot = torch.cat(\n            (\n                torch.zeros_like(dataset['prize'][:, :1]),\n                dataset['prize']\n            ),\n            1\n        )\n        p = prize_with_depot.gather(1, pi)\n\n        # Gather dataset in order of tour\n        loc_with_depot = torch.cat((dataset['depot'][:, None, :], dataset['loc']), 1)\n        d = loc_with_depot.gather(1, pi[..., None].expand(*pi.size(), loc_with_depot.size(-1)))\n\n        length = (\n            (d[:, 1:] - d[:, :-1]).norm(p=2, dim=-1).sum(1)  # Prevent error if len 1 seq\n            + (d[:, 0] - dataset['depot']).norm(p=2, dim=-1)  # Depot to first\n            + (d[:, -1] - dataset['depot']).norm(p=2, dim=-1)  # Last to depot, will be 0 if depot is last\n        )\n        assert (length <= dataset['max_length'] + 1e-5).all(), \\\n            \"Max length exceeded by {}\".format((length - dataset['max_length']).max())\n\n        # We want to maximize total prize but code minimizes so return negative\n        return -p.sum(-1), None\n\n    @staticmethod\n    def make_dataset(*args, **kwargs):\n        return OPDataset(*args, **kwargs)\n\n    @staticmethod\n    def make_state(*args, **kwargs):\n        return StateOP.initialize(*args, **kwargs)\n\n    @staticmethod\n    def beam_search(input, beam_size, expand_size=None,\n                    compress_mask=False, model=None, max_calc_batch_size=4096):\n\n        assert model is not None, \"Provide model\"\n\n        fixed = model.precompute_fixed(input)\n\n        def propose_expansions(beam):\n            return model.propose_expansions(\n                beam, fixed, expand_size, normalize=True, max_calc_batch_size=max_calc_batch_size\n            )\n\n        state = OP.make_state(\n            input, visited_dtype=torch.int64 if compress_mask else torch.uint8\n        )\n\n        return beam_search(state, beam_size, propose_expansions)\n\n\ndef generate_instance(size, prize_type):\n    # Details see paper\n    MAX_LENGTHS = {\n        20: 2.,\n        50: 3.,\n        100: 4.\n    }\n\n    loc = torch.FloatTensor(size, 2).uniform_(0, 1)\n    depot = torch.FloatTensor(2).uniform_(0, 1)\n    # Methods taken from Fischetti et al. 1998\n    if prize_type == 'const':\n        prize = torch.ones(size)\n    elif prize_type == 'unif':\n        prize = (1 + torch.randint(0, 100, size=(size, ))) / 100.\n    else:  # Based on distance to depot\n        assert prize_type == 'dist'\n        prize_ = (depot[None, :] - loc).norm(p=2, dim=-1)\n        prize = (1 + (prize_ / prize_.max(dim=-1, keepdim=True)[0] * 99).int()).float() / 100.\n\n    return {\n        'loc': loc,\n        # Uniform 1 - 9, scaled by capacities\n        'prize': prize,\n        'depot': depot,\n        'max_length': torch.tensor(MAX_LENGTHS[size])\n    }\n\n\nclass OPDataset(Dataset):\n    \n    def __init__(self, filename=None, size=50, num_samples=1000000, offset=0, distribution='const'):\n        super(OPDataset, self).__init__()\n        assert distribution is not None, \"Data distribution must be specified for OP\"\n        # Currently the distribution can only vary in the type of the prize\n        prize_type = distribution\n\n        self.data_set = []\n        if filename is not None:\n            assert os.path.splitext(filename)[1] == '.pkl'\n\n            with open(filename, 'rb') as f:\n                data = pickle.load(f)\n                self.data = [\n                    {\n                        'loc': torch.FloatTensor(loc),\n                        'prize': torch.FloatTensor(prize),\n                        'depot': torch.FloatTensor(depot),\n                        'max_length': torch.tensor(max_length)\n                    }\n                    for depot, loc, prize, max_length in (data[offset:offset+num_samples])\n                ]\n        else:\n            self.data = [\n                generate_instance(size, prize_type)\n                for i in range(num_samples)\n            ]\n\n        self.size = len(self.data)\n\n    def __len__(self):\n        return self.size\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\n# ==========================================\n# File: Sym-NCO-AM/problems/tsp/problem_tsp.py\n# Function/Context: TSP.get_costs\n# ==========================================\nfrom torch.utils.data import Dataset\nimport torch\nimport os\nimport pickle\nfrom problems.tsp.state_tsp import StateTSP\nfrom utils.beam_search import beam_search\n\n\nclass TSP(object):\n\n    NAME = 'tsp'\n\n    @staticmethod\n    def get_costs(dataset, pi):\n        # Check that tours are valid, i.e. contain 0 to n -1\n        assert (\n            torch.arange(pi.size(1), out=pi.data.new()).view(1, -1).expand_as(pi) ==\n            pi.data.sort(1)[0]\n        ).all(), \"Invalid tour\"\n\n        # Gather dataset in order of tour\n        d = dataset.gather(1, pi.unsqueeze(-1).expand_as(dataset))\n\n        # Length is distance (L2-norm of difference) from each next location from its prev and of last from first\n        return (d[:, 1:] - d[:, :-1]).norm(p=2, dim=2).sum(1) + (d[:, 0] - d[:, -1]).norm(p=2, dim=1), None\n\n    @staticmethod\n    def make_dataset(*args, **kwargs):\n        return TSPDataset(*args, **kwargs)\n\n    @staticmethod\n    def make_state(*args, **kwargs):\n        return StateTSP.initialize(*args, **kwargs)\n\n    @staticmethod\n    def beam_search(input, beam_size, expand_size=None,\n                    compress_mask=False, model=None, max_calc_batch_size=4096):\n\n        assert model is not None, \"Provide model\"\n\n        fixed = model.precompute_fixed(input)\n\n        def propose_expansions(beam):\n            return model.propose_expansions(\n                beam, fixed, expand_size, normalize=True, max_calc_batch_size=max_calc_batch_size\n            )\n\n        state = TSP.make_state(\n            input, visited_dtype=torch.int64 if compress_mask else torch.uint8\n        )\n\n        return beam_search(state, beam_size, propose_expansions)\n\n\nclass TSPDataset(Dataset):\n    \n    def __init__(self, filename=None, size=50, num_samples=1000000, offset=0, distribution=None):\n        super(TSPDataset, self).__init__()\n\n        self.data_set = []\n        if filename is not None:\n            assert os.path.splitext(filename)[1] == '.pkl'\n\n            with open(filename, 'rb') as f:\n                data = pickle.load(f)\n                self.data = [torch.FloatTensor(row) for row in (data[offset:offset+num_samples])]\n        else:\n            # Sample points randomly in [0, 1] square\n            self.data = [torch.FloatTensor(size, 2).uniform_(0, 1) for i in range(num_samples)]\n\n        self.size = len(self.data)\n\n    def __len__(self):\n        return self.size\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\n# ==========================================\n# File: Sym-NCO-AM/train.py\n# Function/Context: train_batch\n# ==========================================\nimport os\nimport time\nfrom tqdm import tqdm\nimport torch\nimport math\n\nfrom torch.utils.data import DataLoader\nfrom torch.nn import DataParallel\nfrom torch.nn import CosineSimilarity\nfrom nets.attention_model import set_decode_type\nfrom utils.log_utils import log_values\nfrom utils import move_to\n\n\ndef augment_xy_data_by_N_fold(problems, N, depot=None):\n    x = problems[:, :, [0]]\n    y = problems[:, :, [1]]\n\n    if depot is not None:\n        x_depot = depot[:, :, [0]]\n        y_depot = depot[:, :, [1]]\n    idx = torch.rand(N - 1)\n\n    for i in range(N - 1):\n\n        problems = torch.cat((problems, SR_transform(x, y, idx[i])), dim=0)\n        if depot is not None:\n            depot = torch.cat((depot, SR_transform(x_depot, y_depot, idx[i])), dim=0)\n\n    if depot is not None:\n        return problems, depot.view(-1, 2)\n\n    return problems\n\n\ndef SR_transform(x, y, idx):\n    if idx < 0.5:\n        phi = idx * 4 * math.pi\n    else:\n        phi = (idx - 0.5) * 4 * math.pi\n\n    x = x - 1 / 2\n    y = y - 1 / 2\n\n    x_prime = torch.cos(phi) * x - torch.sin(phi) * y\n    y_prime = torch.sin(phi) * x + torch.cos(phi) * y\n\n    if idx < 0.5:\n        dat = torch.cat((x_prime + 1 / 2, y_prime + 1 / 2), dim=2)\n    else:\n        dat = torch.cat((y_prime + 1 / 2, x_prime + 1 / 2), dim=2)\n    return dat\n\n\ndef augment(input, N, problem):\n    is_vrp = problem.NAME == 'cvrp' or problem.NAME == 'sdvrp'\n    is_orienteering = problem.NAME == 'op'\n    is_pctsp = problem.NAME == 'pctsp'\n    if is_vrp or is_orienteering or is_pctsp:\n        if is_vrp:\n            features = ('demand',)\n        elif is_orienteering:\n            features = ('prize', 'max_length')\n        else:\n            assert is_pctsp\n            features = ('deterministic_prize', 'penalty')\n\n        input['loc'], input['depot'] = augment_xy_data_by_N_fold(input['loc'], N, depot=input['depot'].view(-1, 1, 2))\n\n        for feat in features:\n            input[feat] = input[feat].repeat(N, 1)\n        if is_orienteering:\n            input['max_length'] = input['max_length'].view(-1)\n        return input\n\n        # TSP\n    return augment_xy_data_by_N_fold(input, N)\n\n\ndef train_batch(\n        model,\n        optimizer,\n        baseline,\n        epoch,\n        batch_id,\n        step,\n        batch,\n        opts, problem\n):\n    x, bl_val = baseline.unwrap_batch(batch)\n    x = move_to(x, opts.device)\n    bl_val = move_to(bl_val, opts.device) if bl_val is not None else None\n\n    # Evaluate model, get costs and log probabilities\n\n    x_aug = augment(x, opts.N_aug, problem)\n\n    cost, log_likelihood, proj_nodes = model(x_aug, return_proj=True)\n\n    ######################################################################\n    # rotational invariant consistancy learning\n    ######################################################################\n    proj_nodes = proj_nodes.reshape(opts.N_aug, -1, proj_nodes.shape[1], proj_nodes.shape[2])\n\n    cos = torch.nn.CosineSimilarity(dim=-1)\n    similarity = 0\n    for i in range(opts.N_aug - 1):\n        similarity = similarity + cos(proj_nodes[0], proj_nodes[i + 1])\n\n    similarity /= (opts.N_aug - 1)\n    ######################################################################\n    # rotational invariant consistancy learning\n    ######################################################################\n\n    cost = cost.view(opts.N_aug, -1).permute(1, 0)\n    log_likelihood = log_likelihood.view(opts.N_aug, -1).permute(1, 0)\n\n    # problem symmetric loss\n    advantage = cost - cost.mean(dim=1).view(-1, 1)\n\n    loss = ((advantage) * log_likelihood).mean() - opts.alpha * similarity.mean()\n\n    optimizer.zero_grad()\n    loss.backward()\n    # Clip gradient norms and get (clipped) gradient norms for logging\n    grad_norms = clip_grad_norms(optimizer.param_groups, opts.max_grad_norm)\n    optimizer.step()",
  "description": "Combined Analysis:\n- [Sym-NCO-AM/problems/op/problem_op.py]: This file implements the core Orienteering Problem (OP) environment for the Sym-NCO framework. It contains: 1) The OP class with methods for computing tour costs (get_costs), creating datasets, and beam search; 2) The mathematical model for OP with prize collection objective and maximum tour length constraint; 3) Instance generation with different prize distributions (const, unif, dist); 4) Dataset handling for training/evaluation. The get_costs method implements the optimization objective: maximizing total prize collected while respecting maximum tour length constraint, with Euclidean distance calculations. This serves as the problem environment for the Sym-NCO algorithm to operate on.\n- [Sym-NCO-AM/problems/tsp/problem_tsp.py]: This file implements the core TSP optimization model and data handling. The TSP.get_costs() method directly computes the objective function: min Σ_{i=1}^{N-1} ‖x_{π_i} - x_{π_{i+1}}‖ + ‖x_{π_N} - x_{π_1}‖, where the constraint (π being a permutation) is enforced via the assertion that sorted indices equal 0..n-1. The method uses PyTorch operations to gather coordinates in tour order and compute Euclidean distances (L2 norm). The TSPDataset class handles data loading and generation, supporting both file-based and random uniform sampling in [0,1]². This provides the fundamental problem definition and cost computation that Sym-NCO builds upon with symmetry-aware regularization.\n- [Sym-NCO-AM/train.py]: This file implements the core Sym-NCO training logic described in the paper. The key components are:\n1. Symmetric Data Augmentation: Functions `augment_xy_data_by_N_fold` and `SR_transform` generate multiple symmetric variants of problem instances through rotations and reflections.\n2. Symmetry-Aware Loss: The `train_batch` function computes two loss components:\n   - REINFORCE loss with baseline: Uses advantage (cost - average cost) and log-likelihood\n   - Invariant representation loss: Encourages consistent node representations across symmetric variants via cosine similarity\n3. The total loss combines these with weight α (opts.alpha), implementing the symmetry regularization described in the paper.\n4. The implementation handles multiple COPs (TSP, CVRP, PCTSP, OP) through the `augment` function that adapts to problem-specific features.\nThis directly corresponds to the paper's method of leveraging problem symmetries to improve neural combinatorial optimization solvers.",
  "dependencies": [
    "nets.attention_model.set_decode_type",
    "utils.beam_search.beam_search",
    "torch.nn.CosineSimilarity",
    "os",
    "pickle",
    "problems.tsp.state_tsp.StateTSP",
    "problems.op.state_op.StateOP",
    "torch.utils.data.Dataset",
    "torch",
    "utils.move_to",
    "math"
  ]
}