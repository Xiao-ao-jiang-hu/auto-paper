{
  "file_path": "gflownet/algorithm.py, gflownet/main.py, gflownet/util.py",
  "function_name": "DetailedBalanceTransitionBuffer.train_step, main, GraphCombOptMDP, MaxIndSetMDP, MaxCliqueMDP, MinDominateSetMDP, MaxCutMDP",
  "code_snippet": "\n\n# ==========================================\n# File: gflownet/algorithm.py\n# Function/Context: DetailedBalanceTransitionBuffer.train_step\n# ==========================================\nimport random\nimport networkx as nx\nimport copy\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport dgl\nfrom einops import rearrange, reduce, repeat\n\nfrom util import get_decided, pad_batch, get_parent\nfrom network import GIN\n\n\ndef sample_from_logits(pf_logits, gb, state, done, rand_prob=0.):\n    numnode_per_graph = gb.batch_num_nodes().tolist()\n    pf_logits[get_decided(state)] = -np.inf\n    pf_logits = pad_batch(pf_logits, numnode_per_graph, padding_value=-np.inf)\n\n    # use -1 to denote impossible action (e.g. for done graphs)\n    action = torch.full([gb.batch_size,], -1, dtype=torch.long, device=gb.device)\n    pf_undone = pf_logits[~done].softmax(dim=1)\n    action[~done] = torch.multinomial(pf_undone, num_samples=1).squeeze(-1)\n\n    if rand_prob > 0.:\n        unif_pf_undone = torch.isfinite(pf_logits[~done]).float()\n        rand_action_unodone = torch.multinomial(unif_pf_undone, num_samples=1).squeeze(-1)\n        rand_mask = torch.rand_like(rand_action_unodone.float()) < rand_prob\n        action[~done][rand_mask] = rand_action_unodone[rand_mask]\n    return action\n\n\nclass DetailedBalance(object):\n    def __init__(self, cfg, device):\n        self.cfg = cfg\n        self.task = cfg.task\n        self.device = device\n\n        assert cfg.arch in [\"gin\"]\n        gin_dict = {\"hidden_dim\": cfg.hidden_dim, \"num_layers\": cfg.hidden_layer,\n                    \"dropout\": cfg.dropout, \"learn_eps\": cfg.learn_eps,\n                    \"aggregator_type\": cfg.aggr}\n        self.model = GIN(3, 1, graph_level_output=0, **gin_dict).to(device)\n        self.model_flow = GIN(3, 0, graph_level_output=1, **gin_dict).to(device)\n        self.params = [\n            {\"params\": self.model.parameters(), \"lr\": cfg.lr},\n            {\"params\": self.model_flow.parameters(), \"lr\": cfg.zlr},\n        ]\n        self.optimizer = torch.optim.Adam(self.params)\n        self.leaf_coef = cfg.leaf_coef\n\n    def parameters(self):\n        return list(self.model.parameters()) + list(self.model_flow.parameters())\n\n    @torch.no_grad()\n    def sample(self, gb, state, done, rand_prob=0., temperature=1., reward_exp=None):\n        self.model.eval()\n        pf_logits = self.model(gb, state, reward_exp)[..., 0]\n        return sample_from_logits(pf_logits / temperature, gb, state, done, rand_prob=rand_prob)\n\n    def save(self, path):\n        save_dict = {\n            \"model\": self.model.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n        }\n        save_dict.update({\"model_flow\": self.model_flow.state_dict()})\n        torch.save(save_dict, path)\n        print(f\"Saved to {path}\")\n\n    def load(self, path):\n        save_dict = torch.load(path, map_location=self.device)\n        self.model.load_state_dict(save_dict[\"model\"])\n        self.model_flow.load_state_dict(save_dict[\"model_flow\"])\n        self.optimizer.load_state_dict(save_dict[\"optimizer\"])\n        print(f\"Loaded from {path}\")\n\n    def train_step(self, *batch):\n        raise NotImplementedError\n\n\nclass DetailedBalanceTransitionBuffer(DetailedBalance):\n    def __init__(self, cfg, device):\n        assert cfg.alg in [\"db\", \"fl\"]\n        self.forward_looking = (cfg.alg == \"fl\")\n        super(DetailedBalanceTransitionBuffer, self).__init__(cfg, device)\n\n    def train_step(self, *batch, reward_exp=None, logr_scaler=None):\n        self.model.train()\n        self.model_flow.train()\n        torch.cuda.empty_cache()\n\n        gb, s, logr, a, s_next, logr_next, d = batch\n        gb, s, logr, a, s_next, logr_next, d = gb.to(self.device), s.to(self.device), logr.to(self.device), \\\n                    a.to(self.device), s_next.to(self.device), logr_next.to(self.device), d.to(self.device)\n        logr, logr_next = logr_scaler(logr), logr_scaler(logr_next)\n        numnode_per_graph = gb.batch_num_nodes().tolist()\n        batch_size = gb.batch_size\n\n        total_num_nodes = gb.num_nodes()\n        gb_two = dgl.batch([gb, gb])\n        s_two = torch.cat([s, s_next], dim=0)\n        logits = self.model(gb_two, s_two, reward_exp)\n        _, flows_out = self.model_flow(gb_two, s_two, reward_exp) # (2 * num_graphs, 1)\n        flows, flows_next = flows_out[:batch_size, 0], flows_out[batch_size:, 0]\n\n        pf_logits = logits[:total_num_nodes, ..., 0]\n        pf_logits[get_decided(s)] = -np.inf\n        pf_logits = pad_batch(pf_logits, numnode_per_graph, padding_value=-np.inf)\n        log_pf = F.log_softmax(pf_logits, dim=1)[torch.arange(batch_size), a]\n\n        log_pb = torch.tensor([torch.log(1 / get_parent(s_, self.task).sum())\n         for s_ in torch.split(s_next, numnode_per_graph, dim=0)]).to(self.device)\n\n        if self.forward_looking:\n            flows_next.masked_fill_(d, 0.) # \\tilde F(x) = F(x) / R(x) = 1, log 1 = 0\n            lhs = logr + flows + log_pf # (bs,)\n            rhs = logr_next + flows_next + log_pb\n            loss = (lhs - rhs).pow(2)\n            loss = loss.mean()\n        else:\n            flows_next = torch.where(d, logr_next, flows_next)\n            lhs = flows + log_pf # (bs,)\n            rhs = flows_next + log_pb\n            losses = (lhs - rhs).pow(2)\n            loss = (losses[d].sum() * self.leaf_coef + losses[~d].sum()) / batch_size\n\n        return_dict = {\"train/loss\": loss.item()}\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        return return_dict\n\n# ==========================================\n# File: gflownet/main.py\n# Function/Context: main\n# ==========================================\nimport sys, os\nimport gzip, pickle\nfrom time import time, sleep\nfrom tqdm import tqdm\nimport hydra\nfrom omegaconf import DictConfig, open_dict, OmegaConf\n\nimport random\nimport numpy as np\nimport torch\nimport dgl\nfrom einops import rearrange, reduce, repeat\n\nfrom data import get_data_loaders\nfrom util import seed_torch, TransitionBuffer, get_mdp_class\nfrom algorithm import DetailedBalanceTransitionBuffer\n\ntorch.backends.cudnn.benchmark = True\n\n\ndef get_alg_buffer(cfg, device):\n    assert cfg.alg in [\"db\", \"fl\"]\n    buffer = TransitionBuffer(cfg.tranbuff_size, cfg)\n    alg = DetailedBalanceTransitionBuffer(cfg, device)\n    return alg, buffer\n\ndef get_logr_scaler(cfg, process_ratio=1., reward_exp=None):\n    if reward_exp is None:\n        reward_exp = float(cfg.reward_exp)\n\n    if cfg.anneal == \"linear\":\n        process_ratio = max(0., min(1., process_ratio)) # from 0 to 1\n        reward_exp = reward_exp * process_ratio +\\\n                     float(cfg.reward_exp_init) * (1 - process_ratio)\n    elif cfg.anneal == \"none\":\n        pass\n    else:\n        raise NotImplementedError\n\n    # (R/T)^beta -> (log R - log T) * beta\n    def logr_scaler(sol_size, gbatch=None):\n        logr = sol_size\n        return logr * reward_exp\n    return logr_scaler\n\ndef refine_cfg(cfg):\n    with open_dict(cfg):\n        cfg.device = cfg.d\n        cfg.work_directory = os.getcwd()\n\n        if cfg.task in [\"mis\", \"maxindset\", \"maxindependentset\",]:\n            cfg.task = \"MaxIndependentSet\"\n            cfg.wandb_project_name = \"MIS\"\n        elif cfg.task in [\"mds\", \"mindomset\", \"mindominateset\",]:\n            cfg.task = \"MinDominateSet\"\n            cfg.wandb_project_name = \"MDS\"\n        elif cfg.task in [\"mc\", \"maxclique\",]:\n            cfg.task = \"MaxClique\"\n            cfg.wandb_project_name = \"MaxClique\"\n        elif cfg.task in [\"mcut\", \"maxcut\",]:\n            cfg.task = \"MaxCut\"\n            cfg.wandb_project_name = \"MaxCut\"\n        else:\n            raise NotImplementedError\n\n        # architecture\n        assert cfg.arch in [\"gin\"]\n\n        # log reward shape\n        cfg.reward_exp = cfg.rexp\n        cfg.reward_exp_init = cfg.rexpit\n        if cfg.anneal in [\"lin\"]:\n            cfg.anneal = \"linear\"\n\n        # training\n        cfg.batch_size = cfg.bs\n        cfg.batch_size_interact = cfg.bsit\n        cfg.leaf_coef = cfg.lc\n        cfg.same_graph_across_batch = cfg.sameg\n\n        # data\n        cfg.test_batch_size = cfg.tbs\n        if \"rb\" in cfg.input:\n            cfg.data_type = cfg.input.upper()\n        elif \"ba\" in cfg.input:\n            cfg.data_type = cfg.input.upper()\n        else:\n            raise NotImplementedError\n\n    del cfg.d, cfg.rexp, cfg.rexpit, cfg.bs, cfg.bsit, cfg.lc, cfg.sameg, cfg.tbs\n    return cfg\n\n@torch.no_grad()\ndef rollout(gbatch, cfg, alg):\n    env = get_mdp_class(cfg.task)(gbatch, cfg)\n    state = env.state\n\n    ##### sample traj\n    reward_exp_eval = None\n    traj_s, traj_r, traj_a, traj_d = [], [], [], []\n    while not all(env.done):\n        action = alg.sample(gbatch, state, env.done, rand_prob=cfg.randp, reward_exp=reward_exp_eval)\n\n        traj_s.append(state)\n        traj_r.append(env.get_log_reward())\n        traj_a.append(action)\n        traj_d.append(env.done)\n        state = env.step(action)\n\n    ##### save last state\n    traj_s.append(state)\n    traj_r.append(env.get_log_reward())\n    traj_d.append(env.done)\n    assert len(traj_s) == len(traj_a) + 1 == len(traj_r) == len(traj_d)\n\n    traj_s = torch.stack(traj_s, dim=1) # (sum of #node per graph in batch, max_traj_len)\n    traj_r = torch.stack(traj_r, dim=1) # (batch_size, max_traj_len)\n    traj_a = torch.stack(traj_a, dim=1) # (batch_size, max_traj_len-1)\n    \"\"\"\n    traj_a is tensor like \n    [ 4, 30, 86, 95, 96, 29, -1, -1],\n    [47, 60, 41, 11, 55, 64, 80, -1],\n    [26, 38, 13,  5,  9, -1, -1, -1]\n    \"\"\"\n    traj_d = torch.stack(traj_d, dim=1) # (batch_size, max_traj_len)\n    \"\"\"\n    traj_d is tensor like \n    [False, False, False, False, False, False,  True,  True,  True],\n    [False, False, False, False, False, False, False,  True,  True],\n    [False, False, False, False, False,  True,  True,  True,  True]\n    \"\"\"\n    traj_len = 1 + torch.sum(~traj_d, dim=1) # (batch_size, )\n\n    ##### graph, state, action, done, reward, trajectory length\n    batch = gbatch.cpu(), traj_s.cpu(), traj_a.cpu(), traj_d.cpu(), traj_r.cpu(), traj_len.cpu()\n    return batch, env.batch_metric(state)\n\n\n@hydra.main(config_path=\"configs\", config_name=\"main\") # for hydra-core==1.1.0\n# @hydra.main(version_base=None, config_path=\"configs\", config_name=\"main\") # for newer hydra\ndef main(cfg: DictConfig):\n    cfg = refine_cfg(cfg)\n    device = torch.device(f\"cuda:{cfg.device:d}\" if torch.cuda.is_available() and cfg.device>=0 else \"cpu\")\n    print(f\"Device: {device}\")\n    alg, buffer = get_alg_buffer(cfg, device)\n    seed_torch(cfg.seed)\n    print(str(cfg))\n    print(f\"Work directory: {os.getcwd()}\")\n\n    train_loader, test_loader = get_data_loaders(cfg)\n    trainset_size = len(train_loader.dataset)\n    print(f\"Trainset size: {trainset_size}\")\n    alg_save_path = os.path.abspath(\"./alg.pt\")\n    alg_save_path_best = os.path.abspath(\"./alg_best.pt\")\n    train_data_used = 0\n    train_step = 0\n    train_logr_scaled_ls = []\n    train_metric_ls = []\n    metric_best = 0.\n    result = {\"set_size\": {}, \"logr_scaled\": {}, \"train_data_used\": {}, \"train_step\": {}, }\n\n    @torch.no_grad()\n    def evaluate(ep, train_step, train_data_used, logr_scaler):\n        torch.cuda.empty_cache()\n        num_repeat = 20\n        mis_ls, mis_top20_ls = [], []\n        logr_ls = []\n        pbar = tqdm(enumerate(test_loader))\n        pbar.set_description(f\"Test Epoch {ep:2d} Data used {train_data_used:5d}\")\n        for batch_idx, gbatch in pbar:\n            gbatch = gbatch.to(device)\n            gbatch_rep = dgl.batch([gbatch] * num_repeat)\n\n            env = get_mdp_class(cfg.task)(gbatch_rep, cfg)\n            state = env.state\n            while not all(env.done):\n                action = alg.sample(gbatch_rep, state, env.done, rand_prob=0.)\n                state = env.step(action)\n\n            logr_rep = logr_scaler(env.get_log_reward())\n            logr_ls += logr_rep.tolist()\n            curr_mis_rep = torch.tensor(env.batch_metric(state))\n            curr_mis_rep = rearrange(curr_mis_rep, \"(rep b) -> b rep\", rep=num_repeat).float()\n            mis_ls += curr_mis_rep.mean(dim=1).tolist()\n            mis_top20_ls += curr_mis_rep.max(dim=1)[0].tolist()\n            pbar.set_postfix({\"Metric\": f\"{np.mean(mis_ls):.2f}+-{np.std(mis_ls):.2f}\"})\n\n        print(f\"Test Epoch{ep:2d} Data used{train_data_used:5d}: \"\n              f\"Metric={np.mean(mis_ls):.2f}+-{np.std(mis_ls):.2f}, \"\n              f\"top20={np.mean(mis_top20_ls):.2f}, \"\n              f\"LogR scaled={np.mean(logr_ls):.2e}+-{np.std(logr_ls):.2e}\")\n\n        result[\"set_size\"][ep] = np.mean(mis_ls)\n        result[\"logr_scaled\"][ep] = np.mean(logr_ls)\n        result[\"train_step\"][ep] = train_step\n        result[\"train_data_used\"][ep] = train_data_used\n        pickle.dump(result, gzip.open(\"./result.json\", 'wb'))\n\n    for ep in range(cfg.epochs):\n        for batch_idx, gbatch in enumerate(train_loader):\n            reward_exp = None\n            process_ratio = max(0., min(1., train_data_used / cfg.annend))\n            logr_scaler = get_logr_scaler(cfg, process_ratio=process_ratio, reward_exp=reward_exp)\n\n            train_logr_scaled_ls = train_logr_scaled_ls[-5000:]\n            train_metric_ls = train_metric_ls[-5000:]\n            gbatch = gbatch.to(device)\n            if cfg.same_graph_across_batch:\n                gbatch = dgl.batch([gbatch] * cfg.batch_size_interact)\n            train_data_used += gbatch.batch_size\n\n            ###### rollout\n            batch, metric_ls = rollout(gbatch, cfg, alg)\n            buffer.add_batch(batch)\n\n            logr = logr_scaler(batch[-2][:, -1])\n            train_logr_scaled_ls += logr.tolist()\n            train_logr_scaled = logr.mean().item()\n            train_metric_ls += metric_ls\n            train_traj_len = batch[-1].float().mean().item()\n\n            ##### train\n            batch_size = min(len(buffer), cfg.batch_size)\n            indices = list(range(len(buffer)))\n            for _ in range(cfg.tstep):\n                if len(indices) == 0:\n                    break\n                curr_indices = random.sample(indices, min(len(indices), batch_size))\n                batch = buffer.sample_from_indices(curr_indices)\n                train_info = alg.train_step(*batch, reward_exp=reward_exp, logr_scaler=logr_scaler)\n                indices = [i for i in indices if i not in curr_indices]\n\n            if cfg.onpolicy:\n                buffer.reset()\n\n            if train_step % cfg.print_freq == 0:\n                print(f\"Epoch {ep:2d} Data used {train_data_used:.3e}: loss={train_info['train/loss']:.2e}, \"\n                      + (f\"LogZ={train_info['train/logZ']:.2e}, \" if cfg.alg in [\"tb\", \"tbbw\"] else \"\")\n                      + f\"metric size={np.mean(train_metric_ls):.2f}+-{np.std(train_metric_ls):.2f}, \"\n                      + f\"LogR scaled={train_logr_scaled:.2e} traj_len={train_traj_len:.2f}\")\n\n            train_step += 1\n\n            ##### eval\n            if batch_idx == 0 or train_step % cfg.eval_freq == 0:\n                alg.save(alg_save_path)\n                metric_curr = np.mean(train_metric_ls[-1000:])\n                if metric_curr > metric_best:\n                    metric_best = metric_curr\n                    print(f\"best metric: {metric_best:.2f} at step {train_data_used:.3e}\")\n                    alg.save(alg_save_path_best)\n                if cfg.eval:\n                    evaluate(ep, train_step, train_data_used, logr_scaler)\n\n    evaluate(cfg.epochs, train_step, train_data_used, logr_scaler)\n    alg.save(alg_save_path)\n\n\nif __name__ == \"__main__\":\n    main()\n\n# ==========================================\n# File: gflownet/util.py\n# Function/Context: GraphCombOptMDP, MaxIndSetMDP, MaxCliqueMDP, MinDominateSetMDP, MaxCutMDP\n# ==========================================\nimport os, sys\nfrom itertools import count\nimport random\nimport pathlib\nimport ipdb\nimport functools\n\nimport math\nimport numpy as np\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\nimport dgl\nimport dgl.function as fn\n\n\ndef get_decided(state, task=\"MaxIndependentSet\") -> torch.bool:\n    assert state.dtype == torch.long\n    if task in [\"MaxIndependentSet\", \"MinDominateSet\", \"MaxClique\", \"MaxCut\"]:\n        return state != 2\n    else:\n        raise NotImplementedError\n\ndef get_parent(state, task=\"MaxIndependentSet\") -> torch.bool:\n    assert state.dtype == torch.long\n    if task in [\"MaxIndependentSet\", \"MaxClique\", \"MaxCut\"]:\n        return state == 1\n    elif task in [\"MinDominateSet\"]:\n        return state == 0\n    else:\n        raise NotImplementedError\n\n\nclass GraphCombOptMDP(object):\n    def __init__(self, gbatch, cfg):\n        self.cfg = cfg\n        self.task = cfg.task\n        self.device = gbatch.device\n        self.gbatch = gbatch\n        self.batch_size = gbatch.batch_size\n        self.numnode_per_graph = gbatch.batch_num_nodes().tolist()\n        cum_num_node = gbatch.batch_num_nodes().cumsum(dim=0)\n        self.cum_num_node = torch.cat([torch.tensor([0]).to(cum_num_node), cum_num_node])[:-1]\n        self._state = torch.full((gbatch.num_nodes(),), 2, dtype=torch.long, device=self.device)\n        self.done = torch.full((self.batch_size,), False, dtype=torch.bool, device=self.device)\n\n    @property\n    def state(self):\n        return self._state\n\n    def set_state(self, state):\n        self._state = state\n\n    def get_decided_mask(self, state=None):\n        state = self._state if state is None else state\n        return get_decided(state, self.task)\n\n    def step(self, action):\n        raise NotImplementedError\n\n    def get_log_reward(self):\n        raise NotImplementedError\n\n    def batch_metric(self, state): # return a list of metric\n        raise NotImplementedError\n\ndef get_mdp_class(task):\n    if task == \"MaxIndependentSet\":\n        return MaxIndSetMDP\n    elif task == \"MaxClique\":\n        return MaxCliqueMDP\n    elif task == \"MinDominateSet\":\n        return MinDominateSetMDP\n    elif task == \"MaxCut\":\n        return MaxCutMDP\n    else:\n        raise NotImplementedError\n\nclass MaxIndSetMDP(GraphCombOptMDP):\n    def __init__(self, gbatch, cfg):\n        assert cfg.task == \"MaxIndependentSet\"\n        super(MaxIndSetMDP, self).__init__(gbatch, cfg)\n\n    def step(self, action):\n        state = self._state.clone()\n\n        action_node_idx = (self.cum_num_node + action)[~self.done]\n        assert torch.all(~self.get_decided_mask(state[action_node_idx]))\n        state[action_node_idx] = 1\n\n        with self.gbatch.local_scope():\n            self.gbatch.ndata[\"h\"] = (state == 1).float()\n            self.gbatch.update_all(fn.copy_u(\"h\", \"m\"), fn.sum(\"m\", \"h\"))\n            x1_deg = self.gbatch.ndata.pop('h')\n        undecided = ~get_decided(state)\n        state[undecided & (x1_deg > 0)] = 0\n        self._state = state\n\n        decided_tensor = pad_batch(self.get_decided_mask(state), self.numnode_per_graph, padding_value=True)\n        self.done = torch.all(decided_tensor, dim=1)\n        return state\n\n    def get_log_reward(self):\n        state = pad_batch(self._state, self.numnode_per_graph, padding_value=2)\n        sol = (state == 1).sum(dim=1).float()\n        return sol\n\n    def batch_metric(self, vec_state):\n        state_per_graph = torch.split(vec_state, self.numnode_per_graph, dim=0)\n        return [(s == 1).sum().item() for s in state_per_graph]\n\nclass MaxCliqueMDP(GraphCombOptMDP):\n    def __init__(self, gbatch, cfg,):\n        super(MaxCliqueMDP, self).__init__(gbatch, cfg)\n\n    def step(self, action):\n        state = self._state.clone()\n\n        action_node_idx = (self.cum_num_node + action)[~self.done]\n        assert torch.all(~self.get_decided_mask(state[action_node_idx]))\n        state[action_node_idx] = 1\n\n        num1 = pad_batch(state == 1, self.numnode_per_graph, padding_value=0).sum(dim=1)\n        num1 = [num * torch.ones(count).to(self.device) for count, num in zip(self.numnode_per_graph, num1)]\n        num1 = torch.cat(num1)\n        with self.gbatch.local_scope():\n            self.gbatch.ndata[\"h\"] = (state == 1).float()\n            self.gbatch.update_all(fn.copy_u(\"h\", \"m\"), fn.sum(\"m\", \"h\"))\n            x1_deg = self.gbatch.ndata.pop('h')\n        undecided = ~get_decided(state)\n        state[undecided & (x1_deg < num1)] = 0\n        self._state = state\n\n        decided_tensor = pad_batch(self.get_decided_mask(state), self.numnode_per_graph, padding_value=True)\n        self.done = torch.all(decided_tensor, dim=1)\n        return state\n\n    def get_log_reward(self):\n        state = pad_batch(self._state, self.numnode_per_graph, padding_value=2)\n        sol = (state == 1).sum(dim=1).float()\n        return sol\n\n    def batch_metric(self, vec_state):\n        state_per_graph = torch.split(vec_state, self.numnode_per_graph, dim=0)\n        return [(s == 1).sum().item() for s in state_per_graph]\n\nclass MinDominateSetMDP(GraphCombOptMDP):\n    def __init__(self, gbatch, cfg):\n        super(MinDominateSetMDP, self).__init__(gbatch, cfg)\n        assert not cfg.back_trajectory\n\n    def step(self, action):\n        state = self._state.clone()\n\n        action_node_idx = (self.cum_num_node + action)[~self.done]\n        assert torch.all(~self.get_decided_mask(state[action_node_idx]))\n        state[action_node_idx] = 0\n\n        undecided = ~get_decided(state)\n        with self.gbatch.local_scope():\n            self.gbatch.ndata[\"h\"] = ((state == 1) | (state == 2)).float()\n            self.gbatch.update_all(fn.copy_u(\"h\", \"m\"), fn.sum(\"m\", \"h\"))\n            x12_deg = self.gbatch.ndata.pop('h').int()\n        state[undecided & (x12_deg == 0)] = 1\n\n        special0 = (state == 0) & (x12_deg <= 1)\n        with self.gbatch.local_scope():\n            self.gbatch.ndata[\"h\"] = special0.float()\n            self.gbatch.update_all(fn.copy_u(\"h\", \"m\"), fn.sum(\"m\", \"h\"))\n            xsp0_deg = self.gbatch.ndata.pop('h').int()\n        state[undecided & (xsp0_deg >= 1)] = 1\n\n        self._state = state\n\n        decided_tensor = pad_batch(self.get_decided_mask(state), self.numnode_per_graph, padding_value=True)\n        self.done = torch.all(decided_tensor, dim=1)\n        return state\n\n    def get_log_reward(self):\n        state = pad_batch(self._state, self.numnode_per_graph, padding_value=2)\n        sol = - (state == 1).sum(dim=1).float()\n        return sol\n\n    def batch_metric(self, vec_state):\n        state_per_graph = torch.split(vec_state, self.numnode_per_graph, dim=0)\n        return [-(s == 1).sum().item() for s in state_per_graph]\n\nclass MaxCutMDP(GraphCombOptMDP):\n    def __init__(self, gbatch, cfg):\n        super(MaxCutMDP, self).__init__(gbatch, cfg)\n        assert not cfg.back_trajectory\n\n    def step(self, action):\n        state = self._state.clone()\n\n        action_node_idx = (self.cum_num_node + action)[~self.done]\n        assert torch.all(~self.get_decided_mask(state[action_node_idx]))\n        state[action_node_idx] = 1\n\n        undecided = ~get_decided(state)\n        with self.gbatch.local_scope():\n            self.gbatch.ndata[\"h\"] = (state == 1).float()\n            self.gbatch.update_all(fn.copy_u(\"h\", \"m\"), fn.sum(\"m\", \"h\"))\n            x1_deg = self.gbatch.ndata.pop('h').int()\n        with self.gbatch.local_scope():\n            self.gbatch.ndata[\"h\"] = ((state == 0) | (state == 2)).float()\n            self.gbatch.update_all(fn.copy_u(\"h\", \"m\"), fn.sum(\"m\", \"h\"))\n            x02_deg = self.gbatch.ndata.pop('h').int()\n        state[undecided & (x1_deg > x02_deg)] = 0\n        self._state = state\n\n        decided_tensor = pad_batch(self.get_decided_mask(state), self.numnode_per_graph, padding_value=True)\n        self.done = torch.all(decided_tensor, dim=1)\n        return state\n\n    def get_log_reward(self, state=None):\n        if state is None:\n            state = self._state.clone()\n        state[state == 2] = 0\n        with self.gbatch.local_scope():\n            self.gbatch.ndata[\"h\"] = state.float()\n            self.gbatch.apply_edges(fn.u_add_v(\"h\", \"h\", \"e\"))\n            self.gbatch.edata[\"e\"] = (self.gbatch.edata[\"e\"] == 1).float()\n            cut = dgl.sum_edges(self.gbatch, 'e')\n        cut = cut / 2\n        return cut\n\n    def batch_metric(self, vec_state):\n        return self.get_log_reward(vec_state).tolist()",
  "description": "Combined Analysis:\n- [gflownet/algorithm.py]: This file implements the core GFlowNet training algorithm described in the paper. The DetailedBalanceTransitionBuffer class implements the transition-based forward-looking detailed balance loss (FL-DB) for training GFlowNets on graph combinatorial optimization problems. Key aspects implemented: 1) Two GIN neural networks for forward policy (model) and state flows (model_flow), 2) The detailed balance loss computation with forward-looking adjustment (when cfg.alg='fl'), 3) Action sampling with masking of invalid actions (sample_from_logits), 4) Handling of terminal states and reward scaling. The mathematical formulation matches the paper's GFlowNet approach where solutions are sampled proportionally to rewards exp(-energy/temperature).\n- [gflownet/main.py]: This file implements the core training and evaluation pipeline for GFlowNet-based combinatorial optimization. It directly maps to the paper's algorithm steps by: 1) Configuring the specific optimization problem (MIS, MDS, MC, MaxCut) via refine_cfg(); 2) Implementing the sequential decision-making process through the rollout() function that samples trajectories using GFlowNet policies; 3) Training GFlowNets using detailed balance loss via alg.train_step(); 4) Evaluating solution quality through the evaluate() function. The code orchestrates the complete GFlowNet training loop, including trajectory collection, reward shaping via logr_scaler, and periodic evaluation that measures solution metrics aligned with the paper's objectives.\n- [gflownet/util.py]: This file implements the core Markov Decision Process (MDP) formulation for the four graph combinatorial optimization problems described in the paper. Each problem (MaxIndependentSet, MaxClique, MinDominateSet, MaxCut) has a dedicated MDP class that defines the state representation (0/1/2 for different node statuses), transition dynamics (step method that enforces problem constraints), and reward computation (get_log_reward). The MDPs exactly implement the sequential decision-making process where actions select nodes and states evolve while maintaining feasibility through constraint propagation, matching the paper's GFlowNet approach to combinatorial optimization.",
  "dependencies": [
    "util.get_parent",
    "get_decided",
    "pickle",
    "pad_batch",
    "util.pad_batch",
    "torch.nn.functional",
    "dgl",
    "tqdm",
    "util.get_decided",
    "hydra",
    "util.get_mdp_class",
    "util.seed_torch",
    "util.TransitionBuffer",
    "torch.nn.utils.rnn.pad_sequence",
    "network.GIN",
    "get_parent",
    "algorithm.DetailedBalanceTransitionBuffer",
    "random",
    "numpy",
    "omegaconf",
    "dgl.function",
    "einops",
    "torch",
    "data.get_data_loaders",
    "gzip"
  ]
}