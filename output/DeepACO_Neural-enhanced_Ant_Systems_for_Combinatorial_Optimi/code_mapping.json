{
  "file_path": "cvrp/aco.py, cvrp_nls/aco.py, cvrp_nls/net.py, cvrp_nls/swapstar.py, mkp/aco.py, tsp/aco.py, tsp/net.py, tsp_nls/aco.py, tsp_nls/two_opt.py",
  "function_name": "ACO, ACO, Net, swapstar, ACO, ACO, Net, ACO, two_opt_once, _two_opt_python, batched_two_opt_python",
  "code_snippet": "\n\n# ==========================================\n# File: cvrp/aco.py\n# Function/Context: ACO\n# ==========================================\nimport torch\nfrom torch.distributions import Categorical\nimport random\nimport itertools\nimport numpy as np\n\nCAPACITY = 50\n\nclass ACO():\n\n    def __init__(self,  # 0: depot\n                 distances, # (n, n)\n                 demand,   # (n, )\n                 n_ants=20, \n                 decay=0.9,\n                 alpha=1,\n                 beta=1,\n                 elitist=False,\n                 min_max=False,\n                 pheromone=None,\n                 heuristic=None,\n                 min=None,\n                 device='cpu',\n                 adaptive=False,\n                 capacity=CAPACITY\n                 ):\n        \n        self.problem_size = len(distances)\n        self.distances = distances\n        self.capacity = capacity\n        self.demand = demand\n        \n        self.n_ants = n_ants\n        self.decay = decay\n        self.alpha = alpha\n        self.beta = beta\n        self.elitist = elitist or adaptive\n        self.min_max = min_max\n        self.adaptive = adaptive\n        \n        if min_max:\n            if min is not None:\n                assert min > 1e-9\n            else:\n                min = 0.1\n            self.min = min\n            self.max = None\n        \n        if pheromone is None:\n            self.pheromone = torch.ones_like(self.distances)\n            if min_max:\n                self.pheromone = self.pheromone * self.min\n        else:\n            self.pheromone = pheromone\n        \n        if self.adaptive:\n            self.elite_pool = []\n\n        self.heuristic = 1 / distances if heuristic is None else heuristic # TODO\n\n        self.shortest_path = None\n        self.lowest_cost = float('inf')\n\n        self.device = device\n    \n    def sample(self):\n        paths, log_probs = self.gen_path(require_prob=True)\n        costs = self.gen_path_costs(paths)\n        return costs, log_probs\n        \n\n    @torch.no_grad()\n    def run(self, n_iterations):\n        for _ in range(n_iterations):\n            paths = self.gen_path(require_prob=False)\n            costs = self.gen_path_costs(paths)\n            \n            if self.adaptive:\n                self.improvement_phase(paths, costs)\n            \n            improved = False\n            best_cost, best_idx = costs.min(dim=0)\n            if best_cost < self.lowest_cost:\n                self.shortest_path = paths[:, best_idx]\n                self.lowest_cost = best_cost\n                if self.adaptive:\n                    self.intensification_phase(paths, costs, best_idx)\n                if self.min_max:\n                    max = self.problem_size / self.lowest_cost\n                    if self.max is None:\n                        self.pheromone *= max / self.pheromone.max()\n                    self.max = max\n                improved = True\n\n            if not self.adaptive or improved:           \n                self.update_pheronome(paths, costs)\n                if self.adaptive:\n                    self.elite_pool.insert(0, (self.shortest_path, self.lowest_cost))\n                    if len(self.elite_pool) > 5:  # pool_size = 5\n                        del self.elite_pool[5:]\n            else:\n                self.diversification_phase()\n\n        return self.lowest_cost\n       \n    @torch.no_grad()\n    def update_pheronome(self, paths, costs):\n        '''\n        Args:\n            paths: torch tensor with shape (problem_size, n_ants)\n            costs: torch tensor with shape (n_ants,)\n        '''\n        self.pheromone = self.pheromone * self.decay \n        \n        if self.elitist:\n            best_cost, best_idx = costs.min(dim=0)\n            best_tour = paths[:, best_idx]\n            self.pheromone[best_tour[:-1], torch.roll(best_tour, shifts=-1)[:-1]] += 1.0/best_cost\n        \n        else:\n            for i in range(self.n_ants):\n                path = paths[:, i]\n                cost = costs[i]\n                self.pheromone[path[:-1], torch.roll(path, shifts=-1)[:-1]] += 1.0/cost\n                \n        if self.min_max:\n            self.pheromone[(self.pheromone > 1e-9) * (self.pheromone) < self.min] = self.min\n            self.pheromone[self.pheromone > self.max] = self.max\n        \n        self.pheromone[self.pheromone < 1e-10] = 1e-10\n    \n    @torch.no_grad()\n    def gen_path_costs(self, paths):\n        u = paths.permute(1, 0) # shape: (n_ants, max_seq_len)\n        v = torch.roll(u, shifts=-1, dims=1)  \n        return torch.sum(self.distances[u[:, :-1], v[:, :-1]], dim=1)\n\n    def gen_path(self, require_prob=False):\n        actions = torch.zeros((self.n_ants,), dtype=torch.long, device=self.device)\n        visit_mask = torch.ones(size=(self.n_ants, self.problem_size), device=self.device)\n        visit_mask = self.update_visit_mask(visit_mask, actions)\n        used_capacity = torch.zeros(size=(self.n_ants,), device=self.device)\n        \n        used_capacity, capacity_mask = self.update_capacity_mask(actions, used_capacity)\n        \n        paths_list = [actions] # paths_list[i] is the ith move (tensor) for all ants\n        \n        log_probs_list = [] # log_probs_list[i] is the ith log_prob (tensor) for all ants' actions\n        \n        done = self.check_done(visit_mask, actions)\n        while not done:\n            # print(paths_list)\n            actions, log_probs = self.pick_move(actions, visit_mask, capacity_mask, require_prob)\n            paths_list.append(actions)\n            if require_prob:\n                log_probs_list.append(log_probs)\n                visit_mask = visit_mask.clone()\n            visit_mask = self.update_visit_mask(visit_mask, actions)\n            used_capacity, capacity_mask = self.update_capacity_mask(actions, used_capacity)\n            done = self.check_done(visit_mask, actions)\n            \n        if require_prob:\n            return torch.stack(paths_list), torch.stack(log_probs_list)\n        else:\n            return torch.stack(paths_list)\n        \n    def pick_move(self, prev, visit_mask, capacity_mask, require_prob):\n        pheromone = self.pheromone[prev] # shape: (n_ants, p_size)\n        heuristic = self.heuristic[prev] # shape: (n_ants, p_size)\n        dist = ((pheromone ** self.alpha) * (heuristic ** self.beta) * visit_mask * capacity_mask) # shape: (n_ants, p_size)\n        dist = Categorical(dist)\n        actions = dist.sample() # shape: (n_ants,)\n        log_probs = dist.log_prob(actions) if require_prob else None # shape: (n_ants,)\n        return actions, log_probs\n    \n    def update_visit_mask(self, visit_mask, actions):\n        visit_mask[torch.arange(self.n_ants, device=self.device), actions] = 0\n        visit_mask[:, 0] = 1 # depot can be revisited with one exception\n        visit_mask[(actions==0) * (visit_mask[:, 1:]!=0).any(dim=1), 0] = 0 # one exception is here\n        return visit_mask\n    \n    def update_capacity_mask(self, cur_nodes, used_capacity):\n        '''\n        Args:\n            cur_nodes: shape (n_ants, )\n            used_capacity: shape (n_ants, )\n            capacity_mask: shape (n_ants, p_size)\n        Returns:\n            ant_capacity: updated capacity\n            capacity_mask: updated mask\n        '''\n        capacity_mask = torch.ones(size=(self.n_ants, self.problem_size), device=self.device)\n        # update capacity\n        used_capacity[cur_nodes==0] = 0\n        used_capacity = used_capacity + self.demand[cur_nodes]\n        # update capacity_mask\n        remaining_capacity = self.capacity - used_capacity # (n_ants,)\n        remaining_capacity_repeat = remaining_capacity.unsqueeze(-1).repeat(1, self.problem_size) # (n_ants, p_size)\n        demand_repeat = self.demand.unsqueeze(0).repeat(self.n_ants, 1) # (n_ants, p_size)\n        capacity_mask[demand_repeat > remaining_capacity_repeat] = 0\n        \n        return used_capacity, capacity_mask\n    \n    def check_done(self, visit_mask, actions):\n        return (visit_mask[:, 1:] == 0).all() and (actions == 0).all()\n\n# ==========================================\n# File: cvrp_nls/aco.py\n# Function/Context: ACO\n# ==========================================\nimport torch\nfrom torch.distributions import Categorical\nimport random\nimport itertools\nimport numpy as np\nfrom swapstar import swapstar\nfrom functools import cached_property\nimport concurrent.futures\n\nCAPACITY = 1.0 # The input demands shall be normalized\n\ndef get_subroutes(route, end_with_zero = True):\n    x = torch.nonzero(route == 0).flatten()\n    subroutes = []\n    for i, j in zip(x, x[1:]):\n        if j-i>1:\n            if end_with_zero:\n                j=j+1\n            subroutes.append(route[i:j])\n    return subroutes\n\ndef merge_subroutes(subroutes, length, device):\n    route = torch.zeros(length, dtype = torch.long, device=device)\n    i=0\n    for r in subroutes:\n        if len(r)>2:\n            if isinstance(r, list):\n                r = torch.tensor(r[:-1])\n            else:\n                r = r[:-1].clone().detach()\n            route[i: i+len(r)] = r\n            i+=len(r)\n    return route\n    \nclass ACO():\n\n    def __init__(self,  # 0: depot\n                 distances, # (n, n)\n                 demand,   # (n, )\n                 n_ants=20, \n                 decay=0.9,\n                 alpha=1,\n                 beta=1,\n                 elitist=False,\n                 min_max=False,\n                 pheromone=None,\n                 heuristic=None,\n                 min=None,\n                 device='cpu',\n                 adaptive=False,\n                 capacity=CAPACITY,\n                 swapstar = False,\n                 positions = None,\n                 inference = False,\n                 ):\n        \n        self.problem_size = len(distances)\n        self.distances = distances\n        self.capacity = capacity\n        self.demand = demand\n        \n        self.n_ants = n_ants\n        self.decay = decay\n        self.alpha = alpha\n        self.beta = beta\n        self.elitist = elitist or adaptive\n        self.min_max = min_max\n        self.adaptive = adaptive\n        self.swapstar = swapstar\n        self.positions = positions\n        self.inference = inference\n\n        assert positions is not None if swapstar else True\n        \n        if min_max:\n            if min is not None:\n                assert min > 1e-9\n            else:\n                min = 0.1\n            self.min = min\n            self.max = None\n        \n        if pheromone is None:\n            self.pheromone = torch.ones_like(self.distances)\n            if min_max:\n                self.pheromone = self.pheromone * self.min\n        else:\n            self.pheromone = pheromone\n        \n        if self.adaptive:\n            self.elite_pool = []\n\n        self.heuristic = 1 / distances if heuristic is None else heuristic\n\n        self.shortest_path = None\n        self.lowest_cost = float('inf')\n\n        self.device = device\n    \n    def sample(self, inference = False):\n        # TODO\n        paths, log_probs = self.gen_path(require_prob=True)\n        costs = self.gen_path_costs(paths)\n        return costs, log_probs, paths\n\n    def sample_nls(self):\n        paths, log_probs = self.gen_path(require_prob=True)\n        costs_raw = self.gen_path_costs(paths).detach()\n        self.multiple_swap_star(paths)\n        costs = self.gen_path_costs(paths).detach()\n        return costs, log_probs, costs_raw\n    \n    @torch.no_grad()\n    def multiple_swap_star(self, paths, indexes = None):\n        subroutes_all = []\n        for i in range(paths.size(1)) if indexes is None else indexes:\n            subroutes = get_subroutes(paths[:, i])\n            subroutes_all.append((i, subroutes))\n        with concurrent.futures.ThreadPoolExecutor() as executor:\n            futures = []\n            for i, p in subroutes_all:\n                future = executor.submit(neural_swapstar, self.demand_cpu, self.distances_cpu, self.heuristic_dist, self.positions_cpu, p,\n                                        limit = 100000 if self.inference else max(self.problem_size, 50))\n                futures.append((i, future))\n            for i, future in futures:\n                paths[:, i] = merge_subroutes(future.result(), paths.size(0), self.device)\n        \n    @cached_property\n    @torch.no_grad()\n    def heuristic_dist(self):\n        heu = self.heuristic.detach().cpu().numpy()\n        return (1 / (heu/heu.max(-1, keepdims=True) + 1e-5))\n\n    @torch.no_grad()\n    def run(self, n_iterations, inference = False):\n        for _ in range(n_iterations):\n            paths = self.gen_path(require_prob=False)\n            costs = self.gen_path_costs(paths)\n            \n            if self.adaptive:\n                self.improvement_phase(paths, costs)\n            \n            if self.swapstar:\n                indexes = costs.topk(8, largest=False).indices\n                self.multiple_swap_star(paths, indexes=indexes)\n                costs = self.gen_path_costs(paths)\n\n            improved = False\n            best_cost, best_idx = costs.min(dim=0)\n            if best_cost < self.lowest_cost:\n                self.shortest_path = paths[:, best_idx].clone()\n                self.lowest_cost = best_cost\n                if self.adaptive:\n                    self.intensification_phase(paths, costs, best_idx)\n                if self.min_max:\n                    max = self.problem_size / self.lowest_cost\n                    if self.max is None:\n                        self.pheromone *= max / self.pheromone.max()\n                    self.max = max\n                improved = True\n\n            if not self.adaptive or improved:           \n                self.update_pheronome(paths, costs)\n                if self.adaptive:\n                    self.elite_pool.insert(0, (self.shortest_path, self.lowest_cost))\n                    if len(self.elite_pool) > 5:  # pool_size = 5\n                        del self.elite_pool[5:]\n            else:\n                self.diversification_phase()\n\n        return self.lowest_cost\n       \n    @torch.no_grad()\n    def update_pheronome(self, paths, costs):\n        '''\n        Args:\n            paths: torch tensor with shape (problem_size, n_ants)\n            costs: torch tensor with shape (n_ants,)\n        '''\n        self.pheromone = self.pheromone * self.decay \n        \n        if self.elitist:\n            best_cost, best_idx = costs.min(dim=0)\n            best_tour = paths[:, best_idx]\n            self.pheromone[best_tour[:-1], torch.roll(best_tour, shifts=-1)[:-1]] += 1.0/best_cost\n        \n        else:\n            for i in range(self.n_ants):\n                path = paths[:, i]\n                cost = costs[i]\n                self.pheromone[path[:-1], torch.roll(path, shifts=-1)[:-1]] += 1.0/cost\n                \n        if self.min_max:\n            self.pheromone[(self.pheromone > 1e-9) * (self.pheromone) < self.min] = self.min\n            self.pheromone[self.pheromone > self.max] = self.max\n        \n        self.pheromone[self.pheromone < 1e-10] = 1e-10\n    \n    @torch.no_grad()\n    def gen_path_costs(self, paths):\n        u = paths.permute(1, 0) # shape: (n_ants, max_seq_len)\n        v = torch.roll(u, shifts=-1, dims=1)  \n        return torch.sum(self.distances[u[:, :-1], v[:, :-1]], dim=1)\n\n    def gen_path(self, require_prob=False):\n        actions = torch.zeros((self.n_ants,), dtype=torch.long, device=self.device)\n        visit_mask = torch.ones(size=(self.n_ants, self.problem_size), device=self.device)\n        visit_mask = self.update_visit_mask(visit_mask, actions)\n        used_capacity = torch.zeros(size=(self.n_ants,), device=self.device)\n        \n        used_capacity, capacity_mask = self.update_capacity_mask(actions, used_capacity)\n        \n        paths_list = [actions] # paths_list[i] is the ith move (tensor) for all ants\n        \n        log_probs_list = [] # log_probs_list[i] is the ith log_prob (tensor) for all ants' actions\n        \n        done = self.check_done(visit_mask, actions)\n        while not done:\n            # print(paths_list)\n            actions, log_probs = self.pick_move(actions, visit_mask, capacity_mask, require_prob)\n            paths_list.append(actions)\n            if require_prob:\n                log_probs_list.append(log_probs)\n                visit_mask = visit_mask.clone()\n            visit_mask = self.update_visit_mask(visit_mask, actions)\n            used_capacity, capacity_mask = self.update_capacity_mask(actions, used_capacity)\n            done = self.check_done(visit_mask, actions)\n            \n        if require_prob:\n            return torch.stack(paths_list), torch.stack(log_probs_list)\n        else:\n            return torch.stack(paths_list)\n        \n    def pick_move(self, prev, visit_mask, capacity_mask, require_prob):\n        pheromone = self.pheromone[prev] # shape: (n_ants, p_size)\n        heuristic = self.heuristic[prev] # shape: (n_ants, p_size)\n        dist = ((pheromone ** self.alpha) * (heuristic ** self.beta) * visit_mask * capacity_mask) # shape: (n_ants, p_size)\n        dist = Categorical(dist)\n        actions = dist.sample() # shape: (n_ants,)\n        log_probs = dist.log_prob(actions) if require_prob else None # shape: (n_ants,)\n        return actions, log_probs\n    \n    def update_visit_mask(self, visit_mask, actions):\n        visit_mask[torch.arange(self.n_ants, device=self.device), actions] = 0\n        visit_mask[:, 0] = 1 # depot can be revisited with one exception\n        visit_mask[(actions==0) * (visit_mask[:, 1:]!=0).any(dim=1), 0] = 0 # one exception is here\n        return visit_mask\n    \n    def update_capacity_mask(self, cur_nodes, used_capacity):\n        '''\n        Args:\n            cur_nodes: shape (n_ants, )\n            used_capacity: shape (n_ants, )\n            capacity_mask: shape (n_ants, p_size)\n        Returns:\n            ant_capacity: updated capacity\n            capacity_mask: updated mask\n        '''\n        capacity_mask = torch.ones(size=(self.n_ants, self.problem_size), device=self.device)\n        # update capacity\n        used_capacity[cur_nodes==0] = 0\n        used_capacity = used_capacity + self.demand[cur_nodes]\n        # update capacity_mask\n        remaining_capacity = self.capacity - used_capacity # (n_ants,)\n        remaining_capacity_repeat = remaining_capacity.unsqueeze(-1).repeat(1, self.problem_size) # (n_ants, p_size)\n        demand_repeat = self.demand.unsqueeze(0).repeat(self.n_ants, 1) # (n_ants, p_size)\n        capacity_mask[demand_repeat > remaining_capacity_repeat] = 0\n        \n        return used_capacity, capacity_mask\n    \n    def check_done(self, visit_mask, actions):\n        return (visit_mask[:, 1:] == 0).all() and (actions == 0).all()\n    \n    @cached_property\n    @torch.no_grad()\n    def distances_cpu(self):\n        return self.distances.cpu().numpy()\n    \n    @cached_property\n    @torch.no_grad()\n    def demand_cpu(self):\n        return self.demand.cpu().numpy()\n    \n    @cached_property\n    @torch.no_grad()\n    def positions_cpu(self):\n        return self.positions.cpu().numpy() if self.positions is not None else None\n\n# ==========================================\n# File: cvrp_nls/net.py\n# Function/Context: Net\n# ==========================================\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nimport torch_geometric.nn as gnn\n\n# GNN for edge embeddings\nclass EmbNet(nn.Module):\n    def __init__(self, depth=12, feats=1, units=32, act_fn='silu', agg_fn='mean'):\n        super().__init__()\n        self.depth = depth\n        self.feats = feats\n        self.units = units\n        self.act_fn = getattr(F, act_fn)\n        self.agg_fn = getattr(gnn, f'global_{agg_fn}_pool')\n        self.v_lin0 = nn.Linear(self.feats, self.units)\n        self.v_lins1 = nn.ModuleList([nn.Linear(self.units, self.units) for i in range(self.depth)])\n        self.v_lins2 = nn.ModuleList([nn.Linear(self.units, self.units) for i in range(self.depth)])\n        self.v_lins3 = nn.ModuleList([nn.Linear(self.units, self.units) for i in range(self.depth)])\n        self.v_lins4 = nn.ModuleList([nn.Linear(self.units, self.units) for i in range(self.depth)])\n        self.v_bns = nn.ModuleList([gnn.BatchNorm(self.units) for i in range(self.depth)])\n        self.e_lin0 = nn.Linear(1, self.units)\n        self.e_lins0 = nn.ModuleList([nn.Linear(self.units, self.units) for i in range(self.depth)])\n        self.e_bns = nn.ModuleList([gnn.BatchNorm(self.units) for i in range(self.depth)])\n    def reset_parameters(self):\n        raise NotImplementedError\n    def forward(self, x, edge_index, edge_attr):\n        x = x\n        w = edge_attr\n        x = self.v_lin0(x)\n        x = self.act_fn(x)\n        w = self.e_lin0(w)\n        w = self.act_fn(w)\n        for i in range(self.depth):\n            x0 = x\n            x1 = self.v_lins1[i](x0)\n            x2 = self.v_lins2[i](x0)\n            x3 = self.v_lins3[i](x0)\n            x4 = self.v_lins4[i](x0)\n            w0 = w\n            w1 = self.e_lins0[i](w0)\n            w2 = torch.sigmoid(w0)\n            x = x0 + self.act_fn(self.v_bns[i](x1 + self.agg_fn(w2 * x2[edge_index[1]], edge_index[0])))\n            w = w0 + self.act_fn(self.e_bns[i](w1 + x3[edge_index[0]] + x4[edge_index[1]]))\n        return w\n\n# general class for MLP\nclass MLP(nn.Module):\n    @property\n    def device(self):\n        return self._dummy.device\n    def __init__(self, units_list, act_fn):\n        super().__init__()\n        self._dummy = nn.Parameter(torch.empty(0), requires_grad = False)\n        self.units_list = units_list\n        self.depth = len(self.units_list) - 1\n        self.act_fn = getattr(F, act_fn)\n        self.lins = nn.ModuleList([nn.Linear(self.units_list[i], self.units_list[i + 1]) for i in range(self.depth)])\n    def forward(self, x):\n        for i in range(self.depth):\n            x = self.lins[i](x)\n            if i < self.depth - 1:\n                x = self.act_fn(x)\n            else:\n                x = torch.sigmoid(x) # last layer\n        return x\n\n# MLP for predicting parameterization theta\nclass ParNet(MLP):\n    def __init__(self, depth=3, units=32, preds=1, act_fn='silu'):\n        self.units = units\n        self.preds = preds\n        super().__init__([self.units] * depth + [self.preds], act_fn)\n    def forward(self, x):\n        return super().forward(x).squeeze(dim = -1)\n    \n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.emb_net = EmbNet()\n        self.par_net_heu = ParNet()\n    def forward(self, pyg):\n        x, edge_index, edge_attr = pyg.x, pyg.edge_index, pyg.edge_attr\n        emb = self.emb_net(x, edge_index, edge_attr)\n        heu = self.par_net_heu(emb)\n        return heu\n    \n    def freeze_gnn(self):\n        for param in self.emb_net.parameters():\n            param.requires_grad = False\n            \n    @staticmethod\n    def reshape(pyg, vector):\n        '''Turn phe/heu vector into matrix with zero padding \n        '''\n        n_nodes = pyg.x.shape[0]\n        device = pyg.x.device\n        matrix = torch.zeros(size=(n_nodes, n_nodes), device=device)\n        matrix[pyg.edge_index[0], pyg.edge_index[1]] = vector\n        return matrix\n\n# ==========================================\n# File: cvrp_nls/swapstar.py\n# Function/Context: swapstar\n# ==========================================\nimport os\nimport platform\nfrom ctypes import (\n    Structure,\n    CDLL,\n    POINTER,\n    c_int,\n    c_double,\n    c_char,\n    sizeof,\n    cast,\n    byref,\n    c_char_p,\n)\nfrom dataclasses import dataclass\nfrom typing import List\nimport numpy as np\nimport sys\nimport torch\nimport random\nimport time\n\ndef write_routes(routes: List[List[int]], filepath: str):\n    with open(filepath, \"w\") as f:\n        for i, r in enumerate(routes):\n            f.write(f\"Route #{i+1}: \"+' '.join([str(x.item()) for x in r if x>0])+\"\\n\")\n    return\n\ndef read_routes(filepath):\n    routes = []\n    with open(filepath, \"r\") as f:\n        while 1:\n            line = f.readline().strip()\n            if line.startswith(\"Route\"):\n                routes.append(torch.tensor([0, *map(int, line.split(\":\")[1].split()), 0]))\n            else:\n                break\n    return routes\n\ndef get_lib_filename():\n    path = \"HGS-CVRP-main/build/libhgscvrp.so\"\n    if os.path.isfile(path):\n        return path\n    else:\n        raise FileNotFoundError(f\"Shared library file `{path}` not found\")\n\nbasedir = os.path.dirname(os.path.realpath(__file__))\nHGS_LIBRARY_FILEPATH = os.path.join(basedir, get_lib_filename())\n\nc_double_p = POINTER(c_double)\nc_int_p = POINTER(c_int)\nC_INT_MAX = 2 ** (sizeof(c_int) * 8 - 1) - 1\nC_DBL_MAX = sys.float_info.max\n\nclass CAlgorithmParameters(Structure):\n    _fields_ = [\n        (\"nbGranular\", c_int),\n        (\"mu\", c_int),\n        (\"lambda\", c_int),\n        (\"nbElite\", c_int),\n        (\"nbClose\", c_int),\n        (\"targetFeasible\", c_double),\n        (\"seed\", c_int),\n        (\"nbIter\", c_int),\n        (\"timeLimit\", c_double),\n        (\"useSwapStar\", c_int),\n    ]\n\n@dataclass\nclass AlgorithmParameters:\n    nbGranular: int = 20\n    mu: int = 25\n    lambda_: int = 40\n    nbElite: int = 4\n    nbClose: int = 5\n    targetFeasible: float = 0.2\n    seed: int = 0\n    nbIter: int = 20000\n    timeLimit: float = 0.0\n    useSwapStar: bool = True\n\n    @property\n    def ctypes(self) -> CAlgorithmParameters:\n        return CAlgorithmParameters(\n            self.nbGranular,\n            self.mu,\n            self.lambda_,\n            self.nbElite,\n            self.nbClose,\n            self.targetFeasible,\n            self.seed,\n            self.nbIter,\n            self.timeLimit,\n            int(self.useSwapStar),\n        )\n\nclass _SolutionRoute(Structure):\n    _fields_ = [(\"length\", c_int), (\"path\", c_int_p)]\n\nclass _Solution(Structure):\n    _fields_ = [\n        (\"cost\", c_double),\n        (\"time\", c_double),\n        (\"n_routes\", c_int),\n        (\"routes\", POINTER(_SolutionRoute)),\n    ]\n\nclass RoutingSolution:\n    def __init__(self, sol_ptr):\n        if not sol_ptr:\n            raise TypeError(\"The solution pointer is null.\")\n\n        self.cost = sol_ptr[0].cost\n        self.time = sol_ptr[0].time\n        self.n_routes = sol_ptr[0].n_routes\n        self.routes = []\n        for i in range(self.n_routes):\n            r = sol_ptr[0].routes[i]\n            path = r.path[0 : r.length]\n            self.routes.append(path)\n\nclass Solver:\n    def __init__(self, parameters=AlgorithmParameters(), verbose=False):\n        if platform.system() == \"Windows\":\n            hgs_library = CDLL(HGS_LIBRARY_FILEPATH, winmode=0)\n        else:\n            hgs_library = CDLL(HGS_LIBRARY_FILEPATH)\n\n        self.algorithm_parameters = parameters\n        self.verbose = verbose\n\n        self._c_api_solve_cvrp = hgs_library.solve_cvrp\n        self._c_api_solve_cvrp.argtypes = [\n            c_int,\n            c_double_p,\n            c_double_p,\n            c_double_p,\n            c_double_p,\n            c_double,\n            c_double,\n            c_char,\n            c_char,\n            c_int,\n            POINTER(CAlgorithmParameters),\n            c_char,\n        ]\n        self._c_api_solve_cvrp.restype = POINTER(_Solution)\n\n        self._c_api_local_search = hgs_library.local_search\n        self._c_api_local_search.argtypes = [\n            c_int,\n            c_double_p,\n            c_double_p,\n            c_double_p,\n            c_double_p,\n            c_double_p,\n            c_double,\n            c_double,\n            c_char,\n            c_int,\n            POINTER(CAlgorithmParameters),\n            c_char,\n            c_int,\n            c_int,\n        ]\n        self._c_api_local_search.restype = c_int\n\n        self._c_api_delete_sol = hgs_library.delete_solution\n        self._c_api_delete_sol.restype = None\n        self._c_api_delete_sol.argtypes = [POINTER(_Solution)]\n    \n    def local_search(self, data, routes: List[List[int]], count:int = 1,rounding=True,):\n        demand = np.asarray(data[\"demands\"])\n        vehicle_capacity = data[\"vehicle_capacity\"]\n        n_nodes = len(demand)\n\n        depot = data.get(\"depot\", 0)\n        if depot != 0:\n            raise ValueError(\"In HGS, the depot location must be 0.\")\n\n        maximum_number_of_vehicles = data.get(\"num_vehicles\", C_INT_MAX)\n\n        service_times = data.get(\"service_times\")\n        if service_times is None:\n            service_times = np.zeros(n_nodes)\n        else:\n            service_times = np.asarray(service_times)\n\n        duration_limit = data.get(\"duration_limit\")\n        if duration_limit is None:\n            is_duration_constraint = False\n            duration_limit = C_DBL_MAX\n        else:\n            is_duration_constraint = True\n\n        is_rounding_integer = rounding\n\n        x_coords = data.get(\"x_coordinates\")\n        y_coords = data.get(\"y_coordinates\")\n        dist_mtx = data.get(\"distance_matrix\")\n\n        if x_coords is None or y_coords is None:\n            assert dist_mtx is not None\n            x_coords = np.zeros(n_nodes)\n            y_coords = np.zeros(n_nodes)\n        else:\n            x_coords = np.asarray(x_coords)\n            y_coords = np.asarray(y_coords)\n\n        assert len(x_coords) == len(y_coords) == len(service_times) == len(demand)\n        assert (x_coords >= 0.0).all()\n        assert (y_coords >= 0.0).all()\n        assert (service_times >= 0.0).all()\n        assert (demand >= 0.0).all()\n\n        dist_mtx = np.asarray(dist_mtx)\n        assert dist_mtx.shape[0] == dist_mtx.shape[1]\n        assert (dist_mtx >= 0.0).all()\n\n        callid = (time.time_ns()*10000+random.randint(0,10000))%C_INT_MAX\n\n        tmppath = \"/tmp/route-{}\".format(callid)\n        resultpath = \"/tmp/swapstar-result-{}\".format(callid)\n        write_routes(routes, tmppath)\n        try:\n            self._local_search(\n                x_coords,\n                y_coords,\n                dist_mtx,\n                service_times,\n                demand,\n                vehicle_capacity,\n                duration_limit,\n                is_duration_constraint,\n                maximum_number_of_vehicles,\n                self.algorithm_parameters,\n                self.verbose,\n                callid,\n                count,\n            )\n\n            result = read_routes(resultpath)\n        except Exception as e:\n            print(routes)\n            print([demand[r].sum() for r in routes])\n        else:\n            os.remove(resultpath)\n        finally:\n            os.remove(tmppath)\n        \n        return result\n\n    def _local_search(\n        self,\n        x_coords: np.ndarray,\n        y_coords: np.ndarray,\n        dist_mtx: np.ndarray,\n        service_times: np.ndarray,\n        demand: np.ndarray,\n        vehicle_capacity: int,\n        duration_limit: float,\n        is_duration_constraint: bool,\n        maximum_number_of_vehicles: int,\n        algorithm_parameters: AlgorithmParameters,\n        verbose: bool,\n        callid: int,\n        count:int,\n    ):\n        n_nodes = x_coords.size\n\n        x_ct = x_coords.astype(c_double).ctypes\n        y_ct = y_coords.astype(c_double).ctypes\n        s_ct = service_times.astype(c_double).ctypes\n        d_ct = demand.astype(c_double).ctypes\n\n        m_ct = dist_mtx.reshape(n_nodes * n_nodes).astype(c_double).ctypes\n        ap_ct = algorithm_parameters.ctypes\n\n        sol_p = self._c_api_local_search(\n            n_nodes,\n            cast(x_ct, c_double_p),\n            cast(y_ct, c_double_p),\n            cast(m_ct, c_double_p),\n            cast(s_ct, c_double_p),\n            cast(d_ct, c_double_p),\n            vehicle_capacity,\n            duration_limit,\n            is_duration_constraint,\n            maximum_number_of_vehicles,\n            byref(ap_ct),\n            verbose,\n            callid,\n            count,\n        )\n\n        result = sol_p\n        return result\n\ndef swapstar(demands, matrix, positions, routes, count=1):\n    ap = AlgorithmParameters()\n    hgs_solver = Solver(parameters=ap, verbose=False)\n\n    data = dict()\n    x = positions[:, 0]\n    y = positions[:, 1]\n    data['x_coordinates'] = x\n    data['y_coordinates'] = y\n\n    data['depot'] = 0\n    data['demands'] = demands*1000\n    data[\"num_vehicles\"] = len(routes)\n    data['vehicle_capacity'] = 1000.001  # to avoid floating-point error\n\n    data['distance_matrix'] = matrix\n    try:\n        result = hgs_solver.local_search(data, routes, count)\n    except Exception as e:\n        print(e)\n        return routes\n    return result\n\n# ==========================================\n# File: mkp/aco.py\n# Function/Context: ACO\n# ==========================================\nimport torch\nfrom torch.distributions import Categorical\nimport numpy as np\n\nclass ACO():\n\n    def __init__(self,  # constraints are set to n//2 after norm\n                 prize,  # shape [n,]\n                 weight, # shape [n, m]\n                 n_ants=20, \n                 decay=0.9,\n                 alpha=1,\n                 beta=1,\n                 elitist=False,\n                 min_max=False,\n                 pheromone=None,\n                 heuristic=None,\n                 min=None,\n                 device='cpu'\n                 ):\n\n        self.n = prize.size(0)\n        self.m = weight.size(1)\n        \n        self.prize = prize\n        self.weight = weight\n\n        self.n_ants = n_ants\n        self.decay = decay\n        self.alpha = alpha\n        self.beta = beta\n        self.elitist = elitist\n        self.min_max = min_max\n        \n        if min_max:\n            if min is not None:\n                assert min > 1e-9\n            else:\n                min = 0.1\n            self.min = min\n            self.max = 20\n        \n        if pheromone is None:\n            self.pheromone = torch.ones(size=(self.n+1, self.n+1), device=device)\n            if min_max:\n                self.pheromone = self.pheromone * self.min\n        else:\n            self.pheromone = pheromone\n\n        # Fidanova S. Hybrid ant colony optimization algorithm for multiple knapsack problem\n        self.heuristic = (prize / self.weight.sum(dim=1)).unsqueeze(0).repeat(self.n, 1) if heuristic is None else heuristic\n        # Leguizamon G, Michalewicz Z. A New Version of Ant System for Subset Problems\n        self.Q = 1/self.prize.sum()\n\n        self.alltime_best_sol = None\n        self.alltime_best_obj = 0\n        self.device = device\n        self.add_dummy_node()\n        \n    def add_dummy_node(self):\n        self.prize = torch.cat((self.prize, torch.tensor([0.], device=self.device))) # (n+1,)\n        self.weight = torch.cat((self.weight, torch.zeros((1, self.m), device=self.device)), dim=0) # (n+1, m)\n        heu_added_row = torch.cat((self.heuristic, torch.zeros((1, self.n), device=self.device)), dim=0) # (n+1, n)\n        self.heuristic = torch.cat((heu_added_row, 1e-10*torch.ones((self.n+1, 1), device=self.device)), dim=1)\n        \n    def sample(self):\n        sols, log_probs = self.gen_sol(require_prob=True)\n        objs = self.gen_sol_obj(sols)\n        return objs, log_probs\n\n    @torch.no_grad()\n    def run(self, n_iterations):\n        for _ in range(n_iterations):\n            sols = self.gen_sol(require_prob=False) # (n_ants, max_horizon)\n            objs = self.gen_sol_obj(sols)             # (n_ants,)\n            sols = sols.T\n            best_obj, best_idx = objs.max(dim=0)\n            if best_obj > self.alltime_best_obj:\n                self.alltime_best_obj = best_obj\n                self.alltime_best_sol = sols[best_idx]\n            self.update_pheronome(sols, objs, best_obj.item(), best_idx.item())\n\n        return self.alltime_best_obj, self.alltime_best_sol\n\n    @torch.no_grad()\n    def update_pheronome(self, sols, objs, best_obj, best_idx):\n        self.pheromone = self.pheromone * self.decay \n        if self.elitist:\n            best_sol= sols[best_idx]\n            self.pheromone[best_sol[:-1], torch.roll(best_sol, shifts=-1)[:-1]] += self.Q * best_obj\n        else:\n            for i in range(self.n_ants):\n                sol = sols[i]\n                obj = objs[i]\n                self.pheromone[sol[:-1], torch.roll(sol, shifts=-1)[:-1]] += self.Q * obj\n                \n        if self.min_max:\n            self.pheromone[(self.pheromone>1e-9) * (self.pheromone)<self.min] = self.min\n            self.pheromone[self.pheromone>self.max] = self.max\n        \n        self.pheromone[self.pheromone<1e-10] = 1e-10\n        \n    @torch.no_grad()\n    def gen_sol_obj(self, solutions):\n        '''\n        Args:\n            solutions: (n_ants, max_horizon)\n        Return:\n            obj: (n_ants,)\n        '''\n        return self.prize[solutions.T].sum(dim=1) # (n_ants,)\n\n    def gen_sol(self, require_prob=False):\n        '''\n        Solution contruction for all ants\n        '''\n        log_probs_list = []\n\n        items = torch.randint(low=0, high=self.n, size=(self.n_ants,), device=self.device)\n        solutions = [items]\n        \n        knapsack = torch.zeros(size=(self.n_ants, self.m), device=self.device)  # used capacity\n        mask = torch.ones(size=(self.n_ants, self.n+1), device=self.device)\n\n        dummy_mask = torch.ones(size=(self.n_ants, self.n+1), device=self.device)\n        dummy_mask[:, -1] = 0\n        \n        mask, knapsack = self.update_knapsack(mask, knapsack, items)\n        dummy_mask = self.update_dummy_state(mask, dummy_mask)\n        done = self.check_done(mask)\n        while not done:\n            items, log_probs = self.pick_item(items, mask, dummy_mask, require_prob)\n            solutions.append(items)\n            log_probs_list.append(log_probs)\n            if require_prob:\n                mask = mask.clone()\n                dummy_mask = dummy_mask.clone()\n            mask, knapsack = self.update_knapsack(mask, knapsack, items)\n            dummy_mask = self.update_dummy_state(mask, dummy_mask)\n            done = self.check_done(mask)\n        if require_prob:\n            return torch.stack(solutions), torch.stack(log_probs_list)  # shape: [max_horizon, n_ants]\n        else:\n            return torch.stack(solutions)\n    \n    def pick_item(self, items, mask, dummy_mask, require_prob):\n        phe = self.pheromone[items]\n        heu = self.heuristic[items]\n        dist = ((phe ** self.alpha) * (heu ** self.beta) * mask * dummy_mask) # (n_ants, n+1)\n        dist = Categorical(dist)\n        item = dist.sample()\n        log_prob = dist.log_prob(item) if require_prob else None\n        return item, log_prob # (n_ants,)\n    \n    def check_done(self, mask):\n        # is mask all zero except for the dummy node?\n        return (mask[:, :-1] == 0).all()\n    \n    def update_dummy_state(self, mask, dummy_mask):\n        finished = (mask[: ,:-1] == 0).all(dim=1)\n        dummy_mask[finished] = 1\n        return dummy_mask\n    \n    def update_knapsack(self, mask, knapsack, new_item):\n        '''\n        Args:\n            mask: (n_ants, n+1)\n            knapsack: (n_ants, m)\n            new_item: (n_ants)\n        '''\n        if new_item is not None:\n            mask[torch.arange(self.n_ants), new_item] = 0\n            knapsack += self.weight[new_item] # (n_ants, m)\n        for ant_idx in range(self.n_ants):\n            candidates = torch.nonzero(mask[ant_idx]) # (x, 1)\n            if len(candidates) > 1:\n                candidates.squeeze_()\n                test_knapsack = knapsack[ant_idx].unsqueeze(0).repeat(len(candidates), 1) # (x, m)\n                new_knapsack = test_knapsack + self.weight[candidates] # (x, m)\n                infeasible_idx = candidates[(new_knapsack > self.n // 2).any(dim=1)]\n                mask[ant_idx, infeasible_idx] = 0\n        mask[:, -1] = 1\n        return mask, knapsack\n\nif __name__ == '__main__':\n    torch.set_printoptions(precision=3,sci_mode=False)\n    torch.manual_seed(1234)\n    from utils import gen_instance\n    prize, weight = gen_instance(100, 5, 'cpu') \n    aco = ACO(prize=prize, weight=weight, n_ants=10)\n    for i in range(200):\n        obj, sol = aco.run(1)\n        print(obj)\n        print(sol)\n    print(aco.pheromone)\n\n# ==========================================\n# File: tsp/aco.py\n# Function/Context: ACO\n# ==========================================\nimport torch\nfrom torch.distributions import Categorical\n\nclass ACO():\n\n    def __init__(self, \n                 distances,\n                 n_ants=20, \n                 decay=0.9,\n                 alpha=1,\n                 beta=1,\n                 elitist=False,\n                 min_max=False,\n                 pheromone=None,\n                 heuristic=None,\n                 min=None,\n                 device='cpu'\n                 ):\n        \n        self.problem_size = len(distances)\n        self.distances  = distances\n        self.n_ants = n_ants\n        self.decay = decay\n        self.alpha = alpha\n        self.beta = beta\n        self.elitist = elitist\n        self.min_max = min_max\n        \n        if min_max:\n            if min is not None:\n                assert min > 1e-9\n            else:\n                min = 0.1\n            self.min = min\n            self.max = None\n        \n        if pheromone is None:\n            self.pheromone = torch.ones_like(self.distances)\n            if min_max:\n                self.pheromone = self.pheromone * self.min\n        else:\n            self.pheromone = pheromone\n\n        self.heuristic = 1 / distances if heuristic is None else heuristic\n\n        self.shortest_path = None\n        self.lowest_cost = float('inf')\n\n        self.device = device\n\n    @torch.no_grad()\n    def sparsify(self, k_sparse):\n        '''\n        Sparsify the TSP graph to obtain the heuristic information \n        Used for vanilla ACO baselines\n        '''\n        _, topk_indices = torch.topk(self.distances, \n                                        k=k_sparse, \n                                        dim=1, largest=False)\n        edge_index_u = torch.repeat_interleave(\n            torch.arange(len(self.distances), device=self.device),\n            repeats=k_sparse\n            )\n        edge_index_v = torch.flatten(topk_indices)\n        sparse_distances = torch.ones_like(self.distances) * 1e10\n        sparse_distances[edge_index_u, edge_index_v] = self.distances[edge_index_u, edge_index_v]\n        self.heuristic = 1 / sparse_distances\n    \n    def sample(self):\n        paths, log_probs = self.gen_path(require_prob=True)\n        costs = self.gen_path_costs(paths)\n        return costs, log_probs\n\n    @torch.no_grad()\n    def run(self, n_iterations):\n        for _ in range(n_iterations):\n            paths = self.gen_path(require_prob=False)\n            costs = self.gen_path_costs(paths)\n            \n            best_cost, best_idx = costs.min(dim=0)\n            if best_cost < self.lowest_cost:\n                self.shortest_path = paths[:, best_idx]\n                self.lowest_cost = best_cost\n                if self.min_max:\n                    max = self.problem_size / self.lowest_cost\n                    if self.max is None:\n                        self.pheromone *= max/self.pheromone.max()\n                    self.max = max\n            \n            self.update_pheronome(paths, costs)\n\n        return self.lowest_cost\n       \n    @torch.no_grad()\n    def update_pheronome(self, paths, costs):\n        '''\n        Args:\n            paths: torch tensor with shape (problem_size, n_ants)\n            costs: torch tensor with shape (n_ants,)\n        '''\n        self.pheromone = self.pheromone * self.decay \n        \n        if self.elitist:\n            best_cost, best_idx = costs.min(dim=0)\n            best_tour= paths[:, best_idx]\n            self.pheromone[best_tour, torch.roll(best_tour, shifts=1)] += 1.0/best_cost\n            self.pheromone[torch.roll(best_tour, shifts=1), best_tour] += 1.0/best_cost\n        \n        else:\n            for i in range(self.n_ants):\n                path = paths[:, i]\n                cost = costs[i]\n                self.pheromone[path, torch.roll(path, shifts=1)] += 1.0/cost\n                self.pheromone[torch.roll(path, shifts=1), path] += 1.0/cost\n                \n        if self.min_max:\n            self.pheromone[(self.pheromone > 1e-9) * (self.pheromone) < self.min] = self.min\n            self.pheromone[self.pheromone > self.max] = self.max\n    \n    @torch.no_grad()\n    def gen_path_costs(self, paths):\n        '''\n        Args:\n            paths: torch tensor with shape (problem_size, n_ants)\n        Returns:\n                Lengths of paths: torch tensor with shape (n_ants,)\n        '''\n        assert paths.shape == (self.problem_size, self.n_ants)\n        u = paths.T # shape: (n_ants, problem_size)\n        v = torch.roll(u, shifts=1, dims=1)  # shape: (n_ants, problem_size)\n        assert (self.distances[u, v] > 0).all()\n        return torch.sum(self.distances[u, v], dim=1)\n\n    def gen_path(self, require_prob=False):\n        '''\n        Tour contruction for all ants\n        Returns:\n            paths: torch tensor with shape (problem_size, n_ants), paths[:, i] is the constructed tour of the ith ant\n            log_probs: torch tensor with shape (problem_size, n_ants), log_probs[i, j] is the log_prob of the ith action of the jth ant\n        '''\n        start = torch.randint(low=0, high=self.problem_size, size=(self.n_ants,), device=self.device)\n        mask = torch.ones(size=(self.n_ants, self.problem_size), device=self.device)\n        mask[torch.arange(self.n_ants, device=self.device), start] = 0\n        \n        paths_list = [] # paths_list[i] is the ith move (tensor) for all ants\n        paths_list.append(start)\n        \n        log_probs_list = [] # log_probs_list[i] is the ith log_prob (tensor) for all ants' actions\n        \n        prev = start\n        for _ in range(self.problem_size-1):\n            actions, log_probs = self.pick_move(prev, mask, require_prob)\n            paths_list.append(actions)\n            if require_prob:\n                log_probs_list.append(log_probs)\n                mask = mask.clone()\n            prev = actions\n            mask[torch.arange(self.n_ants, device=self.device), actions] = 0\n        \n        if require_prob:\n            return torch.stack(paths_list), torch.stack(log_probs_list)\n        else:\n            return torch.stack(paths_list)\n        \n    def pick_move(self, prev, mask, require_prob):\n        '''\n        Args:\n            prev: tensor with shape (n_ants,), previous nodes for all ants\n            mask: bool tensor with shape (n_ants, p_size), masks (0) for the visited cities\n        '''\n        pheromone = self.pheromone[prev] # shape: (n_ants, p_size)\n        heuristic = self.heuristic[prev] # shape: (n_ants, p_size)\n        dist = ((pheromone ** self.alpha) * (heuristic ** self.beta) * mask) # shape: (n_ants, p_size)\n        dist = Categorical(dist)\n        actions = dist.sample() # shape: (n_ants,)\n        log_probs = dist.log_prob(actions) if require_prob else None # shape: (n_ants,)\n        return actions, log_probs\n\n# ==========================================\n# File: tsp/net.py\n# Function/Context: Net\n# ==========================================\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom copy import deepcopy\nimport torch_geometric.nn as gnn\n\n# GNN for edge embeddings\nclass EmbNet(nn.Module):\n    def __init__(self, depth=12, feats=2, units=32, act_fn='silu', agg_fn='mean'): # TODO feats=1\n        super().__init__()\n        self.depth = depth\n        self.feats = feats\n        self.units = units\n        self.act_fn = getattr(F, act_fn)\n        self.agg_fn = getattr(gnn, f'global_{agg_fn}_pool')\n        self.v_lin0 = nn.Linear(self.feats, self.units)\n        self.v_lins1 = nn.ModuleList([nn.Linear(self.units, self.units) for i in range(self.depth)])\n        self.v_lins2 = nn.ModuleList([nn.Linear(self.units, self.units) for i in range(self.depth)])\n        self.v_lins3 = nn.ModuleList([nn.Linear(self.units, self.units) for i in range(self.depth)])\n        self.v_lins4 = nn.ModuleList([nn.Linear(self.units, self.units) for i in range(self.depth)])\n        self.v_bns = nn.ModuleList([gnn.BatchNorm(self.units) for i in range(self.depth)])\n        self.e_lin0 = nn.Linear(1, self.units)\n        self.e_lins0 = nn.ModuleList([nn.Linear(self.units, self.units) for i in range(self.depth)])\n        self.e_bns = nn.ModuleList([gnn.BatchNorm(self.units) for i in range(self.depth)])\n    def reset_parameters(self):\n        raise NotImplementedError\n    def forward(self, x, edge_index, edge_attr):\n        x = x\n        w = edge_attr\n        x = self.v_lin0(x)\n        x = self.act_fn(x)\n        w = self.e_lin0(w)\n        w = self.act_fn(w)\n        for i in range(self.depth):\n            x0 = x\n            x1 = self.v_lins1[i](x0)\n            x2 = self.v_lins2[i](x0)\n            x3 = self.v_lins3[i](x0)\n            x4 = self.v_lins4[i](x0)\n            w0 = w\n            w1 = self.e_lins0[i](w0)\n            w2 = torch.sigmoid(w0)\n            x = x0 + self.act_fn(self.v_bns[i](x1 + self.agg_fn(w2 * x2[edge_index[1]], edge_index[0])))\n            w = w0 + self.act_fn(self.e_bns[i](w1 + x3[edge_index[0]] + x4[edge_index[1]]))\n        return w\n\n# general class for MLP\nclass MLP(nn.Module):\n    @property\n    def device(self):\n        return self._dummy.device\n    def __init__(self, units_list, act_fn):\n        super().__init__()\n        self._dummy = nn.Parameter(torch.empty(0), requires_grad = False)\n        self.units_list = units_list\n        self.depth = len(self.units_list) - 1\n        self.act_fn = getattr(F, act_fn)\n        self.lins = nn.ModuleList([nn.Linear(self.units_list[i], self.units_list[i + 1]) for i in range(self.depth)])\n    def forward(self, x):\n        for i in range(self.depth):\n            x = self.lins[i](x)\n            if i < self.depth - 1:\n                x = self.act_fn(x)\n            else:\n                x = torch.sigmoid(x) # last layer\n        return x\n\n# MLP for predicting parameterization theta\nclass ParNet(MLP):\n    def __init__(self, depth=3, units=32, preds=1, act_fn='silu'):\n        self.units = units\n        self.preds = preds\n        super().__init__([self.units] * depth + [self.preds], act_fn)\n    def forward(self, x):\n        return super().forward(x).squeeze(dim = -1)\n    \n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.emb_net = EmbNet()\n        self.par_net_phe = ParNet()\n        self.par_net_heu = ParNet()\n    def forward(self, pyg):\n        x, edge_index, edge_attr = pyg.x, pyg.edge_index, pyg.edge_attr\n        emb = self.emb_net(x, edge_index, edge_attr)\n        heu = self.par_net_heu(emb)\n        return heu\n    \n    def freeze_gnn(self):\n        for param in self.emb_net.parameters():\n            param.requires_grad = False\n            \n    @staticmethod\n    def reshape(pyg, vector):\n        '''Turn phe/heu vector into matrix with zero padding \n        '''\n        n_nodes = pyg.x.shape[0]\n        device = pyg.x.device\n        matrix = torch.zeros(size=(n_nodes, n_nodes), device=device)\n        matrix[pyg.edge_index[0], pyg.edge_index[1]] = vector\n        return matrix\n\n# ==========================================\n# File: tsp_nls/aco.py\n# Function/Context: ACO\n# ==========================================\nimport torch\nimport numpy as np\nimport numba as nb\nfrom torch.distributions import Categorical\nfrom two_opt import batched_two_opt_python\nimport random\nimport concurrent.futures\nfrom functools import cached_property\n\nclass ACO():\n\n    def __init__(self, \n                 distances,\n                 n_ants=20, \n                 decay=0.9,\n                 alpha=1,\n                 beta=1,\n                 elitist=False,\n                 min_max=False,\n                 pheromone=None,\n                 heuristic=None,\n                 min=None,\n                 two_opt = False, # for compatibility\n                 device='cpu',\n                 local_search = 'nls',\n                 ):\n        \n        self.problem_size = len(distances)\n        self.distances = distances.to(device)\n        self.n_ants = n_ants\n        self.decay = decay\n        self.alpha = alpha\n        self.beta = beta\n        self.elitist = elitist\n        self.min_max = min_max\n        \n        if min_max:\n            if min is not None:\n                assert min > 1e-9\n            else:\n                min = 0.1\n            self.min = min\n            self.max = None\n        \n        if pheromone is None:\n            self.pheromone = torch.ones_like(self.distances)\n            if min_max:\n                self.pheromone = self.pheromone * self.min\n        else:\n            self.pheromone = pheromone.to(device)\n        \n        assert local_search in [None, \"2opt\", \"nls\"]\n        self.local_search_type = '2opt' if two_opt else local_search\n\n        self.heuristic = 1 / distances if heuristic is None else heuristic\n\n        self.shortest_path = None\n        self.lowest_cost = float('inf')\n\n        self.device = device\n\n    @torch.no_grad()\n    def sparsify(self, k_sparse):\n        '''\n        Sparsify the TSP graph to obtain the heuristic information \n        Used for vanilla ACO baselines\n        '''\n        _, topk_indices = torch.topk(self.distances, \n                                        k=k_sparse, \n                                        dim=1, largest=False)\n        edge_index_u = torch.repeat_interleave(\n            torch.arange(len(self.distances), device=self.device),\n            repeats=k_sparse\n            )\n        edge_index_v = torch.flatten(topk_indices)\n        sparse_distances = torch.ones_like(self.distances) * 1e10\n        sparse_distances[edge_index_u, edge_index_v] = self.distances[edge_index_u, edge_index_v]\n        self.heuristic = 1 / sparse_distances\n    \n    def sample(self, inference = False):\n        if inference:\n            probmat = (self.pheromone ** self.alpha) * (self.heuristic ** self.beta)\n            paths = inference_batch_sample(probmat.cpu().numpy(), self.n_ants, 0)\n            paths = torch.from_numpy(paths.T.astype(np.int64)).to(self.device)\n            costs = self.gen_path_costs(paths)\n            return costs, None, paths\n        else:\n            paths, log_probs = self.gen_path(require_prob=True)\n            costs = self.gen_path_costs(paths)\n            return costs, log_probs, paths\n    \n    def sample_2opt(self, paths):\n        paths = self.local_search(paths)\n        costs = self.gen_path_costs(paths)\n        return costs, paths\n    \n    def local_search(self, paths, inference = False):\n        if self.local_search_type == \"2opt\":\n            paths = self.two_opt(paths, inference)\n        elif self.local_search_type == \"nls\":\n            paths = self.nls(paths, inference)\n        return paths\n\n    @torch.no_grad()\n    def run(self, n_iterations, inference = False):\n        for _ in range(n_iterations):\n            if inference:\n                probmat = (self.pheromone ** self.alpha) * (self.heuristic ** self.beta)\n                paths = inference_batch_sample(probmat.cpu().numpy(), self.n_ants, 0)\n                paths = torch.from_numpy(paths.T.astype(np.int64)).to(self.device)\n            else:\n                paths = self.gen_path(require_prob=False)\n\n            paths = self.local_search(paths, inference)\n            costs = self.gen_path_costs(paths)\n            \n            best_cost, best_idx = costs.min(dim=0)\n            if best_cost < self.lowest_cost:\n                self.shortest_path = paths[:, best_idx]\n                self.lowest_cost = best_cost.item()\n                if self.min_max:\n                    max = self.problem_size / self.lowest_cost\n                    if self.max is None:\n                        self.pheromone *= max/self.pheromone.max()\n                    self.max = max\n            \n            self.update_pheronome(paths, costs)\n\n        return self.lowest_cost\n       \n    @torch.no_grad()\n    def update_pheronome(self, paths, costs):\n        '''\n        Args:\n            paths: torch tensor with shape (problem_size, n_ants)\n            costs: torch tensor with shape (n_ants,)\n        '''\n        self.pheromone = self.pheromone * self.decay \n        \n        if self.elitist:\n            best_cost, best_idx = costs.min(dim=0)\n            best_tour= paths[:, best_idx]\n            self.pheromone[best_tour, torch.roll(best_tour, shifts=1)] += 1.0/best_cost\n            self.pheromone[torch.roll(best_tour, shifts=1), best_tour] += 1.0/best_cost\n        \n        else:\n            for i in range(self.n_ants):\n                path = paths[:, i]\n                cost = costs[i]\n                self.pheromone[path, torch.roll(path, shifts=1)] += 1.0/cost\n                self.pheromone[torch.roll(path, shifts=1), path] += 1.0/cost\n                \n        if self.min_max:\n            self.pheromone[(self.pheromone > 1e-9) * (self.pheromone) < self.min] = self.min\n            self.pheromone[self.pheromone > self.max] = self.max\n    \n    @torch.no_grad()\n    def gen_path_costs(self, paths):\n        '''\n        Args:\n            paths: torch tensor with shape (problem_size, n_ants)\n        Returns:\n                Lengths of paths: torch tensor with shape (n_ants,)\n        '''\n        assert paths.shape == (self.problem_size, self.n_ants)\n        u = paths.T # shape: (n_ants, problem_size)\n        v = torch.roll(u, shifts=1, dims=1)  # shape: (n_ants, problem_size)\n        assert (self.distances[u, v] > 0).all()\n        return torch.sum(self.distances[u, v], dim=1)\n    \n    def gen_numpy_path_costs(self, paths, numpy_distances):\n        '''\n        Args:\n            paths: numpy ndarray with shape (n_ants, problem_size), note the shape\n        Returns:\n            Lengths of paths: numpy ndarray with shape (n_ants,)\n        '''\n        assert paths.shape == (self.n_ants, self.problem_size)\n        u = paths\n        v = np.roll(u, shift=1, axis=1)  # shape: (n_ants, problem_size)\n        # assert (self.distances[u, v] > 0).all()\n        return np.sum(numpy_distances[u, v], axis=1)\n    \n    def gen_path(self, require_prob=False):\n        '''\n        Tour contruction for all ants\n        Returns:\n            paths: torch tensor with shape (problem_size, n_ants), paths[:, i] is the constructed tour of the ith ant\n            log_probs: torch tensor with shape (problem_size, n_ants), log_probs[i, j] is the log_prob of the ith action of the jth ant\n        '''\n        start = torch.zeros((self.n_ants, ), dtype = torch.long, device=self.device)\n        # start = torch.randint(low=0, high=self.problem_size, size=(self.n_ants,), device=self.device)\n        mask = torch.ones(size=(self.n_ants, self.problem_size), device=self.device)\n        index = torch.arange(self.n_ants, device=self.device)\n        prob_mat = (self.pheromone ** self.alpha) * (self.heuristic ** self.beta)\n\n        mask[index, start] = 0\n        \n        paths_list = [] # paths_list[i] is the ith move (tensor) for all ants\n        paths_list.append(start)\n        \n        log_probs_list = [] # log_probs_list[i] is the ith log_prob (tensor) for all ants' actions\n        prev = start\n        for _ in range(self.problem_size-1):\n            dist = prob_mat[prev] * mask\n            dist = dist / dist.sum(axis=-1, keepdims=True)\n            dist = Categorical(dist, validate_args=False)\n            actions = dist.sample() # shape: (n_ants,)\n            paths_list.append(actions)\n            if require_prob:\n                log_probs = dist.log_prob(actions) # shape: (n_ants,)\n                log_probs_list.append(log_probs)\n                mask = mask.clone()\n            prev = actions\n            mask[index, actions] = 0\n        \n        if require_prob:\n            return torch.stack(paths_list), torch.stack(log_probs_list)\n        else:\n            return torch.stack(paths_list)\n    \n    @cached_property\n    def distances_numpy(self):\n        return self.distances.detach().cpu().numpy().astype(np.float32)\n\n    @cached_property\n    def heuristic_numpy(self):\n        return self.heuristic.detach().cpu().numpy().astype(np.float32)\n    \n    @cached_property\n    def heuristic_dist(self):\n        return 1 / (self.heuristic_numpy/self.heuristic_numpy.max(-1, keepdims=True) + 1e-5)\n    \n    def two_opt(self, paths, inference = False):\n        maxt = 10000 if inference else self.problem_size//4\n        best_paths = batched_two_opt_python(self.distances_numpy, paths.T.cpu().numpy(), max_iterations=maxt)\n        best_paths = torch.from_numpy(best_paths.T.astype(np.int64)).to(self.device)\n\n        return best_paths\n    \n    def nls(self, paths, inference = False, T_nls = 10, T_p = 20):\n        maxt = 10000 if inference else self.problem_size//4\n        best_paths = batched_two_opt_python(self.distances_numpy, paths.T.cpu().numpy(), max_iterations=maxt)\n        best_costs = self.gen_numpy_path_costs(best_paths, self.distances_numpy)\n        new_paths = best_paths\n        \n        for _ in range(T_nls):\n            perturbed_paths = batched_two_opt_python(self.heuristic_dist, new_paths, max_iterations=T_p)\n            new_paths = batched_two_opt_python(self.distances_numpy, perturbed_paths, max_iterations=maxt)\n            new_costs = self.gen_numpy_path_costs(new_paths, self.distances_numpy)\n\n            improved_indices = new_costs < best_costs\n            best_paths[improved_indices] = new_paths[improved_indices]\n            best_costs[improved_indices] = new_costs[improved_indices]\n        \n        best_paths = torch.from_numpy(best_paths.T.astype(np.int64)).to(self.device)\n\n        return best_paths\n\n@nb.jit(nb.uint16[:](nb.float32[:,:],nb.int64), nopython=True, nogil=True)\ndef _inference_sample(probmat: np.ndarray, startnode = 0):\n    n = probmat.shape[0]\n    route = np.zeros(n, dtype=np.uint16)\n    mask = np.ones(n, dtype=np.uint8)\n    route[0] = lastnode = startnode   # fixed starting node\n    for j in range(1, n):\n        mask[lastnode] = 0\n        prob = probmat[lastnode] * mask\n        rand = random.random() * prob.sum()\n        for k in range(n):\n            rand -= prob[k]\n            if rand <= 0:\n                break\n        lastnode = route[j] = k\n    return route\n\n\ndef inference_batch_sample(probmat: np.ndarray, count=1, startnode = None):\n    n = probmat.shape[0]\n    routes = np.zeros((count, n), dtype=np.uint16)\n    probmat = probmat.astype(np.float32)\n    if startnode is None:\n        startnode = np.random.randint(0, n, size=count)\n    else:\n        startnode = np.ones(count) * startnode\n    if count <= 4 and n < 500:\n        for i in range(count):\n            routes[i] = _inference_sample(probmat, startnode[i])\n    else:\n        with concurrent.futures.ThreadPoolExecutor() as executor:\n            futures = []\n            for i in range(count):\n                future = executor.submit(_inference_sample, probmat, startnode[i])\n                futures.append(future)\n            for i, future in enumerate(futures):\n                routes[i] = future.result()\n    return routes\n\n# ==========================================\n# File: tsp_nls/two_opt.py\n# Function/Context: two_opt_once, _two_opt_python, batched_two_opt_python\n# ==========================================\nimport numpy as np\nimport numba as nb\nimport concurrent.futures\nfrom functools import partial\n\n@nb.njit(nb.float32(nb.float32[:,:], nb.uint16[:], nb.uint16), nogil=True)\ndef two_opt_once(distmat, tour, fixed_i = 0):\n    '''in-place operation'''\n    n = tour.shape[0]\n    p = q = 0\n    delta = 0\n    for i in range(1, n - 1) if fixed_i==0 else range(fixed_i, fixed_i+1):\n        for j in range(i + 1, n):\n            node_i, node_j = tour[i], tour[j]\n            node_prev, node_next = tour[i-1], tour[(j+1) % n]\n            if node_prev == node_j or node_next == node_i:\n                continue\n            change = (  distmat[node_prev, node_j] \n                        + distmat[node_i, node_next]\n                        - distmat[node_prev, node_i] \n                        - distmat[node_j, node_next])                    \n            if change < delta:\n                p, q, delta = i, j, change\n    if delta < -1e-6:\n        tour[p: q+1] = np.flip(tour[p: q+1])\n        return delta\n    else:\n        return 0.0\n\n\n@nb.njit(nb.uint16[:](nb.float32[:,:], nb.uint16[:], nb.int64), nogil=True)\ndef _two_opt_python(distmat, tour, max_iterations=1000):\n    iterations = 0\n    tour = tour.copy()\n    min_change = -1.0\n    while min_change < -1e-6 and iterations < max_iterations:\n        min_change = two_opt_once(distmat, tour, 0)\n        iterations += 1\n    return tour\n\ndef batched_two_opt_python(dist: np.ndarray, tours: np.ndarray, max_iterations=1000):\n    dist = dist.astype(np.float32)\n    tours = tours.astype(np.uint16)\n    with concurrent.futures.ThreadPoolExecutor() as executor:\n        futures = []\n        for tour in tours:\n            future = executor.submit(partial(_two_opt_python, distmat=dist, max_iterations=max_iterations), tour = tour)\n            futures.append(future)\n        return np.stack([f.result() for f in futures])",
  "description": "Combined Analysis:\n- [cvrp/aco.py]: This file implements the core ACO algorithm for CVRP as described in the DeepACO paper. The ACO class contains the complete solution construction logic using pheromone trails () and heuristic measures () with parameters  and , following the probability distribution P(s_t|s_{<t},) = (_{ij}^ * _{ij;}^) / (_{il}^ * _{il;}^). It includes path generation with feasibility constraints (capacity and visit masks), pheromone update with elitist and min-max strategies, and cost computation. The heuristic can be provided externally (allowing neural-enhanced heuristics from DeepACO's GNN). The implementation matches the mathematical model's solution construction and ACO framework, though the neural heuristic training and NLS are separate components.\n- [cvrp_nls/aco.py]: This file implements the core ACO algorithm for CVRP as described in the DeepACO paper. It includes:\n1. Solution construction using the probability distribution P(s_t|s_{<t}, ) = (_{ij}^ * _{ij;}^) /  (Eq. 3), where  can be provided as a learned heuristic.\n2. Pheromone update with elitist and min-max strategies.\n3. Neural Local Search (NLS) via swapstar for solution refinement (Algorithm 1).\n4. Adaptive mechanisms for intensification and diversification.\nThe ACO class encapsulates the entire optimization loop, including ant-based solution generation, cost evaluation, and iterative improvement through pheromone updates and local search.\n- [cvrp_nls/net.py]: This file implements the neural heuristic learner component of DeepACO, which corresponds to the GNN that generates heuristic measures _ from problem instances . Specifically, the Net class uses an EmbNet (graph neural network) to compute edge embeddings from graph-structured inputs (node features x, edge indices, edge attributes), then passes these embeddings through a ParNet (multi-layer perceptron) to produce heuristic values for each edge. These learned heuristics _ are used in the ACO solution construction probability distribution P__(s_t|s_<t,) = (_ij^ * _ij;^) / (_il^ * _il;^). The reshape method converts the heuristic vector into a matrix format compatible with the ACO algorithm. This directly implements the heuristic learning component of the optimization model's Step 1 (Heuristic Learner Training) and provides _ for use in Steps 2-3 (Inference with ACO and Neural-guided Local Search).\n- [cvrp_nls/swapstar.py]: This file implements the swapstar local search operator for the Capacitated Vehicle Routing Problem (CVRP) as part of the Neural-guided Local Search (NLS) component in DeepACO. It provides a Python interface to the HGS-CVRP C++ library's local search functionality. The swapstar function takes current routes and applies the swapstar operator to improve them, which corresponds to the local search minimization step in Algorithm 1 of the paper. While it doesn't directly implement the neural-guided perturbation (which would maximize heuristic measures), it serves as the core local search engine that can be called during both minimization and perturbation phases of NLS.\n- [mkp/aco.py]: This file implements the core ACO algorithm for the Multiple Knapsack Problem (MKP) as part of the DeepACO framework. It directly corresponds to the paper's ACO component with the following matches:\n\n1. **Solution Construction**: The `gen_sol` method implements step-by-step construction using the probability distribution from Eq. (3): `P(s_t|s_{<t}, )  _{ij}^  _{ij;}^`. Here, `` is a fixed heuristic (prize/weight ratio) rather than a learned one, but the structure matches the paper's ACO formulation.\n\n2. **Probability Computation**: In `pick_item`, the probability is computed as `(phe**alpha) * (heu**beta) * mask * dummy_mask`, where `phe` is pheromone (), `heu` is heuristic (), and `mask`/`dummy_mask` enforce feasibility constraints (`N(s_{<t})`).\n\n3. **Pheromone Update**: The `update_pheronome` method implements evaporation (`decay`) and reinforcement based on solution quality (using `Q` and objective values), supporting both standard and elitist strategies.\n\n4. **ACO Variants**: The class supports elitist and min-max pheromone updates (lines 37-41, 82-88), aligning with the paper's mention of different ACO variants.\n\n5. **Objective Calculation**: `gen_sol_obj` computes the total prize of selected items, matching the MKP objective.\n\n6. **Feasibility Handling**: The `update_knapsack` and `check_done` methods manage the knapsack constraints, ensuring only feasible items are selected.\n\nHowever, this file does **not** include the neural network component (heuristic learner) or NLS local search from DeepACO. It is a standalone ACO implementation that could be enhanced with learned heuristics (as described in the paper) by passing a learned `heuristic` matrix to the constructor. The code thus represents the base ACO algorithm that DeepACO builds upon.\n- [tsp/aco.py]: This file implements the core ACO algorithm from the DeepACO framework, specifically the solution construction and pheromone update mechanisms. The code directly implements the probability distribution P(s_t|s_<t,) = (_ij^ * _ij^) / (_il^ * _il^) for feasible components (via mask), where  can be provided as an external heuristic (allowing neural enhancement). It supports key ACO variants (Elitist AS, MAX-MIN AS) through configuration parameters. The implementation handles: 1) Ant-based solution construction with probabilistic selection (pick_move), 2) Pheromone update with evaporation and reinforcement (update_pheromone), 3) Optional heuristic sparsification. While this file doesn't include the neural network training (heuristic learner), it provides the algorithmic backbone that uses learned heuristics when supplied via the 'heuristic' parameter.\n- [tsp/net.py]: This file implements the neural network architecture (GNN + MLP heads) that serves as the heuristic learner in DeepACO. Specifically, it defines the graph neural network (EmbNet) that processes problem instances and two MLP heads (ParNet) that output parameterized heuristic (_) and pheromone (_) values for edges. The forward method returns the learned heuristic measures, which are used in the ACO solution construction probability distribution (Eq. 3 in the paper). The network architecture directly corresponds to the 'Heuristic Learner Training' step where  is learned to minimize the expected objective via reinforcement learning. The reshape helper function converts edge-wise outputs to adjacency matrices for compatibility with ACO operations.\n- [tsp_nls/aco.py]: This file implements the core ACO algorithm with neural-enhanced heuristics and Neural-guided Local Search (NLS) as described in the DeepACO paper. Key implementations include:\n1. Solution construction probability (Eq.3): `prob_mat = (self.pheromone ** self.alpha) * (self.heuristic ** self.beta)` in `gen_path()`\n2. Neural-guided local search (Algorithm 1): `nls()` method alternates between distance minimization (2-opt) and heuristic-guided perturbation\n3. Pheromone update rules: `update_pheronome()` implements elitist/standard updates with MIN-MAX bounds\n4. Complete ACO iteration loop: `run()` method performs construction, local search, and pheromone updates\n5. The heuristic (`self.heuristic`) can be provided externally (e.g., from a trained neural network), enabling neural enhancement.\nThe implementation matches the mathematical model where  is replaced by learned heuristics, and includes both standard ACO and NLS-enhanced variants.\n- [tsp_nls/two_opt.py]: This file implements the 2-opt local search algorithm for the Traveling Salesman Problem (TSP), which is a core component of the Neural-guided Local Search (NLS) procedure in DeepACO. The 2-opt algorithm performs local optimization by iteratively swapping edges to reduce tour length, directly corresponding to the local search (LS) minimization step in Algorithm 1 of the paper. The functions provide both single-tour optimization (two_opt_once, _two_opt_python) and batched parallel execution (batched_two_opt_python) for efficiency. This implementation serves as the objective-minimizing local search within the NLS framework, where solutions are refined until local optima before neural-guided perturbation.",
  "dependencies": [
    "concurrent.futures",
    "torch_geometric.nn",
    "functools.cached_property",
    "gnn.BatchNorm",
    "numba",
    "CAlgorithmParameters",
    "swapstar",
    "get_lib_filename",
    "typing",
    "RoutingSolution",
    "write_routes",
    "_Solution",
    "AlgorithmParameters",
    "numpy",
    "platform",
    "time",
    "inference_batch_sample",
    "copy.deepcopy",
    "gnn.global_mean_pool",
    "torch",
    "two_opt.batched_two_opt_python",
    "functools.partial",
    "_SolutionRoute",
    "torch.distributions.Categorical",
    "torch.nn.functional",
    "ctypes",
    "os",
    "dataclasses",
    "itertools",
    "read_routes",
    "torch.nn",
    "Solver",
    "_inference_sample",
    "sys",
    "random",
    "utils.gen_instance"
  ]
}