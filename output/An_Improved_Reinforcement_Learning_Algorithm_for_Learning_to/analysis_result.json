{
  "paper_id": "An_Improved_Reinforcement_Learning_Algorithm_for_Learning_to",
  "title": "An Improved Reinforcement Learning Algorithm for Learning to Branch",
  "abstract": "Most combinatorial optimization problems can be formulated as mixed integer linear programming (MILP), in which branch-and-bound (B&B) is a general and widely used method. Recently, learning to branch has become a hot research topic in the intersection of machine learning and combinatorial optimization. In this paper, we propose a novel reinforcement learning-based B&B algorithm. Similar to offline reinforcement learning, we initially train on the demonstration data to accelerate learning massively. With the improvement of the training effect, the agent starts to interact with the environment with its learned policy gradually. It is critical to improve the performance of the algorithm by determining the mixing ratio between demonstration and self-generated data. Thus, we propose a prioritized storage mechanism to control this ratio automatically. In order to improve the robustness of the training process, a superior network is additionally introduced based on Double DQN, which always serves as a Q-network with competitive performance. We evaluate the performance of the proposed algorithm over three public research benchmarks and compare it against strong baselines, including three classical heuristics and one state-of-the-art imitation learning-based branching algorithm. The results show that the proposed algorithm achieves the best performance among compared algorithms and possesses the potential to improve B&B algorithm performance continuously.",
  "problem_description_natural": "The paper addresses the problem of variable selection in the branch-and-bound (B&B) algorithm for solving mixed integer linear programming (MILP) problems. Efficient branching—choosing which fractional variable to split on at each node of the B&B tree—is crucial for reducing the total number of nodes explored and thus speeding up the solution process. Traditional methods rely on handcrafted heuristics like Strong Branching or Pseudocost Branching, which either require heavy computation or extensive manual tuning. The authors propose a reinforcement learning approach that leverages expert demonstration data early in training and gradually incorporates self-generated experience, using a novel architecture with a superior Q-network and a prioritized replay mechanism to stabilize and accelerate learning.",
  "problem_type": "MILP",
  "datasets": [
    "Balanced Item Placement (BIP)",
    "Workload Apportionment (WA)",
    "Anonymous Problem (AP)"
  ],
  "performance_metrics": [
    "Dual Integral",
    "Scores",
    "Wins"
  ],
  "lp_model": {
    "objective": "$\\min \\mathbf{c}^T \\mathbf{x}$",
    "constraints": [
      "$\\mathbf{A} \\mathbf{x} \\leq \\mathbf{b}$",
      "$\\mathbf{l} \\leq \\mathbf{x} \\leq \\mathbf{u}$",
      "$\\mathbf{x} \\in \\mathbb{Z}^p \\times \\mathbb{R}^{n-p}$"
    ],
    "variables": [
      "$\\mathbf{x} \\in \\mathbb{R}^n$: decision variable vector, where $p$ components are integer and $n-p$ are real"
    ]
  },
  "raw_latex_model": "$$\\arg \\min_{\\mathbf{x}} \\left\\{ \\mathbf{c}^T \\mathbf{x} \\mid \\mathbf{A} \\mathbf{x} \\leq \\mathbf{b}, \\mathbf{l} \\leq \\mathbf{x} \\leq \\mathbf{u}, \\mathbf{x} \\in \\mathbb{Z}^p \\times \\mathbb{R}^{n-p} \\right\\}$$",
  "algorithm_description": "A reinforcement learning algorithm based on Double DQN with a superior network and prioritized storage mechanism to learn variable selection policies in branch-and-bound for solving mixed integer linear programs."
}