{
  "paper_id": "Boosting_Neural_Combinatorial_Optimization_for_Large-Scale_V",
  "title": "BOOSTING NEURAL COMBINATORIAL OPTIMIZATION FOR LARGE-SCALE VEHICLE ROUTING PROBLEMS",
  "abstract": "Neural Combinatorial Optimization (NCO) methods have exhibited promising performance in solving Vehicle Routing Problems (VRPs). However, most NCO methods rely on the conventional self-attention mechanism that induces excessive computational complexity, thereby struggling to contend with large-scale VRPs and hindering their practical applicability. In this paper, we propose a lightweight cross-attention mechanism with linear complexity, by which a Transformer network is developed to learn efficient and favorable solutions for large-scale VRPs. We also propose a Self-Improved Training (SIT) algorithm that enables direct model training on large-scale VRP instances, bypassing extensive computational overhead for attaining labels. By iterating solution reconstruction, the Transformer network itself can generate improved partial solutions as pseudo-labels to guide the model training. Experimental results on the Travelling Salesman Problem (TSP) and the Capacitated Vehicle Routing Problem (CVRP) with up to 100K nodes indicate that our method consistently achieves superior performance for synthetic and real-world benchmarks, significantly boosting the scalability of NCO methods.",
  "problem_description_natural": "The paper addresses large-scale Vehicle Routing Problems (VRPs), specifically the Traveling Salesman Problem (TSP) and the Capacitated Vehicle Routing Problem (CVRP). These problems involve finding optimal or near-optimal tours that visit a set of nodes (e.g., cities or delivery locations) exactly once while minimizing total travel distance. In CVRP, additional constraints include vehicle capacity limits and the requirement to start and end routes at a central depot. The challenge lies in efficiently solving these NP-hard combinatorial optimization problems when the number of nodes scales up to tens of thousands or even 100,000, where traditional and existing neural methods become computationally infeasible due to quadratic complexity in attention mechanisms and difficulties in obtaining training labels.",
  "problem_type": "combinatorial optimization",
  "datasets": [
    "TSP1K",
    "TSP5K",
    "TSP10K",
    "TSP50K",
    "TSP100K",
    "CVRP1K",
    "CVRP5K",
    "CVRP10K",
    "CVRP50K",
    "CVRP100K",
    "TSPLIB",
    "CVRPLIB"
  ],
  "performance_metrics": [
    "Objective Value",
    "Optimality Gap",
    "Inference Time"
  ],
  "lp_model": {
    "objective": "$\\min \\sum_{i,j} c_{ij} x_{ij}$",
    "constraints": [
      "$\\sum_{j} x_{ij} = 1, \\quad \\forall i \\in \\{1,\\dots,n\\}$",
      "$\\sum_{i} x_{ij} = 1, \\quad \\forall j \\in \\{1,\\dots,n\\}$",
      "$\\sum_{j} x_{0j} = m$ and $\\sum_{i} x_{i0} = m$, where $m$ is the number of vehicles (routes)",
      "$\\sum_{i \\notin S, j \\in S} x_{ij} \\geq \\left\\lceil \\frac{\\sum_{i \\in S} q_i}{Q} \\right\\rceil, \\quad \\forall S \\subseteq \\{1,\\dots,n\\}$"
    ],
    "variables": [
      "$x_{ij}$: binary variable indicating if edge $(i,j)$ is used in the solution",
      "$m$: integer variable representing the number of vehicles (routes)"
    ]
  },
  "raw_latex_model": "$$\\begin{aligned} &\\min \\sum_{i,j} c_{ij} x_{ij} \\\\ &\\text{s.t.} \\\\ &\\sum_{j} x_{ij} = 1, \\quad \\forall i \\in \\{1,\\dots,n\\} \\\\ &\\sum_{i} x_{ij} = 1, \\quad \\forall j \\in \\{1,\\dots,n\\} \\\\ &\\sum_{j} x_{0j} = m, \\quad \\sum_{i} x_{i0} = m \\\\ &\\sum_{i \\notin S, j \\in S} x_{ij} \\geq \\left\\lceil \\frac{\\sum_{i \\in S} q_i}{Q} \\right\\rceil, \\quad \\forall S \\subseteq \\{1,\\dots,n\\} \\\\ &x_{ij} \\in \\{0,1\\}, \\quad m \\in \\mathbb{Z}^+ \\end{aligned}$$",
  "algorithm_description": "The paper proposes a neural combinatorial optimization (NCO) method for large-scale vehicle routing problems (VRPs), specifically the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP). It introduces a lightweight cross-attention mechanism with linear computational complexity to replace the conventional quadratic self-attention in Transformer networks, enabling efficient handling of large-scale instances. Additionally, a Self-Improved Training (SIT) algorithm is developed, which allows the model to be trained directly on large-scale VRP instances without labeled data by iteratively reconstructing solutions and using the improved solutions as pseudo-labels for training."
}