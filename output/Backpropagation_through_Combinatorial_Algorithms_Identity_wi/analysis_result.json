{
  "paper_id": "Backpropagation_through_Combinatorial_Algorithms_Identity_wi",
  "title": "Backpropagation Through Combinatorial Algorithms: Identity with Projection Works",
  "abstract": "Embedding discrete solvers as differentiable layers has given modern deep learning architectures combinatorial expressivity and discrete reasoning capabilities. The derivative of these solvers is zero or undefined, therefore a meaningful replacement is crucial for effective gradient-based learning. Prior works rely on smoothing the solver with input perturbations, relaxing the solver to continuous problems, or interpolating the loss landscape with techniques that typically require additional solver calls, introduce extra hyper-parameters, or compromise performance. We propose a principled approach to exploit the geometry of the discrete solution space to treat the solver as a negative identity on the backward pass and further provide a theoretical justification. Our experiments demonstrate that such a straightforward hyper-parameter-free approach is able to compete with previously more complex methods on numerous experiments such as backpropagation through discrete samplers, deep graph matching, and image retrieval. Furthermore, we substitute the previously proposed problem-specific and label-dependent margin with a generic regularization procedure that prevents cost collapse and increases robustness.",
  "problem_description_natural": "The paper addresses the challenge of end-to-end training of deep neural networks that include embedded discrete combinatorial solvers. These solvers output solutions to optimization problems of the form y(ω) = argmin_{y ∈ Y} ⟨ω, y⟩, where ω is a cost vector produced by a neural network and Y is a finite set of feasible discrete solutions (e.g., permutations, matchings, paths). Because the true gradient of such a solver is zero or undefined almost everywhere due to its piecewise-constant nature, standard backpropagation fails. The authors propose replacing the zero Jacobian with a 'negative identity' mapping during backpropagation—i.e., setting dℓ/dω = −dℓ/dy—and combine this with projections that respect invariances of the solver (e.g., invariance to constant shifts in cost for ranking problems). They also introduce noise-based regularization to prevent cost collapse and eliminate the need for ground-truth-dependent margins.",
  "problem_type": "combinatorial optimization",
  "datasets": [
    "MNIST",
    "BeerAdvocate",
    "Pascal VOC",
    "SPair-71k",
    "CUB-200-2011",
    "Globe TSP",
    "Warcraft Shortest Path"
  ],
  "performance_metrics": [
    "N-ELBO",
    "Test MSE",
    "Matching accuracy",
    "Recall R@1",
    "Test accuracy",
    "Path cost ratio"
  ],
  "lp_model": {
    "objective": "\\min_{y \\in Y} \\langle \\omega, y \\rangle",
    "constraints": [
      "y \\in Y",
      "Y \\subset \\mathbb{R}^n \\text{ is a finite set of solutions}"
    ],
    "variables": [
      "y \\in \\mathbb{R}^n \\text{ (solution vector)}",
      "\\omega \\in W \\subseteq \\mathbb{R}^n \\text{ (cost vector, input to solver)}"
    ]
  },
  "raw_latex_model": "y(\\omega) = \\arg\\min_{y \\in Y} \\langle \\omega, y \\rangle",
  "algorithm_description": "The Identity method for backpropagation through combinatorial solvers involves the following steps during training: 1. Forward pass: Predict the cost vector ω from the input using a backbone neural network. Optionally, apply a projection P(ω) (e.g., standardization P_std) to handle solver invariants. Then, solve the combinatorial problem y = argmin_{y' in Y} ⟨ω, y'⟩ to obtain the discrete solution. 2. Compute the loss ℓ based on y and the ground truth. 3. Backward pass: Compute the gradient dℓ/dy. For the solver layer, replace the true zero gradient with -P'(ω) dℓ/dy (or -dℓ/dy if no projection is used), effectively treating the solver as a negative identity. Backpropagate this gradient to update the backbone network weights. Optionally, add noise ξ to ω before solving in the forward pass to induce a margin and prevent cost collapse."
}