{
  "file_path": "nets/attention_model.py, nets/graph_encoder.py, nets/model_search.py, problems/tsp/state_tsp.py, problems/vrp/state_cvrp.py, utils/beam_search.py",
  "function_name": "AttentionModel.forward, GraphAttentionEncoder, AttentionModel.forward, StateTSP, StateCVRP, beam_search, _beam_search, BatchBeam",
  "code_snippet": "\n\n# ==========================================\n# File: nets/attention_model.py\n# Function/Context: AttentionModel.forward\n# ==========================================\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.checkpoint import checkpoint\nimport math\nfrom typing import NamedTuple\nfrom utils.tensor_functions import compute_in_batches\n\nfrom nets.graph_encoder import GraphAttentionEncoder\nfrom torch.nn import DataParallel\nfrom utils.beam_search import CachedLookup\nfrom utils.functions import sample_many\n\n\ndef set_decode_type(model, decode_type):\n    if isinstance(model, DataParallel):\n        model = model.module\n    model.set_decode_type(decode_type)\n\n\nclass AttentionModelFixed(NamedTuple):\n    \"\"\"\n    Context for AttentionModel decoder that is fixed during decoding so can be precomputed/cached\n    This class allows for efficient indexing of multiple Tensors at once\n    \"\"\"\n    node_embeddings: torch.Tensor\n    context_node_projected: torch.Tensor\n    glimpse_key: torch.Tensor\n    glimpse_val: torch.Tensor\n    logit_key: torch.Tensor\n\n    def __getitem__(self, key):\n        if torch.is_tensor(key) or isinstance(key, slice):\n            return AttentionModelFixed(\n                node_embeddings=self.node_embeddings[key],\n                context_node_projected=self.context_node_projected[key],\n                glimpse_key=self.glimpse_key[:, key],  # dim 0 are the heads\n                glimpse_val=self.glimpse_val[:, key],  # dim 0 are the heads\n                logit_key=self.logit_key[key]\n            )\n        return super(AttentionModelFixed, self).__getitem__(key)\n\n\nclass AttentionModel(nn.Module):\n\n    def __init__(self,\n                 embedding_dim,\n                 hidden_dim,\n                 problem,\n                 n_encode_layers=2,\n                 tanh_clipping=10.,\n                 mask_inner=True,\n                 mask_logits=True,\n                 normalization='batch',\n                 n_heads=8,\n                 checkpoint_encoder=False,\n                 shrink_size=None,\n                 n_paths=None,\n                 n_EG=None):\n        super(AttentionModel, self).__init__()\n\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.n_encode_layers = n_encode_layers\n        self.decode_type = None\n        self.temp = 1.0\n        self.is_tsp = problem.NAME == 'tsp'\n        self.allow_partial = problem.NAME == 'sdvrp'\n        self.is_vrp = problem.NAME == 'cvrp' or problem.NAME == 'sdvrp'\n        self.is_orienteering = problem.NAME == 'op'\n        self.is_pctsp = problem.NAME == 'pctsp'\n\n        self.tanh_clipping = tanh_clipping\n\n        self.mask_inner = mask_inner\n        self.mask_logits = mask_logits\n\n        self.problem = problem\n        self.n_heads = n_heads\n        self.checkpoint_encoder = checkpoint_encoder\n        self.shrink_size = shrink_size\n        self.n_paths = n_paths\n        self.n_EG = n_EG\n\n        # Problem specific context parameters (placeholder and step context dimension)\n        if self.is_vrp or self.is_orienteering or self.is_pctsp:\n            # Embedding of last node + remaining_capacity / remaining length / remaining prize to collect\n            step_context_dim = embedding_dim + 1\n\n            if self.is_pctsp:\n                node_dim = 4  # x, y, expected_prize, penalty\n            else:\n                node_dim = 3  # x, y, demand / prize\n\n            # Special embedding projection for depot node\n            self.init_embed_depot = nn.Linear(2, embedding_dim)\n            \n            if self.is_vrp and self.allow_partial:  # Need to include the demand if split delivery allowed\n                self.project_node_step = nn.Linear(1, 3 * embedding_dim, bias=False)\n        else:  # TSP\n            assert problem.NAME == \"tsp\", \"Unsupported problem: {}\".format(problem.NAME)\n            step_context_dim = 2 * embedding_dim  # Embedding of first and last node\n            node_dim = 2  # x, y\n            \n            # Learned input symbols for first action\n            self.W_placeholder = nn.Parameter(torch.Tensor(2 * embedding_dim))\n            self.W_placeholder.data.uniform_(-1, 1)  # Placeholder should be in range of activations\n\n        self.init_embed = nn.Linear(node_dim, embedding_dim)\n\n        self.embedder = GraphAttentionEncoder(\n            n_heads=n_heads,\n            embed_dim=embedding_dim,\n            n_layers=self.n_encode_layers,\n            normalization=normalization\n        )\n\n        # For each node we compute (glimpse key, glimpse value, logit key) so 3 * embedding_dim\n        self.project_node_embeddings = [nn.Linear(embedding_dim, 3 * embedding_dim, bias=False) for i in range(self.n_paths)]\n        self.project_fixed_context = [nn.Linear(embedding_dim, embedding_dim, bias=False) for i in range(self.n_paths)]\n        self.project_step_context = [nn.Linear(step_context_dim, embedding_dim, bias=False) for i in range(self.n_paths)]\n        assert embedding_dim % n_heads == 0\n        # Note n_heads * val_dim == embedding_dim so input to project_out is embedding_dim\n        self.project_out = [nn.Linear(embedding_dim, embedding_dim, bias=False) for i in range(self.n_paths)]\n        \n        self.project_node_embeddings = nn.ModuleList(self.project_node_embeddings)\n        self.project_fixed_context = nn.ModuleList(self.project_fixed_context)\n        self.project_step_context = nn.ModuleList(self.project_step_context)\n        self.project_out = nn.ModuleList(self.project_out)\n\n    def forward(self, input, opts=None, baseline=None, bl_val=None, n_EG=None, return_pi=False, return_kl=False):\n        \"\"\"\n        :param input: (batch_size, graph_size, node_dim) input node features or dictionary with multiple tensors\n        :param return_pi: whether to return the output sequences, this is optional as it is not compatible with\n        using DataParallel as the results may be of different lengths on different GPUs\n        :return:\n        \"\"\"\n\n        n_EG = self.n_EG\n\n        embeddings, init_context, attn, V, h_old = self.embedder(self._init_embed(input))\n\n        costs, lls = [], []\n\n        outputs = []\n\n        # Perform decoding steps\n\n        states = [self.problem.make_state(input) for i in range(self.n_paths)]\n        # first step\n        for i in range(self.n_paths):  \n            # Compute keys, values for the glimpse and keys for the logits once as they can be reused in every step\n            fixed = self._precompute(embeddings, path_index=i)\n            log_p, mask = self._get_log_p(fixed, states[i], i)\n            # Collect output of step\n            if self.is_vrp:\n                outputs.append(log_p[:, 0, 1:]) # ignore the depot for the first step\n            else:\n                outputs.append(log_p[:, 0, :])\n            outputs[-1] = torch.max(outputs[-1], torch.ones(outputs[-1].shape, dtype=outputs[-1].dtype, device=outputs[-1].device) * (-1e9)) # for the kl loss\n\n        if self.n_paths > 1 and baseline != None:\n            kl_divergences = []\n            for _i in range(self.n_paths):\n                for _j in range(self.n_paths):\n                    if _i==_j:\n                        continue\n                    kl_divergence = torch.sum(torch.exp(outputs[_i]) * (outputs[_i] - outputs[_j]), -1)\n                    kl_divergences.append(kl_divergence)\n            loss_kl_divergence = -opts.kl_loss * torch.stack(kl_divergences, 0).mean()\n            loss_kl_divergence.backward()\n\n        states = [self.problem.make_state(input) for i in range(self.n_paths)]\n        for i in range(self.n_paths):\n            output, sequence = [], []\n            embeddings, init_context, attn, V, h_old = self.embedder(self._init_embed(input))\n            fixed = self._precompute(embeddings, path_index=i)\n            j = 0\n            while not (self.shrink_size is None and states[i].all_finished()):\n                if j > 1 and j % n_EG == 0:\n                    if not self.is_vrp:\n                        mask_attn = mask ^ mask_first\n                    else:\n                        mask_attn = mask\n                    embeddings, init_context = self.embedder.change(attn, V, h_old, mask_attn, self.is_tsp)\n                    fixed = self._precompute(embeddings, path_index=i)\n                log_p, mask = self._get_log_p(fixed, states[i], i)\n                if j == 0:\n                    mask_first = mask\n                # Select the indices of the next nodes in the sequences, result (batch_size) long\n                selected = self._select_node(log_p.exp()[:, 0, :], mask[:, 0, :])  # Squeeze out steps dimension\n\n                states[i] = states[i].update(selected)\n\n                # Collect output of step\n                output.append(log_p[:, 0, :])\n                sequence.append(selected)\n\n                j += 1\n\n            _log_p = torch.stack(output, 1)\n            pi = torch.stack(sequence, 1)\n\n            cost, mask = self.problem.get_costs(input, pi)\n            costs.append(cost.detach())\n        \n            ll = self._calc_log_likelihood(_log_p, pi, mask)\n\n            if baseline!=None:    \n                if i==0:\n\n                    bl_val, _ = baseline.eval(input, costs[0]) if bl_val is None else (bl_val, 0)\n\n                reinforce_loss = ((cost - bl_val) * ll).mean()\n                loss = reinforce_loss / self.n_paths\n\n                loss.backward()\n\n        costs = torch.stack(costs, 1)\n\n        if baseline!=None:\n            return costs, ll, reinforce_loss\n\n        if return_pi:\n            return costs, lls, pis\n\n        if return_kl:\n            return costs, lls, loss_kl_divergence\n        return costs, lls\n\n    def _precompute(self, embeddings, num_steps=1, path_index=None):\n\n        # The fixed context projection of the graph embedding is calculated only once for efficiency\n        graph_embed = embeddings.mean(1)\n\n        # fixed context = (batch_size, 1, embed_dim) to make broadcastable with parallel timesteps\n        fixed_context = self.project_fixed_context[path_index](graph_embed)[:, None, :]\n\n        # The projection of the node embeddings for the attention is calculated once up front\n        glimpse_key_fixed, glimpse_val_fixed, logit_key_fixed = \\\n            self.project_node_embeddings[path_index](embeddings[:, None, :, :]).chunk(3, dim=-1)\n\n        return AttentionModelFixed(\n            node_embeddings=embeddings,\n            context_node_projected=fixed_context,\n            glimpse_key=glimpse_key_fixed.contiguous(),\n            glimpse_val=glimpse_val_fixed.contiguous(),\n            logit_key=logit_key_fixed.contiguous()\n        )\n\n# ==========================================\n# File: nets/graph_encoder.py\n# Function/Context: GraphAttentionEncoder\n# ==========================================\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch import nn\nimport math\nfrom torch.autograd import Variable\n\n\nclass SkipConnection(nn.Module):\n\n    def __init__(self, module):\n        super(SkipConnection, self).__init__()\n        self.module = module\n\n    def forward(self, input):\n        return input + self.module(input)\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(\n            self,\n            n_heads,\n            input_dim,\n            embed_dim=None,\n            val_dim=None,\n            key_dim=None,\n            last_one=False\n    ):\n        super(MultiHeadAttention, self).__init__()\n\n        if val_dim is None:\n            assert embed_dim is not None, \"Provide either embed_dim or val_dim\"\n            val_dim = embed_dim // n_heads\n        if key_dim is None:\n            key_dim = val_dim\n\n        self.n_heads = n_heads\n        self.input_dim = input_dim\n        self.embed_dim = embed_dim\n        self.val_dim = val_dim\n        self.key_dim = key_dim\n\n        self.norm_factor = 1 / math.sqrt(key_dim)  # See Attention is all you need\n\n        self.W_query = nn.Parameter(torch.Tensor(n_heads, input_dim, key_dim))\n        self.W_key = nn.Parameter(torch.Tensor(n_heads, input_dim, key_dim))\n        self.W_val = nn.Parameter(torch.Tensor(n_heads, input_dim, val_dim))\n\n        if embed_dim is not None:\n            self.W_out = nn.Parameter(torch.Tensor(n_heads, key_dim, embed_dim))\n\n        self.init_parameters()\n        self.last_one = last_one\n\n    def init_parameters(self):\n\n        for param in self.parameters():\n            stdv = 1. / math.sqrt(param.size(-1))\n            param.data.uniform_(-stdv, stdv)\n\n    def forward(self, q, h=None, mask=None):\n        \"\"\"\n\n        :param q: queries (batch_size, n_query, input_dim)\n        :param h: data (batch_size, graph_size, input_dim)\n        :param mask: mask (batch_size, n_query, graph_size) or viewable as that (i.e. can be 2 dim if n_query == 1)\n        Mask should contain 1 if attention is not possible (i.e. mask is negative adjacency)\n        :return:\n        \"\"\"\n        if h is None:\n            h = q  # compute self-attention\n\n        # h should be (batch_size, graph_size, input_dim)\n        batch_size, graph_size, input_dim = h.size()\n        n_query = q.size(1)\n        assert q.size(0) == batch_size\n        assert q.size(2) == input_dim\n        assert input_dim == self.input_dim, \"Wrong embedding dimension of input\"\n\n        hflat = h.contiguous().view(-1, input_dim)\n        qflat = q.contiguous().view(-1, input_dim)\n\n        # last dimension can be different for keys and values\n        shp = (self.n_heads, batch_size, graph_size, -1)\n        shp_q = (self.n_heads, batch_size, n_query, -1)\n\n        # Calculate queries, (n_heads, n_query, graph_size, key/val_size)\n        Q = torch.matmul(qflat, self.W_query).view(shp_q)\n        # Calculate keys and values (n_heads, batch_size, graph_size, key/val_size)\n        K = torch.matmul(hflat, self.W_key).view(shp)\n        V = torch.matmul(hflat, self.W_val).view(shp)\n\n        # Calculate compatibility (n_heads, batch_size, n_query, graph_size)\n        compatibility = self.norm_factor * torch.matmul(Q, K.transpose(2, 3))\n\n        # Optionally apply mask to prevent attention\n        if mask is not None:\n            mask = mask.view(1, batch_size, n_query, graph_size).expand_as(compatibility)\n            compatibility[mask] = -np.inf\n\n        attn = F.softmax(compatibility, dim=-1)\n\n        # If there are nodes with no neighbours then softmax returns nan so we fix them to 0\n        if mask is not None:\n            attnc = attn.clone()\n            attnc[mask] = 0\n            attn = attnc\n\n        heads = torch.matmul(attn, V)\n\n        out = torch.mm(\n            heads.permute(1, 2, 0, 3).contiguous().view(-1, self.n_heads * self.val_dim),\n            self.W_out.view(-1, self.embed_dim)\n        ).view(batch_size, n_query, self.embed_dim)\n        if self.last_one:\n            return (out, attn, V)\n        return out\n\n\nclass Normalization(nn.Module):\n\n    def __init__(self, embed_dim, normalization='batch'):\n        super(Normalization, self).__init__()\n\n        normalizer_class = {\n            'batch': nn.BatchNorm1d,\n            'instance': nn.InstanceNorm1d\n        }.get(normalization, None)\n\n        self.normalizer = normalizer_class(embed_dim, affine=True)\n\n        # Normalization by default initializes affine parameters with bias 0 and weight unif(0,1) which is too large!\n        # self.init_parameters()\n\n    def init_parameters(self):\n\n        for name, param in self.named_parameters():\n            stdv = 1. / math.sqrt(param.size(-1))\n            param.data.uniform_(-stdv, stdv)\n\n    def forward(self, input):\n\n        if isinstance(self.normalizer, nn.BatchNorm1d):\n            return self.normalizer(input.view(-1, input.size(-1))).view(*input.size())\n        elif isinstance(self.normalizer, nn.InstanceNorm1d):\n            return self.normalizer(input.permute(0, 2, 1)).permute(0, 2, 1)\n        else:\n            assert self.normalizer is None, \"Unknown normalizer type\"\n            return input\n\n\nclass MultiHeadAttentionLayer(nn.Sequential):\n\n    def __init__(\n            self,\n            n_heads,\n            embed_dim,\n            num_layers,\n            feed_forward_hidden=512,\n            normalization='batch',\n    ):\n        args_tuple = []\n        for _ in range(num_layers):\n            args_tuple += [SkipConnection(\n                    MultiHeadAttention(\n                        n_heads,\n                        input_dim=embed_dim,\n                        embed_dim=embed_dim\n                    )\n                ),\n                Normalization(embed_dim, normalization),\n                SkipConnection(\n                    nn.Sequential(\n                        nn.Linear(embed_dim, feed_forward_hidden),\n                        nn.ReLU(),\n                        nn.Linear(feed_forward_hidden, embed_dim)\n                    ) if feed_forward_hidden > 0 else nn.Linear(embed_dim, embed_dim)\n                ),\n                Normalization(embed_dim, normalization)]\n        args_tuple = tuple(args_tuple)\n\n        super(MultiHeadAttentionLayer, self).__init__(*args_tuple)\n\n\nclass GraphAttentionEncoder(nn.Module):\n    def __init__(\n            self,\n            n_heads,\n            embed_dim,\n            n_layers,\n            node_dim=None,\n            normalization='batch',\n            feed_forward_hidden=512\n    ):\n        super(GraphAttentionEncoder, self).__init__()\n\n        self.device = torch.device(\"cuda:0\")\n        # To map input to embedding space\n        self.init_embed = nn.Linear(node_dim, embed_dim) if node_dim is not None else None\n\n        self.layers = MultiHeadAttentionLayer(n_heads, embed_dim, n_layers - 1, feed_forward_hidden, normalization)\n        self.attention_layer = MultiHeadAttention(n_heads, input_dim=embed_dim, embed_dim=embed_dim, last_one=True)\n        self.BN1 = Normalization(embed_dim, normalization)\n        self.projection = SkipConnection(\n                    nn.Sequential(\n                        nn.Linear(embed_dim, feed_forward_hidden),\n                        nn.ReLU(),\n                        nn.Linear(feed_forward_hidden, embed_dim)\n                    ) if feed_forward_hidden > 0 else nn.Linear(embed_dim, embed_dim)\n                )\n        self.BN2 = Normalization(embed_dim, normalization)\n\n    def forward(self, x, mask=None, return_transform_loss=False):\n\n        assert mask is None, \"TODO mask not yet supported!\"\n\n        batch_size, graph_size, feat_dim = x.size()\n\n        # Batch multiply to get initial embeddings of nodes\n        # h_embeded = self.init_embed(x.view(-1, x.size(-1))).view(*x.size()[:2], -1) if self.init_embed is not None else x\n        h_embeded = x\n\n        h_old = self.layers(h_embeded)\n        h_new, attn, V = self.attention_layer(h_old)\n        h = h_new + h_old\n        h = self.BN1(h)\n        h = self.projection(h)\n        h = self.BN2(h)\n\n        return (h, h.mean(dim=1), attn, V, h_old)\n\n\n\n    def change(self, attn, V, h_old, mask, is_tsp=False):\n        n_heads, batch_size, graph_size, feat_size = V.size()\n        attn = (1 - mask.float()).view(1, batch_size, 1, graph_size).repeat(n_heads, 1, graph_size, 1) * attn\n        if is_tsp:\n            attn = attn / (torch.sum(attn, dim = -1).view(n_heads, batch_size, graph_size, 1))\n        else:\n            attn = attn / (torch.sum(attn, dim = -1).view(n_heads, batch_size, graph_size, 1) + 1e-9)\n        heads = torch.matmul(attn, V)\n\n        h_new = torch.mm(\n            heads.permute(1, 2, 0, 3).contiguous().view(-1, self.attention_layer.n_heads * self.attention_layer.val_dim),\n            self.attention_layer.W_out.view(-1, self.attention_layer.embed_dim)\n        ).view(batch_size, graph_size, self.attention_layer.embed_dim)\n        h = h_new + h_old\n        h = self.BN1(h)\n        h = self.projection(h)\n        h = self.BN2(h)\n\n        return (h, h.mean(dim=1))\n\n# ==========================================\n# File: nets/model_search.py\n# Function/Context: AttentionModel.forward\n# ==========================================\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.checkpoint import checkpoint\nimport math\nfrom typing import NamedTuple\nfrom utils.tensor_functions import compute_in_batches\n\nfrom nets.graph_encoder import GraphAttentionEncoder\nfrom torch.nn import DataParallel\nfrom utils.beam_search import CachedLookup\nfrom utils.functions import sample_many\nimport copy\nfrom problems.vrp.problem_vrp import CVRP\nfrom problems.tsp.problem_tsp import TSP\nfrom problems.op.problem_op import OP\nfrom problems.vrp.problem_vrp import SDVRP\n\nfrom problems.pctsp.problem_pctsp import PCTSPDet\nfrom problems.pctsp.problem_pctsp import PCTSPStoch\n\ndef set_decode_type(model, decode_type):\n    if isinstance(model, DataParallel):\n        model = model.module\n    model.set_decode_type(decode_type)\n\n\nclass AttentionModelFixed(NamedTuple):\n    \"\"\"\n    Context for AttentionModel decoder that is fixed during decoding so can be precomputed/cached\n    This class allows for efficient indexing of multiple Tensors at once\n    \"\"\"\n    node_embeddings: torch.Tensor\n    context_node_projected: torch.Tensor\n    glimpse_key: torch.Tensor\n    glimpse_val: torch.Tensor\n    logit_key: torch.Tensor\n\n    def __getitem__(self, key):\n        if torch.is_tensor(key) or isinstance(key, slice):\n            return AttentionModelFixed(\n                node_embeddings=self.node_embeddings[key],\n                context_node_projected=self.context_node_projected[key],\n                glimpse_key=self.glimpse_key[:, key],  # dim 0 are the heads\n                glimpse_val=self.glimpse_val[:, key],  # dim 0 are the heads\n                logit_key=self.logit_key[key]\n            )\n        return super(AttentionModelFixed, self).__getitem__(key)\n\n\nclass AttentionModel(nn.Module):\n\n    def __init__(self,\n                 embedding_dim,\n                 hidden_dim,\n                 problem,\n                 n_encode_layers=2,\n                 tanh_clipping=10.,\n                 mask_inner=True,\n                 mask_logits=True,\n                 normalization='batch',\n                 n_heads=8,\n                 checkpoint_encoder=False,\n                 shrink_size=None,\n                 n_paths=None,\n                 n_EG=None):\n        super(AttentionModel, self).__init__()\n\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.n_encode_layers = n_encode_layers\n        self.decode_type = None\n        self.temp = 1.0\n        self.is_tsp = problem.NAME == 'tsp'\n        self.allow_partial = problem.NAME == 'sdvrp'\n        self.is_vrp = problem.NAME == 'cvrp' or problem.NAME == 'sdvrp'\n        self.is_op = self.is_orienteering = problem.NAME == 'op'\n        self.is_pctsp = problem.NAME == 'pctsp'\n        if self.is_pctsp:\n            self.stochastic = problem.stochastic\n\n        self.tanh_clipping = tanh_clipping\n\n        self.mask_inner = mask_inner\n        self.mask_logits = mask_logits\n\n        self.problem = problem\n        self.n_heads = n_heads\n        self.checkpoint_encoder = checkpoint_encoder\n        self.shrink_size = shrink_size\n        self.n_paths = n_paths\n        self.n_EG = n_EG\n\n        # Problem specific context parameters (placeholder and step context dimension)\n        if self.is_vrp or self.is_orienteering or self.is_pctsp:\n            # Embedding of last node + remaining_capacity / remaining length / remaining prize to collect\n            step_context_dim = embedding_dim + 1\n\n            if self.is_pctsp:\n                node_dim = 4  # x, y, expected_prize, penalty\n            else:\n                node_dim = 3  # x, y, demand / prize\n\n            # Special embedding projection for depot node\n            self.init_embed_depot = nn.Linear(2, embedding_dim)\n            \n            if self.is_vrp and self.allow_partial:  # Need to include the demand if split delivery allowed\n                self.project_node_step = nn.Linear(1, 3 * embedding_dim, bias=False)\n        else:  # TSP\n            assert problem.NAME == \"tsp\", \"Unsupported problem: {}\".format(problem.NAME)\n            step_context_dim = 2 * embedding_dim  # Embedding of first and last node\n            node_dim = 2  # x, y\n            \n            # Learned input symbols for first action\n            self.W_placeholder = nn.Parameter(torch.Tensor(2 * embedding_dim))\n            self.W_placeholder.data.uniform_(-1, 1)  # Placeholder should be in range of activations\n\n        self.init_embed = nn.Linear(node_dim, embedding_dim)\n\n        self.embedder = GraphAttentionEncoder(\n            n_heads=n_heads,\n            embed_dim=embedding_dim,\n            n_layers=self.n_encode_layers,\n            normalization=normalization\n        )\n\n        # For each node we compute (glimpse key, glimpse value, logit key) so 3 * embedding_dim\n        self.project_node_embeddings = [nn.Linear(embedding_dim, 3 * embedding_dim, bias=False) for i in range(self.n_paths)]\n        self.project_fixed_context = [nn.Linear(embedding_dim, embedding_dim, bias=False) for i in range(self.n_paths)]\n        self.project_step_context = [nn.Linear(step_context_dim, embedding_dim, bias=False) for i in range(self.n_paths)]\n        assert embedding_dim % n_heads == 0\n        # Note n_heads * val_dim == embedding_dim so input to project_out is embedding_dim\n        self.project_out = [nn.Linear(embedding_dim, embedding_dim, bias=False) for i in range(self.n_paths)]\n        \n        self.project_node_embeddings = nn.ModuleList(self.project_node_embeddings)\n        self.project_fixed_context = nn.ModuleList(self.project_fixed_context)\n        self.project_step_context = nn.ModuleList(self.project_step_context)\n        self.project_out = nn.ModuleList(self.project_out)\n\n    def set_decode_type(self, decode_type, temp=None):\n        self.decode_type = decode_type\n        if temp is not None:  # Do not change temperature if not provided\n            self.temp = temp\n\n    def forward(self, input, opts=None, baseline=None, bl_val=None, n_EG=None, return_pi=False, return_kl=False, beam_size=None, fst=None):\n        \"\"\"\n        :param input: (batch_size, graph_size, node_dim) input node features or dictionary with multiple tensors\n        :param return_pi: whether to return the output sequences, this is optional as it is not compatible with\n        using DataParallel as the results may be of different lengths on different GPUs\n        :return:\n        \"\"\"\n\n        assert fst==0 or fst==1\n\n        assert beam_size!=None\n\n        if self.is_vrp and n_EG:\n            n_EG = n_EG\n        else:\n            n_EG = self.n_EG\n\n        embeddings_init, init_context, attn, V, h_old = self.embedder(self._init_embed(input))\n\n        batch_size, graph_size, _ = embeddings_init.size()\n        expand_size = min(graph_size, beam_size)\n        expand_size = 3\n        if self.is_vrp:\n            demand = input['demand']\n            loc_with_depot = torch.cat((input['depot'][:, None, :], input['loc']), 1)\n        if self.is_op:\n            loc_with_depot = torch.cat((input['depot'][:, None, :], input['loc']), 1)\n            max_length=input[\"max_length\"][:, None] - (loc_with_depot[:, 0:1] - loc_with_depot).norm(p=2, dim=-1)\n            prize_with_depot = torch.cat((torch.zeros_like(input['prize'][:, :1]), input['prize']), 1)\n        if self.is_pctsp:\n            loc_with_depot = torch.cat((input['depot'][:, None, :], input['loc']), 1)\n            penalty_with_depot = torch.cat((torch.zeros_like(input['penalty'][:, :1]), input['penalty']), 1)\n            expected_prize = input['deterministic_prize']\n            real_prize = input['stochastic_prize' if self.stochastic else 'deterministic_prize']\n            real_prize_with_depot = torch.cat((torch.zeros_like(real_prize[:, :1]), real_prize), -1)\n\n        costs = []\n        seq_beam_list = []\n\n        for i in range(self.n_paths):\n            seq_beam = torch.zeros(batch_size, 1, 1, dtype=torch.long, device=embeddings_init.device)\n            ll_beam = torch.zeros(batch_size, 1, device=embeddings_init.device)\n            distance_beam = torch.zeros(batch_size, 1, device=embeddings_init.device)\n            fixed_beam = self._precompute(embeddings_init, path_index=i)\n            mask_beam = torch.zeros(batch_size, 1, graph_size, dtype=torch.uint8, device=embeddings_init.device) > 0\n            if self.is_vrp:\n                used_beam = demand.new_zeros(batch_size, 1)\n                visited_beam = torch.zeros(batch_size, 1, graph_size, dtype=torch.uint8, device=embeddings_init.device)\n                if self.allow_partial:\n                    demand_with_depot = torch.cat((demand.new_zeros(batch_size, 1), demand[:, :]), 1)[:, None, :]\n                    mask_beam[:, :, 0] = True\n            if self.is_op:\n                visited_beam = torch.zeros(batch_size, 1, graph_size, dtype=torch.uint8, device=embeddings_init.device)\n                coord_all = loc_with_depot.view(batch_size, 1, -1, 2)\n                coord_cur = loc_with_depot[torch.arange(batch_size).view(-1, 1), seq_beam[:, :, -1]].view(batch_size, seq_beam.size(1), 1, -1)\n                mask_beam = ((coord_all - coord_cur).norm(p=2, dim=-1)) > max_length.view(batch_size, 1, -1)\n            if self.is_pctsp:\n                visited_beam = torch.zeros(batch_size, 1, graph_size, dtype=torch.uint8, device=embeddings_init.device)\n                prize_beam = penalty_with_depot.new_zeros(batch_size, 1)\n                mask_beam[:, :, 0] = True\n\n            j = 0\n\n            while (self.is_op and (j == 0 or not (seq_beam[:, :, -1][ll_beam!=float(\"-inf\")] == 0).all())) \\\n                    or (not self.is_vrp and not self.is_op and not self.is_pctsp and not mask_beam[ll_beam!=float(\"-inf\")].all()) \\\n                    or (self.is_vrp and not self.allow_partial and not visited_beam[ll_beam!=float(\"-inf\")].all()) \\\n                    or (self.is_vrp and self.allow_partial and (j < loc_with_depot.size(1) or (demand_with_depot[ll_beam!=float(\"-inf\")] > 0).any())) \\\n                    or (self.is_pctsp and (j == 0 or not (seq_beam[:, :, -1][ll_beam!=float(\"-inf\")] == 0).all())):\n\n                assert (ll_beam.max(-1)[0]!=float(\"-inf\")).all()\n                ll_beam_new = []\n                fixed_beam_new = []\n                node_index_beam = []\n\n                n_beam = distance_beam.size(-1)\n                distance_beam = distance_beam.view(batch_size, n_beam, -1)\n\n                for beam_index in range(ll_beam.size(1)):\n                    ll = ll_beam[:, beam_index]\n                    seq = seq_beam[:, beam_index, :]\n                    mask = mask_beam[:, beam_index, :].view(batch_size, 1, -1)\n                    fixed = fixed_beam[beam_index * batch_size:(beam_index + 1) * batch_size]\n                    if self.is_vrp:\n                        if self.allow_partial:\n                            log_p = self._get_log_p(fixed, mask, seq, i, used=used_beam[:, beam_index], demand_with_depot=demand_with_depot[:, beam_index:beam_index+1, :])[:, 0, :]\n                        else:\n                            log_p = self._get_log_p(fixed, mask, seq, i, used=used_beam[:, beam_index])[:, 0, :]\n                        # log_p[(ll_beam[:, beam_index]!=float(\"-inf\")) & (visited_beam[:, beam_index].all(dim=-1)), 0] = 0\n                    elif self.is_op:\n                        remaining_beam = input[\"max_length\"].view(batch_size, 1) - distance_beam[:, beam_index]\n\n                        log_p = self._get_log_p(fixed, mask, seq, i, remaining = remaining_beam)[:, 0, :]\n                    elif self.is_pctsp: \n                        remaining_beam = torch.clamp(1 - prize_beam[:, beam_index, None], min = 0)\n                        mask[ll==float(\"-inf\"), :, 0] = 0\n                        log_p = self._get_log_p(fixed, mask, seq, i, remaining = remaining_beam)[:, 0, :]\n                    else:\n                        log_p = self._get_log_p(fixed, mask, seq, i)[:, 0, :]\n\n                    log_p, node_index = torch.topk(log_p, expand_size)\n\n                    ll_beam_new.append(ll.view(batch_size, 1) + log_p)\n                    node_index_beam.append(node_index)\n\n                node_index_beam = torch.cat(node_index_beam, dim=-1)\n                ll_beam_new = torch.cat(ll_beam_new, dim=-1)\n\n                if self.is_vrp or self.is_op or self.is_pctsp:\n                    distance_beam = distance_beam + (\n                            loc_with_depot[torch.arange(batch_size).view(-1, 1), seq_beam[:, :, -1]].view(batch_size, n_beam, 1, -1)\n                            - loc_with_depot[torch.arange(batch_size).view(-1, 1), node_index_beam].view(batch_size, n_beam, expand_size, -1)\n                            ).norm(p=2, dim=-1)\n                else:\n                    distance_beam = distance_beam + (\n                            input[torch.arange(batch_size).view(-1, 1), seq_beam[:, :, -1]].view(batch_size, n_beam, 1, -1)\n                            - input[torch.arange(batch_size).view(-1, 1), node_index_beam].view(batch_size, n_beam, expand_size, -1)\n                            ).norm(p=2, dim=-1)\n                distance_beam = distance_beam.view(batch_size, -1)\n\n                if self.is_vrp:\n                    if self.allow_partial:\n                        used_beam_last = used_beam\n                        selected_demand = demand_with_depot.view(-1, graph_size)[torch.arange(batch_size * demand_with_depot.size(1)).view(-1, 1), node_index_beam.view(batch_size * demand_with_depot.size(1), -1)].view(batch_size, -1)\n                    else:\n                        selected_demand = demand[torch.arange(batch_size).view(-1, 1), torch.clamp(node_index_beam - 1, 0, graph_size - 1)]\n                    used_beam = torch.min((used_beam.view(batch_size, n_beam, 1).expand(batch_size, n_beam, expand_size).contiguous().view(batch_size, -1) + selected_demand) * (node_index_beam != 0).float(), self.problem.VEHICLE_CAPACITY * torch.ones_like(selected_demand))\n                if fst == 1 and j > 0:\n                    distance_beam[ll_beam_new==float(\"-inf\")] = 99999\n                    ll_beam_new1 = ll_beam_new + 0.0\n                    if self.is_vrp or self.is_op or self.is_pctsp:\n                        if self.allow_partial:\n                            b_index = (demand_with_depot.view(batch_size, 1, n_beam, -1)==demand_with_depot.view(batch_size, n_beam, 1, -1)).all(dim=-1)\n                        else:\n                            b_index = (visited_beam.view(batch_size, 1, n_beam, -1)==visited_beam.view(batch_size, n_beam, 1, -1)).all(dim=-1)\n                        b_index[:, torch.arange(n_beam), torch.arange(n_beam)] = 0\n\n                        b_index = b_index.view(batch_size, n_beam, 1, n_beam, 1).expand(-1, -1, expand_size, -1, expand_size)\n                        b_index = b_index.contiguous().view(batch_size, n_beam * expand_size, n_beam * expand_size)\n                        n_index = (node_index_beam.view(batch_size, 1, -1) == node_index_beam.view(batch_size, -1, 1))\n                        n\n\n# ==========================================\n# File: problems/tsp/state_tsp.py\n# Function/Context: StateTSP\n# ==========================================\nimport torch\nfrom typing import NamedTuple\nfrom utils.boolmask import mask_long2bool, mask_long_scatter\n\n\nclass StateTSP(NamedTuple):\n    # Fixed input\n    loc: torch.Tensor\n    dist: torch.Tensor\n\n    # If this state contains multiple copies (i.e. beam search) for the same instance, then for memory efficiency\n    # the loc and dist tensors are not kept multiple times, so we need to use the ids to index the correct rows.\n    ids: torch.Tensor  # Keeps track of original fixed data index of rows\n\n    # State\n    first_a: torch.Tensor\n    prev_a: torch.Tensor\n    visited_: torch.Tensor  # Keeps track of nodes that have been visited\n    lengths: torch.Tensor\n    cur_coord: torch.Tensor\n    i: torch.Tensor  # Keeps track of step\n\n    @property\n    def visited(self):\n        if self.visited_.dtype == torch.uint8:\n            return self.visited_\n        else:\n            return mask_long2bool(self.visited_, n=self.loc.size(-2))\n\n    def __getitem__(self, key):\n        if torch.is_tensor(key) or isinstance(key, slice):  # If tensor, idx all tensors by this tensor:\n            return self._replace(\n                ids=self.ids[key],\n                first_a=self.first_a[key],\n                prev_a=self.prev_a[key],\n                visited_=self.visited_[key],\n                lengths=self.lengths[key],\n                cur_coord=self.cur_coord[key] if self.cur_coord is not None else None,\n            )\n        return super(StateTSP, self).__getitem__(key)\n\n    @staticmethod\n    def initialize(loc, visited_dtype=torch.uint8):\n\n        batch_size, n_loc, _ = loc.size()\n        prev_a = torch.zeros(batch_size, 1, dtype=torch.long, device=loc.device)\n        return StateTSP(\n            loc=loc,\n            dist=(loc[:, :, None, :] - loc[:, None, :, :]).norm(p=2, dim=-1),\n            ids=torch.arange(batch_size, dtype=torch.int64, device=loc.device)[:, None],  # Add steps dimension\n            first_a=prev_a,\n            prev_a=prev_a,\n            # Keep visited with depot so we can scatter efficiently (if there is an action for depot)\n            visited_=(  # Visited as mask is easier to understand, as long more memory efficient\n                torch.zeros(\n                    batch_size, 1, n_loc,\n                    dtype=torch.uint8, device=loc.device\n                )\n                if visited_dtype == torch.uint8\n                else torch.zeros(batch_size, 1, (n_loc + 63) // 64, dtype=torch.int64, device=loc.device)  # Ceil\n            ),\n            lengths=torch.zeros(batch_size, 1, device=loc.device),\n            cur_coord=None,\n            i=torch.zeros(1, dtype=torch.int64, device=loc.device)  # Vector with length num_steps\n        )\n\n    def get_final_cost(self):\n\n        assert self.all_finished()\n        # assert self.visited_.\n\n        return self.lengths + (self.loc[self.ids, self.first_a, :] - self.cur_coord).norm(p=2, dim=-1)\n\n    def update(self, selected):\n\n        # Update the state\n        prev_a = selected[:, None]  # Add dimension for step\n\n        # Add the length\n        # cur_coord = self.loc.gather(\n        #     1,\n        #     selected[:, None, None].expand(selected.size(0), 1, self.loc.size(-1))\n        # )[:, 0, :]\n        cur_coord = self.loc[self.ids, prev_a]\n        lengths = self.lengths\n        if self.cur_coord is not None:  # Don't add length for first action (selection of start node)\n            lengths = self.lengths + (cur_coord - self.cur_coord).norm(p=2, dim=-1)  # (batch_dim, 1)\n\n        # Update should only be called with just 1 parallel step, in which case we can check this way if we should update\n        first_a = prev_a if self.i.item() == 0 else self.first_a\n\n        if self.visited_.dtype == torch.uint8:\n            # Add one dimension since we write a single value\n            visited_ = self.visited_.scatter(-1, prev_a[:, :, None], 1)\n        else:\n            visited_ = mask_long_scatter(self.visited_, prev_a)\n\n        return self._replace(first_a=first_a, prev_a=prev_a, visited_=visited_,\n                             lengths=lengths, cur_coord=cur_coord, i=self.i + 1)\n\n    def all_finished(self):\n        # Exactly n steps\n        return self.i.item() >= self.loc.size(-2)\n\n    def get_current_node(self):\n        return self.prev_a\n\n    def get_mask(self):\n        return self.visited > 0\n\n    def get_nn(self, k=None):\n        # Insert step dimension\n        # Nodes already visited get inf so they do not make it\n        if k is None:\n            k = self.loc.size(-2) - self.i.item()  # Number of remaining\n        return (self.dist[self.ids, :, :] + self.visited.float()[:, :, None, :] * 1e6).topk(k, dim=-1, largest=False)[1]\n\n    def get_nn_current(self, k=None):\n        assert False, \"Currently not implemented, look into which neighbours to use in step 0?\"\n        # Note: if this is called in step 0, it will have k nearest neighbours to node 0, which may not be desired\n        # so it is probably better to use k = None in the first iteration\n        if k is None:\n            k = self.loc.size(-2)\n        k = min(k, self.loc.size(-2) - self.i.item())  # Number of remaining\n        return (\n            self.dist[\n                self.ids,\n                self.prev_a\n            ] +\n            self.visited.float() * 1e6\n        ).topk(k, dim=-1, largest=False)[1]\n\n    def construct_solutions(self, actions):\n        return actions\n\n# ==========================================\n# File: problems/vrp/state_cvrp.py\n# Function/Context: StateCVRP\n# ==========================================\nimport torch\nfrom typing import NamedTuple\nfrom utils.boolmask import mask_long2bool, mask_long_scatter\n\n\nclass StateCVRP(NamedTuple):\n    # Fixed input\n    coords: torch.Tensor  # Depot + loc\n    demand: torch.Tensor\n\n    # If this state contains multiple copies (i.e. beam search) for the same instance, then for memory efficiency\n    # the coords and demands tensors are not kept multiple times, so we need to use the ids to index the correct rows.\n    ids: torch.Tensor  # Keeps track of original fixed data index of rows\n\n    # State\n    prev_a: torch.Tensor\n    used_capacity: torch.Tensor\n    visited_: torch.Tensor  # Keeps track of nodes that have been visited\n    lengths: torch.Tensor\n    cur_coord: torch.Tensor\n    i: torch.Tensor  # Keeps track of step\n\n    VEHICLE_CAPACITY = 1.0  # Hardcoded\n\n    @property\n    def visited(self):\n        if self.visited_.dtype == torch.uint8:\n            return self.visited_\n        else:\n            return mask_long2bool(self.visited_, n=self.demand.size(-1))\n\n    @property\n    def dist(self):\n        return (self.coords[:, :, None, :] - self.coords[:, None, :, :]).norm(p=2, dim=-1)\n\n    def __getitem__(self, key):\n        if torch.is_tensor(key) or isinstance(key, slice):  # If tensor, idx all tensors by this tensor:\n            return self._replace(\n                ids=self.ids[key],\n                prev_a=self.prev_a[key],\n                used_capacity=self.used_capacity[key],\n                visited_=self.visited_[key],\n                lengths=self.lengths[key],\n                cur_coord=self.cur_coord[key],\n            )\n        return super(StateCVRP, self).__getitem__(key)\n\n    # Warning: cannot override len of NamedTuple, len should be number of fields, not batch size\n    # def __len__(self):\n    #     return len(self.used_capacity)\n\n    @staticmethod\n    def initialize(input, visited_dtype=torch.uint8):\n\n        depot = input['depot']\n        loc = input['loc']\n        demand = input['demand']\n\n        batch_size, n_loc, _ = loc.size()\n        return StateCVRP(\n            coords=torch.cat((depot[:, None, :], loc), -2),\n            demand=demand,\n            ids=torch.arange(batch_size, dtype=torch.int64, device=loc.device)[:, None],  # Add steps dimension\n            prev_a=torch.zeros(batch_size, 1, dtype=torch.long, device=loc.device),\n            used_capacity=demand.new_zeros(batch_size, 1),\n            visited_=(  # Visited as mask is easier to understand, as long more memory efficient\n                # Keep visited_ with depot so we can scatter efficiently\n                torch.zeros(\n                    batch_size, 1, n_loc + 1,\n                    dtype=torch.uint8, device=loc.device\n                )\n                if visited_dtype == torch.uint8\n                else torch.zeros(batch_size, 1, (n_loc + 63) // 64, dtype=torch.int64, device=loc.device)  # Ceil\n            ),\n            lengths=torch.zeros(batch_size, 1, device=loc.device),\n            cur_coord=input['depot'][:, None, :],  # Add step dimension\n            i=torch.zeros(1, dtype=torch.int64, device=loc.device)  # Vector with length num_steps\n        )\n\n    def get_final_cost(self):\n\n        assert self.all_finished()\n\n        return self.lengths + (self.coords[self.ids, 0, :] - self.cur_coord).norm(p=2, dim=-1)\n\n    def update(self, selected):\n\n        assert self.i.size(0) == 1, \"Can only update if state represents single step\"\n\n        # Update the state\n        selected = selected[:, None]  # Add dimension for step\n        prev_a = selected\n        n_loc = self.demand.size(-1)  # Excludes depot\n\n        # Add the length\n        cur_coord = self.coords[self.ids, selected]\n        # cur_coord = self.coords.gather(\n        #     1,\n        #     selected[:, None].expand(selected.size(0), 1, self.coords.size(-1))\n        # )[:, 0, :]\n        lengths = self.lengths + (cur_coord - self.cur_coord).norm(p=2, dim=-1)  # (batch_dim, 1)\n\n        # Not selected_demand is demand of first node (by clamp) so incorrect for nodes that visit depot!\n        #selected_demand = self.demand.gather(-1, torch.clamp(prev_a - 1, 0, n_loc - 1))\n        selected_demand = self.demand[self.ids, torch.clamp(prev_a - 1, 0, n_loc - 1)]\n\n        # Increase capacity if depot is not visited, otherwise set to 0\n        #used_capacity = torch.where(selected == 0, 0, self.used_capacity + selected_demand)\n        used_capacity = (self.used_capacity + selected_demand) * (prev_a != 0).float()\n\n        if self.visited_.dtype == torch.uint8:\n            # Note: here we do not subtract one as we have to scatter so the first column allows scattering depot\n            # Add one dimension since we write a single value\n            visited_ = self.visited_.scatter(-1, prev_a[:, :, None], 1)\n        else:\n            # This works, will not set anything if prev_a -1 == -1 (depot)\n            visited_ = mask_long_scatter(self.visited_, prev_a - 1)\n\n        return self._replace(\n            prev_a=prev_a, used_capacity=used_capacity, visited_=visited_,\n            lengths=lengths, cur_coord=cur_coord, i=self.i + 1\n        )\n\n    def all_finished(self):\n        return self.i.item() >= self.demand.size(-1) and self.visited.all()\n\n    def get_finished(self):\n        return self.visited.sum(-1) == self.visited.size(-1)\n\n    def get_current_node(self):\n        return self.prev_a\n\n    def get_mask(self):\n        \"\"\"\n        Gets a (batch_size, n_loc + 1) mask with the feasible actions (0 = depot), depends on already visited and\n        remaining capacity. 0 = feasible, 1 = infeasible\n        Forbids to visit depot twice in a row, unless all nodes have been visited\n        :return:\n        \"\"\"\n\n        if self.visited_.dtype == torch.uint8:\n            visited_loc = self.visited_[:, :, 1:]\n        else:\n            visited_loc = mask_long2bool(self.visited_, n=self.demand.size(-1))\n\n        # Nodes that cannot be visited are already visited or too much demand to be served now\n        mask_loc = (\n            visited_loc |\n            # For demand steps_dim is inserted by indexing with id, for used_capacity insert node dim for broadcasting\n            (self.demand[self.ids, :] + self.used_capacity[:, :, None] > self.VEHICLE_CAPACITY)\n        )\n\n        # Cannot visit the depot if just visited and still unserved nodes\n        mask_depot = (self.prev_a == 0) & ((mask_loc == 0).int().sum(-1) > 0)\n        return torch.cat((mask_depot[:, :, None], mask_loc > 0), -1)\n\n    def construct_solutions(self, actions):\n        return actions\n\n# ==========================================\n# File: utils/beam_search.py\n# Function/Context: beam_search, _beam_search, BatchBeam\n# ==========================================\nimport time\nimport torch\nfrom typing import NamedTuple\nfrom utils.lexsort import torch_lexsort\n\n\ndef beam_search(*args, **kwargs):\n    beams, final_state = _beam_search(*args, **kwargs)\n    return get_beam_search_results(beams, final_state)\n\n\ndef get_beam_search_results(beams, final_state):\n    beam = beams[-1]  # Final beam\n    if final_state is None:\n        return None, None, None, None, beam.batch_size\n\n    # First state has no actions/parents and should be omitted when backtracking\n    actions = [beam.action for beam in beams[1:]]\n    parents = [beam.parent for beam in beams[1:]]\n\n    solutions = final_state.construct_solutions(backtrack(parents, actions))\n    return beam.score, solutions, final_state.get_final_cost()[:, 0], final_state.ids.view(-1), beam.batch_size\n\n\ndef _beam_search(state, beam_size, propose_expansions=None,\n                keep_states=False):\n\n    beam = BatchBeam.initialize(state)\n\n    # Initial state\n    beams = [beam if keep_states else beam.clear_state()]\n\n    # Perform decoding steps\n    while not beam.all_finished():\n\n        # Use the model to propose and score expansions\n        parent, action, score = beam.propose_expansions() if propose_expansions is None else propose_expansions(beam)\n        if parent is None:\n            return beams, None\n\n        # Expand and update the state according to the selected actions\n        beam = beam.expand(parent, action, score=score)\n\n        # Get topk\n        beam = beam.topk(beam_size)\n\n        # Collect output of step\n        beams.append(beam if keep_states else beam.clear_state())\n\n    # Return the final state separately since beams may not keep state\n    return beams, beam.state\n\n\nclass BatchBeam(NamedTuple):\n    \"\"\"\n    Class that keeps track of a beam for beam search in batch mode.\n    Since the beam size of different entries in the batch may vary, the tensors are not (batch_size, beam_size, ...)\n    but rather (sum_i beam_size_i, ...), i.e. flattened. This makes some operations a bit cumbersome.\n    \"\"\"\n    score: torch.Tensor  # Current heuristic score of each entry in beam (used to select most promising)\n    state: None  # To track the state\n    parent: torch.Tensor\n    action: torch.Tensor\n    batch_size: int  # Can be used for optimizations if batch_size = 1\n    device: None  # Track on which device\n\n    # Indicates for each row to which batch it belongs (0, 0, 0, 1, 1, 2, ...), managed by state\n    @property\n    def ids(self):\n        return self.state.ids.view(-1)  # Need to flat as state has steps dimension\n\n    def __getitem__(self, key):\n        if torch.is_tensor(key) or isinstance(key, slice):  # If tensor, idx all tensors by this tensor:\n            return self._replace(\n                # ids=self.ids[key],\n                score=self.score[key] if self.score is not None else None,\n                state=self.state[key],\n                parent=self.parent[key] if self.parent is not None else None,\n                action=self.action[key] if self.action is not None else None\n            )\n        return super(BatchBeam, self).__getitem__(key)\n\n    # Do not use __len__ since this is used by namedtuple internally and should be number of fields\n    # def __len__(self):\n    #     return len(self.ids)\n\n    @staticmethod\n    def initialize(state):\n        batch_size = len(state.ids)\n        device = state.ids.device\n        return BatchBeam(\n            score=torch.zeros(batch_size, dtype=torch.float, device=device),\n            state=state,\n            parent=None,\n            action=None,\n            batch_size=batch_size,\n            device=device\n        )\n\n    def propose_expansions(self):\n        mask = self.state.get_mask()\n        # Mask always contains a feasible action\n        expansions = torch.nonzero(mask[:, 0, :] == 0)\n        parent, action = torch.unbind(expansions, -1)\n        return parent, action, None\n\n    def expand(self, parent, action, score=None):\n        return self._replace(\n            score=score,  # The score is cleared upon expanding as it is no longer valid, or it must be provided\n            state=self.state[parent].update(action),  # Pass ids since we replicated state\n            parent=parent,\n            action=action\n        )\n\n    def topk(self, k):\n        idx_topk = segment_topk_idx(self.score, k, self.ids)\n        return self[idx_topk]\n\n    def all_finished(self):\n        return self.state.all_finished()\n\n    def cpu(self):\n        return self.to(torch.device('cpu'))\n\n    def to(self, device):\n        if device == self.device:\n            return self\n        return self._replace(\n            score=self.score.to(device) if self.score is not None else None,\n            state=self.state.to(device),\n            parent=self.parent.to(device) if self.parent is not None else None,\n            action=self.action.to(device) if self.action is not None else None\n        )\n\n    def clear_state(self):\n        return self._replace(state=None)\n\n    def size(self):\n        return self.state.ids.size(0)\n\n\ndef segment_topk_idx(x, k, ids):\n    \"\"\"\n    Finds the topk per segment of data x given segment ids (0, 0, 0, 1, 1, 2, ...).\n    Note that there may be fewer than k elements in a segment so the returned length index can vary.\n    x[result], ids[result] gives the sorted elements per segment as well as corresponding segment ids after sorting.\n    :param x:\n    :param k:\n    :param ids:\n    :return:\n    \"\"\"\n    assert x.dim() == 1\n    assert ids.dim() == 1\n\n    # Since we may have varying beam size per batch entry we cannot reshape to (batch_size, beam_size)\n    # And use default topk along dim -1, so we have to be creative\n    # Now we have to get the topk per segment which is really annoying :(\n    # we use lexsort on (ids, score), create array with offset per id\n    # offsets[ids] then gives offsets repeated and only keep for which arange(len) < offsets + k\n    splits_ = torch.nonzero(ids[1:] - ids[:-1])\n\n    if len(splits_) == 0:  # Only one group\n        _, idx_topk = x.topk(min(k, x.size(0)))\n        return idx_topk\n\n    splits = torch.cat((ids.new_tensor([0]), splits_[:, 0] + 1))\n    # Make a new array in which we store for each id the offset (start) of the group\n    # This way ids does not need to be increasing or adjacent, as long as each group is a single range\n    group_offsets = splits.new_zeros((splits.max() + 1,))\n    group_offsets[ids[splits]] = splits\n    offsets = group_offsets[ids]  # Look up offsets based on ids, effectively repeating for the repetitions per id\n\n    # We want topk so need to sort x descending so sort -x (be careful with unsigned data type!)\n    idx_sorted = torch_lexsort((-(x if x.dtype != torch.uint8 else x.int()).detach(), ids))\n\n    # This will filter first k per group (example k = 2)\n    # ids     = [0, 0, 0, 1, 1, 1, 1, 2]\n    # splits  = [0, 3, 7]\n    # offsets = [0, 0, 0, 3, 3, 3, 3, 7]\n    # offs+2  = [2, 2, 2, 5, 5, 5, 5, 9]\n    # arange  = [0, 1, 2, 3, 4, 5, 6, 7]\n    # filter  = [1, 1, 0, 1, 1, 0, 0, 1]\n    # Use filter to get only topk of sorting idx\n    return idx_sorted[torch.arange(ids.size(0), out=ids.new()) < offsets + k]\n\n\ndef backtrack(parents, actions):\n\n    # Now backtrack to find aligned action sequences in reversed order\n    cur_parent = parents[-1]\n    reversed_aligned_sequences = [actions[-1]]\n    for parent, sequence in reversed(list(zip(parents[:-1], actions[:-1]))):\n        reversed_aligned_sequences.append(sequence.gather(-1, cur_parent))\n        cur_parent = parent.gather(-1, cur_parent)\n\n    return torch.stack(list(reversed(reversed_aligned_sequences)), -1)\n\n\nclass CachedLookup(object):\n\n    def __init__(self, data):\n        self.orig = data\n        self.key = None\n        self.current = None\n\n    def __getitem__(self, key):\n        assert not isinstance(key, slice), \"CachedLookup does not support slicing, \" \\\n                                           \"you can slice the result of an index operation instead\"\n\n        if torch.is_tensor(key):  # If tensor, idx all tensors by this tensor:\n\n            if self.key is None:\n                self.key = key\n                self.current = self.orig[key]\n            elif len(key) != len(self.key) or (key != self.key).any():\n                self.key = key\n                self.current = self.orig[key]\n\n            return self.current\n\n        return super(CachedLookup, self).__getitem__(key)",
  "description": "Combined Analysis:\n- [nets/attention_model.py]: This file implements the core MDAM (Multi-Decoder Attention Model) architecture described in the paper. Key implementations include: 1) Multiple decoders (n_paths) with separate projection layers for diversity, 2) Embedding Glimpse (EG) mechanism that updates node embeddings every n_EG steps by masking visited nodes, 3) KL divergence regularization between different decoder outputs for training diversity, 4) REINFORCE algorithm with baseline for training. The forward method orchestrates the multi-decoder training with embedding updates, KL regularization, and REINFORCE loss computation.\n- [nets/graph_encoder.py]: This file implements the core graph attention encoder component of the MDAM model, which is essential for processing node embeddings in vehicle routing problems. The GraphAttentionEncoder class uses multi-head attention layers to encode graph nodes, and the 'change' method implements the Embedding Glimpse mechanism that dynamically updates node embeddings by masking visited nodes during solution construction. This directly corresponds to the paper's algorithm step of refining embeddings to exclude visited nodes, enabling the model to maintain solution feasibility constraints.\n- [nets/model_search.py]: This file implements the core Multi-Decoder Attention Model (MDAM) with multiple decoding paths (n_paths) and beam search for solving vehicle routing problems. The AttentionModel class contains the neural network architecture with multiple decoders (projection layers per path) and the forward method performs a customized beam search across multiple problem types (TSP, CVRP, SDVRP, OP, PCTSP). The code handles problem-specific constraints (capacity, distance limits, prizes) and uses the Embedding Glimpse mechanism through the _get_log_p method (not fully shown) to compute log probabilities for node selection. The beam search maintains multiple sequences (beams) and expands them using top-k selection, updating problem-specific state variables (used capacity, remaining distance, collected prize). This directly implements the algorithm steps described in the paper: multi-decoder attention model with embedding glimpse and beam search for generating diverse, high-quality solutions.\n- [problems/tsp/state_tsp.py]: This file implements the state management for the Traveling Salesman Problem (TSP) within the MDAM framework. The StateTSP class maintains the problem state during solution construction, including node locations, distances, visited nodes, current position, and tour length. It provides methods for state initialization, updating after node selection, computing final tour cost, and generating masks for visited nodes. This directly supports the sequential decision-making process in the paper's attention-based decoder, where the model selects unvisited nodes at each step to build tours. The state tracks the optimization objective (tour length) and enforces the TSP constraint that each node is visited exactly once through the visited mask.\n- [problems/vrp/state_cvrp.py]: This file implements the core state management and constraint logic for the Capacitated Vehicle Routing Problem (CVRP) within the MDAM framework. The StateCVRP class maintains the problem state during solution construction, including: 1) Tracking visited nodes and vehicle capacity usage, 2) Computing Euclidean distances between locations, 3) Enforcing CVRP constraints through the get_mask() method (capacity constraints and visit restrictions), 4) Updating state when nodes are selected, and 5) Calculating final route costs. This directly implements the constraint handling for the mathematical model's capacity constraints and forms the foundation for the sequential decision-making process in the attention-based decoder.\n- [utils/beam_search.py]: This file implements the core beam search algorithm used in the MDAM paper for constructing solutions to vehicle routing problems. The beam search operates in batch mode with variable beam sizes per batch entry, using a flattened representation. Key components include: 1) The main beam_search function that orchestrates the search, 2) BatchBeam class managing beam states with methods for initialization, expansion, and top-k selection, 3) segment_topk_idx for selecting top-k elements per batch segment, and 4) backtrack for reconstructing solution sequences. The algorithm iteratively proposes expansions (using either a default mask-based method or a custom propose_expansions function), expands states, and maintains the top-k beams until all solutions are complete. This directly implements the 'customized beam search scheme' mentioned in the paper's algorithm steps.",
  "dependencies": [
    "typing.NamedTuple",
    "utils.functions.sample_many",
    "copy",
    "utils.boolmask.mask_long2bool",
    "torch.autograd.Variable",
    "problems.tsp.problem_tsp.TSP",
    "torch.utils.checkpoint",
    "utils.lexsort.torch_lexsort",
    "torch.nn.functional",
    "utils.boolmask.mask_long_scatter",
    "utils.beam_search.CachedLookup",
    "problems.pctsp.problem_pctsp.PCTSPDet",
    "problems.pctsp.problem_pctsp.PCTSPStoch",
    "utils.tensor_functions.compute_in_batches",
    "math",
    "CachedLookup",
    "segment_topk_idx",
    "numpy",
    "torch.nn.DataParallel",
    "problems.vrp.problem_vrp.CVRP",
    "torch.nn",
    "backtrack",
    "problems.op.problem_op.OP",
    "problems.vrp.problem_vrp.SDVRP",
    "torch",
    "nets.graph_encoder.GraphAttentionEncoder"
  ]
}