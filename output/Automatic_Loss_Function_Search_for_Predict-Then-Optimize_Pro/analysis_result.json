{
  "paper_id": "Automatic_Loss_Function_Search_for_Predict-Then-Optimize_Pro",
  "title": "AUTOMATIC LOSS FUNCTION SEARCH FOR PREDICT-THEN-OPTIMIZE PROBLEMS WITH STRONG RANKING PROPERTY",
  "abstract": "Combinatorial optimization problems with parameters to be predicted from side information are commonly seen in a variety of problems during the paradigm shifts from reactive decision making to proactive decision making. Due to the misalignment between the continuous prediction results and the discrete decisions in optimization problems, it is hard to achieve a satisfactory prediction result with the ordinary $l_2$ loss in the prediction phase. To properly connect the prediction loss with the optimization goal, in this paper we propose a *total group preorder* (TGP) loss and its differential version called *approximate total group preorder* (ATGP) loss for predict-then-optimize (PTO) problems with strong ranking property. These new losses are provably more robust than the usual $l_2$ loss in a linear regression setting and have great potential to extend to other settings. We also propose an automatic searching algorithm that adapts the ATGP loss to PTO problems with different combinatorial structures. Extensive experiments on the ranking problem, the knapsack problem, and the shortest path problem have demonstrated that our proposed method can achieve a significantly better performance compared to the other methods designed for PTO problems.",
  "problem_description_natural": "The paper addresses predict-then-optimize (PTO) problems where unknown parameters of a combinatorial optimization problem must first be predicted from contextual features before solving the optimization. The key challenge is that standard prediction losses like $l_2$ do not align with the downstream optimization objective because small prediction errors may lead to large decision-quality losses due to the discrete and combinatorial nature of the optimization. The authors focus on a class of PTO problems exhibiting a 'strong ranking property,' meaning the optimal solution is fully determined by the total group preorder (i.e., comparisons of sums of subsets of parameters). Examples include ranking, knapsack (with fixed weights and capacity), and shortest path problems. The goal is to design a differentiable surrogate loss that better reflects the optimization outcome and can be automatically adapted to different problem structures.",
  "problem_type": "combinatorial optimization",
  "datasets": [
    "Irish Single Electricity Market Operator (SEM-O) energy price data",
    "Quandl WIKI SP500 dataset",
    "Twitter ego network (McAuley & Leskovec, 2012)"
  ],
  "performance_metrics": [
    "SPO loss"
  ],
  "lp_model": {
    "objective": "$\\max \\sum_{i=1}^{d} c_i z_i$",
    "constraints": [
      "$\\sum_{i=1}^{d} w_i z_i \\leq B$",
      "$z_i \\in \\{0,1\\}, \\forall i=1,\\dots,d$"
    ],
    "variables": [
      "$z_i$: binary decision variable indicating whether item $i$ is selected"
    ]
  },
  "raw_latex_model": "$$\\begin{aligned} \\max_{z} & \\sum_{i=1}^{d} c_i z_i \\\\ \\text{s.t.} & \\sum_{i=1}^{d} w_i z_i \\leq B \\\\ & z_i \\in \\{0,1\\}, \\forall i=1,\\dots,d \\end{aligned}$$",
  "algorithm_description": "The paper addresses predict-then-optimize (PTO) problems where the parameters of an optimization problem (e.g., item values in knapsack) are predicted from features using a neural network. The neural network is trained with a loss function (ATGP) that is automatically searched by the APOC algorithm. APOC uses policy gradient methods (REINFORCE or PPO2) to search over TGP matrices parameterized by Derivative-of-Gaussian (DoG) wavelet filters, aiming to minimize the SPO loss (regret) on a validation set. After training, the predicted parameters are fed into a traditional optimization solver (e.g., Gurobi) to obtain the final solution."
}