{
  "paper_id": "Deep_Graph_Matching_Consensus",
  "title": "DEEP GRAPH MATCHING CONSENSUS",
  "abstract": "This work presents a two-stage neural architecture for learning and refining structural correspondences between graphs. First, we use localized node embeddings computed by a graph neural network to obtain an initial ranking of soft correspondences between nodes. Secondly, we employ synchronous message passing networks to iteratively re-rank the soft correspondences to reach a matching consensus in local neighborhoods between graphs. We show, theoretically and empirically, that our message passing scheme computes a well-founded measure of consensus for corresponding neighborhoods, which is then used to guide the iterative re-ranking process. Our purely local and sparsity-aware architecture scales well to large, real-world inputs while still being able to recover global correspondences consistently. We demonstrate the practical effectiveness of our method on real-world tasks from the fields of computer vision and entity alignment between knowledge graphs, on which we improve upon the current state-of-the-art.",
  "problem_description_natural": "The paper addresses the problem of graph matching, which involves establishing injective node correspondences between a source graph and a target graph by maximizing structural similarity—specifically, by preserving edge relationships between matched nodes. This is traditionally formulated as a quadratic assignment problem that maximizes the number of aligned edges under one-to-one mapping constraints. The authors aim to solve this in a data-driven, differentiable manner using deep learning, avoiding NP-hard combinatorial optimization during inference. Their approach uses an initial soft correspondence matrix derived from node embeddings, then iteratively refines it by enforcing neighborhood consensus: ensuring that neighbors of matched nodes are also consistently matched. The goal is to produce a correspondence matrix that reflects both local node similarity and global structural coherence without solving an explicit optimization problem at test time.",
  "problem_type": "Graph Matching",
  "datasets": [
    "Generated ER Graphs",
    "PASCALVOC",
    "WILLOW-OBJECTCLASS",
    "PASCALPF",
    "DBP15K"
  ],
  "performance_metrics": [
    "Hits@1",
    "Hits@10"
  ],
  "lp_model": {
    "objective": "\\argmax_{\\boldsymbol{S}} \\sum_{\\substack{i,i' \\in \\mathcal{V}_s \\\\ j,j' \\in \\mathcal{V}_t}} A^{(s)}_{i,i'} A^{(t)}_{j,j'} S_{i,j} S_{i',j'}",
    "constraints": [
      "\\sum_{j \\in \\mathcal{V}_t} S_{i,j} = 1~\\forall i \\in \\mathcal{V}_s",
      "\\sum_{i \\in \\mathcal{V}_s} S_{i,j} \\leq 1~\\forall j \\in \\mathcal{V}_t"
    ],
    "variables": [
      "\\boldsymbol{S} \\in \\{0, 1\\}^{|\\mathcal{V}_s| \\times |\\mathcal{V}_t|} - correspondence matrix where S_{i,j} = 1 if node i in source graph matches node j in target graph"
    ]
  },
  "raw_latex_model": "\\argmax_{\\boldsymbol{S}} \\sum_{\\substack{i,i' \\in \\mathcal{V}_s \\\\ j,j' \\in \\mathcal{V}_t}} A^{(s)}_{i,i'} A^{(t)}_{j,j'} S_{i,j} S_{i',j'} \\quad \\text{subject to} \\quad \\sum_{j \\in \\mathcal{V}_t} S_{i,j} = 1~\\forall i \\in \\mathcal{V}_s, \\sum_{i \\in \\mathcal{V}_s} S_{i,j} \\leq 1~\\forall j \\in \\mathcal{V}_t",
  "algorithm_description": "The algorithm is a two-stage neural architecture for graph matching consensus, as described in Algorithm 1. Steps: 1. Compute node embeddings for source and target graphs using a shared graph neural network Ψ_θ1. 2. Compute initial soft correspondence scores Ŝ^(0) by taking the dot product of node embeddings. 3. Sparsify Ŝ^(0) by retaining only the top k candidates for each source node. 4. Perform L refinement iterations: a. Normalize current scores row-wise using softmax to get S^(l-1). b. Sample a random node function R_s^(l) from a normal distribution. c. Map R_s^(l) to the target graph via S^(l-1)⊤ to get R_t^(l). d. Apply a shared graph neural network Ψ_θ2 to distribute functions on both graphs, obtaining O_s and O_t. e. Compute neighborhood consensus measure d_{i,j} = o_i^(s) - o_j^(t) for each correspondence. f. Update correspondence scores using a neural network Φ_θ3: Ŝ_{i,j}^(l) = Ŝ_{i,j}^(l-1) + Φ_θ3(d_{i,j}). 5. After L iterations, apply final row-wise softmax normalization to get refined correspondence matrix S^(L). 6. Output the sparse soft correspondence matrix S^(L) with k non-zero entries per row."
}