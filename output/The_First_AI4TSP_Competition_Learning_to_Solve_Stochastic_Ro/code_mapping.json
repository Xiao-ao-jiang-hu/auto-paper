{
  "file_path": "baseline_rl/neuralconet.py, baseline_rl/train_baseline.py, baseline_surrogate/demo_surrogate.py, env.py, env_rl.py, op_utils/op.py",
  "function_name": "NeuralCombOptRL, objective, check_surrogate_solution, objBO, x_to_route, Env, EnvRL, tour_check",
  "code_snippet": "\n\n# ==========================================\n# File: baseline_rl/neuralconet.py\n# Function/Context: NeuralCombOptRL\n# ==========================================\nimport math\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\n\nclass Encoder(nn.Module):\n    \"\"\"Maps a graph represented as an input sequence\n    to a hidden vector\"\"\"\n\n    def __init__(self, input_dim, hidden_dim, use_cuda):\n        super(Encoder, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.lstm = nn.LSTM(input_dim, hidden_dim)\n        self.use_cuda = use_cuda\n        self.enc_init_state = self.init_hidden(hidden_dim)\n\n    def forward(self, x, hidden):\n        output, hidden = self.lstm(x, hidden)\n        return output, hidden\n\n    def init_hidden(self, hidden_dim):\n        \"\"\"Trainable initial hidden state\"\"\"\n        enc_init_hx = Variable(torch.zeros(hidden_dim), requires_grad=False)\n        if self.use_cuda:\n            enc_init_hx = enc_init_hx.cuda()\n\n        # enc_init_hx.data.uniform_(-(1. / math.sqrt(hidden_dim)),\n        #        1. / math.sqrt(hidden_dim))\n\n        enc_init_cx = Variable(torch.zeros(hidden_dim), requires_grad=False)\n        if self.use_cuda:\n            enc_init_cx = enc_init_cx.cuda()\n\n        # enc_init_cx = nn.Parameter(enc_init_cx)\n        # enc_init_cx.data.uniform_(-(1. / math.sqrt(hidden_dim)),\n        #        1. / math.sqrt(hidden_dim))\n        return (enc_init_hx, enc_init_cx)\n\n\nclass Attention(nn.Module):\n    \"\"\"A generic attention module for a decoder in seq2seq\"\"\"\n\n    def __init__(self, dim, use_tanh=False, C=10, use_cuda=True):\n        super(Attention, self).__init__()\n        self.use_tanh = use_tanh\n        self.project_query = nn.Linear(dim, dim)\n        self.project_ref = nn.Conv1d(dim, dim, 1, 1)\n        self.C = C  # tanh exploration\n        self.tanh = nn.Tanh()\n\n        v = torch.FloatTensor(dim)\n        if use_cuda:\n            v = v.cuda()\n        self.v = nn.Parameter(v)\n        self.v.data.uniform_(-(1. / math.sqrt(dim)), 1. / math.sqrt(dim))\n\n    def forward(self, query, ref):\n        \"\"\"\n        Args:\n            query: is the hidden state of the decoder at the current\n                time step. batch x dim\n            ref: the set of hidden states from the encoder.\n                sourceL x batch x hidden_dim\n        \"\"\"\n        # ref is now [batch_size x hidden_dim x sourceL]\n        ref = ref.permute(1, 2, 0)\n        q = self.project_query(query).unsqueeze(2)  # batch x dim x 1\n        e = self.project_ref(ref)  # batch_size x hidden_dim x sourceL\n        # expand the query by sourceL\n        # batch x dim x sourceL\n        expanded_q = q.repeat(1, 1, e.size(2))\n        # batch x 1 x hidden_dim\n        v_view = self.v.unsqueeze(0).expand(\n            expanded_q.size(0), len(self.v)).unsqueeze(1)\n        # [batch_size x 1 x hidden_dim] * [batch_size x hidden_dim x sourceL]\n        u = torch.bmm(v_view, self.tanh(expanded_q + e)).squeeze(1)\n        if self.use_tanh:\n            logits = self.C * self.tanh(u)\n        else:\n            logits = u\n        return e, logits\n\n\nclass Decoder(nn.Module):\n    def __init__(self,\n                 embedding_dim,\n                 hidden_dim,\n                 max_length,\n                 tanh_exploration,\n                 terminating_symbol,\n                 use_tanh,\n                 decode_type,\n                 n_glimpses=1,\n                 use_cuda=True):\n        super(Decoder, self).__init__()\n\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.n_glimpses = n_glimpses\n        self.max_length = max_length\n        self.terminating_symbol = terminating_symbol\n        self.decode_type = decode_type\n        self.use_cuda = use_cuda\n\n        self.input_weights = nn.Linear(embedding_dim, 4 * hidden_dim)\n        self.hidden_weights = nn.Linear(hidden_dim, 4 * hidden_dim)\n\n        self.pointer = Attention(hidden_dim, use_tanh=use_tanh, C=tanh_exploration, use_cuda=self.use_cuda)\n        self.glimpse = Attention(hidden_dim, use_tanh=False, use_cuda=self.use_cuda)\n        self.sm = nn.Softmax(dim=1)\n\n    def apply_mask_to_logits(self, step, logits, mask, prev_idxs):\n        if mask is None:\n            mask = torch.zeros(logits.size()).byte()\n            if self.use_cuda:\n                mask = mask.cuda()\n\n        maskk = mask.clone()\n\n        if step == 0:\n            maskk[[x for x in range(logits.size(0))], 0] = 1\n            logits[maskk.bool()] = -np.inf\n\n        else:\n\n            # to prevent them from being reselected.\n            # Or, allow re-selection and penalize in the objective function\n            if prev_idxs is not None:\n                if step == 1:\n                    maskk[[x for x in range(logits.size(0))], 0] = 0\n                # set most recently selected idx values to 1\n                maskk[[x for x in range(logits.size(0))],\n                      prev_idxs.data] = 1\n\n            logits[maskk.bool()] = -np.inf\n\n        return logits, maskk\n\n    def forward(self, decoder_input, embedded_inputs, hidden, context):\n        \"\"\"\n        Args:\n            decoder_input: The initial input to the decoder\n                size is [batch_size x embedding_dim]. Trainable parameter.\n            embedded_inputs: [sourceL x batch_size x embedding_dim]\n            hidden: the prev hidden state, size is [batch_size x hidden_dim].\n                Initially this is set to (enc_h[-1], enc_c[-1])\n            context: encoder outputs, [sourceL x batch_size x hidden_dim]\n        \"\"\"\n\n        def recurrence(x, hidden, logit_mask, prev_idxs, step):\n\n            hx, cx = hidden  # batch_size x hidden_dim\n\n            gates = self.input_weights(x) + self.hidden_weights(hx)\n            ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n\n            ingate = torch.sigmoid(ingate)\n            forgetgate = torch.sigmoid(forgetgate)\n            cellgate = torch.tanh(cellgate)\n            outgate = torch.sigmoid(outgate)\n\n            cy = (forgetgate * cx) + (ingate * cellgate)\n            hy = outgate * torch.tanh(cy)  # batch_size x hidden_dim\n\n            g_l = hy\n            for i in range(self.n_glimpses):\n                ref, logits = self.glimpse(g_l, context)\n                logits, logit_mask = self.apply_mask_to_logits(step, logits, logit_mask, prev_idxs)\n                # [batch_size x h_dim x sourceL] * [batch_size x sourceL x 1] =\n                # [batch_size x h_dim x 1]\n                g_l = torch.bmm(ref, self.sm(logits).unsqueeze(2)).squeeze(2)\n            _, logits = self.pointer(g_l, context)\n\n            logits, logit_mask = self.apply_mask_to_logits(step, logits, logit_mask, prev_idxs)\n            probs = self.sm(logits)\n            return hy, cy, probs, logit_mask\n\n        batch_size = context.size(1)\n        outputs = []\n        selections = []\n        steps = range(self.max_length)  # or until terminating symbol ?\n        inps = []\n        idxs = None\n        mask = None\n\n        for i in steps:\n            hx, cx, probs, mask = recurrence(decoder_input, hidden, mask, idxs, i)\n            hidden = (hx, cx)\n            # select the next inputs for the decoder [batch_size x hidden_dim]\n            if self.decode_type == \"stochastic\":\n                decoder_input, idxs = self.decode_stochastic(\n                    probs,\n                    embedded_inputs,\n                    selections)\n            if self.decode_type == 'greedy':\n                decoder_input, idxs = self.decode_greedy(\n                    probs,\n                    embedded_inputs,\n                    selections)\n\n            inps.append(decoder_input)\n            # use outs to point to next object\n            outputs.append(probs)\n            selections.append(idxs)\n        return (outputs, selections), hidden\n\n    def decode_stochastic(self, probs, embedded_inputs, selections):\n        \"\"\"\n        Return the next input for the decoder by selecting the\n        input corresponding to the max output\n\n        Args:\n            probs: [batch_size x sourceL]\n            embedded_inputs: [sourceL x batch_size x embedding_dim]\n            selections: list of all of the previously selected indices during decoding\n       Returns:\n            Tensor of size [batch_size x sourceL] containing the embeddings\n            from the inputs corresponding to the [batch_size] indices\n            selected for this iteration of the decoding, as well as the\n            corresponding indicies\n        \"\"\"\n        batch_size = probs.size(0)\n        # idxs is [batch_size]\n        # idxs = probs.multinomial().squeeze(1)\n\n        idxs = probs.multinomial(1).squeeze(1)\n\n        # due to race conditions, might need to resample here\n        for old_idxs in selections:\n            # compare new idxs\n            # elementwise with the previous idxs. If any matches,\n            # then need to resample\n            if old_idxs.eq(idxs).data.any():\n                print(' [!] resampling due to race condition')\n                idxs = probs.multinomial(1).squeeze(1)\n                break\n\n        sels = embedded_inputs[idxs.data, [i for i in range(batch_size)], :]\n        return sels, idxs\n\n    def decode_greedy(self, probs, embedded_inputs, selections):\n        \"\"\"\n        Return the next input for the decoder by selecting the\n        input corresponding to the max output\n\n        Args:\n            probs: [batch_size x sourceL]\n            embedded_inputs: [sourceL x batch_size x embedding_dim]\n            selections: list of all of the previously selected indices during decoding\n       Returns:\n            Tensor of size [batch_size x sourceL] containing the embeddings\n            from the inputs corresponding to the [batch_size] indices\n            selected for this iteration of the decoding, as well as the\n            corresponding indicies\n        \"\"\"\n        batch_size = probs.size(0)\n        # idxs is [batch_size]\n        # idxs = probs.multinomial().squeeze(1)\n\n        idxs = torch.argmax(probs, 1)\n\n        # due to race conditions, might need to resample here\n        for old_idxs in selections:\n            # compare new idxs\n            # elementwise with the previous idxs. If any matches,\n            # then need to resample\n            if old_idxs.eq(idxs).data.any():\n                print(' [!] resampling due to race condition')\n                idxs = probs.multinomial(1).squeeze(1)\n                break\n\n        sels = embedded_inputs[idxs.data, [i for i in range(batch_size)], :]\n        return sels, idxs\n\n\nclass PointerNetwork(nn.Module):\n    \"\"\"The pointer network, which is the core seq2seq\n    model\"\"\"\n\n    def __init__(self,\n                 embedding_dim,\n                 hidden_dim,\n                 max_decoding_len,\n                 terminating_symbol,\n                 n_glimpses,\n                 tanh_exploration,\n                 use_tanh,\n                 use_cuda):\n        super(PointerNetwork, self).__init__()\n\n        self.encoder = Encoder(\n            embedding_dim,\n            hidden_dim,\n            use_cuda)\n\n        self.decoder = Decoder(\n            embedding_dim,\n            hidden_dim,\n            max_length=max_decoding_len,\n            tanh_exploration=tanh_exploration,\n            use_tanh=use_tanh,\n            terminating_symbol=terminating_symbol,\n            decode_type=\"stochastic\",\n            n_glimpses=n_glimpses,\n            use_cuda=use_cuda)\n\n        # Trainable initial hidden states\n        dec_in_0 = torch.FloatTensor(embedding_dim)\n        if use_cuda:\n            dec_in_0 = dec_in_0.cuda()\n\n        self.decoder_in_0 = nn.Parameter(dec_in_0)\n        self.decoder_in_0.data.uniform_(-(1. / math.sqrt(embedding_dim)),\n                                        1. / math.sqrt(embedding_dim))\n\n    def forward(self, inputs):\n        \"\"\" Propagate inputs through the network\n        Args:\n            inputs: [sourceL x batch_size x embedding_dim]\n        \"\"\"\n\n        (encoder_hx, encoder_cx) = self.encoder.enc_init_state\n        encoder_hx = encoder_hx.unsqueeze(0).repeat(inputs.size(1), 1).unsqueeze(0)\n        encoder_cx = encoder_cx.unsqueeze(0).repeat(inputs.size(1), 1).unsqueeze(0)\n\n        # encoder forward pass\n        enc_h, (enc_h_t, enc_c_t) = self.encoder(inputs, (encoder_hx, encoder_cx))\n\n        dec_init_state = (enc_h_t[-1], enc_c_t[-1])\n\n        # repeat decoder_in_0 across batch\n        decoder_input = self.decoder_in_0.unsqueeze(0).repeat(inputs.size(1), 1)\n\n        (pointer_probs, input_idxs), dec_hidden_t = self.decoder(decoder_input,\n                                                                 inputs,\n                                                                 dec_init_state,\n                                                                 enc_h)\n\n        return pointer_probs, input_idxs\n\n\nclass NeuralCombOptRL(nn.Module):\n    \"\"\"\n    This module contains the PointerNetwork (actor) and\n    CriticNetwork (critic). It requires\n    an application-specific reward function\n    \"\"\"\n\n    def __init__(self,\n                 input_dim,\n                 embedding_dim,\n                 hidden_dim,\n                 max_decoding_len,\n                 terminating_symbol,\n                 n_glimpses,\n                 n_process_block_iters,\n                 tanh_exploration,\n                 use_tanh,\n                 is_train,\n                 use_cuda):\n        super(NeuralCombOptRL, self).__init__()\n        self.input_dim = input_dim\n        self.is_train = is_train\n        self.use_cuda = use_cuda\n\n        self.actor_net = PointerNetwork(\n            embedding_dim,\n            hidden_dim,\n            max_decoding_len,\n            terminating_symbol,\n            n_glimpses,\n            tanh_exploration,\n            use_tanh,\n            use_cuda)\n\n        embedding_ = torch.FloatTensor(input_dim,\n                                       embedding_dim)\n        if self.use_cuda:\n            embedding_ = embedding_.cuda()\n        self.embedding = nn.Parameter(embedding_)\n        self.embedding.data.uniform_(-(1. / math.sqrt(embedding_dim)),\n                                     1. / math.sqrt(embedding_dim))\n\n    def forward(self, inputs):\n        \"\"\"\n        Args:\n            inputs: [batch_size, input_dim, sourceL]\n        \"\"\"\n        batch_size = inputs.size(0)\n        input_dim = inputs.size(1)\n        sourceL = inputs.size(2)\n\n        # repeat embeddings across batch_size\n        # result is [batch_size x input_dim x embedding_dim]\n        embedding = self.embedding.repeat(batch_size, 1, 1)\n        embedded_inputs = []\n        # result is [batch_size, 1, input_dim, sourceL]\n        ips = inputs.unsqueeze(1)\n\n        for i in range(sourceL):\n            # [batch_size x 1 x input_dim] * [batch_size x input_dim x embedding_dim]\n            # result is [batch_size, embedding_dim]\n            embedded_inputs.append(torch.bmm(\n                ips[:, :, :, i].float(),\n                embedding).squeeze(1))\n\n        # Result is [sourceL x batch_size x embedding_dim]\n        embedded_inputs = torch.cat(embedded_inputs).view(\n            sourceL,\n            batch_size,\n            embedding.size(2))\n\n        # query th\n\n# ==========================================\n# File: baseline_rl/train_baseline.py\n# Function/Context: \n# ==========================================\nimport numpy as np\nimport os\nimport torch\nimport torch.optim as optim\n\nfrom batch_env_rl import BatchEnvRL\nfrom neuralconet import NeuralCombOptRL\n\nuse_cuda = True\n\nmodel = NeuralCombOptRL(input_dim=3, embedding_dim=128, hidden_dim=128,\n                        max_decoding_len=10, terminating_symbol='<0>', n_glimpses=2,\n                        n_process_block_iters=3, tanh_exploration=10, use_tanh=True, is_train=True, use_cuda=use_cuda)\n\nactor_optim = optim.Adam(model.actor_net.parameters(), lr=1e-4)\n\ncritic_exp_mvg_avg = torch.zeros(1)\nbeta = 0.9\n\nif use_cuda:\n    model = model.cuda()\n    # critic_mse = critic_mse.cuda()\n    critic_exp_mvg_avg = critic_exp_mvg_avg.cuda()\n\nn_epochs = 100\nn_sims = 100\nn_sims_val = 10\nbatch_size = 32\nstep = 0\nlog_step = 1\nfor e in range(0, n_epochs):\n    model.train()\n    env = BatchEnvRL(n_envs=batch_size, n_nodes=10, adaptive=False)\n    # Use beam search decoding for validation\n    model.actor_net.decoder.decode_type = \"stochastic\"\n    for b in range(0, n_sims):\n        batch = env.get_features().copy()\n        # batch in (batch_size, n_nodes, dim)\n        batch = torch.from_numpy(batch)\n\n        if use_cuda:\n            batch = batch.cuda()\n        # for NCO we need (batch_size, dim, n_nodes)\n        batch = batch.transpose(-1, -2)\n\n        _, probs, actions, action_idxs = model(batch)\n\n        action_idxs = torch.stack(action_idxs).transpose(0, 1)\n        nodes = action_idxs + 1\n        ones = torch.ones(batch_size, 1)\n        if use_cuda:\n            ones = ones.cuda()\n\n        nodes = torch.cat([ones, nodes], dim=-1)\n        rwds, pens = env.check_solution(nodes)\n        R = rwds + pens\n        R = torch.from_numpy(R)\n        if use_cuda:\n            R = R.cuda()\n\n        if b == 0:\n            critic_exp_mvg_avg = R.mean()\n        else:\n            critic_exp_mvg_avg = (critic_exp_mvg_avg * beta) + ((1. - beta) * R.mean())\n\n        advantage = R - critic_exp_mvg_avg\n\n        logprobs = 0\n        nll = 0\n        for prob in probs:\n            # compute the sum of the log probs\n            # for each tour in the batch\n            logprob = torch.log(prob)\n            nll += -logprob\n            logprobs += logprob\n        # guard against nan\n        nll[(nll != nll).detach()] = 0.\n        # clamp any -inf's to 0 to throw away this tour\n        logprobs[(logprobs < -1000).detach()] = 0.\n        logprobs = logprobs.unsqueeze(-1)\n\n        # multiply each time step by the advanrate\n        reinforce = - advantage * logprobs\n        actor_loss = reinforce.mean()\n\n        actor_optim.zero_grad()\n        actor_loss.backward()\n        actor_optim.step()\n\n        critic_exp_mvg_avg = critic_exp_mvg_avg.detach()\n\n        env.reset()\n        step += 1\n\n        if step % log_step == 0:\n            print(\n                f'epoch: {e + 1}, batch: {b + 1}, avg reward: {R.mean().data:.2f}, critic: {critic_exp_mvg_avg.data:.2f}')\n\n            model_dir = os.path.join('model', 'run01')\n            if not os.path.exists(model_dir):\n                print(f'Creating a new model directory: {model_dir}')\n                os.makedirs(model_dir)\n\n            checkpoint = {\n                'actor': model.state_dict(),\n                'optimizer': actor_optim.state_dict(),\n                'epoch': e,\n                'avg_rwd': R.mean().data,\n                'step': step,\n            }\n            torch.save(checkpoint, os.path.join(model_dir, f'baseline-epoch{e}-step{step}.pt'))\n\n        # Use beam search decoding for validation\n    model.actor_net.decoder.decode_type = \"greedy\"\n\n    print('validation')\n    model.eval()\n    env_val = BatchEnvRL(n_envs=1, n_nodes=10, adaptive=False, seed=1234)\n    R_ = 0\n    for b_val in (0, n_sims_val):\n\n        batch = env.get_features().copy()\n        # batch in (batch_size, n_nodes, dim)\n        batch = torch.from_numpy(batch)\n\n        if use_cuda:\n            batch = batch.cuda()\n        # for NCO we need (batch_size, dim, n_nodes)\n        batch = batch.transpose(-1, -2)\n\n        _, probs, actions, action_idxs = model(batch)\n\n        action_idxs = torch.stack(action_idxs).transpose(0, 1)\n        nodes = action_idxs + 1\n        ones = torch.ones(batch_size, 1)\n        if use_cuda:\n            ones = ones.cuda()\n\n        nodes = torch.cat([ones, nodes], dim=-1)\n        rwds, pens = env.check_solution(nodes)\n        R = rwds + pens\n        R_ += R\n\n    print(f'validation avg rwds {np.mean(R_)}')\n\n# ==========================================\n# File: baseline_surrogate/demo_surrogate.py\n# Function/Context: objective, check_surrogate_solution, objBO, x_to_route\n# ==========================================\nimport math\nimport numpy as np\nimport time\n\nfrom env import Env\n\ntry:\n    from bayes_opt import BayesianOptimization\nexcept:\n    print(\n        'Please make sure the package bayesianoptimization is installed via conda-forge or pip. \\n(conda install -c conda-forge bayesian-optimization)')\n\n\ndef objective(x, env):\n    '''\n\n\n    Parameters\n    ----------\n    x : array\n        Vector of the form [1, x1, x2, ..., x_n].\n        The integers from 1 to n have to appear\n        in the part [x1,..., x_n], so the number\n        1 appears twice in total.\n    env : environment\n        Environment of the TSP-like problem.\n\n    Returns\n    -------\n    obj : float\n        Objective to be maximized.\n\n    '''\n\n    print('x', x)\n    obj_cost, rewards, pen, feas = env.check_solution(x)\n\n    MonteCarlo = 10000  # Number of Monte Carlo samples. Higher number means less noise.\n    obj = 0  # Objective averaged over Monte Carlo samples, to be maximized with surrogate optimization\n    for _ in range(MonteCarlo):\n        obj_cost, rewards, pen, feas = env.check_solution(x)\n        # print('Time: ', obj_cost)\n        # print('Rewards: ', rewards)\n        # print('Penalty: ', pen)\n        # print('Feasible: ', feas)\n        # print('Objective: ', rewards+pen)\n        obj = obj + (rewards + pen)  # Maximize the rewards + penalties (penalties are negative)\n    obj /= MonteCarlo\n\n    return obj\n\n\ndef check_surrogate_solution(x):\n    '''\n\n\n    Parameters\n    ----------\n    x : array\n        Vector of the form [1, x1, x2, ..., x_n].\n        The integers from 1 to n have to appear\n        in the part [x1,..., x_n], so the number\n        1 appears twice in total.\n\n    Returns\n    -------\n    obj : float\n        Corresponding objective.\n\n    '''\n    n_nodes = 65\n    env = Env(n_nodes, seed=6537855)  # Generate instance with n_nodes nodes\n    obj = objective(x, env)\n    print('Solution quality (higher is better): ', obj)\n    return obj\n\n\nif __name__ == '__main__':\n\n    ##Test phase\n    n_nodes = 65\n    env = Env(n_nodes, seed=6537855)  # Generate instance with n_nodes nodes\n\n    print('Generated instance (n=65). Compute the average over evaluating the same solution multiple times...')\n    # sol = [1, 4, 3, 2, 5, 1]\n    sol = np.arange(1, n_nodes + 1)\n    np.random.shuffle(sol)\n    sol = np.concatenate(([1], sol))  # Make sure solution starts at depot\n    print('Solution: ', sol)\n    MonteCarlo = 10000  # Number of Monte Carlo samples. Higher number means less noise.\n    time1 = time.time()\n    obj = 0  # Objective averaged over Monte Carlo samples, to be used for surrogate modelling\n    for _ in range(MonteCarlo):\n        obj_cost, rewards, pen, feas = env.check_solution(sol)\n        # print('Time: ', obj_cost)\n        # print('Rewards: ', rewards)\n        # print('Penalty: ', pen)\n        # print('Feasible: ', feas)\n        # print('Objective: ', rewards+pen)\n        obj = obj + (rewards + pen) / MonteCarlo\n    time_elapsed1 = time.time() - time1\n    print('Time elapsed: ', time_elapsed1)\n    print('Average objective: ', obj)\n\n    time2 = time.time()\n    obj = 0  # Objective averaged over Monte Carlo samples, to be used for surrogate modelling\n    for _ in range(MonteCarlo):\n        obj_cost, rewards, pen, feas = env.check_solution(sol)\n        # print('Time: ', obj_cost)\n        # print('Rewards: ', rewards)\n        # print('Penalty: ', pen)\n        # print('Feasible: ', feas)\n        # print('Objective: ', rewards+pen)\n        obj = obj + (rewards + pen) / MonteCarlo\n    time_elapsed2 = time.time() - time2\n    print('Time elapsed: ', time_elapsed2)\n    print('Average objective: ', obj)\n\n    time3 = time.time()\n    obj = 0  # Objective averaged over Monte Carlo samples, to be used for surrogate modelling\n    for _ in range(MonteCarlo):\n        obj_cost, rewards, pen, feas = env.check_solution(sol)\n        # print('Time: ', obj_cost)\n        # print('Rewards: ', rewards)\n        # print('Penalty: ', pen)\n        # print('Feasible: ', feas)\n        # print('Objective: ', rewards+pen)\n        obj = obj + (rewards + pen) / MonteCarlo\n    time_elapsed3 = time.time() - time3\n    print('Time elapsed: ', time_elapsed3)\n    print('Average objective: ', obj)\n\n    print('Evaluating the objective function takes about ', (time_elapsed1 + time_elapsed2 + time_elapsed3) / 3,\n          ' seconds on this machine.')\n\n\n    def x_to_route(x):\n        # After rounding x, transform it to a route.\n        nodes = np.arange(1, n_nodes + 1).tolist()\n        xnew = []\n        for xi in x:\n            i = int(xi)\n            xnew.append(nodes[i])\n            nodes.remove(nodes[i])\n        xnew.insert(0, 1)  # Start from starting depot\n        print(xnew)\n\n        return xnew\n\n\n    def objBO(**x):\n        vars = [f'v{i}' for i in range(n_nodes)]\n        xvars = [x[v] for v in vars]\n\n        # Bayesianoptimisation does not naturally support integer variables.\n        # As such we round them.\n        xrounded = np.floor(np.asarray(xvars))\n        xnew = x_to_route(xrounded)\n\n        r = objective(xnew, env)\n\n        # Bayesianoptimization maximizes by default.\n        # Include some random noise to avoid issues if all samples are the same.\n        eps = 1e-6\n        rnoise = r + np.random.standard_normal() * eps\n        return rnoise\n\n\n    varnames = {f'v{i}' for i in range(n_nodes)}\n    pbounds = {f'v{i}': (0.0, max(1e-4, n_nodes - i - 1e-4))\n               # keep upper bound above 0 to avoid numerical errors, but subtract a small number so the np.floor function does the right thing\n               for i in range(n_nodes)}\n\n    optimizer = BayesianOptimization(\n        f=objBO,\n        pbounds=pbounds,\n        verbose=2\n    )\n\n    random_init_evals = 10\n    max_evals = 100\n    optimizer.maximize(\n        init_points=random_init_evals,\n        n_iter=max_evals - random_init_evals)\n\n    print('Finished optimizing with surrogate model.')\n\n    solX = []\n    for i in range(n_nodes):\n        solX.append(optimizer.max['params'][f'v{i}'])\n\n    solY = optimizer.max['target']\n    print('Solution: ', solX)\n    print('Objective: ', solY)\n    route_solX = np.floor(np.asarray(solX))\n    print('Rounded solution: ', route_solX)\n    route_solX = x_to_route(route_solX)\n    print('Route: ', route_solX)\n\n    print('Do one run using this solution.')\n    obj_cost, rewards, pen, feas = env.check_solution(route_solX)\n    print('Time: ', obj_cost)\n    print('Rewards: ', rewards)\n    print('Penalty: ', pen)\n    print('Feasible: ', feas)\n    print('Objective: ', rewards + pen)\n\n    print('Average objective using the found solution:', objective(route_solX, env))\n\n# ==========================================\n# File: env.py\n# Function/Context: Env\n# ==========================================\nimport numpy as np\nimport random\nimport op_utils.instance as u_i\nimport op_utils.op as u_o\n\n\nclass Env:\n    maxT_pen = -1.0\n    tw_pen = -1.0\n\n    def __init__(self, n_nodes=50, seed=None, from_file=False, x_path=None, adj_path=None):\n\n        self.x = None\n        self.adj = None\n        self.seed = seed\n        np.random.seed(self.seed)\n        random.seed(self.seed)\n        self.sim_counter = 0\n        self.name = None\n        if from_file:\n            self.x, self.adj, self.instance_name = u_i.read_instance(x_path, adj_path)\n            self.n_nodes = len(self.x)\n        else:\n            assert n_nodes is not None, 'if no file is given, n_nodes is required'\n            self.n_nodes = n_nodes\n            self.instance_name = ''\n            self.x, self.adj = u_i.make_instance(self.n_nodes, seed=self.seed)\n\n    def get_features(self):\n        return self.x, self.adj\n\n    def check_solution(self, sol):\n\n        assert len(sol) == len(self.x) + 1, 'len(sol) = ' + str(len(sol)) + ', n_nodes+1 = ' + str(len(self.x) + 1)\n        assert len(sol) == len(set(sol)) + 1\n        self.sim_counter += 1\n        self.name = f'tour{self.sim_counter:03}'\n        tour_time, rewards, pen, feas = u_o.tour_check(sol, self.x, self.adj, self.maxT_pen,\n                                                       self.tw_pen, self.n_nodes)\n        return tour_time, rewards, pen, feas\n\n\nif __name__ == '__main__':\n    env = Env(n_nodes=5, seed=1235)\n    sol = [1, 2, 1, 4, 3, 5]\n    print(sol)\n    for _ in range(10):\n        print(env.check_solution(sol))\n\n# ==========================================\n# File: env_rl.py\n# Function/Context: EnvRL\n# ==========================================\nimport env\nimport numpy as np\nimport op_utils.op as u_o\n\n\nclass EnvRL(env.Env):\n\n    def __init__(self, n_nodes=None, seed=None, from_file=False, x_path=None, adj_path=None, verbose=False,\n                 adaptive=True):\n        super().__init__(n_nodes, seed, from_file, x_path, adj_path)\n        self.sim_counter = 0\n        self.verbose = verbose\n        self.adaptive = adaptive\n\n        self.current_node = None\n        self.mask = None\n        self.tour_time = None\n        self.time_t = None\n\n        self.feas = None\n        self.return_to_depot = None\n        self.rewards = None\n        self.pen = None\n        self.tw_high = None\n        self.tw_low = None\n        self.prizes = None\n        self.maxT = None\n        self.tour = None\n        self.violation_t = None\n        self.name = None\n\n        self.reset()\n\n    def get_seed(self):\n        return self.seed\n\n    def get_sim_name(self):\n        return self.name\n\n    def get_instance_name(self):\n        return self.instance_name\n\n    def visited(self, node):\n        return bool(self.mask[node - 1])\n\n    def check_solution(self, sol):\n        if self.adaptive:\n            pass\n        else:\n            # this is will generate a different randomness than 'step()'\n            return u_o.tour_check(sol, self.x, self.adj, self.maxT_pen,\n                                  self.tw_pen, self.n_nodes)\n\n    def get_remaining_time(self):\n        return self.maxT - self.tour_time\n\n    def get_collected_rewards(self):\n        return self.rewards\n\n    def get_incurred_penalties(self):\n        return self.pen\n\n    def get_feasibility(self):\n        return self.feas\n\n    def get_current_violation(self):\n        return self.violation_t\n\n    def get_current_node(self):\n        return self.current_node\n\n    def is_tour_done(self):\n        return self.return_to_depot\n\n    def get_current_node_features(self):\n        return self.x[self.current_node - 1]\n\n    def _get_rewards(self, node):\n\n        self.pen_t = 0\n        self.rwd_t = 0\n        self.violation_t = 0\n\n        # only compute stuff if you are not back to depot\n        if not self.return_to_depot:\n            # make sure a node is not visited twice\n            assert not self.visited(node), f'node: {node} already visited in the tour'\n            assert node != 0, 'node: 0 (zero) is not allowed.'\n\n            if self.tour_time > self.tw_high[node - 1]:\n                self.feas = False\n                # penalty added for each missed tw\n                self.pen += self.tw_pen\n                self.pen_t = self.tw_pen\n                self.violation_t = 1\n\n            elif self.tour_time < self.tw_low[node - 1]:\n                # time added for being too early\n                self.tour_time += self.tw_low[node - 1] - self.tour_time\n                self.rewards += self.prizes[node - 1]\n                self.rwd_t = self.prizes[node - 1]\n            else:\n                # within the time window - nothing to fix\n                self.rewards += self.prizes[node - 1]\n                self.rwd_t = self.prizes[node - 1]\n\n            if node == 1:\n                self.return_to_depot = True\n\n            if self.tour_time > self.maxT:\n                # penalty added for taking longer than maxT\n                self.pen += self.maxT_pen * self.n_nodes\n                self.pen_t += self.maxT_pen * self.n_nodes\n                self.feas = False\n                self.violation_t = 2\n\n            # add the next node to the tour\n            self.tour.append(node)\n            self.mask[node - 1] = 1\n\n    def step(self, node):\n        \n        if len(self.tour) >= self.n_nodes + 1:\n            return None\n        assert node <= self.n_nodes, f'node {node} does not exist for instance of size {self.n_nodes}'\n\n        previous_tour_time = self.tour_time\n        time = self.adj[self.current_node - 1, node - 1]\n        noise = np.random.randint(1, 101, size=1)[0] / 100\n        self.tour_time += np.round(noise * time, 2)\n        self._get_rewards(node)\n        self.time_t = self.tour_time - previous_tour_time\n        self.current_node = node\n\n        return self.tour_time, self.time_t, self.rwd_t, self.pen_t, self.feas, self.violation_t, self.return_to_depot\n\n    def reset(self):\n\n        self.current_node = 1\n        self.mask = [0] * self.n_nodes\n        self.tour_time = 0\n        self.time_t = 0\n        self.feas = True\n        self.return_to_depot = False\n        self.rewards = 0\n        self.pen = 0\n        self.tw_high = self.x[:, -3]\n        self.tw_low = self.x[:, -4]\n        self.prizes = self.x[:, -2]\n        self.maxT = self.x[0, -1]\n        self.tour = [self.current_node]\n        self.violation_t = 0  # 0: none, 1: tw, 2: maxT (takes precedence on tw)\n        self.sim_counter += 1\n        self.name = f'tour{self.sim_counter:03}'\n        if self.verbose:\n            print(f'[*] Starting a new simulation: {self.name}')\n\n# ==========================================\n# File: op_utils/op.py\n# Function/Context: tour_check\n# ==========================================\nimport math\nimport numpy as np\n\n\ndef dist_l2_closest_integer(x1, x2):\n    \"\"\"Compute the L2-norm (Euclidean) distance between two points.\n    The distance is rounded to the closest integer, for compatibility\n    with the TSPLIB convention.\n    The two points are located on coordinates (x1,y1) and (x2,y2),\n    sent as parameters\"\"\"\n    x_diff = x2[0] - x1[0]\n    y_diff = x2[1] - x1[1]\n    return int(math.sqrt(x_diff * x_diff + y_diff * y_diff) + .5)\n\n\ndef dist_l2(x1, x2, rd=5):\n    \"\"\"Compute the L2-norm (Euclidean) distance between two points.\n    The two points are located on coordinates (x1,y1) and (x2,y2),\n    sent as parameters\"\"\"\n    x_diff = x2[0] - x1[0]\n    y_diff = x2[1] - x1[1]\n    return round(math.sqrt(x_diff * x_diff + y_diff * y_diff), rd)\n\n\ndef make_dist_matrix(points, dist=dist_l2_closest_integer, to_integer=True, rd=4):\n    \"\"\"Compute a distance matrix for a set of points.\n    Uses function 'dist' to calculate distance between\n    any two points.\n    \"\"\"\n    n = len(points)\n    dist_matrix = np.zeros((n, n))\n    for i in range(n - 1):\n        for j in range(i + 1, n):\n            x1 = points[i]\n            x2 = points[j]\n            x1 = x1 * 10 ** rd\n            x2 = x2 * 10 ** rd\n            dist_matrix[i, j] = dist(x1, x2)\n            dist_matrix[j, i] = dist_matrix[i, j]\n    if to_integer:\n        dist_matrix = dist_matrix.astype(int)\n    else:\n        dist_matrix = dist_matrix / 10 ** rd\n\n    return dist_matrix\n\n\ndef tsp_tour_cost(tour, cost_matrix):\n    cost = 0\n\n    for i in range(len(tour) - 1):\n        node = int(tour[i])\n        succ = int(tour[i + 1])\n        cost += cost_matrix[node][succ]\n\n    return cost\n\n\ndef tour_check(tour, x, time_matrix, maxT_pen, tw_pen, n_nodes):\n    \"\"\"\n    Calculate a tour times and the penalties for constraint violation\n    \"\"\"\n    tw_high = x[:, -3]\n    tw_low = x[:, -4]\n    prizes = x[:, -2]\n    maxT = x[0, -1]\n\n    feas = True\n    return_to_depot = False\n    tour_time = 0\n    rewards = 0\n    pen = 0\n\n    for i in range(len(tour) - 1):\n\n        node = int(tour[i])\n        if i == 0:\n            assert node == 1, 'A tour must start from the depot - node: 1'\n\n        succ = int(tour[i + 1])\n        time = time_matrix[node - 1][succ - 1]\n        noise = np.random.randint(1, 101, size=1)[0]/100\n        tour_time += np.round(noise * time, 2)\n        if tour_time > tw_high[succ - 1]:\n            feas = False\n            # penalty added for each missed tw\n            pen += tw_pen\n        elif tour_time < tw_low[succ - 1]:\n            tour_time += tw_low[succ - 1] - tour_time\n            rewards += prizes[succ - 1]\n        else:\n            rewards += prizes[succ - 1]\n\n        if succ == 1:\n            return_to_depot = True\n            break\n\n    if not return_to_depot:\n        raise Exception('A tour must reconnect back to the depot - node: 1')\n\n    if tour_time > maxT:\n        # penalty added for each\n        pen += maxT_pen * n_nodes\n        feas = False\n\n    return tour_time, rewards, pen, feas",
  "description": "Combined Analysis:\n- [baseline_rl/neuralconet.py]: This file implements the core neural network architecture for the reinforcement learning track (DRL) of the AI4TSP competition. It provides a Neural Combinatorial Optimization model using a Pointer Network (actor) with attention mechanisms, which is designed to generate sequences (tours) for combinatorial optimization problems like TSP variants. The model encodes node features, uses an LSTM-based decoder with attention to select nodes sequentially, and supports both stochastic and greedy decoding. While it doesn't directly handle stochastic travel times, time windows, or penalties, it serves as the policy network that can be trained via RL to maximize expected reward in the TD-OPSWTW environment. The code matches the algorithm step 'neural combinatorial optimization with pointer networks' from the paper.\n- [baseline_rl/train_baseline.py]: This file implements the core reinforcement learning algorithm for the TD-OPSWTW problem described in the paper. It uses a Neural Combinatorial Optimization (NCO) model with pointer networks (NeuralCombOptRL) to construct tours. The training follows the REINFORCE algorithm with baseline (moving average critic) for policy gradient optimization. Key aspects implemented: 1) Tour construction via sequential decoding (max_decoding_len=10 for 10-node instances), 2) Reward computation via env.check_solution() that evaluates prizes and penalties, 3) Stochastic travel times handled by the BatchEnvRL environment, 4) Depot handling by prepending node 1 to tours, 5) Training objective maximizes expected reward via policy gradient. The code matches the DRL approach described in the paper's Track 2, specifically neural combinatorial optimization with REINFORCE.\n- [baseline_surrogate/demo_surrogate.py]: This file implements a Bayesian Optimization (BO) surrogate model for the TD-OPSWTW problem. The core logic matches the paper's Track 1 (surrogate-based optimization) approach: 1) The objective function computes the expected reward via Monte Carlo averaging (10000 samples) over stochastic travel times, aligning with the mathematical objective max_s E[f(s,I)]. 2) The solution representation is a permutation of nodes starting and ending at depot (node 1), satisfying the route constraints. 3) The BO framework (via bayesian-optimization library) optimizes continuous variables that are rounded to integers and mapped to routes via x_to_route, handling the combinatorial space. 4) The env.check_solution evaluates each route, computing rewards, penalties, and feasibility under stochastic travel times and time windows. However, the code assumes visiting all nodes (full permutation), while the problem allows subset selection; this is a limitation but the core BO and Monte Carlo evaluation structure directly implements the surrogate optimization algorithm described.\n- [env.py]: This file implements the core environment for evaluating solutions to the TD-OPSWTW problem. The Env class initializes problem instances (either from file or randomly generated) and provides a check_solution method that validates and scores candidate tours. The method enforces the constraint that the tour must start and end at the depot (node 1) and be a valid permutation. It delegates the detailed constraint checking and objective calculation (including stochastic travel times, time windows, penalties, and feasibility) to the external function u_o.tour_check. The class also manages instance features (node coordinates and adjacency matrix) and simulation state. This aligns with the paper's need for an evaluation environment that computes the expected reward f(s, I) under stochastic travel times, though the stochastic sampling is handled inside the helper function.\n- [env_rl.py]: This file implements the core environment for the TD-OPSWTW problem as described in the paper. The EnvRL class provides a reinforcement learning environment that models the stochastic routing problem with time windows and prizes. Key implementations include: 1) Stochastic travel times using uniform noise (η/100) applied to base distances (adjacency matrix), 2) Time window constraints with waiting for early arrivals and penalties for late arrivals, 3) Prize collection only within time windows, 4) Global time budget constraint with penalty, 5) Tour construction starting and ending at depot (node 1). The step() method implements the transition logic including stochastic travel time calculation and reward/penalty computation, while reset() initializes a new episode. This directly corresponds to the mathematical model's objective of maximizing expected rewards minus penalties under stochastic travel times and time window constraints.\n- [op_utils/op.py]: The file implements core evaluation logic for the TD-OPSWTW problem. Specifically, the 'tour_check' function evaluates a candidate tour against the stochastic constraints and objective described in the paper. It validates the tour structure (starts/ends at depot), computes stochastic travel times using uniform noise (η ~ U{1,100}/100), enforces time windows (waiting if early, penalty if late), applies a global time limit penalty, and calculates collected prizes. This directly corresponds to the optimization model's objective and constraints, including stochastic travel time generation and penalty calculations. The distance functions support Euclidean distance computations for constructing time matrices, which underlie the stochastic travel times.",
  "dependencies": [
    "dist_l2",
    "Decoder",
    "Encoder",
    "torch.autograd.Variable",
    "torch.optim",
    "tsp_tour_cost",
    "time",
    "op_utils.instance.read_instance",
    "BayesianOptimization (from bayes_opt)",
    "op_utils.op.tour_check",
    "op_utils.op",
    "Attention",
    "neuralconet.NeuralCombOptRL",
    "math",
    "batch_env_rl.BatchEnvRL",
    "os",
    "op_utils.instance.make_instance",
    "make_dist_matrix",
    "PointerNetwork",
    "env",
    "dist_l2_closest_integer",
    "random",
    "numpy",
    "Env (from env)",
    "torch.nn",
    "torch"
  ]
}