{
  "paper_id": "Combinatorial_Optimization_with_Policy_Adaptation_using_Late",
  "title": "Combinatorial Optimization with Policy Adaptation using Latent Space Search",
  "abstract": "Combinatorial Optimization underpins many real-world applications and yet, designing performant algorithms to solve these complex, typically NP-hard, problems remains a significant research challenge. Reinforcement Learning (RL) provides a versatile framework for designing heuristics across a broad spectrum of problem domains. However, despite notable progress, RL has not yet supplanted industrial solvers as the go-to solution. Current approaches emphasize pre-training heuristics that construct solutions but often rely on search procedures with limited variance, such as stochastically sampling numerous solutions from a single policy or employing computationally expensive fine-tuning of the policy on individual problem instances. Building on the intuition that performant search at inference time should be anticipated during pre-training, we propose COMPASS, a novel RL approach that parameterizes a distribution of diverse and specialized policies conditioned on a continuous latent space. We evaluate COMPASS across three canonical problems - Travelling Salesman, Capacitated Vehicle Routing, and Job-Shop Scheduling - and demonstrate that our search strategy (i) outperforms state-of-the-art approaches in 9 out of 11 standard benchmarking tasks and (ii) generalizes better, surpassing all other approaches on a set of 18 procedurally transformed instance distributions.",
  "problem_description_natural": "The paper addresses the challenge of solving combinatorial optimization (CO) problems—specifically the Travelling Salesman Problem (TSP), Capacitated Vehicle Routing Problem (CVRP), and Job-Shop Scheduling Problem (JSSP)—which involve finding an optimal ordering, labeling, or subset of discrete elements to minimize or maximize an objective function under constraints. These problems are typically NP-hard, making exact solutions infeasible for large instances, so the focus is on learning effective heuristics using reinforcement learning that can adapt efficiently at inference time without retraining.",
  "problem_type": "combinatorial optimization",
  "datasets": [
    "TSP100",
    "TSP125",
    "TSP150",
    "TSP200",
    "CVRP100",
    "CVRP125",
    "CVRP150",
    "CVRP200",
    "JSSP 10x10",
    "JSSP 15x15",
    "JSSP 20x15",
    "Procedurally mutated TSP instances (Bossek et al., 2019)",
    "Procedurally mutated CVRP instances (Bossek et al., 2019)"
  ],
  "performance_metrics": [
    "Objective Value (Obj.)",
    "Optimality Gap (%)",
    "Runtime (Time)"
  ],
  "lp_model": {
    "objective": "\\max_{\\theta} J_{\\text{compass}}, \\text{ where } J_{\\text{compass}} = \\mathbb{E}_{\\rho \\sim \\mathcal{D}} \\mathbb{E}_{z_1, \\dots, z_N \\sim \\mathcal{P}_z} [R(\\tau_{i^*})] \\text{ with } i^* = \\arg\\max_{i} R(\\tau_i) \\text{ and } \\tau_i \\sim \\pi_\\theta(\\cdot | z_i)",
    "constraints": [
      "s_{t+1} = T(s_t, a_t) \\text{ for } t = 0, \\dots, H-1, \\text{ where } T \\text{ is the transition function}",
      "z_i \\sim \\mathcal{P}_z \\text{ for } i = 1, \\dots, N, \\text{ with } \\mathcal{P}_z \\text{ being the prior distribution over the latent space}"
    ],
    "variables": [
      "\\theta \\text{ - policy parameters}",
      "z_i \\text{ - latent vectors sampled from the latent space}",
      "s_t \\text{ - states at time step } t",
      "a_t \\text{ - actions at time step } t"
    ]
  },
  "raw_latex_model": "The Markov Decision Process is defined as $M = (S, A, R, T, \\gamma, H)$. The objective is $\\pi^* = \\argmax_\\pi \\mathbb{E}[\\sum_{t=0}^{H} \\gamma^t R(s_t, a_t)]$. For COMPASS, the gradient of the objective is given by $\\nabla_\\theta J_{\\text{compass}} = \\mathbb{E}_{\\rho \\sim \\mathcal{D}} \\mathbb{E}_{z_1, \\dots, z_N \\sim \\mathcal{P}_z} \\mathbb{E}_{\\tau_i \\sim \\pi_\\theta(\\cdot | z_i)} [\\nabla_\\theta \\log \\pi_\\theta(\\tau_i | z_{i^*}) R_{i^*} - \\mathcal{B}_{\\rho,\\theta}]$, where $i^* = \\arg\\max_{i \\in [1,N]} R(\\tau_i)$.",
  "algorithm_description": "COMPASS consists of two main phases: training and inference. Training Phase: 1. Sample N latent vectors uniformly from the latent space. 2. Condition the policy on each latent vector to obtain N conditioned policies. 3. Evaluate each policy on the current problem instance to generate trajectories and compute rewards. 4. Identify the best-performing latent vector based on the highest reward. 5. Update the policy parameters using the gradient from the trajectory generated by the best latent vector, as per Equation (1). Inference Phase: 1. For a given problem instance, use Covariance Matrix Adaptation Evolution Strategy (CMA-ES) to search the latent space. 2. Initialize multiple CMA-ES components with means set to centroids from a Voronoi Tessellation of the latent space to ensure diverse exploration. 3. Iteratively sample latent vectors from the CMA-ES distributions, condition the policy on them, and evaluate the resulting policies. 4. Update the CMA-ES distributions (mean and covariance) based on the performance of sampled vectors to focus on high-performing regions. 5. Continue until the search budget is exhausted, and select the best solution found."
}