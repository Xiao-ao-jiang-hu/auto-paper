{
  "file_path": "difusco/pl_meta_model.py, difusco/pl_mis_model.py, difusco/pl_tsp_model.py, difusco/utils/mis_utils.py, difusco/utils/tsp_utils.py, tsp_mcts/code/TSP_2Opt.h, tsp_mcts/code/TSP_MCTS.h",
  "function_name": "COMetaModel, MISModel.test_step, TSPModel, mis_decode_np, merge_tours, MCTS",
  "code_snippet": "\n\n# ==========================================\n# File: difusco/pl_meta_model.py\n# Function/Context: COMetaModel\n# ==========================================\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.data\nfrom torch_geometric.data import DataLoader as GraphDataLoader\nfrom pytorch_lightning.utilities import rank_zero_info\n\nfrom models.gnn_encoder import GNNEncoder\nfrom utils.lr_schedulers import get_schedule_fn\nfrom utils.diffusion_schedulers import CategoricalDiffusion, GaussianDiffusion\n\n\nclass COMetaModel(pl.LightningModule):\n  def __init__(self,\n               param_args,\n               node_feature_only=False):\n    super(COMetaModel, self).__init__()\n    self.args = param_args\n    self.diffusion_type = self.args.diffusion_type\n    self.diffusion_schedule = self.args.diffusion_schedule\n    self.diffusion_steps = self.args.diffusion_steps\n    self.sparse = self.args.sparse_factor > 0 or node_feature_only\n\n    if self.diffusion_type == 'gaussian':\n      out_channels = 1\n      self.diffusion = GaussianDiffusion(\n          T=self.diffusion_steps, schedule=self.diffusion_schedule)\n    elif self.diffusion_type == 'categorical':\n      out_channels = 2\n      self.diffusion = CategoricalDiffusion(\n          T=self.diffusion_steps, schedule=self.diffusion_schedule)\n    else:\n      raise ValueError(f\"Unknown diffusion type {self.diffusion_type}\")\n\n    self.model = GNNEncoder(\n        n_layers=self.args.n_layers,\n        hidden_dim=self.args.hidden_dim,\n        out_channels=out_channels,\n        aggregation=self.args.aggregation,\n        sparse=self.sparse,\n        use_activation_checkpoint=self.args.use_activation_checkpoint,\n        node_feature_only=node_feature_only,\n    )\n    self.num_training_steps_cached = None\n\n  def categorical_posterior(self, target_t, t, x0_pred_prob, xt):\n    \"\"\"Sample from the categorical posterior for a given time step.\n       See https://arxiv.org/pdf/2107.03006.pdf for details.\n    \"\"\"\n    diffusion = self.diffusion\n\n    if target_t is None:\n      target_t = t - 1\n    else:\n      target_t = torch.from_numpy(target_t).view(1)\n\n    # Thanks to Daniyar and Shengyu, who found the \"target_t == 0\" branch is not needed :)\n    # if target_t > 0:\n    Q_t = np.linalg.inv(diffusion.Q_bar[target_t]) @ diffusion.Q_bar[t]\n    Q_t = torch.from_numpy(Q_t).float().to(x0_pred_prob.device)\n    # else:\n    #   Q_t = torch.eye(2).float().to(x0_pred_prob.device)\n    Q_bar_t_source = torch.from_numpy(diffusion.Q_bar[t]).float().to(x0_pred_prob.device)\n    Q_bar_t_target = torch.from_numpy(diffusion.Q_bar[target_t]).float().to(x0_pred_prob.device)\n\n    xt = F.one_hot(xt.long(), num_classes=2).float()\n    xt = xt.reshape(x0_pred_prob.shape)\n\n    x_t_target_prob_part_1 = torch.matmul(xt, Q_t.permute((1, 0)).contiguous())\n    x_t_target_prob_part_2 = Q_bar_t_target[0]\n    x_t_target_prob_part_3 = (Q_bar_t_source[0] * xt).sum(dim=-1, keepdim=True)\n\n    x_t_target_prob = (x_t_target_prob_part_1 * x_t_target_prob_part_2) / x_t_target_prob_part_3\n\n    sum_x_t_target_prob = x_t_target_prob[..., 1] * x0_pred_prob[..., 0]\n    x_t_target_prob_part_2_new = Q_bar_t_target[1]\n    x_t_target_prob_part_3_new = (Q_bar_t_source[1] * xt).sum(dim=-1, keepdim=True)\n\n    x_t_source_prob_new = (x_t_target_prob_part_1 * x_t_target_prob_part_2_new) / x_t_target_prob_part_3_new\n\n    sum_x_t_target_prob += x_t_source_prob_new[..., 1] * x0_pred_prob[..., 1]\n\n    if target_t > 0:\n      xt = torch.bernoulli(sum_x_t_target_prob.clamp(0, 1))\n    else:\n      xt = sum_x_t_target_prob.clamp(min=0)\n\n    if self.sparse:\n      xt = xt.reshape(-1)\n    return xt\n\n  def gaussian_posterior(self, target_t, t, pred, xt):\n    \"\"\"Sample (or deterministically denoise) from the Gaussian posterior for a given time step.\n       See https://arxiv.org/pdf/2010.02502.pdf for details.\n    \"\"\"\n    diffusion = self.diffusion\n    if target_t is None:\n      target_t = t - 1\n    else:\n      target_t = torch.from_numpy(target_t).view(1)\n\n    atbar = diffusion.alphabar[t]\n    atbar_target = diffusion.alphabar[target_t]\n\n    if self.args.inference_trick is None or t <= 1:\n      # Use DDPM posterior\n      at = diffusion.alpha[t]\n      z = torch.randn_like(xt)\n      atbar_prev = diffusion.alphabar[t - 1]\n      beta_tilde = diffusion.beta[t - 1] * (1 - atbar_prev) / (1 - atbar)\n\n      xt_target = (1 / np.sqrt(at)).item() * (xt - ((1 - at) / np.sqrt(1 - atbar)).item() * pred)\n      xt_target = xt_target + np.sqrt(beta_tilde).item() * z\n    elif self.args.inference_trick == 'ddim':\n      xt_target = np.sqrt(atbar_target / atbar).item() * (xt - np.sqrt(1 - atbar).item() * pred)\n      xt_target = xt_target + np.sqrt(1 - atbar_target).item() * pred\n    else:\n      raise ValueError('Unknown inference trick {}'.format(self.args.inference_trick))\n    return xt_target\n\n# ==========================================\n# File: difusco/pl_mis_model.py\n# Function/Context: MISModel.test_step\n# ==========================================\nimport os\nimport numpy as np\nimport scipy.sparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data\n\nfrom co_datasets.mis_dataset import MISDataset\nfrom utils.diffusion_schedulers import InferenceSchedule\nfrom pl_meta_model import COMetaModel\nfrom utils.mis_utils import mis_decode_np\n\nclass MISModel(COMetaModel):\n  def __init__(self,\n               param_args=None):\n    super(MISModel, self).__init__(param_args=param_args, node_feature_only=True)\n\n    data_label_dir = None\n    if self.args.training_split_label_dir is not None:\n      data_label_dir = os.path.join(self.args.storage_path, self.args.training_split_label_dir)\n\n    self.train_dataset = MISDataset(\n        data_file=os.path.join(self.args.storage_path, self.args.training_split),\n        data_label_dir=data_label_dir,\n    )\n\n    self.test_dataset = MISDataset(\n        data_file=os.path.join(self.args.storage_path, self.args.test_split),\n    )\n\n    self.validation_dataset = MISDataset(\n        data_file=os.path.join(self.args.storage_path, self.args.validation_split),\n    )\n\n  def test_step(self, batch, batch_idx, draw=False, split='test'):\n    device = batch[-1].device\n\n    real_batch_idx, graph_data, point_indicator = batch\n    node_labels = graph_data.x\n    edge_index = graph_data.edge_index\n\n    stacked_predict_labels = []\n    edge_index = edge_index.to(node_labels.device).reshape(2, -1)\n    edge_index_np = edge_index.cpu().numpy()\n    adj_mat = scipy.sparse.coo_matrix(\n        (np.ones_like(edge_index_np[0]), (edge_index_np[0], edge_index_np[1])),\n    )\n\n    for _ in range(self.args.sequential_sampling):\n      xt = torch.randn_like(node_labels.float())\n      if self.args.parallel_sampling > 1:\n        xt = xt.repeat(self.args.parallel_sampling, 1, 1)\n        xt = torch.randn_like(xt)\n\n      if self.diffusion_type == 'gaussian':\n        xt.requires_grad = True\n      else:\n        xt = (xt > 0).long()\n      xt = xt.reshape(-1)\n\n      if self.args.parallel_sampling > 1:\n        edge_index = self.duplicate_edge_index(edge_index, node_labels.shape[0], device)\n\n      batch_size = 1\n      steps = self.args.inference_diffusion_steps\n      time_schedule = InferenceSchedule(inference_schedule=self.args.inference_schedule,\n                                        T=self.diffusion.T, inference_T=steps)\n\n      for i in range(steps):\n        t1, t2 = time_schedule(i)\n        t1 = np.array([t1 for _ in range(batch_size)]).astype(int)\n        t2 = np.array([t2 for _ in range(batch_size)]).astype(int)\n\n        if self.diffusion_type == 'gaussian':\n          xt = self.gaussian_denoise_step(\n              xt, t1, device, edge_index, target_t=t2)\n        else:\n          xt = self.categorical_denoise_step(\n              xt, t1, device, edge_index, target_t=t2)\n\n      if self.diffusion_type == 'gaussian':\n        predict_labels = xt.float().cpu().detach().numpy() * 0.5 + 0.5\n      else:\n        predict_labels = xt.float().cpu().detach().numpy() + 1e-6\n      stacked_predict_labels.append(predict_labels)\n\n    predict_labels = np.concatenate(stacked_predict_labels, axis=0)\n    all_sampling = self.args.sequential_sampling * self.args.parallel_sampling\n\n    splitted_predict_labels = np.split(predict_labels, all_sampling)\n    solved_solutions = [mis_decode_np(predict_labels, adj_mat) for predict_labels in splitted_predict_labels]\n    solved_costs = [solved_solution.sum() for solved_solution in solved_solutions]\n    best_solved_cost = np.max(solved_costs)\n\n    gt_cost = node_labels.cpu().numpy().sum()\n    metrics = {\n        f\"{split}/gt_cost\": gt_cost,\n    }\n    for k, v in metrics.items():\n      self.log(k, v, on_epoch=True, sync_dist=True)\n    self.log(f\"{split}/solved_cost\", best_solved_cost, prog_bar=True, on_epoch=True, sync_dist=True)\n    return metrics\n\n  def categorical_denoise_step(self, xt, t, device, edge_index=None, target_t=None):\n    with torch.no_grad():\n      t = torch.from_numpy(t).view(1)\n      x0_pred = self.forward(\n          xt.float().to(device),\n          t.float().to(device),\n          edge_index.long().to(device) if edge_index is not None else None,\n      )\n      x0_pred_prob = x0_pred.reshape((1, xt.shape[0], -1, 2)).softmax(dim=-1)\n      xt = self.categorical_posterior(target_t, t, x0_pred_prob, xt)\n      return xt\n\n  def gaussian_denoise_step(self, xt, t, device, edge_index=None, target_t=None):\n    with torch.no_grad():\n      t = torch.from_numpy(t).view(1)\n      pred = self.forward(\n          xt.float().to(device),\n          t.float().to(device),\n          edge_index.long().to(device) if edge_index is not None else None,\n      )\n      pred = pred.squeeze(1)\n      xt = self.gaussian_posterior(target_t, t, pred, xt)\n      return xt\n\n# ==========================================\n# File: difusco/pl_tsp_model.py\n# Function/Context: TSPModel\n# ==========================================\nimport os\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data\nfrom pytorch_lightning.utilities import rank_zero_info\n\nfrom co_datasets.tsp_graph_dataset import TSPGraphDataset\nfrom pl_meta_model import COMetaModel\nfrom utils.diffusion_schedulers import InferenceSchedule\nfrom utils.tsp_utils import TSPEvaluator, batched_two_opt_torch, merge_tours\n\n\nclass TSPModel(COMetaModel):\n  def __init__(self,\n               param_args=None):\n    super(TSPModel, self).__init__(param_args=param_args, node_feature_only=False)\n\n    self.train_dataset = TSPGraphDataset(\n        data_file=os.path.join(self.args.storage_path, self.args.training_split),\n        sparse_factor=self.args.sparse_factor,\n    )\n\n    self.test_dataset = TSPGraphDataset(\n        data_file=os.path.join(self.args.storage_path, self.args.test_split),\n        sparse_factor=self.args.sparse_factor,\n    )\n\n    self.validation_dataset = TSPGraphDataset(\n        data_file=os.path.join(self.args.storage_path, self.args.validation_split),\n        sparse_factor=self.args.sparse_factor,\n    )\n\n  def forward(self, x, adj, t, edge_index):\n    return self.model(x, t, adj, edge_index)\n\n  def categorical_training_step(self, batch, batch_idx):\n    edge_index = None\n    if not self.sparse:\n      _, points, adj_matrix, _ = batch\n      t = np.random.randint(1, self.diffusion.T + 1, points.shape[0]).astype(int)\n    else:\n      _, graph_data, point_indicator, edge_indicator, _ = batch\n      t = np.random.randint(1, self.diffusion.T + 1, point_indicator.shape[0]).astype(int)\n      route_edge_flags = graph_data.edge_attr\n      points = graph_data.x\n      edge_index = graph_data.edge_index\n      num_edges = edge_index.shape[1]\n      batch_size = point_indicator.shape[0]\n      adj_matrix = route_edge_flags.reshape((batch_size, num_edges // batch_size))\n\n    # Sample from diffusion\n    adj_matrix_onehot = F.one_hot(adj_matrix.long(), num_classes=2).float()\n    if self.sparse:\n      adj_matrix_onehot = adj_matrix_onehot.unsqueeze(1)\n\n    xt = self.diffusion.sample(adj_matrix_onehot, t)\n    xt = xt * 2 - 1\n    xt = xt * (1.0 + 0.05 * torch.rand_like(xt))\n\n    if self.sparse:\n      t = torch.from_numpy(t).float()\n      t = t.reshape(-1, 1).repeat(1, adj_matrix.shape[1]).reshape(-1)\n      xt = xt.reshape(-1)\n      adj_matrix = adj_matrix.reshape(-1)\n      points = points.reshape(-1, 2)\n      edge_index = edge_index.float().to(adj_matrix.device).reshape(2, -1)\n    else:\n      t = torch.from_numpy(t).float().view(adj_matrix.shape[0])\n\n    # Denoise\n    x0_pred = self.forward(\n        points.float().to(adj_matrix.device),\n        xt.float().to(adj_matrix.device),\n        t.float().to(adj_matrix.device),\n        edge_index,\n    )\n\n    # Compute loss\n    loss_func = nn.CrossEntropyLoss()\n    loss = loss_func(x0_pred, adj_matrix.long())\n    self.log(\"train/loss\", loss)\n    return loss\n\n  def gaussian_training_step(self, batch, batch_idx):\n    if self.sparse:\n      # TODO: Implement Gaussian diffusion with sparse graphs\n      raise ValueError(\"DIFUSCO with sparse graphs are not supported for Gaussian diffusion\")\n    _, points, adj_matrix, _ = batch\n\n    adj_matrix = adj_matrix * 2 - 1\n    adj_matrix = adj_matrix * (1.0 + 0.05 * torch.rand_like(adj_matrix))\n    # Sample from diffusion\n    t = np.random.randint(1, self.diffusion.T + 1, adj_matrix.shape[0]).astype(int)\n    xt, epsilon = self.diffusion.sample(adj_matrix, t)\n\n    t = torch.from_numpy(t).float().view(adj_matrix.shape[0])\n    # Denoise\n    epsilon_pred = self.forward(\n        points.float().to(adj_matrix.device),\n        xt.float().to(adj_matrix.device),\n        t.float().to(adj_matrix.device),\n        None,\n    )\n    epsilon_pred = epsilon_pred.squeeze(1)\n\n    # Compute loss\n    loss = F.mse_loss(epsilon_pred, epsilon.float())\n    self.log(\"train/loss\", loss)\n    return loss\n\n  def training_step(self, batch, batch_idx):\n    if self.diffusion_type == 'gaussian':\n      return self.gaussian_training_step(batch, batch_idx)\n    elif self.diffusion_type == 'categorical':\n      return self.categorical_training_step(batch, batch_idx)\n\n  def categorical_denoise_step(self, points, xt, t, device, edge_index=None, target_t=None):\n    with torch.no_grad():\n      t = torch.from_numpy(t).view(1)\n      x0_pred = self.forward(\n          points.float().to(device),\n          xt.float().to(device),\n          t.float().to(device),\n          edge_index.long().to(device) if edge_index is not None else None,\n      )\n\n      if not self.sparse:\n        x0_pred_prob = x0_pred.permute((0, 2, 3, 1)).contiguous().softmax(dim=-1)\n      else:\n        x0_pred_prob = x0_pred.reshape((1, points.shape[0], -1, 2)).softmax(dim=-1)\n\n      xt = self.categorical_posterior(target_t, t, x0_pred_prob, xt)\n      return xt\n\n  def gaussian_denoise_step(self, points, xt, t, device, edge_index=None, target_t=None):\n    with torch.no_grad():\n      t = torch.from_numpy(t).view(1)\n      pred = self.forward(\n          points.float().to(device),\n          xt.float().to(device),\n          t.float().to(device),\n          edge_index.long().to(device) if edge_index is not None else None,\n      )\n      pred = pred.squeeze(1)\n      xt = self.gaussian_posterior(target_t, t, pred, xt)\n      return xt\n\n  def test_step(self, batch, batch_idx, split='test'):\n    edge_index = None\n    np_edge_index = None\n    device = batch[-1].device\n    if not self.sparse:\n      real_batch_idx, points, adj_matrix, gt_tour = batch\n      np_points = points.cpu().numpy()[0]\n      np_gt_tour = gt_tour.cpu().numpy()[0]\n    else:\n      real_batch_idx, graph_data, point_indicator, edge_indicator, gt_tour = batch\n      route_edge_flags = graph_data.edge_attr\n      points = graph_data.x\n      edge_index = graph_data.edge_index\n      num_edges = edge_index.shape[1]\n      batch_size = point_indicator.shape[0]\n      adj_matrix = route_edge_flags.reshape((batch_size, num_edges // batch_size))\n      points = points.reshape((-1, 2))\n      edge_index = edge_index.reshape((2, -1))\n      np_points = points.cpu().numpy()\n      np_gt_tour = gt_tour.cpu().numpy().reshape(-1)\n      np_edge_index = edge_index.cpu().numpy()\n\n    stacked_tours = []\n    ns, merge_iterations = 0, 0\n\n    if self.args.parallel_sampling > 1:\n      if not self.sparse:\n        points = points.repeat(self.args.parallel_sampling, 1, 1)\n      else:\n        points = points.repeat(self.args.parallel_sampling, 1)\n        edge_index = self.duplicate_edge_index(edge_index, np_points.shape[0], device)\n\n    for _ in range(self.args.sequential_sampling):\n      xt = torch.randn_like(adj_matrix.float())\n      if self.args.parallel_sampling > 1:\n        if not self.sparse:\n          xt = xt.repeat(self.args.parallel_sampling, 1, 1)\n        else:\n          xt = xt.repeat(self.args.parallel_sampling, 1)\n        xt = torch.randn_like(xt)\n\n      if self.diffusion_type == 'gaussian':\n        xt.requires_grad = True\n      else:\n        xt = (xt > 0).long()\n\n      if self.sparse:\n        xt = xt.reshape(-1)\n\n      steps = self.args.inference_diffusion_steps\n      time_schedule = InferenceSchedule(inference_schedule=self.args.inference_schedule,\n                                        T=self.diffusion.T, inference_T=steps)\n\n      # Diffusion iterations\n      for i in range(steps):\n        t1, t2 = time_schedule(i)\n        t1 = np.array([t1]).astype(int)\n        t2 = np.array([t2]).astype(int)\n\n        if self.diffusion_type == 'gaussian':\n          xt = self.gaussian_denoise_step(\n              points, xt, t1, device, edge_index, target_t=t2)\n        else:\n          xt = self.categorical_denoise_step(\n              points, xt, t1, device, edge_index, target_t=t2)\n\n      if self.diffusion_type == 'gaussian':\n        adj_mat = xt.cpu().detach().numpy() * 0.5 + 0.5\n      else:\n        adj_mat = xt.float().cpu().detach().numpy() + 1e-6\n\n      if self.args.save_numpy_heatmap:\n        self.run_save_numpy_heatmap(adj_mat, np_points, real_batch_idx, split)\n\n      tours, merge_iterations = merge_tours(\n          adj_mat, np_points, np_edge_index,\n          sparse_graph=self.sparse,\n          parallel_sampling=self.args.parallel_sampling,\n      )\n\n      # Refine using 2-opt\n      solved_tours, ns = batched_two_opt_torch(\n          np_points.astype(\"float64\"), np.array(tours).astype('int64'),\n          max_iterations=self.args.two_opt_iterations, device=device)\n      stacked_tours.append(solved_tours)\n\n    solved_tours = np.concatenate(stacked_tours, axis=0)\n\n    tsp_solver = TSPEvaluator(np_points)\n    gt_cost = tsp_solver.evaluate(np_gt_tour)\n\n    total_sampling = self.args.parallel_sampling * self.args.sequential_sampling\n    all_solved_costs = [tsp_solver.evaluate(solved_tours[i]) for i in range(total_sampling)]\n    best_solved_cost = np.min(all_solved_costs)\n\n    metrics = {\n        f\"{split}/gt_cost\": gt_cost,\n        f\"{split}/2opt_iterations\": ns,\n        f\"{split}/merge_iterations\": merge_iterations,\n    }\n    for k, v in metrics.items():\n      self.log(k, v, on_epoch=True, sync_dist=True)\n    self.log(f\"{split}/solved_cost\", best_solved_cost, prog_bar=True, on_epoch=True, sync_dist=True)\n    return metrics\n\n  def run_save_numpy_heatmap(self, adj_mat, np_points, real_batch_idx, split):\n    if self.args.parallel_sampling > 1 or self.args.sequential_sampling > 1:\n      raise NotImplementedError(\"Save numpy heatmap only support single sampling\")\n    exp_save_dir = os.path.join(self.logger.save_dir, self.logger.name, self.logger.version)\n    heatmap_path = os.path.join(exp_save_dir, 'numpy_heatmap')\n    rank_zero_info(f\"Saving heatmap to {heatmap_path}\")\n    os.makedirs(heatmap_path, exist_ok=True)\n    real_batch_idx = real_batch_idx.cpu().numpy().reshape(-1)[0]\n    np.save(os.path.join(heatmap_path, f\"{split}-heatmap-{real_batch_idx}.npy\"), adj_mat)\n    np.save(os.path.join(heatmap_path, f\"{split}-points-{real_batch_idx}.npy\"), np_points)\n\n  def validation_step(self, batch, batch_idx):\n    return self.test_step(batch, batch_idx, split='val')\n\n# ==========================================\n# File: difusco/utils/mis_utils.py\n# Function/Context: mis_decode_np\n# ==========================================\nimport numpy as np\n\ndef mis_decode_np(predictions, adj_matrix):\n  \"\"\"Decode the labels to the MIS.\"\"\"\n  solution = np.zeros_like(predictions.astype(int))\n  sorted_predict_labels = np.argsort(- predictions)\n  csr_adj_matrix = adj_matrix.tocsr()\n\n  for i in sorted_predict_labels:\n    next_node = i\n\n    if solution[next_node] == -1:\n      continue\n\n    solution[csr_adj_matrix[next_node].nonzero()[1]] = -1\n    solution[next_node] = 1\n\n  return (solution == 1).astype(int)\n\n# ==========================================\n# File: difusco/utils/tsp_utils.py\n# Function/Context: merge_tours\n# ==========================================\nimport os\nimport warnings\nfrom multiprocessing import Pool\n\nimport numpy as np\nimport scipy.sparse\nimport scipy.spatial\nimport torch\nfrom utils.cython_merge.cython_merge import merge_cython\n\n\ndef batched_two_opt_torch(points, tour, max_iterations=1000, device=\"cpu\"):\n  iterator = 0\n  tour = tour.copy()\n  with torch.inference_mode():\n    cuda_points = torch.from_numpy(points).to(device)\n    cuda_tour = torch.from_numpy(tour).to(device)\n    batch_size = cuda_tour.shape[0]\n    min_change = -1.0\n    while min_change < 0.0:\n      points_i = cuda_points[cuda_tour[:, :-1].reshape(-1)].reshape((batch_size, -1, 1, 2))\n      points_j = cuda_points[cuda_tour[:, :-1].reshape(-1)].reshape((batch_size, 1, -1, 2))\n      points_i_plus_1 = cuda_points[cuda_tour[:, 1:].reshape(-1)].reshape((batch_size, -1, 1, 2))\n      points_j_plus_1 = cuda_points[cuda_tour[:, 1:].reshape(-1)].reshape((batch_size, 1, -1, 2))\n\n      A_ij = torch.sqrt(torch.sum((points_i - points_j) ** 2, axis=-1))\n      A_i_plus_1_j_plus_1 = torch.sqrt(torch.sum((points_i_plus_1 - points_j_plus_1) ** 2, axis=-1))\n      A_i_i_plus_1 = torch.sqrt(torch.sum((points_i - points_i_plus_1) ** 2, axis=-1))\n      A_j_j_plus_1 = torch.sqrt(torch.sum((points_j - points_j_plus_1) ** 2, axis=-1))\n\n      change = A_ij + A_i_plus_1_j_plus_1 - A_i_i_plus_1 - A_j_j_plus_1\n      valid_change = torch.triu(change, diagonal=2)\n\n      min_change = torch.min(valid_change)\n      flatten_argmin_index = torch.argmin(valid_change.reshape(batch_size, -1), dim=-1)\n      min_i = torch.div(flatten_argmin_index, len(points), rounding_mode='floor')\n      min_j = torch.remainder(flatten_argmin_index, len(points))\n\n      if min_change < -1e-6:\n        for i in range(batch_size):\n          cuda_tour[i, min_i[i] + 1:min_j[i] + 1] = torch.flip(cuda_tour[i, min_i[i] + 1:min_j[i] + 1], dims=(0,))\n        iterator += 1\n      else:\n        break\n\n      if iterator >= max_iterations:\n        break\n    tour = cuda_tour.cpu().numpy()\n  return tour, iterator\n\n\ndef numpy_merge(points, adj_mat):\n  dists = np.linalg.norm(points[:, None] - points, axis=-1)\n\n  components = np.zeros((adj_mat.shape[0], 2)).astype(int)\n  components[:] = np.arange(adj_mat.shape[0])[..., None]\n  real_adj_mat = np.zeros_like(adj_mat)\n  merge_iterations = 0\n  for edge in (-adj_mat / dists).flatten().argsort():\n    merge_iterations += 1\n    a, b = edge // adj_mat.shape[0], edge % adj_mat.shape[0]\n    if not (a in components and b in components):\n      continue\n    ca = np.nonzero((components == a).sum(1))[0][0]\n    cb = np.nonzero((components == b).sum(1))[0][0]\n    if ca == cb:\n      continue\n    cca = sorted(components[ca], key=lambda x: x == a)\n    ccb = sorted(components[cb], key=lambda x: x == b)\n    newc = np.array([[cca[0], ccb[0]]])\n    m, M = min(ca, cb), max(ca, cb)\n    real_adj_mat[a, b] = 1\n    components = np.concatenate([components[:m], components[m + 1:M], components[M + 1:], newc], 0)\n    if len(components) == 1:\n      break\n  real_adj_mat[components[0, 1], components[0, 0]] = 1\n  real_adj_mat += real_adj_mat.T\n  return real_adj_mat, merge_iterations\n\n\ndef cython_merge(points, adj_mat):\n  with warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    real_adj_mat, merge_iterations = merge_cython(points.astype(\"double\"), adj_mat.astype(\"double\"))\n    real_adj_mat = np.asarray(real_adj_mat)\n  return real_adj_mat, merge_iterations\n\n\ndef merge_tours(adj_mat, np_points, edge_index_np, sparse_graph=False, parallel_sampling=1):\n  \"\"\"\n  To extract a tour from the inferred adjacency matrix A, we used the following greedy edge insertion\n  procedure.\n  • Initialize extracted tour with an empty graph with N vertices.\n  • Sort all the possible edges (i, j) in decreasing order of Aij/kvi − vjk (i.e., the inverse edge weight,\n  multiplied by inferred likelihood). Call the resulting edge list (i1, j1),(i2, j2), . . . .\n  • For each edge (i, j) in the list:\n    – If inserting (i, j) into the graph results in a complete tour, insert (i, j) and terminate.\n    – If inserting (i, j) results in a graph with cycles (of length < N), continue.\n    – Otherwise, insert (i, j) into the tour.\n  • Return the extracted tour.\n  \"\"\"\n  splitted_adj_mat = np.split(adj_mat, parallel_sampling, axis=0)\n\n  if not sparse_graph:\n    splitted_adj_mat = [\n        adj_mat[0] + adj_mat[0].T for adj_mat in splitted_adj_mat\n    ]\n  else:\n    splitted_adj_mat = [\n        scipy.sparse.coo_matrix(\n            (adj_mat, (edge_index_np[0], edge_index_np[1])),\n        ).toarray() + scipy.sparse.coo_matrix(\n            (adj_mat, (edge_index_np[1], edge_index_np[0])),\n        ).toarray() for adj_mat in splitted_adj_mat\n    ]\n\n  splitted_points = [\n      np_points for _ in range(parallel_sampling)\n  ]\n\n  if np_points.shape[0] > 1000 and parallel_sampling > 1:\n    with Pool(parallel_sampling) as p:\n      results = p.starmap(\n          cython_merge,\n          zip(splitted_points, splitted_adj_mat),\n      )\n  else:\n    results = [\n        cython_merge(_np_points, _adj_mat) for _np_points, _adj_mat in zip(splitted_points, splitted_adj_mat)\n    ]\n\n  splitted_real_adj_mat, splitted_merge_iterations = zip(*results)\n\n  tours = []\n  for i in range(parallel_sampling):\n    tour = [0]\n    while len(tour) < splitted_adj_mat[i].shape[0] + 1:\n      n = np.nonzero(splitted_real_adj_mat[i][tour[-1]])[0]\n      if len(tour) > 1:\n        n = n[n != tour[-2]]\n      tour.append(n.max())\n    tours.append(tour)\n\n  merge_iterations = np.mean(splitted_merge_iterations)\n  return tours, merge_iterations\n\n\nclass TSPEvaluator(object):\n  def __init__(self, points):\n    self.dist_mat = scipy.spatial.distance_matrix(points, points)\n\n  def evaluate(self, route):\n    total_cost = 0\n    for i in range(len(route) - 1):\n      total_cost += self.dist_mat[route[i], route[i + 1]]\n    return total_cost\n\n# ==========================================\n# File: tsp_mcts/code/TSP_2Opt.h\n# Function/Context: \n# ==========================================\n// Evaluate the delta after applying a 2-opt move (delta >0 indicates an improving solution)\nDistance_Type Get_2Opt_Delta(int First_City, int Second_City)\n{\n    if (Check_If_Two_City_Same_Or_Adjacent(First_City, Second_City) == true)\n        return -Inf_Cost;\n\n    int First_Next_City = All_Node[First_City].Next_City;\n    int Second_Next_City = All_Node[Second_City].Next_City;\n\n    Distance_Type Delta = Get_Distance(First_City, First_Next_City) + Get_Distance(Second_City, Second_Next_City) - Get_Distance(First_City, Second_City) - Get_Distance(First_Next_City, Second_Next_City);\n\n    // Update the Chosen_Times[][] and Total_Simulation_Times which are used in MCTS\n    Chosen_Times[First_City][Second_City]++;\n    Chosen_Times[Second_City][First_City]++;\n    Chosen_Times[First_Next_City][Second_Next_City]++;\n    Chosen_Times[Second_Next_City][First_Next_City]++;\n    Total_Simulation_Times++;\n\n    return Delta;\n}\n\n// Apply a chosen 2-opt move\nvoid Apply_2Opt_Move(int First_City, int Second_City)\n{\n    Distance_Type Before_Distance = Get_Solution_Total_Distance();\n    Distance_Type Delta = Get_2Opt_Delta(First_City, Second_City);\n\n    int First_Next_City = All_Node[First_City].Next_City;\n    int Second_Next_City = All_Node[Second_City].Next_City;\n\n    Reverse_Sub_Path(First_Next_City, Second_City);\n    All_Node[First_City].Next_City = Second_City;\n    All_Node[Second_City].Pre_City = First_City;\n    All_Node[First_Next_City].Next_City = Second_Next_City;\n    All_Node[Second_Next_City].Pre_City = First_Next_City;\n\n    // Update the values of matrix Weight[][] by back propagation, which would be used in MCTS\n    double Increase_Rate = Beta * (pow(2.718, (double)(Delta) / (double)(Before_Distance)) - 1);\n\n    Weight[First_City][Second_City] += Increase_Rate;\n    Weight[Second_City][First_City] += Increase_Rate;\n    Weight[First_Next_City][Second_Next_City] += Increase_Rate;\n    Weight[Second_Next_City][First_Next_City] += Increase_Rate;\n}\n\nbool Improve_By_2Opt_Move()\n{\n    bool If_Improved = false;\n    for (int i = 0; i < Virtual_City_Num; i++)\n        for (int j = 0; j < Candidate_Num[i]; j++)\n        {\n            int Candidate_City = Candidate[i][j];\n            if (Get_2Opt_Delta(i, Candidate_City) > 0)\n            {\n                Apply_2Opt_Move(i, Candidate_City);\n                If_Improved = true;\n                break;\n            }\n        }\n\n    return If_Improved;\n}\n\n// Iteratively apply an improving 2-opt move until no improvement is possible\nvoid Local_Search_by_2Opt_Move()\n{\n    while (Improve_By_2Opt_Move() == true)\n        ;\n\n    Distance_Type Cur_Solution_Total_Distance = Get_Solution_Total_Distance();\n    if (Cur_Solution_Total_Distance < Current_Instance_Best_Distance)\n    {\n        // Store the information of the best found solution to Struct_Node *Best_All_Node\n        Current_Instance_Best_Distance = Cur_Solution_Total_Distance;\n        Store_Best_Solution();\n    }\n}\n\n# ==========================================\n# File: tsp_mcts/code/TSP_MCTS.h\n# Function/Context: MCTS\n# ==========================================\n// Initialize the parameters used in MCTS\nvoid MCTS_Init()\n{\n\tfor(int i=0;i<Virtual_City_Num;i++)\n\t\tfor(int j=0;j<Virtual_City_Num;j++)\n\t\t{\n\t\t\t//Weight[i][j]=1;\n\t\t\tWeight[i][j]=Edge_Heatmap[i][j]*100;\n\t\t\tChosen_Times[i][j]=0;\n\t\t}\n\n\t/*\n\tfor(int i=0;i<Virtual_City_Num;i++)\n\t\tfor(int j=0;j<Virtual_City_Num;j++)\n\t\t\tWeight[i][j]+=Edge_Heatmap[i][j]*100;\n\t*/\n\n\tTotal_Simulation_Times=0;\n}\n\n//Get the average weight of all the edge relative to Cur_City\ndouble Get_Avg_Weight(int Cur_City)\n{\n\tdouble Total_Weight=0;\n\tfor(int i=0;i<Virtual_City_Num;i++)\n\t{\n\t\tif(i==Cur_City)\n\t\t\tcontinue;\n\n\t\tTotal_Weight+=Weight[Cur_City][i];\n\t}\n\n\treturn Total_Weight/(Virtual_City_Num-1);\n}\n\n//Estimate the potential of each edge by upper bound confidence function\ndouble Get_Potential(int First_City, int Second_City)\n{\n\tdouble Potential=Weight[First_City][Second_City]/Avg_Weight+Alpha*sqrt( log(Total_Simulation_Times+1) / ( log(2.718)*(Chosen_Times[First_City][Second_City]+1) ) );\n\n\treturn Potential;\n}\n\n// Indentify the promising cities as candidates which are possible to connect to Cur_City\nvoid Identify_Promising_City(int Cur_City, int Begin_City)\n{\n\tPromising_City_Num=0;\n\tfor(int i=0;i<Candidate_Num[Cur_City];i++)\n\t{\n\t\tint Temp_City = Candidate[Cur_City][i];\n\t\tif(Temp_City == Begin_City)\n\t\t\tcontinue;\n\t\tif(Temp_City == All_Node[Cur_City].Next_City)\n\t\t\tcontinue;\n\t\tif(Get_Potential(Cur_City, Temp_City) < 1)\n\t\t\tcontinue;\n\n\t\tPromising_City[Promising_City_Num++]=Temp_City;\n\t}\n}\n\n// Set the probability (stored in Probabilistic[]) of selecting each candidate city (proportion to the potential of the corresponding edge)\nbool Get_Probabilistic(int Cur_City)\n{\n\tif(Promising_City_Num==0)\n\t\treturn false;\n\n\tdouble Total_Potential=0;\n\tfor(int i=0;i<Promising_City_Num;i++)\n\t\tTotal_Potential+=Get_Potential(Cur_City, Promising_City[i]);\n\n\tProbabilistic[0]=(int)(1000*Get_Potential(Cur_City, Promising_City[0])/Total_Potential);\n\tfor(int i=1;i<Promising_City_Num-1;i++)\n\t\tProbabilistic[i]=Probabilistic[i-1]+(int)(1000*Get_Potential(Cur_City, Promising_City[i])/Total_Potential);\n\tProbabilistic[Promising_City_Num-1]=1000;\n\n\treturn true;\n}\n\n// Probabilistically choose a city, controled by the values stored in Probabilistic[]\nint Probabilistic_Get_City_To_Connect()\n{\n\tint Random_Num=Get_Random_Int(1000);\n\tfor(int i=0;i<Promising_City_Num;i++)\n\t\tif(Random_Num < Probabilistic[i])\n\t\t\treturn Promising_City[i];\n\n\treturn Null;\n}\n\n// The whole process of choosing a city (a_{i+1} in the paper) to connect Cur_City (b_i in the paper)\nint Choose_City_To_Connect(int Cur_City, int Begin_City)\n{\n\tAvg_Weight=Get_Avg_Weight(Cur_City);\n\tIdentify_Promising_City(Cur_City, Begin_City);\n\tGet_Probabilistic(Cur_City);\n\n\treturn Probabilistic_Get_City_To_Connect();\n}\n\n// Generate an action starting form Begin_City (corresponding to a_1 in the paper), return the delta value\nDistance_Type Get_Simulated_Action_Delta(int Begin_City)\n{\n\t// Store the current solution to Solution[]\n\tif(Convert_All_Node_To_Solution()==false)\n\t\treturn -Inf_Cost;\n\n\tint Next_City=All_Node[Begin_City].Next_City;   // a_1=Begin city, b_1=Next_City\n\n\t// Break edge (a_1,b_1)\n\tAll_Node[Begin_City].Next_City=Null;\n\tAll_Node[Next_City].Pre_City=Null;\n\n\t// The elements of an action is stored in City_Sequence[], where a_{i+1}=City_Sequence[2*i], b_{i+1}=City_Sequence[2*i+1]\n\tCity_Sequence[0]=Begin_City;\n\tCity_Sequence[1]=Next_City;\n\n\tGain[0]=Get_Distance(Begin_City,Next_City);                // Gain[i] stores the delta (before connecting to a_1) at the (i+1)th iteration\n\tReal_Gain[0]=Gain[0]-Get_Distance(Next_City,Begin_City);   // Real_Gain[i] stores the delta (after connecting to a_1) at the (i+1)th iteration\n\tPair_City_Num=1;                                            // Pair_City_Num indicates the depth (k in the paper) of the action\n\n\tbool If_Changed=false;\n\tint Cur_City=Next_City;     // b_i = Cur_City (1 <= i <= k)\n\twhile(true)\n\t{\n\t\tint Next_City_To_Connect=Choose_City_To_Connect(Cur_City,Begin_City); //  Probabilistically choose one city as a_{i+1}\n\t\tif(Next_City_To_Connect == Null)\n\t\t\tbreak;\n\n\t\t//Update the chosen times, used in MCTS\n\t\tChosen_Times[Cur_City][Next_City_To_Connect] ++;\n\t\tChosen_Times[Next_City_To_Connect][Cur_City] ++;\n\n\t\tint Next_City_To_Disconnect=All_Node[Next_City_To_Connect].Pre_City;   // Determine b_{i+1}\n\n\t\t// Update City_Sequence[], Gain[], Real_Gain[] and Pair_City_Num\n\t\tCity_Sequence[2*Pair_City_Num]=Next_City_To_Connect;\n\t\tCity_Sequence[2*Pair_City_Num+1]=Next_City_To_Disconnect;\n\t\tGain[Pair_City_Num]=Gain[Pair_City_Num-1]-Get_Distance(Cur_City,Next_City_To_Connect)+Get_Distance(Next_City_To_Connect,Next_City_To_Disconnect);\n\t\tReal_Gain[Pair_City_Num]=Gain[Pair_City_Num]-Get_Distance(Next_City_To_Disconnect,Begin_City);\n\t\tPair_City_Num++;\n\n\t\t// Reverse the cities between b_i and b_{i+1}\n\t\tReverse_Sub_Path(Cur_City,Next_City_To_Disconnect);\n\t\tAll_Node[Cur_City].Next_City=Next_City_To_Connect;\n\t\tAll_Node[Next_City_To_Connect].Pre_City=Cur_City;\n\t\tAll_Node[Next_City_To_Disconnect].Pre_City=Null;\n\t\tIf_Changed=true;\n\n\t\t// Turns to the next iteration\n\t\tCur_City=Next_City_To_Disconnect;\n\n\t\t// Close the loop is meeting an improving action, or the depth reaches its upper bound\n\t\tif(Real_Gain[Pair_City_Num-1] > 0 || Pair_City_Num > Max_Depth)\n\t\t\tbreak;\n\t}\n\n\t// Restore the solution before simulation\n\tif(If_Changed)\n\t\tConvert_Solution_To_All_Node();\n\telse\n\t{\n\t\tAll_Node[Begin_City].Next_City=Next_City;\n\t\tAll_Node[Next_City].Pre_City=Begin_City;\n\t}\n\n\t// Identify the best depth of the simulated action\n\tint Max_Real_Gain=-Inf_Cost;\n\tint Best_Index=1;\n\tfor(int i=1;i<Pair_City_Num;i++)\n\t\tif(Real_Gain[i] > Max_Real_Gain)\n\t\t{\n\t\t\tMax_Real_Gain=Real_Gain[i];\n\t\t\tBest_Index=i;\n\t\t}\n\n\tPair_City_Num=Best_Index+1;\n\n\treturn Max_Real_Gain;\n}\n\n// If the delta of an action is greater than zero, use the information of this action (stored in City_Sequence[]) to update the parameters by back propagation\nvoid Back_Propagation(Distance_Type Before_Simulation_Distance, Distance_Type Action_Delta)\n{\n\tfor(int i=0;i<Pair_City_Num;i++)\n\t{\n\t\tint First_City=City_Sequence[2*i];\n\t\tint Second_City=City_Sequence[2*i+1];\n\t\tint Third_City;\n\t\tif(i<Pair_City_Num-1)\n\t\t\tThird_City=City_Sequence[2*i+2];\n\t\telse\n\t\t\tThird_City=City_Sequence[0];\n\n\t\tif(Action_Delta >0)\n\t\t{\n\t\t\tdouble Increase_Rate=Beta*(pow(2.718, (double) (Action_Delta) / (double)(Before_Simulation_Distance) )-1);\n\t\t\tWeight[Second_City][Third_City] += Increase_Rate;\n\t\t\tWeight[Third_City][Second_City] += Increase_Rate;\n\t\t}\n\t}\n}\n\n// Sampling at most Max_Simulation_Times actions\nDistance_Type Simulation(int Max_Simulation_Times)\n{\n\tDistance_Type Best_Action_Delta = -Inf_Cost;\n\tfor(int i=0;i<Max_Simulation_Times;i++)\n\t{\n\t\tint Begin_City=Get_Random_Int(Virtual_City_Num);\n\t\tDistance_Type Action_Delta=Get_Simulated_Action_Delta(Begin_City);\n\t\tTotal_Simulation_Times++;\n\n\t\t//Store the action with the best delta, stored in Temp_City_Sequence[] and Temp_Pair_Num\n\t\tif(Action_Delta > Best_Action_Delta)\n\t\t{\n\t\t\tBest_Action_Delta = Action_Delta;\n\n\t\t\tTemp_Pair_Num = Pair_City_Num;\n\t\t\tfor(int j=0;j<2*Pair_City_Num;j++)\n\t\t\t\tTemp_City_Sequence[j]=City_Sequence[j];\n\t\t}\n\n\t\tif(Best_Action_Delta >0)\n\t\t\tbreak;\n\t}\n\n\t// Restore the action with the best delta\n\tPair_City_Num=Temp_Pair_Num;\n\tfor(int i=0;i<2*Pair_City_Num;i++)\n\t\tCity_Sequence[i]=Temp_City_Sequence[i];\n\n\treturn Best_Action_Delta;\n}\n\n//Execute the best action stored in City_Sequence[] with depth Pair_City_Num\nbool Execute_Best_Action()\n{\n\tint Begin_City=City_Sequence[0];\n\tint Cur_City=City_Sequence[1];\n\tAll_Node[Begin_City].Next_City=Null;\n\tAll_Node[Cur_City].Pre_City=Null;\n\tfor(int i=1;i<Pair_City_Num;i++)\n\t{\n\t\tint Next_City_To_Connect=City_Sequence[2*i];\n\t\tint Next_City_To_Disconnect=City_Sequence[2*i+1];\n\n\t\tReverse_Sub_Path(Cur_City,Next_City_To_Disconnect);\n\n\t\tAll_Node[Cur_City].Next_City=Next_City_To_Connect;\n\t\tAll_Node[Next_City_To_Connect].Pre_City=Cur_City;\n\t\tAll_Node[Next_City_To_Disconnect].Pre_City=Null;\n\n\t\tCur_City=Next_City_To_Disconnect;\n\t}\n\n\tAll_Node[Begin_City].Next_City=Cur_City;\n\tAll_Node[Cur_City].Pre_City=Begin_City;\n\n\tif(Check_Solution_Feasible()==false)\n\t{\n\t\tprintf(\"\\nError! The solution after applying action from %d is unfeasible\\n\",Begin_City+1);\n\t\tPrint_TSP_Tour(Begin_City);\n\t\tgetchar();\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n// Process of the MCTS\nvoid MCTS()\n{\n\t//while(true)\n\twhile(((double)clock()-Current_Instance_Begin_Time) /CLOCKS_PER_SEC<Param_T*Virtual_City_Num)\n\t{\n\t\tDistance_Type Before_Simulation_Distance = Get_Solution_Total_Distance();\n\n\t\t//Simulate a number of (controled by Param_H) actions\n\t\tDistance_Type Best_Delta=Simulation(Param_H*Virtual_City_Num);\n\n\t\t// Use the information of the best action to update the parameters of MCTS by back propagation\n\t\tBack_Propagation(Before_Simulation_Distance,Best_Delta);\n\n\t\tif(Best_Delta > 0)\n\t\t{\n\t\t\t// Select the best action to execute\n\t\t\tExecute_Best_Action();\n\n\t\t\t// Store the best found solution to Struct_Node *Best_All_Node\n\t\t\tDistance_Type Cur_Solution_Total_Distance=Get_Solution_Total_Distance();\n\t\t\tif(Cur_Solution_Total_Distance < Current_Instance_Best_Distance)\n\t\t\t{\n\t\t\t\tCurrent_Instance_Best_Distance = Cur_Solution_Total_Distance;\n\t\t\t\tStore_Best_Solution();\n\t\t\t}\n\t\t}\n\t\telse\n\t\t\tbreak;      // The MCTS terminates if no improving action is found among the sampling pool\n\t}\n}",
  "description": "Combined Analysis:\n- [difusco/pl_meta_model.py]: This file implements the core diffusion model infrastructure for DIFUSCO's algorithm steps. The COMetaModel class initializes the diffusion process (Gaussian or Categorical) and the GNN encoder that parameterizes the denoising model. The key algorithm steps are implemented in the posterior sampling methods (categorical_posterior and gaussian_posterior), which perform the denoising steps during inference. These methods correspond to the diffusion model's reverse process described in the paper, using either DDPM or DDIM sampling strategies. The model handles both edge-based (TSP) and node-based (MIS) representations through the sparse flag and output channels configuration. While this file doesn't directly implement the TSP/MIS objective functions or constraints, it provides the essential diffusion framework that generates the solution heatmaps, which are then decoded into discrete solutions in problem-specific modules.\n- [difusco/pl_mis_model.py]: This file implements the core inference algorithm for the MIS problem as described in the DIFUSCO paper. The test_step method performs the complete diffusion-based solving pipeline: 1) Initializes noise samples (Gaussian or Bernoulli), 2) Executes iterative denoising using either categorical_denoise_step or gaussian_denoise_step with a cosine schedule, 3) Decodes the final heatmap into binary solutions using mis_decode_np (greedy decoding for MIS), 4) Evaluates solution quality by comparing independent set sizes. This directly corresponds to the paper's algorithm steps for MIS: graph-based diffusion model inference with parallel denoising and greedy decoding.\n- [difusco/pl_tsp_model.py]: This file implements the core DIFUSCO algorithm for TSP as described in the paper. It contains the TSPModel class which handles both training and inference using graph-based diffusion models. The implementation includes:\n1. Training steps for both Gaussian and categorical diffusion models (categorical_training_step, gaussian_training_step)\n2. Denoising steps for inference (categorical_denoise_step, gaussian_denoise_step)\n3. Complete test/validation pipeline (test_step) that performs:\n   - Parallel and sequential sampling\n   - Diffusion denoising iterations with cosine schedule\n   - Heatmap merging to tours\n   - 2-opt refinement\n   - Cost evaluation\n4. The code maps directly to the paper's algorithm steps: diffusion model parameterization by GNN, parallel denoising with cosine schedules, and decoding with 2-opt refinement.\n5. The binary edge selection representation (adjacency matrix) corresponds to the paper's {0,1}-vector formulation for TSP edges.\n6. The TSPEvaluator computes the objective function (tour length).\n7. Constraints are implicitly enforced through the diffusion model's training on valid tours and the merge_tours decoding step.\n- [difusco/utils/mis_utils.py]: This file implements the greedy decoding algorithm for the Maximal Independent Set (MIS) problem as described in the DIFUSCO paper. The function 'mis_decode_np' takes predicted node probabilities (heatmap from diffusion model) and converts them to a valid MIS solution using a greedy algorithm that selects nodes in order of decreasing probability while enforcing the independence constraint (no two adjacent nodes selected). This corresponds to the 'greedy decoding' step mentioned in the algorithm steps for MIS. The mathematical optimization model for MIS is implicitly enforced through the constraint that selected nodes cannot be adjacent, which is implemented by marking neighbors of selected nodes as excluded (-1).\n- [difusco/utils/tsp_utils.py]: This file implements key post-processing and decoding steps for the TSP optimization model. The core logic includes:\n1. **Greedy Edge Insertion Decoding (merge_tours)**: Implements the exact algorithm described in the paper (Section 3.2) to convert diffusion-generated heatmaps (adjacency matrices) into valid tours. It sorts edges by A_ij/distance(i,j) and incrementally builds a Hamiltonian cycle while avoiding subtours.\n2. **2-opt Local Search (batched_two_opt_torch)**: Implements the 2-opt heuristic mentioned in the paper (Algorithm 2) to refine tours by iteratively swapping edges to reduce tour length.\n3. **Tour Evaluation (TSPEvaluator)**: Computes the objective function ∑ d_e x_e for a given route.\n\nThe code directly supports the optimization model's objective (minimizing tour length) and enforces constraints through the decoding procedure (ensuring a single cycle covering all nodes). The merge_tours function explicitly handles the subtour elimination constraint via component tracking, while the degree constraint is implicitly enforced by the edge insertion logic.\n- [tsp_mcts/code/TSP_2Opt.h]: This file implements the 2-opt local search algorithm for TSP, which is a key component of the decoding strategy mentioned in the DIFUSCO paper. The 2-opt algorithm improves TSP tours by iteratively swapping pairs of edges to reduce tour length. The code specifically implements: 1) Delta calculation for potential 2-opt moves, 2) Application of improving moves with path reversal, 3) Iterative local search until convergence. This aligns with the paper's description of using 'greedy decoding with 2-opt' for TSP solutions. The implementation includes MCTS-related statistics tracking (Chosen_Times, Weight updates) which suggests integration with Monte Carlo Tree Search as mentioned in the algorithm steps.\n- [tsp_mcts/code/TSP_MCTS.h]: This file implements the Monte Carlo Tree Search (MCTS) component of DIFUSCO's TSP solver. It specifically implements the local search refinement algorithm that operates on the heatmap output from the diffusion model. The code performs: 1) Weight initialization using edge heatmaps from diffusion models, 2) UCB-based node selection for promising edges, 3) Simulation of k-opt moves (edge exchanges) to evaluate potential tour improvements, 4) Back-propagation to update edge weights based on simulation results, and 5) Execution of the best-found improving moves. This implements the 'greedy decoding with Monte Carlo Tree Search' mentioned in the algorithm steps, working directly with the TSP objective of minimizing tour distance through edge selection optimization.",
  "dependencies": [
    "All_Node",
    "Get_Distance",
    "Get_Solution_Total_Distance",
    "self.duplicate_edge_index",
    "self.categorical_posterior",
    "Distance_Type",
    "torch.utils.data",
    "Promising_City_Num",
    "Param_T",
    "Convert_Solution_To_All_Node",
    "Check_If_Two_City_Same_Or_Adjacent",
    "multiprocessing.Pool",
    "Struct_Node",
    "City_Sequence",
    "Weight",
    "torch_geometric.data.DataLoader",
    "COMetaModel",
    "Beta",
    "Virtual_City_Num",
    "scipy.sparse",
    "Max_Depth",
    "torch.nn.functional",
    "InferenceSchedule",
    "scipy.sparse matrix (adj_matrix parameter)",
    "warnings",
    "Candidate",
    "Gain",
    "MISDataset",
    "scipy.spatial",
    "pytorch_lightning.utilities.rank_zero_info",
    "utils.lr_schedulers.get_schedule_fn",
    "Edge_Heatmap",
    "Get_Random_Int",
    "utils.cython_merge.cython_merge.merge_cython",
    "Print_TSP_Tour",
    "utils.diffusion_schedulers.CategoricalDiffusion",
    "co_datasets.tsp_graph_dataset.TSPGraphDataset",
    "utils.diffusion_schedulers.InferenceSchedule",
    "Reverse_Sub_Path",
    "Check_Solution_Feasible",
    "os",
    "utils.tsp_utils.TSPEvaluator",
    "Inf_Cost",
    "Probabilistic",
    "Real_Gain",
    "Chosen_Times",
    "mis_decode_np",
    "numpy",
    "utils.tsp_utils.batched_two_opt_torch",
    "Promising_City",
    "Candidate_Num",
    "Current_Instance_Best_Distance",
    "Pair_City_Num",
    "Param_H",
    "Current_Instance_Begin_Time",
    "Alpha",
    "utils.diffusion_schedulers.GaussianDiffusion",
    "Convert_All_Node_To_Solution",
    "torch.nn",
    "pl_meta_model.COMetaModel",
    "self.gaussian_posterior",
    "utils.tsp_utils.merge_tours",
    "torch",
    "pytorch_lightning",
    "Store_Best_Solution",
    "Total_Simulation_Times",
    "models.gnn_encoder.GNNEncoder"
  ]
}