{
  "paper_id": "Generalization_of_Neural_Combinatorial_Solvers_Through_the_L",
  "title": "Generalization of Neural Combinatorial Solvers Through the Lens of Adversarial Robustness",
  "abstract": "End-to-end (geometric) deep learning has seen first successes in approximating the solution of combinatorial optimization problems. However, generating data in the realm of NP-hard/-complete tasks brings practical and theoretical challenges, resulting in evaluation protocols that are too optimistic. Specifically, most datasets only capture a simpler subproblem and likely suffer from spurious features. We investigate these effects by studying adversarial robustness—a local generalization property—to reveal hard, model-specific instances and spurious features. For this purpose, we derive perturbation models for SAT and TSP. Unlike in other applications, where perturbation models are designed around subjective notions of imperceptibility, our perturbation models are efficient and sound, allowing us to determine the true label of perturbed samples without a solver. Surprisingly, with such perturbations, a sufficiently expressive neural solver does not suffer from the limitations of the accuracy-robustness trade-off common in supervised learning. Although such robust solvers exist, we show empirically that the assessed neural solvers do not generalize well w.r.t. small perturbations of the problem instance.",
  "problem_description_natural": "The paper focuses on two canonical NP-complete combinatorial optimization problems: the Boolean Satisfiability Problem (SAT) and the Traveling Salesperson Problem (TSP). In SAT, the goal is to determine whether a given Boolean formula in Conjunctive Normal Form (CNF) can be satisfied by some assignment of truth values to its variables. In TSP, the objective is to find the shortest possible route that visits each city exactly once and returns to the origin city. The authors study the generalization capabilities of neural solvers for these problems by evaluating their robustness to small, structured perturbations of the input instances.",
  "problem_type": "combinatorial optimization",
  "datasets": [
    "10-40",
    "50-100",
    "100-300",
    "3-10",
    "SATLIB",
    "UNI3SAT"
  ],
  "performance_metrics": [
    "Accuracy",
    "Adversarial Accuracy",
    "Optimality Gap",
    "Recall"
  ],
  "lp_model": {
    "objective": "$ \\min_{Y' \\in g(\\boldsymbol{x})} c(\\boldsymbol{x}, Y') $",
    "constraints": [
      "$ Y' \\in g(\\boldsymbol{x}) $, where $ g(\\boldsymbol{x}) $ is the finite set of feasible solutions for problem instance $ \\boldsymbol{x} $."
    ],
    "variables": [
      "$ Y' $: a feasible solution from the set $ g(\\boldsymbol{x}) $."
    ]
  },
  "raw_latex_model": "$$ Y = \\arg\\min_{Y' \\in g(\\boldsymbol{x})} c(\\boldsymbol{x}, Y') $$",
  "algorithm_description": "The paper focuses on neural combinatorial solvers for specific NP-hard problems: the Boolean Satisfiability (SAT) problem and the Traveling Salesperson Problem (TSP). For SAT, the problem is to determine if a boolean formula in Conjunctive Normal Form (CNF) is satisfiable, with decision variables as truth assignments for boolean variables and constraints that each clause must have at least one true literal. For TSP, the problem is to find a Hamiltonian cycle of minimum cost in a weighted graph, with the objective function defined as minimizing the sum of edge weights in the cycle, and constraints ensuring the solution is a permutation covering all nodes exactly once. The neural solvers, such as NeuroSAT for SAT and DTSP/ConvTSP for TSP, use graph neural networks to approximate solutions, and the paper introduces adversarial robustness to evaluate and improve their generalization."
}