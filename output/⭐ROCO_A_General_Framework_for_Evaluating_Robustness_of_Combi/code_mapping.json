{
  "file_path": "fc/fc_env.py, mc/mc_algorithms.py, mc/mc_gurobi_tune.py, mc/mc_ppo_pytorch.py",
  "function_name": "FCEnv, greedy_search_naive, gurobi_search, ortools_search, gurobi_search, train_PPO",
  "code_snippet": "\n\n# ==========================================\n# File: fc/fc_env.py\n# Function/Context: FCEnv\n# ==========================================\nimport functools\nimport networkx as nx\nimport numpy as np\nimport random\nimport torch\nimport os\nimport time\nfrom torch_geometric.data import Data, DataLoader\nfrom fc_algorithms import greedy_search_naive, greedy_average_search, local_search, gurobi_search\nfrom copy import deepcopy\nfrom torch_sparse import SparseTensor\n\nclass FCEnv(object):\n    def __init__(self, solver_type='greedy_naive', strategy_size=30, event_size=3000, is_attack=False, modify_nodes=False):\n        self.strategy_size = strategy_size\n        self.event_size = event_size\n        self.solver_type = solver_type\n        self.process_dataset()\n        # self.available_solvers = ('greedy_naive', 'greedy_average', 'local', 'gurobi')\n        self.available_solvers = (solver_type,)\n        self.is_attack = is_attack\n        self.modify_nodes = modify_nodes\n        assert solver_type in self.available_solvers\n    \n    def process_dataset(self):\n        \"\"\"\n        Downloading and processing dataset.\n        \"\"\"\n        print(\"\\nPreparing dataset.\\n\")\n        dirpath = f'fraud_data/{self.strategy_size}_{self.event_size}'\n        fcfiles = []\n        self.num_edges = []\n\n        for file in os.listdir(dirpath):\n            # optionally add files\n            _, strategy_num, event_num, _, _ = file.split('_')\n            if int(strategy_num) == self.strategy_size and int(event_num) == self.event_size:\n                fcfiles.append(dirpath+'/'+file)\n        self.graphs = [self.construct_sparse_matrix(gf) for gf in fcfiles]\n        print(np.mean(self.num_edges))\n        \n    def construct_sparse_matrix(self, graph_file):\n        with open(graph_file,'r') as data:\n            strategy_num, events_num, limit = next(data).strip().split(',')\n            row = []; col = []; val = []\n            for line in data:\n                _, event_id, _, strategy_id, amt, flag = line.strip().split(',')\n                if float(flag) > 0:\n                    row.append(int(strategy_id)); col.append(int(event_id)); val.append(float(amt))\n                else:\n                    row.append(int(strategy_id)); col.append(int(event_id)); val.append(-float(amt))\n        self.num_edges.append(len(row))\n        sizes = (int(strategy_num), int(events_num))\n        return (SparseTensor(torch.tensor(row), None, torch.tensor(col), torch.tensor(val), sizes), int(limit))\n        \n    def construct_matrix(self, graph_file):\n        with open(graph_file,'r') as data:\n            strategy_num, events_num, limit = next(data).strip().split(',')\n            matrix = torch.zeros((int(strategy_num),int(events_num)))\n            for line in data:\n                _, event_id, _, strategy_id, amt, flag = line.strip().split(',')\n                if float(flag) > 0:\n                    matrix[int(strategy_id)][int(event_id)] = float(amt)\n                else:\n                    matrix[int(strategy_id)][int(event_id)] = -float(amt)\n        \n        return (matrix, int(limit))\n    \n    def generate_tuples(self, num_train_samples, num_test_samples, rand_id):\n        random.seed(int(rand_id))\n        np.random.seed(int(rand_id + num_train_samples))\n        \n        training_tuples = []\n        testing_tuples = []\n        \n        return_tuples = training_tuples\n        sum_num_nodes = 0\n        for i, (sparse_matrix, limit) in enumerate(self.graphs):\n            # edge_candidates = self.get_edge_candidates(matrix)\n            sum_num_nodes += (sparse_matrix.size(0) + sparse_matrix.size(1))\n            \n            fc_solutions = {}\n            fc_times = {}\n            for key in self.available_solvers:\n                obj, sec, S = self.solve_feasible_fc(sparse_matrix, limit, key)\n                fc_solutions[key] = obj\n                fc_times[key] = sec\n                if key == self.solver_type:\n                    if self.is_attack and not self.modify_nodes:\n                        edge_candidates = self.get_attack_edge_candidates(sparse_matrix, S)\n                    elif self.is_attack and self.modify_nodes:\n                        edge_candidates = self.get_node_candidates(sparse_matrix)\n                    else:\n                        edge_candidates = self.get_edge_candidates(sparse_matrix, S)\n            print(f'id {i}'\n                f'{';'.join([f\"{x} amount={fc_solutions[x]:.2f} time={fc_times[x]:.2f}\" for x in self.available_solvers])}')\n            return_tuples.append((\n                sparse_matrix,\n                limit,\n                edge_candidates,\n                fc_solutions[self.solver_type],\n                fc_solutions,\n                fc_times,\n            ))\n            if i == num_train_samples - 1 or i == num_train_samples + num_test_samples - 1:\n                print(f'average number of nodes: {sum_num_nodes / len(return_tuples)}')\n                sum_num_nodes = 0\n                for solver_name in self.available_solvers:\n                    print(f'{solver_name} average amount='\n                        f'{torch.mean(torch.tensor([tup[4][solver_name] for tup in return_tuples], dtype=torch.float)):.4f}')\n                return_tuples = testing_tuples\n            if i == num_train_samples + num_test_samples - 1:\n                break\n        return training_tuples, testing_tuples\n\n    def step(self, sparse_matrix, limit, act, prev_solution, defense = False):\n        if self.is_attack and not self.modify_nodes and not defense:\n            return self.step_attack(sparse_matrix, limit, act, prev_solution)\n        if self.is_attack and self.modify_nodes and not defense:\n            return self.step_node_attack(sparse_matrix, limit, act, prev_solution)\n        \n        if isinstance(act, torch.Tensor):\n            act = (act[0].item(), act[1].item())\n        row, col, val = sparse_matrix.coo()\n        # remove the black edge\n        for i in range(len(row)):\n            if row[i] == act[0] and col[i] == act[1]:\n                break\n        new_row = torch.cat((row[:i],row[i+1:]), dim=0)\n        new_col = torch.cat((col[:i],col[i+1:]), dim=0)\n        new_val = torch.cat((val[:i],val[i+1:]), dim=0)\n        new_sizes = (sparse_matrix.size(0), sparse_matrix.size(1))\n\n        new_sparse_matrix = SparseTensor(new_row, None, new_col, new_val, new_sizes)\n        new_solution, _, new_S = self.solve_feasible_fc(new_sparse_matrix, limit, self.solver_type)\n        new_edge_candidate = self.get_edge_candidates(new_sparse_matrix, new_S)\n        reward = new_solution - prev_solution\n        done = all([len(x) == 0 for x in new_edge_candidate.values()])\n        \n        return reward, new_sparse_matrix, new_edge_candidate, new_solution, done\n    \n    def step_attack(self, sparse_matrix, limit, act, prev_solution):\n        row, col, val = sparse_matrix.coo()\n        new_row = deepcopy(row)\n        new_col = deepcopy(col)\n        new_val = deepcopy(val)\n        new_sizes = (sparse_matrix.size(0), sparse_matrix.size(1))\n        # add additional black edges\n        for i in range(len(row)):\n            if col[i] == act[1]:\n                target_val = val[i]\n                break\n        new_row = torch.cat((new_row, act[0].unsqueeze(-1)))\n        new_col = torch.cat((new_col, act[1].unsqueeze(-1)))\n        new_val = torch.cat((new_val, target_val.unsqueeze(-1)))\n        \n        new_sparse_matrix = SparseTensor(new_row, None, new_col, new_val, new_sizes)\n        new_solution, _, new_S = self.solve_feasible_fc(new_sparse_matrix, limit, self.solver_type)\n        new_edge_candidate = self.get_attack_edge_candidates(new_sparse_matrix, new_S)\n        reward = prev_solution - new_solution\n\n        done = all([len(x) == 0 for x in new_edge_candidate.values()])\n        \n        return reward, new_sparse_matrix, new_edge_candidate, new_solution, done\n    \n    # the attack method and node candidates\n    def step_node_attack(self, sparse_matrix, limit, act, prev_solution):\n        if isinstance(act, torch.Tensor):\n            act = act.item()\n        row, col, val = sparse_matrix.coo()\n        new_row = deepcopy(row)\n        new_col = deepcopy(col)\n        new_val = deepcopy(val)\n        new_sizes = (sparse_matrix.size(0), sparse_matrix.size(1))\n        # change the white event to black\n        for i in range(len(row)):\n            if col[i] == act:\n                new_val[i] = -val[i]\n        # Will the memory be shared or not?\n        new_sparse_matrix = SparseTensor(new_row, None, new_col, new_val, new_sizes)\n        new_solution, _, _ = self.solve_feasible_fc(new_sparse_matrix, limit, self.solver_type)\n        new_node_candidates = self.get_node_candidates(new_sparse_matrix)\n        reward = prev_solution - new_solution\n        done = (len(new_node_candidates) == 0)\n        \n        return reward, new_sparse_matrix, new_node_candidates, new_solution, done\n    \n    @staticmethod\n    def get_edge_candidates(sparse_matrix, strategies):\n        num_strategy = sparse_matrix.size(0)\n        num_events = sparse_matrix.size(1)\n        row, col, val = sparse_matrix.coo()\n        edge_candidate = {x: set() for x in range(num_strategy)}\n        for i in range(len(row)):\n            if row[i].item() in strategies and val[i].item() > 0:\n                edge_candidate[row[i].item()].add(col[i].item())\n            else:\n                pass\n            \n        return edge_candidate\n    \n    @staticmethod\n    def get_attack_edge_candidates(sparse_matrix, strategies):\n        num_strategy = sparse_matrix.size(0)\n        num_events = sparse_matrix.size(1)\n        row, col, val = sparse_matrix.coo()\n        edge_candidate = {x: set() for x in range(num_strategy)}\n        black_events = set()\n        for i in range(len(row)):\n            if val[i].item() > 0:\n                black_events.add(col[i].item())\n                edge_candidate[row[i].item()].add(col[i].item())\n        for i in range(num_strategy):\n            if i in strategies:\n                edge_candidate[i].clear()\n            else:\n                edge_candidate[i] = black_events - edge_candidate[i]\n        return edge_candidate\n\n    @staticmethod\n    def get_node_candidates(sparse_matrix):\n        node_candidates = set()\n        row, col, val = sparse_matrix.coo()\n        for i in range(len(row)):\n            if val[i].item() < 0:\n                node_candidates.add(col[i].item())\n            else:\n                pass\n        return node_candidates\n    \n    def solve_feasible_fc(self, sparse_matrix, limit, key):\n        prev_time = time.time()\n        if key == 'greedy_naive':\n            obj, S = greedy_search_naive(sparse_matrix, limit)\n        elif key == 'greedy_average':\n            obj, S = greedy_average_search(sparse_matrix, limit)\n        elif key == 'local':\n            obj, S = local_search(sparse_matrix, limit)\n        elif key == 'gurobi':\n            obj, S = gurobi_search(sparse_matrix, limit)\n        else:\n            raise ValueError(f'{self.solver_type} is not implemented.')\n        comp_time = time.time() - prev_time\n        return obj, comp_time, S\n\n# ==========================================\n# File: mc/mc_algorithms.py\n# Function/Context: greedy_search_naive, gurobi_search, ortools_search\n# ==========================================\nfrom torch_sparse import SparseTensor\nfrom ortools.linear_solver import pywraplp\nimport gurobipy as gp\nimport time\n\ndef evaluate(sparse_matrix):\n    row, col, val = sparse_matrix.coo()\n    subset_num = sparse_matrix.size(0)\n    element_num = sparse_matrix.size(1)\n    weights = [0] * element_num\n\n    subset_dict = {x:set() for x in range(subset_num)}\n    for i in range(len(row)):\n        subset_dict[row[i].item()].add(col[i].item())\n        weights[col[i].item()] = val[i].item()\n    return subset_num, element_num, weights, subset_dict\n\ndef greedy_search_naive(sparse_matrix, limit):\n    prev_time = time.time()\n    subset_num, _, weight_array, subset_dict = evaluate(sparse_matrix)\n    S = []\n    V = list(range(subset_num))\n    element_set = set()\n    obj = 0\n    for _ in range(limit):\n        best_incr = 0\n        best_v = None\n        for v in V:\n            obj_incr = 0\n            new_elements = subset_dict[v] - element_set\n            for i in new_elements:\n                obj_incr += weight_array[i]\n            if obj_incr >= best_incr:\n                best_incr = obj_incr\n                best_v = v\n        if best_v is not None:\n            S.append(best_v)\n            V.remove(best_v)\n            element_set = element_set | subset_dict[best_v]\n            obj += best_incr\n        else:\n            break\n    comp_time = time.time() - prev_time\n    return obj, comp_time, S\n\ndef gurobi_search(sparse_matrix, limit, time_limit):\n    subset_num, element_num, weight_array, subset_dict = evaluate(sparse_matrix)\n    m = gp.Model(\"MC\")\n\n    # Initialize variables\n    X = m.addVars(subset_num, vtype=gp.GRB.BINARY, name=\"X\")\n    Y = m.addVars(element_num, vtype=gp.GRB.BINARY, name=\"Y\")\n    m.update()\n\n    # Set Objective\n    m.setObjective(sum([Y[i] * weight_array[i] for i in range(element_num)]), gp.GRB.MAXIMIZE)\n\n    # Add Constraints\n    m.addConstr(sum([X[i] for i in range(subset_num)]) <= limit)\n    for j in range(element_num):\n        m.addConstr(sum([(1 if j in subset_dict[i] else 0) * X[i] for i in range(subset_num)]) >= Y[j])\n\n    m.Params.LogToConsole = False\n    m.Params.TimeLimit = time_limit\n\n    prev_time = time.time()\n    m.optimize()\n    comp_time = time.time() - prev_time\n\n    # if m.status == gp.GRB.Status.OPTIMAL:\n    #     print(f\"gurobi has found the optimal solution.\")\n    # else:\n    #     print(f\"gurobi failed to find the optimal solution within the time limit.\")\n\n    obj = m.objVal\n    S = []\n    for k, v in m.getAttr('X', X).items():\n        if v == 1:\n            S.append(k)\n\n    return obj, comp_time, S\n\ndef ortools_search(sparse_matrix, limit, solver_name, time_limit):\n    subset_num, element_num, weight_array, subset_dict = evaluate(sparse_matrix)\n    # create the mip solver with the Gurobi backend\n    if solver_name == 'gurobi':\n        solver = pywraplp.Solver('SolveMC', pywraplp.Solver.GUROBI_MIXED_INTEGER_PROGRAMMING)\n    elif solver_name == 'scip':\n        solver = pywraplp.Solver('SolveMC', pywraplp.Solver.SCIP_MIXED_INTEGER_PROGRAMMING)\n    elif solver_name == 'cbc':\n        solver = pywraplp.Solver('SolveMC', pywraplp.Solver.CBC_MIXED_INTEGER_PROGRAMMING)\n    else:\n        raise ValueError(\"Unrecognized solver.\")\n\n    # Initialize variables\n    X = [0] * subset_num\n    Y = [0] * element_num\n    for i in range(subset_num):\n        X[i] = solver.IntVar(0, 1, f'X[{i}]')\n    for i in range(element_num):\n        Y[i] = solver.IntVar(0, 1, f'Y[{i}]')\n\n    # Add Constraints\n    solver.Add(sum(X) <= limit)\n    for j in range(element_num):\n        sum_j = sum([(1 if j in subset_dict[i] else 0) * X[i] for i in range(subset_num)])\n        solver.Add(sum_j >= Y[j])\n\n    # Set Objective\n    obj_expr = [Y[i] * weight_array[i] for i in range(element_num)]\n    solver.Maximize(solver.Sum(obj_expr))\n\n    # Set time limit\n    solver.SetTimeLimit(int(time_limit * 1000))\n\n    prev_time = time.time()\n    status = solver.Solve()\n    comp_time = time.time() - prev_time\n    # if status == pywraplp.Solver.OPTIMAL:\n    #     print(f\"{solver_name} has found the optimal solution.\")\n    # else:\n    #     print(f\"{solver_name} failed to find the optimal solution within the time limit.\")\n\n    obj = solver.Objective().Value()\n    S = []\n    for i in range(subset_num):\n        if X[i].solution_value() == 1:\n            S.append(i)\n\n    # print(S)\n    return obj, comp_time, S\n\n# ==========================================\n# File: mc/mc_gurobi_tune.py\n# Function/Context: gurobi_search\n# ==========================================\nimport argparse\nimport numpy as np\nimport sys\nimport time\nimport random\nfrom ortools.linear_solver import pywraplp\nimport gurobipy as gp\nfrom mc_ppo_pytorch import parse_arguments\nfrom mc_env import MCEnv\nimport torch\n\ndef evaluate(sparse_matrix):\n    row, col, val = sparse_matrix.coo()\n    subset_num = sparse_matrix.size(0)\n    element_num = sparse_matrix.size(1)\n    weights = [0] * element_num\n\n    subset_dict = {x:set() for x in range(subset_num)}\n    for i in range(len(row)):\n        subset_dict[row[i].item()].add(col[i].item())\n        weights[col[i].item()] = val[i].item()\n    return subset_num, element_num, weights, subset_dict\n\ndef gurobi_search(sparse_matrix, limit, time_limit, MIPFoucus, Heuristics):\n    subset_num, element_num, weight_array, subset_dict = evaluate(sparse_matrix)\n    m = gp.Model(\"MC\")\n\n    # Initialize variables\n    X = m.addVars(subset_num, vtype=gp.GRB.BINARY, name=\"X\")\n    Y = m.addVars(element_num, vtype=gp.GRB.BINARY, name=\"Y\")\n    m.update()\n\n    # Set Objective\n    m.setObjective(sum([Y[i] * weight_array[i] for i in range(element_num)]), gp.GRB.MAXIMIZE)\n\n    # Add Constraints\n    m.addConstr(sum([X[i] for i in range(subset_num)]) <= limit)\n    for j in range(element_num):\n        m.addConstr(sum([(1 if j in subset_dict[i] else 0) * X[i] for i in range(subset_num)]) >= Y[j])\n\n    m.Params.LogToConsole = False\n    m.Params.TimeLimit = time_limit\n    m.Params.MIPFocus = MIPFoucus\n    m.Params.Heuristics = Heuristics\n\n    prev_time = time.time()\n    m.optimize()\n    comp_time = time.time() - prev_time\n\n    # if m.status == gp.GRB.Status.OPTIMAL:\n    #     print(f\"gurobi has found the optimal solution.\")\n    # else:\n    #     print(f\"gurobi failed to find the optimal solution within the time limit.\")\n    # print(m.status)\n    obj = m.objVal\n    if obj < 0:\n        obj = 0\n    S = []\n    # for k, v in m.getAttr('X', X).items():\n    #     if v == 1:\n    #         S.append(k)\n\n    return obj, comp_time, S\n\n# ==========================================\n# File: mc/mc_ppo_pytorch.py\n# Function/Context: train_PPO\n# ==========================================\nimport argparse\nimport torch\nfrom torch import nn\nimport os\nimport time\nimport yaml\nimport random\nimport numpy as np\nfrom torch.multiprocessing import Pool, cpu_count\nfrom copy import deepcopy\n\nfrom mc_model import ActorNet, CriticNet, GraphEncoder\nfrom utils.utils import print_args\nfrom utils.tfboard_helper import get_pai_tensorboard_path, TensorboardUtil\nfrom mc.evaluate_mc import evaluate\nfrom mc_env import MCEnv\n\n\nclass ItemsContainer:\n    def __init__(self):\n        self.__reward = []\n        self.__matrix = []\n        self.__limit = []\n        self.__ori_greedy = []\n        self.__greedy = []\n        self.__edge_candidates = []\n        self.__done = []\n\n    def append(self, reward, matrix, limit, edge_candidates, greedy, done, ori_greedy):\n        self.__reward.append(reward)\n        self.__matrix.append(matrix)\n        self.__limit.append(limit)\n        self.__greedy.append(greedy)\n        self.__edge_candidates.append(edge_candidates)\n        self.__done.append(done)\n        self.__ori_greedy.append(ori_greedy)\n\n    @property\n    def reward(self):\n        return deepcopy(self.__reward)\n\n    @property\n    def matrix(self):\n        return deepcopy(self.__matrix)\n\n    @property\n    def limit(self):\n        return deepcopy(self.__limit)\n\n    @property\n    def greedy(self):\n        return deepcopy(self.__greedy)\n\n    @property\n    def edge_candidates(self):\n        return deepcopy(self.__edge_candidates)\n\n    @property\n    def done(self):\n        return deepcopy(self.__done)\n\n    @property\n    def ori_greedy(self):\n        return deepcopy(self.__ori_greedy)\n\n    def update(self, idx, reward=None, matrix=None, limit=None, greedy=None, edge_candidates=None, done=None,\n               ori_greedy=None):\n        if reward is not None:\n            self.__reward[idx] = reward\n        if matrix is not None:\n            self.__matrix[idx] = matrix\n        if limit is not None:\n            self.__limit[idx] = limit\n        if greedy is not None:\n            self.__greedy[idx] = greedy\n        if edge_candidates is not None:\n            self.__edge_candidates[idx] = edge_candidates\n        if done is not None:\n            self.__done[idx] = done\n        if ori_greedy is not None:\n            self.__ori_greedy[idx] = ori_greedy\n\n\nclass Memory:\n    def __init__(self):\n        self.actions = []\n        self.states = []\n        self.logprobs = []\n        self.rewards = []\n        self.is_terminals = []\n        self.edge_candidates = []\n\n    def clear_memory(self):\n        del self.actions[:]\n        del self.states[:]\n        del self.logprobs[:]\n        del self.rewards[:]\n        del self.is_terminals[:]\n        del self.edge_candidates[:]\n\nclass ActorCritic(nn.Module):\n    def __init__(self, node_feature_dim, node_output_size, batch_norm, one_hot_degree, gnn_layers):\n        super(ActorCritic, self).__init__()\n        self.state_encoder = GraphEncoder(node_feature_dim, node_output_size, batch_norm, one_hot_degree, gnn_layers)\n        self.actor_net = ActorNet(node_output_size * 2, batch_norm)\n        self.value_net = CriticNet(node_output_size * 4, batch_norm)\n\n    def forward(self):\n        raise NotImplementedError\n\n    def act(self, matrix, edge_candidates, memory):\n        element_feat, subset_feat = self.state_encoder(matrix)\n        actions, action_logits, entropy = self.actor_net(element_feat, subset_feat, edge_candidates)\n\n        memory.states.append(matrix)\n        memory.edge_candidates.append(edge_candidates)\n        memory.actions.append(actions)\n        memory.logprobs.append(action_logits)\n\n        return actions\n\n    def evaluate(self, matrix, edge_candidates, action):\n        element_feat, subset_feat = self.state_encoder(matrix)\n        _, action_logits, entropy = self.actor_net(element_feat, subset_feat, edge_candidates, action)\n        state_value = self.value_net(element_feat, subset_feat)\n\n        return action_logits, state_value, entropy\n\nclass PPO:\n    def __init__(self, args, device):\n        self.lr = args.learning_rate\n        self.betas = args.betas\n        self.gamma = args.gamma\n        self.eps_clip = args.eps_clip\n        self.k_epochs = args.k_epochs\n\n        self.device = device\n\n        ac_params = args.node_feature_dim, args.node_output_size, args.batch_norm, args.one_hot_degree, args.gnn_layers\n\n        self.policy = ActorCritic(*ac_params).to(self.device)\n        self.optimizer = torch.optim.Adam(\n            [{'params': self.policy.actor_net.parameters()},\n             {'params': self.policy.value_net.parameters()},\n             {'params': self.policy.state_encoder.parameters(), 'lr': self.lr / 10}],\n            lr=self.lr, betas=self.betas)\n        if len(args.lr_steps) > 0:\n            lr_steps = [step // args.update_timestep for step in args.lr_steps]\n            self.lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, lr_steps, gamma=0.1)\n        else:\n            self.lr_scheduler = None\n        self.policy_old = ActorCritic(*ac_params).to(self.device)\n        self.policy_old.load_state_dict(self.policy.state_dict())\n\n        self.MseLoss = nn.MSELoss()\n\n    def update(self, memory):\n        # Time Difference estimate of state rewards:\n        rewards = []\n\n        with torch.no_grad():\n            logprobs, state_values, dist_entropy = \\\n                self.policy.evaluate(memory.states[-1], memory.edge_candidates[-1], memory.actions[-1].to(self.device))\n        discounted_reward = state_values\n\n        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n            reward = torch.tensor(reward, dtype=torch.float32).to(self.device)\n            discounted_reward = discounted_reward * (1 - torch.tensor(is_terminal, dtype=torch.float32).to(self.device))\n            discounted_reward = reward + (self.gamma * discounted_reward).clone()\n            rewards.insert(0, discounted_reward)\n\n        # Normalizing the rewards:\n        rewards = torch.cat(rewards)\n        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n\n        # convert list to tensor\n        old_states = []\n        for state in memory.states:\n            old_states += state\n        old_actions = torch.cat(memory.actions, dim=-1)\n        old_logprobs = torch.cat(memory.logprobs, dim=-1)\n        old_candidates = []\n        for candi in memory.edge_candidates:\n            old_candidates += candi\n\n        critic_loss_sum = 0\n\n        # Optimize policy for K epochs:\n        for _ in range(self.k_epochs):\n            # Evaluating old actions and values :\n            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_candidates, old_actions)\n\n            # Finding the ratio (pi_theta / pi_theta_old):\n            ratios = torch.exp(logprobs - old_logprobs)\n\n            # Normalizing advantages\n            advantages = rewards - state_values.detach()\n            # advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-5)\n\n            # Finding Surrogate Loss:\n            surr1 = ratios * advantages\n            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n            actor_loss = -torch.min(surr1, surr2)\n            # print(state_values, rewards,flush=True)\n            critic_loss = self.MseLoss(state_values, rewards)\n            entropy_reg = -0.01 * dist_entropy\n            critic_loss_sum += critic_loss.detach().mean()\n\n            # take gradient step\n            self.optimizer.zero_grad()\n            (actor_loss + critic_loss + entropy_reg).mean().backward()\n            self.optimizer.step()\n        if self.lr_scheduler:\n            self.lr_scheduler.step()\n\n        # Copy new weights into old policy:\n        self.policy_old.load_state_dict(self.policy.state_dict())\n\n        return critic_loss_sum / self.k_epochs  # mean critic loss\n\ndef train_PPO(args):\n    # initialize manual seed\n    if args.random_seed is not None:\n        random.seed(args.random_seed)\n        np.random.seed(args.random_seed)\n        torch.manual_seed(args.random_seed)\n\n    # create environment\n    mc_env = MCEnv(args.solver_type, args.subset_size, args.element_size, args.time_limit)\n    args.node_feature_dim = 3\n\n    # get current device\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n    # load training/testing data\n    tuples_train, tuples_test = mc_env.generate_tuples(args.train_sample, args.test_sample, 0)\n\n    # create tensorboard summary writer\n    env_dict = os.environ\n    print('ENV DICT')\n    print(env_dict)\n    try:\n        import tensorflow as tf\n        key = 'POD_NAMESPACE'\n        if key in env_dict and env_dict[key] == 'kubemaker':  # kubemaker mode\n            tfboard_path = get_pai_tensorboard_path()\n        else:  # local mode: logs stored in ./runs/TIME_STAMP-MACHINE_ID\n            tfboard_path = args.tensorboard_save_dir\n        import socket\n        from datetime import datetime\n        current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n        tfboard_path = os.path.join(\n            tfboard_path, current_time + '_' + socket.gethostname())\n        summary_writer = TensorboardUtil(tf.summary.FileWriter(tfboard_path))\n    except (ModuleNotFoundError, ImportError):\n        print('Warning: Tensorboard not loading, please install tensorflow to enable...')\n        summary_writer = None\n\n    # init models\n    memory = Memory()\n    ppo = PPO(args, device)\n    num_workers = cpu_count()\n    mp_pool = Pool(num_workers)\n\n    # logging variables\n    best_test_ratio = 0\n    running_reward = 0\n    critic_loss = []\n    avg_length = 0\n    timestep = 0\n    prev_time = time.time()\n\n    print(\"Begin Training: \", flush=True)\n    # training loop\n    for i_episode in range(1, args.max_episodes + 1):\n        items_batch = ItemsContainer()\n        if i_episode % 10 == 0:\n            print(i_episode, flush=True)\n        for b in range(args.batch_size):\n            graph_index = ((i_episode - 1) * args.batch_size + b) % len(tuples_train)\n            matrix, limit, edge_candidates, ori_greedy, baselines, _ = tuples_train[graph_index]\n            greedy = ori_greedy\n            items_batch.append(0, matrix, limit, edge_candidates, greedy, False, ori_greedy)\n\n        for t in range(args.max_timesteps):\n            timestep += 1\n\n            # Running policy_old\n            with torch.no_grad():\n                action_batch = ppo.policy_old.act(items_batch.matrix, items_batch.edge_candidates, memory)\n\n            def step_func_feeder(batch_size):\n                batch_matrix = items_batch.matrix\n                batch_limit = items_batch.limit\n                action_batch_cpu = action_batch.cpu()\n                batch_greedy = items_batch.greedy\n                for b in range(batch_size):\n                    yield batch_matrix[b], batch_limit[b], action_batch_cpu[:, b], batch_greedy[b]\n\n            if args.batch_size > 1:\n                pool_map = mp_pool.starmap_async(mc_env.step, step_func_feeder(args.batch_size))\n                step_list = pool_map.get()\n            else:\n                step_list = [mc_env.step(*x) for x in step_func_feeder(args.batch_size)]\n\n            for b, item in enumerate(step_list):\n                reward, matrix, edge_candidates, greedy, done = item\n                if t == args.max_timesteps - 1:\n                    done = True\n                items_batch.update(b, reward=reward, matrix=matrix, greedy=greedy,\n                                   edge_candidates=edge_candidates, done=done)\n\n            # Saving reward and is_terminal:\n            memory.rewards.append(items_batch.reward)\n            memory.is_terminals.append(items_batch.done)\n\n            # update if its time\n            if timestep % args.update_timestep == 0:\n                closs = ppo.update(memory)\n                critic_loss.append(closs)\n                if summary_writer:\n                    summary_writer.add_scalar('critic mse/train', closs, timestep)\n                memory.clear_memory()\n\n            running_reward += sum(items_batch.reward) / args.batch_size\n            if any(items_batch.done):\n                break\n\n        avg_length += t+1\n\n        # logging\n        if i_episode % args.log_interval == 0:\n            avg_length = avg_length / args.log_interval\n            running_reward = running_reward / args.log_interval\n            if len(critic_loss) > 0:\n                critic_loss = torch.mean(torch.stack(critic_loss))\n            else:\n                critic_loss = -1\n            now_time = time.time()\n            avg_time = (now_time - prev_time) / args.log_interval\n            prev_time = now_time\n\n            if summary_writer:\n                summary_writer.add_scalar('reward/train', running_reward, timestep)\n                summary_writer.add_scalar('time/train', avg_time, timestep)\n                for lr_id, x in enumerate(ppo.optimizer.param_groups):\n                    summary_writer.add_scalar(f'lr/{lr_id}', x['lr'], timestep)\n\n            print(\n                f'Episode {i_episode} \\t '\n                f'avg length: {avg_length:.2f} \\t '\n                f'critic mse: {critic_loss:.4f} \\t '\n                f'reward: {running_reward:.4f} \\t '\n                f'time per episode: {avg_time:.2f}'\n            )\n\n            running_reward = 0\n            avg_length = 0\n            critic_loss = []\n\n        # testing\n        if i_episode % args.test_interval == 0:\n            with torch.no_grad():\n                prev_test_time = time.time()\n                print(\"######### Evaluate on Test ##########\")\n                test_dict = evaluate(ppo.policy, mc_env, tuples_test, args.max_timesteps, args.search_size, mp_pool,\n                                     args)\n\n                # for key, val in test_dict.items():\n                #     if isinstance(val, dict):\n                #         if summary_writer:\n                #             summary_writer.add_scalars(f'{key}/test', val, timestep)\n                #     else:\n                #         if summary_writer:\n                #             summary_writer.add_scalar(f'{key}/test', val, timestep)\n                print(\"########## Evaluate complete ##########\")\n                # fix running time value\n                prev_time += time.time() - prev_test_time\n\n            if test_dict[\"ratio\"][\"mean\"] > best_test_ratio:\n                best_test_ratio = test_dict[\"ratio\"][\"mean\"]\n\n                file_name = f'{args.checkpoint_save_dir}/PPO_{args.solver_type}_subset{args.subset_size}_element{args.element_size}' \\\n                            f'_beam{args.search_size}_ratio{best_test_ratio:.4f}.pt'\n                torch.save(ppo.policy.state_dict(), file_name)\n                print(f\"Updating the model, Best test_ratio = {best_test_ratio} \\n\")\n\ndef generate_hardcase(args):\n    # initialize manual seed\n    if args.random_seed is not None:\n        random.seed(args.random_seed)\n        np.random.seed(args.random_seed)\n        torch.manual_seed(args.random_seed)\n\n    # create environment\n    mc_env = MCEnv(args.solver_type, args",
  "description": "Combined Analysis:\n- [fc/fc_env.py]: This file implements the core environment for the ROCO framework's fraud detection (FC) problem, which is a specific instance of the Maximum Coverage problem. The FCEnv class encapsulates the bipartite graph structure between strategies and events, implements the optimization model through solver interfaces, and provides mechanisms for evaluating solver robustness via adversarial perturbations. Key components include: 1) Graph construction from fraud data files, 2) Multiple solver interfaces (greedy algorithms and Gurobi) for the FC problem, 3) Step functions that implement edge/node perturbations to test solver robustness, 4) Candidate generation for adversarial actions. The mathematical model matches the Maximum Coverage formulation where strategies correspond to sets, events to elements, and the objective maximizes covered event weights subject to selecting ≤k strategies.\n- [mc/mc_algorithms.py]: This file directly implements the Maximum Coverage (MC) problem solvers as described in the paper's optimization model. The code contains three solver implementations: 1) greedy_search_naive - implements the greedy algorithm with (1-1/e) approximation ratio, 2) gurobi_search - formulates the MC problem as an integer linear program (ILP) and solves it using Gurobi, 3) ortools_search - provides a unified interface to solve the same ILP formulation using multiple solvers (Gurobi, SCIP, CBC) via OR-Tools. The mathematical model matches exactly: Objective maximizes sum of covered element weights, with constraints limiting selected subsets to k. The evaluate() function converts sparse matrix input to problem data structures. All solvers include time tracking and return objective value, computation time, and solution set S'.\n- [mc/mc_gurobi_tune.py]: This file implements the core integer linear programming (ILP) formulation for the Maximum Coverage problem using Gurobi, matching the paper's optimization model. The gurobi_search function directly encodes the objective to maximize weighted element coverage with constraints on subset selection count (≤k) and coverage relationships. It serves as a black-box solver within the ROCO framework for robustness evaluation, supporting parameter tuning (MIPFocus, Heuristics) and time-limited execution.\n- [mc/mc_ppo_pytorch.py]: This file implements the core PPO (Proximal Policy Optimization) training algorithm for learning a Maximum Coverage solver, which aligns with the paper's approach of using learning-based solvers within the ROCO robustness evaluation framework. The code contains the complete training loop, policy evaluation, and optimization steps for the PPO algorithm applied to the Maximum Coverage problem. It interfaces with the MCEnv environment which encapsulates the combinatorial optimization problem, and uses neural network components (ActorNet, CriticNet, GraphEncoder) to learn selection policies. The training process generates hard instances through RL-based search as part of the robustness evaluation methodology described in the paper.",
  "dependencies": [
    "torch.multiprocessing",
    "mc.evaluate_mc",
    "networkx",
    "gurobipy",
    "evaluate",
    "copy",
    "fc_algorithms.gurobi_search",
    "fc_algorithms.greedy_search_naive",
    "yaml",
    "time",
    "torch_sparse",
    "mc_env",
    "utils.utils",
    "fc_algorithms.greedy_average_search",
    "os",
    "mc_ppo_pytorch.parse_arguments",
    "mc_model",
    "fc_algorithms.local_search",
    "utils.tfboard_helper",
    "random",
    "numpy",
    "mc_env.MCEnv",
    "torch_geometric",
    "torch",
    "ortools.linear_solver"
  ]
}