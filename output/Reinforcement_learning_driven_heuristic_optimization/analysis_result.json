{
  "paper_id": "Reinforcement_learning_driven_heuristic_optimization",
  "title": "Reinforcement Learning Driven Heuristic Optimization",
  "abstract": "Heuristic algorithms such as simulated annealing, Concorde, and METIS are effective and widely used approaches to find solutions to combinatorial optimization problems. However, they are limited by the high sample complexity required to reach a reasonable solution from a cold-start. In this paper, we introduce a novel framework to generate better initial solutions for heuristic algorithms using reinforcement learning (RL), named RLHO. We augment the ability of heuristic algorithms to greedily improve upon an existing initial solution generated by RL, and demonstrate novel results where RL is able to leverage the performance of heuristics as a learning signal to generate better initialization. We apply this framework to Proximal Policy Optimization (PPO) and Simulated Annealing (SA). We conduct a series of experiments on the well-known NP-complete bin packing problem, and show that the RLHO method outperforms our baselines. We show that on the bin packing problem, RL can learn to help heuristics perform even better, allowing us to combine the best parts of both approaches.",
  "problem_description_natural": "The paper focuses on the bin packing problem, a classical combinatorial optimization task where the goal is to pack items of different sizes into the minimum number of bins, subject to the constraint that the total size of items in any bin does not exceed the bin capacity. The authors aim to improve the performance of heuristic solvers like Simulated Annealing by using a reinforcement learning agent to generate high-quality initial solutions, rather than starting from random assignments.",
  "problem_type": "combinatorial optimization",
  "datasets": [
    "Generated Bin Packing Instances"
  ],
  "performance_metrics": [
    "Average Number of Used Bins"
  ],
  "lp_model": {
    "objective": "$\\min \\sum_{i=1}^n y_i$",
    "constraints": [
      "$\\sum_{i=1}^n x_{ij} = 1 \\quad \\forall j = 1,\\dots,n$",
      "$\\sum_{j=1}^n v_j x_{ij} \\leq C y_i \\quad \\forall i = 1,\\dots,n$",
      "$x_{ij} \\in \\{0,1\\} \\quad \\forall i,j$",
      "$y_i \\in \\{0,1\\} \\quad \\forall i$"
    ],
    "variables": [
      "$x_{ij}$: binary decision variable, 1 if item $j$ is assigned to bin $i$, 0 otherwise",
      "$y_i$: binary decision variable, 1 if bin $i$ is used, 0 otherwise"
    ]
  },
  "raw_latex_model": "$$\\begin{aligned}\\min & \\sum_{i=1}^n y_i \\\\\\text{s.t.} & \\sum_{i=1}^n x_{ij} = 1, \\quad \\forall j = 1,\\dots,n \\\\& \\sum_{j=1}^n v_j x_{ij} \\leq C y_i, \\quad \\forall i = 1,\\dots,n \\\\& x_{ij} \\in \\{0,1\\}, \\quad \\forall i,j \\\\& y_i \\in \\{0,1\\}, \\quad \\forall i\\end{aligned}$$ where $v_j$ is the size of item $j$ from vector $v$, and $C$ is the bin capacity.",
  "algorithm_description": "The paper proposes the RLHO framework, which combines Proximal Policy Optimization (PPO) for reinforcement learning and Simulated Annealing (SA) as a heuristic optimizer. RL generates initial solutions for the bin packing problem, SA refines them through hill-climbing, and RL learns from SA's performance to improve initialization, creating an alternating optimization loop."
}