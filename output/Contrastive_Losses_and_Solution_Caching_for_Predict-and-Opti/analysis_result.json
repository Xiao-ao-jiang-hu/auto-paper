{
  "paper_id": "Contrastive_Losses_and_Solution_Caching_for_Predict-and-Opti",
  "title": "Contrastive Losses and Solution Caching for Predict-and-Optimize",
  "abstract": "Many decision-making processes involve solving a combinatorial optimization problem with uncertain input that can be estimated from historic data. Recently, problems in this class have been successfully addressed via end-to-end learning approaches, which rely on solving one optimization problem for each training instance at every epoch. In this context, we provide two distinct contributions. First, we use a Noise Contrastive approach to motivate a family of surrogate loss functions, based on viewing non-optimal solutions as negative examples. Second, we address a major bottleneck of all predict-and-optimize approaches, i.e. the need to frequently recompute optimal solutions at training time. This is done via a solver-agnostic solution caching scheme, and by replacing optimization calls with a lookup in the solution cache. The method is formally based on an inner approximation of the feasible space and, combined with a cache lookup strategy, provides a controllable trade-off between training time and accuracy of the loss approximation. We empirically show that even a very slow growth rate is enough to match the quality of state-of-the-art methods, at a fraction of the computational cost.",
  "problem_description_natural": "The paper addresses the predict-and-optimize setting where uncertain parameters of a combinatorial optimization problem must be predicted from historical features so that the resulting decisions (i.e., solutions to the optimization problem using predicted parameters) perform well with respect to the true (unknown) parameters. The goal is to minimize regretâ€”the difference in objective value between the solution obtained using predicted parameters and the true optimal solution under actual parameters. A key challenge is the computational cost of repeatedly solving NP-hard combinatorial optimization problems during training to compute gradients or subgradients for model updates.",
  "problem_type": "combinatorial optimization",
  "datasets": [
    "Ifrim et al. energy price data (2011-2013)",
    "CSPLib Energy-cost Aware Scheduling",
    "CORA citation network"
  ],
  "performance_metrics": [
    "Regret"
  ],
  "lp_model": {
    "objective": "$\\max \\sum_{i=1}^{48} c_i x_i$",
    "constraints": [
      "$\\sum_{i=1}^{48} w_i x_i \\leq C$",
      "$x_i \\in \\{0,1\\}$ for $i=1,\\ldots,48$"
    ],
    "variables": [
      "$x_i$: binary decision variable indicating whether item $i$ is selected"
    ]
  },
  "raw_latex_model": "$$\\begin{aligned} \\max & \\sum_{i=1}^{48} c_i x_i \\\\ \\text{s.t.} & \\sum_{i=1}^{48} w_i x_i \\leq C \\\\ & x_i \\in \\{0,1\\}, \\quad i=1,\\ldots,48 \\end{aligned}$$",
  "algorithm_description": "The paper proposes a predict-and-optimize approach using gradient-descent with contrastive loss functions (e.g., $\\mathcal{L}_{\\text{NCE}}$ or $\\mathcal{L}_{\\text{MAP}}$) and a solution caching mechanism to approximate the feasible region, reducing the need to solve the optimization problem repeatedly during training."
}