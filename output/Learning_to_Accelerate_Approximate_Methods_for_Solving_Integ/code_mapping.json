{
  "file_path": "LinerProgramming/LinearProgramming/cython_solver/LPboxADMMsolver.cpp, LinerProgramming/LinearProgramming/cython_solver/LPboxADMMsolver.h, LinerProgramming/LinearProgramming/trainer.py, Segmentation/Segmentation/cython/src/LPboxADMMsolver.cpp, Segmentation/Segmentation/cython/src/LPboxADMMsolver.h, SparseAttack/SparseAttack/mha.py, SparseAttack/SparseAttack/trainer.py",
  "function_name": "LPboxADMMsolver class, LPboxADMMsolver class, PolicyKL._valid_1, LPboxADMMsolver class and related functions, LPboxADMMsolver::ADMM_bqp_unconstrained_l2f, GraphAttentionEncoder, MLPEncoder, FixingMLPEncoder, PolicyKL",
  "code_snippet": "\n\n# ==========================================\n# File: LinerProgramming/LinearProgramming/cython_solver/LPboxADMMsolver.cpp\n# Function/Context: LPboxADMMsolver class\n# ==========================================\n#include <cstring>\n#include <cmath>\n#include <chrono>\n#include <iostream>\n#include <Eigen4/Dense>\n#include <Eigen4/Sparse>\n#include <fstream>\n#include \"LPboxADMMsolver.h\"\n#include <Eigen/IterativeLinearSolvers>\n#include <unsupported/Eigen/IterativeSolvers>\n\nusing namespace Eigen::internal;\nusing std::abs;\nusing std::sqrt;\n\n/* Core ADMM solver class implementation */\nclass LPboxADMMsolver {\npublic:\n    LPboxADMMsolver();\n    LPboxADMMsolver(int print_fix_info);\n    LPboxADMMsolver(int consistency, _double_t fix_threshold);\n    \n    /* Projection functions for constraints */\n    void project_box(int n, const DenseVector &x, DenseVector &y);\n    void project_shifted_Lp_ball(int n, const DenseVector &x, int p, DenseVector &y);\n    \n    /* Objective computation */\n    _double_t compute_cost(const DenseVector& x, const SparseMatrix& A, const DenseVector& b, DenseVector &temp_vec_for_mat_mul);\n    _double_t compute_cost(const DenseVector& x, const SparseMatrix& A, const DenseVector& b);\n    _double_t compute_cost_lp(DenseVector& x, DenseVector& b);\n    \n    /* Convergence checking */\n    _double_t compute_std_obj(std::vector<_double_t> obj_list, int history_size);\n    _double_t std_dev(std::vector<_double_t>& arr, size_t begin, size_t end);\n    \n    /* ADMM initialization */\n    int ADMM_lp_iters_init();\n    \n    /* Helper projection functions */\n    void project_vec_greater_than(DenseVector &x, DenseVector &res, const int &greater_val, const int &set_val);\n    void project_vec_less_than(DenseVector &x, DenseVector &res, const int &less_val, const int &set_val);\n    void project_box(int n, const double *x, double *y);\n    \nprivate:\n    /* ADMM parameters */\n    _double_t stop_threshold;\n    _double_t std_threshold;\n    _double_t gamma_val;\n    _double_t gamma_factor;\n    int rho_change_step;\n    int max_iters;\n    _double_t initial_rho;\n    int history_size;\n    _double_t learning_fact;\n    _double_t pcg_tol;\n    int pcg_maxiters;\n    _double_t rel_tol;\n    int projection_lp;\n    \n    /* Early fixing parameters */\n    int consistency;\n    _double_t fix_threshold;\n    int print_fix_info;\n    \n    /* Problem structure */\n    struct {\n        int problem_type;  // inequality\n        int update_y3;\n        int update_z3;\n    } instruction;\n};\n\n/* Conjugate Gradient solver for linear systems */\nvoid _conjugate_gradient(const SparseMatrix& mat, const DenseVector& rhs, DenseVector& x,\n        const Eigen::DiagonalPreconditioner<_double_t>& precond, int& iters, typename DenseVector::RealScalar& tol_error);\n\n/* Matrix-vector multiplication optimization */\nvoid mat_mul_vec(const SparseMatrix &mat, const DenseVector &vec, DenseVector& res);\n\n/* Efficient matrix expression computation */\nvoid calculate_mat_expr_multiplication(const std::vector<std::vector<const SparseMatrix*>> &mat_expressions,\n        DenseVector &x, DenseVector &result, DenseVector &temp_vec);\n\n/* CG with matrix expressions (for A^TAx computations) */\nvoid _conjugate_gradient(const std::vector<std::vector<const SparseMatrix*>> &mat_expressions, const DenseVector& rhs,\n        DenseVector& x, const Eigen::DiagonalPreconditioner<_double_t>& precond, \n        int& iters, typename DenseVector::RealScalar& tol_error, \n        DenseVector &temp_vec_for_cg, DenseVector &temp_vec_for_mat_mul, const char* log);\n\n/* ADMM parameter initialization */\nint LPboxADMMsolver::ADMM_lp_iters_init() {\n    stop_threshold = 1e-4;\n    std_threshold = 1e-6;\n    gamma_val = 1.6;\n    gamma_factor = 0.95;\n    rho_change_step = 25;\n    max_iters = 2e4;\n    initial_rho = 25;\n    history_size = 3;\n    learning_fact = 1 + 1.0 / 100;\n    pcg_tol = 1e-3;\n    pcg_maxiters = 1e3;\n    rel_tol = 5e-5;\n    projection_lp = 2;\n    \n    std_threshold = 1e-12;\n    history_size = 10;\n    \n    instruction.problem_type = inequality;\n    instruction.update_y3 = 1;\n    instruction.update_z3 = /* continuation from original code */;\n    \n    return 0;\n}\n\n/* Box projection for x âˆˆ [0,1]^n */\nvoid LPboxADMMsolver::project_box(int n, const DenseVector &x, DenseVector &y) {\n    while (n--) {\n        if (x[n] > 1) {\n            y[n] = 1;\n        } else {\n            if (x[n] < 0) {\n                y[n] = 0;\n            } else {\n                y[n] = x[n];\n            }\n        }\n    }\n}\n\n/* Lp-ball projection for non-convex constraint */\nvoid LPboxADMMsolver::project_shifted_Lp_ball(int n, const DenseVector &x, int p, DenseVector &y) {\n    y.array() = x.array() - 0.5;\n    _double_t normp_shift = y.norm();\n    y.array() = y.array() * std::pow(n, 1.0 / p) / (2 * normp_shift) + 0.5;\n}\n\n/* Objective computation for quadratic + linear terms */\n_double_t LPboxADMMsolver::compute_cost(const DenseVector& x, const SparseMatrix& A, \n        const DenseVector& b, DenseVector &temp_vec_for_mat_mul) {\n    mat_mul_vec(A, x, temp_vec_for_mat_mul);\n    double val = x.transpose().dot(temp_vec_for_mat_mul);\n    double val2 = b.dot(x);\n    return val + val2;\n}\n\n# ==========================================\n# File: LinerProgramming/LinearProgramming/cython_solver/LPboxADMMsolver.h\n# Function/Context: LPboxADMMsolver class\n# ==========================================\n#ifndef CPP_LPBOXADMMSOLVER_H\n#define CPP_LPBOXADMMSOLVER_H\n\n#include <vector>\n#include <Eigen4/Sparse>\n#include <Eigen4/Dense>\n\n#define Dynamic Eigen::Dynamic\n#define PRINT_VEC false\n#define PRINT_SHAPE(mat) printf(\"%s col: %ld, row: %ld\\n\", #mat, mat.cols(), mat.rows())\n#define PRINT_NORM(mat) printf(\"%s col: %ld, row: %ld, norm: %lf\\n\", #mat, mat.cols(), mat.rows(), mat.norm())\ntypedef double _double_t;\ntypedef Eigen::SparseMatrix<_double_t, Eigen::ColMajor> SparseMatrix;     \ntypedef Eigen::Matrix<_double_t, Dynamic, Dynamic> DenseMatrix;  \ntypedef Eigen::Triplet<_double_t> Triplet;                       \ntypedef Eigen::Matrix<_double_t, Dynamic, 1> DenseVector;        \ntypedef Eigen::Matrix<int, Dynamic, Dynamic> DenseIntMatrix;     \ntypedef Eigen::Matrix<int, Dynamic, 1> DenseIntVector;           \n\nenum PreconditionerType {\n\tDIAGONAL = 0,\n\tINCOMPLETE_CHOLESKY = 1,\n\tIDENTITY = 2\n};\n\nstruct Solution {\n\tDenseVector *best_sol;\n\tDenseVector *x_sol;\n\tDenseVector *y1;\n\tDenseVector *y2;\n\tlong time_elapsed;\n};\n\nenum problem_t {\n    unconstrained,\n    equality,\n    inequality,\n    equality_and_inequality\n};\n\nstruct MatrixInfo {\n    problem_t problem_type;\n    int n;\n    const  SparseMatrix *A = nullptr;\n    const  DenseVector *b = nullptr;\n    const  DenseVector *x0 = nullptr;\n    int m;\n    const SparseMatrix *C = nullptr;\n    const DenseVector *d = nullptr;\n    int l;\n    const SparseMatrix *E = nullptr;\n    const DenseVector *f = nullptr;\n};\n\nstruct SolverInstruction{\n    problem_t problem_type;\n    int update_y3;\n    int update_z3;\n    int update_z4;\n\tint update_rho3;\n\tint update_rho4;\n};\n\nclass LPboxADMMsolver {\n\tprivate:\n\t\t_double_t stop_threshold;\n\t\t_double_t std_threshold;\n\t\tint max_iters;\n\t\t_double_t initial_rho;\n\t\tint rho_change_step;\n\t\t_double_t gamma_val;\n\t\t_double_t learning_fact;\n\t\t_double_t history_size;\n\t\t_double_t projection_lp;\n\t\t_double_t gamma_factor;\n\t\t_double_t pcg_tol;\n\t\tint pcg_maxiters;\n\t\t_double_t rho_upper_limit;\n\t\t_double_t rel_tol;\n\t\tint does_log = 1;\n\t\tstd::string log_file_path;\n\t\tPreconditionerType preconditioner;\n\t\t\n\t\tSolverInstruction instruction; \n\t\tSparseMatrix *A_ptr, *C_ptr, *E_ptr, *org_E_ptr;\n\t\tDenseVector *b_ptr, *d_ptr, *f_ptr;\n\t\tint m;\n\t\tint n;\n\t\tint l;\n\t\t_double_t _c; \n\t\tSparseMatrix C_transpose, E_transpose, rho3_C_transpose, rho4_E_transpose, _2A_plus_rho1_rho2;\n\t\tDenseVector y3, z3, z4, Csq_diag, Esq_diag;\n\t\tDenseVector x_sol, y1, y2, z1, z2, prev_idx, best_sol, temp_vec, cur_idx;\n\t\tDenseVector temp_vec_for_cg, temp_vec_for_mat_mul; \n\t\tSparseMatrix temp_mat; \n\t\t_double_t cur_obj = 0;\n\t\tbool rhoUpdated = true;\n\t\tEigen::DiagonalPreconditioner<_double_t> diagonalPreconditioner;\n\t\t_double_t rho1, rho2, rho3, rho4, prev_rho1, prev_rho2, prev_rho3, prev_rho4;\n\t\tstd::vector<_double_t> obj_list;\n\t\t_double_t std_obj = 1;\n\t\t_double_t cvg_test1;\n\t\t_double_t cvg_test2;\n\t\t_double_t rho_change_ratio;\n\t\tSparseMatrix preconditioner_diag_mat;\n\t\tstd::vector<Triplet> preconditioner_diag_mat_triplets;\n\t\tstd::vector<std::vector<const SparseMatrix*>> matrix_expressions;\n\t\tDenseMatrix _2A_plus_rho1_rho2_tmp;\n\t\t_double_t best_bin_obj;\n\t\t_double_t prev_obj=0, prev_sum=0;\n\t\t\n\t\t/* Early Fixing Members */\n\t\tDenseVector x_prev, x_det, x_count, x_flag;\n\t\t_double_t fix_threshold;\n\t\tint consistency;\n\t\tDenseMatrix x_iters; //for the iterations of x_sol;\n\t\tint org_n;\n\t\tint fix_n, fix_sum;\n\t\tDenseVector fix_idx, non_fix_idx, left_idx;\n\t\tDenseVector org_fix_val, org_fix_idx, org_non_fix_idx; \n\t\tDenseVector ret_idx, ret_val;\n\t\tDenseVector ret_idx_prev, ret_val_prev;\n\t\tDenseVector b1,b2,x2;\t\n\t\tSparseMatrix E1,E2;\n\t\tDenseVector f1;\n\t\tDenseMatrix dE;\n\t\t_double_t fix_obj, sum_fix_obj;\n\t\tDenseVector x_sol_try;\n\t\tint file_idx = 0;\n\t\tstd::string path_xiter;\n\t\tstd::string csv;\n\t\tSparseMatrix esq, lhs;\n\t\tint print_fix_info = 0;\n\t\tint iter=0;\n\t\tint node;\n\t\tint problem;\n\t\tstd::string output;\n\t\tstd::string xiter;\n\t\tstd::string xiter_all;\n\t\tint print_info=0;\n\t\tDenseVector x0;\n\n\t\t_double_t std_dev(std::vector<_double_t>& arr, size_t begin, size_t end); \n\t\tvoid project_vec_greater_than(DenseVector &x, DenseVector &res, const int &greater_val, const int &set_val);\n\t\tvoid project_vec_less_than(DenseVector &x, DenseVector &res, const int &less_val, const int &set_val);\n\t\tvoid project_box(int n, const double *x, double *y);\n\t\tvoid project_box(int n, const DenseVector &x, DenseVector &y);\n\t\tvoid project_shifted_Lp_ball(int n, double *x, int p, double *y);\n\t\tvoid project_shifted_Lp_ball(int n, const DenseVector &x, int p, DenseVector &y);\n\t\tdouble compute_cost(int m, int n, double *x, double *A, double *b);\n\t\t_double_t compute_cost(const DenseVector &x, const SparseMatrix &A, const DenseVector &b);\n\t\t_double_t compute_cost(const DenseVector &x, const SparseMatrix &A, const DenseVector &b, \n\t\t\t\tDenseVector &temp_vec_for_mat_mul);\n\t\t_double_t compute_std_obj(std::vector<_double_t> obj_list, int history_size);\n\t\t_double_t compute_cost_lp( DenseVector& x,  DenseVector& b);\n\n\tpublic:\n\t\tLPboxADMMsolver();\n\t\tLPboxADMMsolver(int print_info);\n\t\tLPboxADMMsolver(int consistency, _double_t fix_threshold);\n\t\tvoid ADMM_bqp_unconstrained_init_seg(int node, int problem);\n\t\t_double_t ADMM_bqp_unconstrained_seg(); \n\t\tvoid free();\n\t\tvoid readFile(int i, int k, int j);\n\t\tint ADMM_lp_iters_init();\n\t\tint ADMM_lp_iters(int iter_start, int iter_end);\n\t\tint ADMM_lp_iters_lr(int iter_start, int iter_end);\n\t\tint ADMM_lp_iters_fix(int iter_start, int iter_end);\n\t\tint ADMM_lp_iters_l2f(int iter_start, int iter_end, double* vec, int num); \n\t\tint get_iter(){ return this->iter; }\n\t\tdouble* get_x_sol();\n\t\tdouble* get_final_x_sol();\n\t\tint check_infeasible_lpbox();\n\t\tint check_infeasible_l2f();\n\t\tvoid update_expression(int iter);\n\t\tdouble cal_obj();\n\t\tdouble get_curBinObj();\n\t\tvoid set_fix_threshold(_double_t fix_threshold) { this->fix_threshold = fix_threshold; }\n\t\tvoid set_consistency(int consistency){ this->consistency = consistency; }\n\t\tDenseMatrix get_x_iters(){ return this->x_iters; }\n\t\tdouble* get_x_iters_d(int ws);\n\t\tint get_n(){ return this->n; }\n\t\tint ADMM_cluster_init(int n_,  SparseMatrix &_A,  DenseVector &_b, \n\t\t\t DenseVector &x0, int m_,  SparseMatrix &_C,  DenseVector &_d, Solution& sol);\n\t\tSolution ADMM_cluster_solve_step(int start, int end); \n\t\tvoid ADMM_bqp_linear_eq_init();\n\t\tvoid ADMM_bqp_linear_ineq_init();\n\t\tvoid ADMM_bqp_linear_eq_and_uneq_init();\n\t\tint ADMM_bqp_unconstrained(int n, const SparseMatrix &_A, const DenseVector &_b, \n\t\t\t\tconst DenseVector &x0, Solution& sol);\n\t\tint ADMM_bqp_unconstrained_legacy(int n, const SparseMatrix &_A, const DenseVector &_b, \n\t\t\t\tconst DenseVector &x0, Solution& sol);\n\t\tint ADMM_bqp_unconstrained(int n, _double_t *A, _double_t *b, _double_t *x0, Solution& sol);\n\t\tint ADMM_bqp_linear_eq(int n, const SparseMatrix &_A, const DenseVector &_b, const DenseVector &x0,\n\t\t\t\tint m, const SparseMatrix &_C, const DenseVector &_d, Solution& sol);\n\t\tint ADMM_bqp_linear_eq_kron(int n, const SparseMatrix &_A, const DenseMatrix &kron, const DenseVector &_b,\n\t\t\t\tint m, const SparseMatrix &_C, const DenseVector &_d, Solution& sol);\n\t\tint ADMM_bqp_linear_ineq(int n, const SparseMatrix &_A, const DenseVector &_b, const DenseVector &x0,\n\t\t\t\tint l, const SparseMatrix &_E, const DenseVector &_f, Solution& sol);\n\t\tint ADMM_bqp_linear_ineq(int n, _double_t *A, _double_t *b, _double_t *x0, int l, _double_t *E, _double_t *f,\n\t\t\t\tSolution& sol);\n\t\tint ADMM_bqp_linear_eq_and_uneq(int n, const SparseMatrix &_A, const DenseVector &_b, \n\t\t\t\tconst DenseVector &x0, int m, const SparseMatrix &_C, const DenseVector &_d, int l, const SparseMatrix &_E,\n\t\t\t\tconst DenseVector &_f, Solution& sol);\n};\n\n#endif //CPP_LPBOXADMMSOLVER_H\n\n# ==========================================\n# File: LinerProgramming/LinearProgramming/trainer.py\n# Function/Context: PolicyKL._valid_1\n# ==========================================\nimport torch\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nfrom LinearProgramming.common.consts import DEVICE\nfrom LinearProgramming.common.utils import max_onehot\nimport math\n\nimport torch.nn as nn  \nimport os\nimport numpy as np \n\nfrom LinearProgramming.cython_solver import lpbox \nimport time\nfrom statistics import mean, stdev, variance  \nimport math \nfrom numpy import linalg as LA\nimport math \n\ndef deter_fix_4(sco_sigmoid, labels):\n    data = sco_sigmoid.cpu().detach().numpy() # 500,1 \n    fix_val = []\n    f1 = 0\n    f0 = 0\n    f = 0\n    C = 0.9\n    fix_labels = []\n    non_fix_labels = [] \n    count = 0\n    for i in range(len(data)):\n        if data[i] > C:\n            f1 = f1 + 1\n            fix_val.append(1.0)\n            fix_labels.append(labels[i])\n            if labels[i]!=1.0:\n                count+=1\n        elif data[i] < 1-C:\n            f0 = f0 + 1\n            fix_val.append(0.0)\n            fix_labels.append(labels[i])\n            if labels[i]!=0.0:\n                count+=1 \n        else:\n            fix_val.append(-1.0)\n            non_fix_labels.append(labels[i])\n    fix_val = np.array(fix_val)\n    non_fix_labels = np.array(non_fix_labels) \n    return fix_val, f1, f0, non_fix_labels, count \n\nclass PolicyKL:\n    def __init__(self, args, score_net, optimizer, scheduler):\n        self.args = args\n        self.score_net = score_net\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.epochs = args.num_epochs\n        self.var = self.args.var\n        self.start_epoch = self.args.start_epoch\n        self.ws = self.args.ws \n\n    # for small size instances\n    def _valid_1(self, f):\n        self.score_net.eval()\n        obj_list = []\n        time_list = []\n        iter_list = []\n        obj_gap_list = []\n        x_sol_gap_l1 = []\n        x_sol_gap_l2 = []\n\n        lpbox_obj_list = []\n        lpbox_time_list = []\n        lpbox_iter_list = []\n\n        x_sol_gap_list_1_to_0 = []\n        x_sol_gap_list_0_to_1 = []\n        x_sol_gap_list_pos = [] \n\n        lpbox_info = get_lpbox_info() #[instance, obj, iters, time]\n        \n        for it in range(100,110): #self.args.iters_per_eval\n            cnt = 0 \n            total_loss = 0.0 \n            solver = lpbox.PyLPboxADMMsolver(0)  # 0-donot print fix info; 1-print fix info; 2-get xiters.\n            solver.read_File(it,100,500) \n            solver.solve_init()\n            dataset = readFile(it)  \n            labels = getLabel(dataset) # get labels \n            left_labels = np.squeeze(labels, axis=-1) \n            max_iter = 2e4\n            window_size = self.ws # 100  \n            n = 0\n            vec = np.zeros([self.args.col], dtype=np.double)  # 1000 \n            time_start = time.time() \n            for i in range(int(max_iter/window_size)):\n                start = window_size * i\n                end = window_size * (i+1)\n                ret = solver.solve_iter_l2f(start, end, vec, n)\n                if ret:\n                    break\n                xiters = solver.get_x_iters_2d(self.ws) #  (n, window_size) = (500, 100)\n                a,b = xiters.shape\n                xiters = xiters.reshape(a, 20, int(b/20))\n                \n                input = torch.from_numpy(xiters.astype(np.float32)).to(DEVICE)\n                pred, pred_sigmoid = self.score_net(input)   # input=(500,20), output=(500,1)\n\n                vec, f1, f0, left_labels_tmp, error_count = deter_fix_4(pred_sigmoid, left_labels) # fix to 1: 1 \n                # / fix to 0: 0 / No fix: -1.\n                \n                n = f1 + f0\n                if n<=10: # do not fix. \n                    n = 0 \n                else:     # do fix.\n                    left_labels = left_labels_tmp\n                    cnt += error_count\n                if n>0:\n                    print(f\"In loop [{i}], fixed [{n}] elements; fix One: [{f1}], fix zero [{f0}]\") \n\n            time_end = time.time()\n            time_cost = time_end-time_start\n\n            inf = solver.check_infeasible_l2f() #\n\n# ==========================================\n# File: Segmentation/Segmentation/cython/src/LPboxADMMsolver.cpp\n# Function/Context: LPboxADMMsolver class and related functions\n# ==========================================\n// Core Lp-Box ADMM Implementation\n\n// Conjugate Gradient solver for ADMM subproblems\nvoid _conjugate_gradient(const SparseMatrix& mat, const DenseVector& rhs, DenseVector& x,\n\t\tconst Eigen::DiagonalPreconditioner<_double_t>& precond, int& iters, typename DenseVector::RealScalar& tol_error)\n{\n\n\ttypedef typename DenseVector::RealScalar RealScalar;\n\ttypedef typename DenseVector::Scalar Scalar;\n\ttypedef Eigen::Matrix<Scalar,Dynamic,1> VectorType;\n\n\tRealScalar tol = tol_error;\n\tint maxIters = iters;\n\n\tint n = mat.cols();\n\n\tVectorType residual = rhs - mat * x; //initial residual\n\n\tRealScalar rhsNorm2 = rhs.squaredNorm();\n\n\tif(rhsNorm2 == 0)\n\t{\n\t\tx.setZero();\n\t\titers = 0;\n\t\ttol_error = 0;\n\t\treturn;\n\t}\n\n\tconst RealScalar considerAsZero = (std::numeric_limits<RealScalar>::min)();\n\tRealScalar threshold = Eigen::numext::maxi(RealScalar(tol*tol*rhsNorm2),considerAsZero);\n\tRealScalar residualNorm2 = residual.squaredNorm();\n\n\tif (residualNorm2 < threshold)\n\t{\n\t\titers = 0;\n\t\ttol_error = sqrt(residualNorm2 / rhsNorm2);\n\t\treturn;\n\t}\n\n\tVectorType p(n);\n\tp = precond.solve(residual);      // initial search direction\n\n\tVectorType z(n), tmp(n);\n\tRealScalar absNew = Eigen::numext::real(residual.dot(p));  // the square of the absolute value of r scaled by invM\n\tint i = 0;\n\twhile(i < maxIters)\n\t{\n\t\ttmp.noalias() = mat * p;                    // the bottleneck of the algorithm\n\n\t\tScalar alpha = absNew / p.dot(tmp);         // the amount we travel on dir\n\n\t\tx += alpha * p;                             // update solution\n\n\t\tresidual -= alpha * tmp;                    // update residual\n\n\t\tresidualNorm2 = residual.squaredNorm();\n\n\t\tif(residualNorm2 < threshold) {\n\t\t\ti++;\n\t\t\tbreak;\n\t\t}\n\n\t\tz = precond.solve(residual);                // approximately solve for \"A z = residual\"\n\n\t\tRealScalar absOld = absNew;\n\t\tabsNew = Eigen::numext::real(residual.dot(z));     // update the absolute value of r\n\t\tRealScalar beta = absNew / absOld;          // calculate the Gram-Schmidt value used to create the new search direction\n\t\tp = z + beta * p;                           // update search direction\n\t\ti++;\n\t}\n\ttol_error = sqrt(residualNorm2 / rhsNorm2);\n\titers = i;\n\treturn;\n}\n\n// Projection functions for box constraints [0,1]\nvoid LPboxADMMsolver::project_vec_greater_than(DenseVector &x, DenseVector &res, const int &greater_val, const int &set_val) {\n\tint len = x.size();\n\tfor (int i = 0; i < len; i++) {\n\t\tres[i] = x[i] > greater_val ? set_val : x[i];\n\t}\n}\n\nvoid LPboxADMMsolver::project_vec_less_than(DenseVector &x, DenseVector &res, const int &less_val, const int &set_val) {\n\tint len = x.size();\n\tfor (int i = 0; i < len; i++) {\n\t\tres(i) = x(i) < less_val ? set_val : x(i);\n\t}\n}\n\nvoid LPboxADMMsolver::project_box(int n, const double *x, double *y) {\n\ty = new double[n];\n\twhile (n--) {\n\t\tif (x[n] > 1) {\n\t\t\ty[n] = 1;\n\t\t} else {\n\t\t\tif (x[n] < 0) {\n\t\t\t\ty[n] = 0;\n\t\t} else {\n\t\t\t\ty[n] = x[n];\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Problem formulation for image segmentation\nvoid get_A_b_from_cost(const DenseMatrix &unary_cost, const SparseMatrix &binary_cost, SparseMatrix &A, DenseVector &b, _double_t &c) {\n\tint n = binary_cost.rows();\n\t\n\tDenseMatrix U1 = get_row(unary_cost, 0);\n\tDenseMatrix U2 = get_row(unary_cost, 1);\n\n\tb = (U2 - U1).eval().transpose();\n\t\n\tA = - binary_cost;\n\tDenseVector ones = DenseVector::Ones(binary_cost.cols());\n\tDenseVector We = - A * ones;\n\tA.diagonal().array() += We.array();\n\tA = 2 * A;\n\n\tc = U1.array().sum();\n}\n\n# ==========================================\n# File: Segmentation/Segmentation/cython/src/LPboxADMMsolver.h\n# Function/Context: LPboxADMMsolver::ADMM_bqp_unconstrained_l2f\n# ==========================================\n// Header includes and type definitions\n#include <vector>\n#include <Eigen4/Sparse>\n#include <Eigen4/Dense>\n\n#define Dynamic Eigen::Dynamic\ntypedef double _double_t;\ntypedef Eigen::SparseMatrix<_double_t, Eigen::RowMajorBit> SparseMatrix;\ntypedef Eigen::Matrix<_double_t, Dynamic, Dynamic> DenseMatrix;\ntypedef Eigen::Matrix<_double_t, Dynamic, 1> DenseVector;\ntypedef Eigen::Matrix<int, Dynamic, 1> DenseIntVector;\n\n/* Stores the solution return by the algorithm */\nstruct Solution {\n\tDenseVector *best_sol;\n\tDenseVector *x_sol;\n\tDenseVector *y1;\n\tDenseVector *y2;\n\tlong time_elapsed;\n};\n\nclass LPboxADMMsolver {\nprivate:\n\t/* Parameters for ADMM convergence */\n\t_double_t stop_threshold;\n\t_double_t std_threshold;\n\tint max_iters;\n\t_double_t initial_rho;\n\tint rho_change_step;\n\t_double_t gamma_val;\n\t_double_t learning_fact;\n\t_double_t history_size;\n\t_double_t projection_lp;\n\t_double_t gamma_factor;\n\t_double_t pcg_tol;\n\tint pcg_maxiters;\n\t_double_t rho_upper_limit;\n\t_double_t rel_tol;\n\n\t/* Problem data */\n\tint m;\n\tint n;\n\t_double_t _c;\n\tDenseVector x0, x_sol, y1, y2, z1, z2, prev_idx, best_sol, temp_vec, cur_idx, temp;\n\tDenseVector x_prev;\n\tSparseMatrix temp_mat;\n\tDenseVector temp_vec_for_cg, temp_vec_for_mat_mul, temp_vec_for_mat_mul1, temp_vec_for_mat_mul2;\n\t_double_t cur_obj;\n\tbool rhoUpdated = true;\n\tEigen::DiagonalPreconditioner<_double_t> diagonalPreconditioner;\n\t_double_t rho1, rho2, rho3, rho4, prev_rho1, prev_rho2, prev_rho3, prev_rho4;\n\tstd::vector<_double_t> obj_list;\n\t_double_t std_obj = 1;\n\t_double_t cvg_test1;\n\t_double_t cvg_test2;\n\t_double_t rho_change_ratio=1;\n\t_double_t best_bin_obj;\n\n\t/* Early fixing related members */\n\tDenseMatrix x_iters; //for the iterations of x_sol;\n\tint org_n;\n\tint fix_n, fix_sum;\n\tDenseVector fix_idx, non_fix_idx, left_idx;\n\tDenseVector org_fix_val, org_fix_idx, org_non_fix_idx;\n\tDenseVector ret_idx, ret_val;\n\tDenseVector ret_idx_prev, ret_val_prev;\n\tDenseVector b1,b2,x2;\n\tSparseMatrix Ma, Mb, Mc, Md;\n\tSparseMatrix Mab;\n\tDenseVector f1;\n\t_double_t fix_obj, sum_fix_obj;\n\n\tint node;\n\tint does_log = 0;\n\tstd::string log_file_path;\n\tstd::string xiter;\n\tstd::string xiter_all;\n\tstd::string output;\n\tint print_info;\n\tint problem;\n\n\t/* Helper functions for projections */\n\tvoid project_box(int n, const double *x, double *y);\n\tvoid project_box(int n, const DenseVector &x, DenseVector &y);\n\tvoid project_shifted_Lp_ball(int n, double *x, int p, double *y);\n\tvoid project_shifted_Lp_ball(int n, const DenseVector &x, int p, DenseVector &y);\n\t_double_t compute_cost(const DenseVector &x, const SparseMatrix &A, const DenseVector &b);\n\t_double_t compute_cost(const DenseVector &x, const SparseMatrix &A, const DenseVector &b, DenseVector &temp_vec_for_mat_mul);\n\t_double_t compute_std_obj(std::vector<_double_t> obj_list, int history_size);\n\t_double_t std_dev(std::vector<_double_t>& arr, size_t begin, size_t end);\n\npublic:\n\tLPboxADMMsolver();\n\tLPboxADMMsolver(int node);\n\tLPboxADMMsolver(int node, int problem);\n\tLPboxADMMsolver(int print_info, int node, int problem);\n\n\t/* Core ADMM implementation with early fixing */\n\tint ADMM_bqp_unconstrained_l2f(int iter_start, int iter_end, double* vec, int fix_num);\n\tvoid ADMM_bqp_unconstrained_init();\n\tint ADMM_bqp_unconstrained_legacy();\n\n\t/* Utility functions */\n\tdouble* get_x_iters_d(int ws);\n\tint get_n(){ return this->n; }\n\tint get_org_n(){ return this->org_n; }\n\tdouble cal_obj();\n\tdouble* get_x_sol();\n\tdouble get_final_obj();\n\n\t/* Parameter setters */\n\tvoid set_stop_threshold(_double_t stop_threshold) { this->stop_threshold = stop_threshold; }\n\tvoid set_std_threshold(_double_t std_threshold) { this->std_threshold = std_threshold; }\n\tvoid set_max_iters(int max_iters) { this->max_iters = max_iters; }\n\tvoid set_initial_rho(_double_t initial_rho) { this->initial_rho = initial_rho; }\n\tvoid set_rho_change_step(_double_t rho_change_step) { this->rho_change_step = rho_change_step; }\n\tvoid set_gamma_val(_double_t gamma_val) { this->gamma_val = gamma_val; }\n\tvoid set_learning_fact(_double_t learning_fact) { this->learning_fact = learning_fact; }\n\tvoid set_history_size(_double_t history_size) { this->history_size = history_size; }\n\tvoid set_projection_lp(_double_t projection_lp) { this->projection_lp = projection_lp; }\n\tvoid set_gamma_factor(_double_t gamma_factor) { this->gamma_factor = gamma_factor; }\n\tvoid set_pcg_tol(_double_t pcg_tol) { this->pcg_tol = pcg_tol; }\n\tvoid set_rho_upper_limit(_double_t rho_upper_limit) { this->rho_upper_limit = rho_upper_limit; }\n\tvoid set_rel_tol(_double_t rel_tol) { this->rel_tol = rel_tol; }\n\tvoid set_pcg_maxiters(int pcg_maxiters) { this->pcg_maxiters = pcg_maxiters; }\n};\n\n/* Vector utility functions */\ninline void vec_add(int n, double *a, double *b, double *c) {\n\tfor (int i = 0; i < n; i++) {\n\t\tc[i] = a[i] + b[i];\n\t}\n}\n\ninline void vec_sub(int n, double *a, double *b, double *c) {\n\tfor (int i = 0; i < n; i++) {\n\t\tc[i] = a[i] - b[i];\n\t}\n}\n\ninline void vec_set(int n, double* a, const double val) {\n\tfor (int i = 0; i < n; i++) {\n\t\ta[i] = val;\n\t}\n}\n\n# ==========================================\n# File: SparseAttack/SparseAttack/mha.py\n# Function/Context: GraphAttentionEncoder, MLPEncoder, FixingMLPEncoder\n# ==========================================\nimport torch\nimport numpy as np\nfrom torch import nn\nimport math\n\nfrom SparseAttack.common.consts import DEVICE\nfrom SparseAttack.common.utils import position_encoding\n\n\nclass SkipConnection(nn.Module):\n\n    def __init__(self, module):\n        super(SkipConnection, self).__init__()\n        self.module = module\n\n    def forward(self, input):\n        return input + self.module(input)\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(\n            self,\n            n_heads,\n            input_dim,\n            embed_dim,\n            val_dim=None,\n            key_dim=None\n    ):\n        super(MultiHeadAttention, self).__init__()\n\n        if val_dim is None:\n            val_dim = embed_dim // n_heads\n        if key_dim is None:\n            key_dim = val_dim\n\n        self.n_heads = n_heads\n        self.input_dim = input_dim\n        self.embed_dim = embed_dim\n        self.val_dim = val_dim\n        self.key_dim = key_dim\n\n        self.norm_factor = 1 / math.sqrt(key_dim)  # See Attention is all you need\n\n        self.W_query = nn.Parameter(torch.Tensor(n_heads, input_dim, key_dim))\n        self.W_key = nn.Parameter(torch.Tensor(n_heads, input_dim, key_dim))\n        self.W_val = nn.Parameter(torch.Tensor(n_heads, input_dim, val_dim))\n\n        self.W_out = nn.Parameter(torch.Tensor(n_heads, val_dim, embed_dim))\n\n        self.init_parameters()\n\n    def init_parameters(self):\n\n        for param in self.parameters():\n            stdv = 1. / math.sqrt(param.size(-1))\n            param.data.uniform_(-stdv, stdv)\n\n    def forward(self, q, h=None, mask=None):\n        \"\"\"\n\n        :param q: queries (batch_size, n_query, input_dim)\n        :param h: data (batch_size, graph_size, input_dim)\n        :param mask: mask (batch_size, n_query, graph_size) or viewable as that (i.e. can be 2 dim if n_query == 1)\n        Mask should contain 1 if attention is not possible (i.e. mask is negative adjacency)\n        :return:\n        \"\"\"\n        if h is None:\n            h = q  # compute self-attention\n\n        # h should be (batch_size, graph_size, input_dim)\n        batch_size, graph_size, input_dim = h.size()\n        n_query = q.size(1)\n        assert q.size(0) == batch_size\n        assert q.size(2) == input_dim\n        assert input_dim == self.input_dim, \"Wrong embedding dimension of input\"\n\n        hflat = h.contiguous().view(-1, input_dim)\n        qflat = q.contiguous().view(-1, input_dim)\n\n        # last dimension can be different for keys and values\n        shp = (self.n_heads, batch_size, graph_size, -1)\n        shp_q = (self.n_heads, batch_size, n_query, -1)\n\n        # Calculate queries, (n_heads, n_query, graph_size, key/val_size)\n        Q = torch.matmul(qflat, self.W_query).view(shp_q)\n        # Calculate keys and values (n_heads, batch_size, graph_size, key/val_size)\n        K = torch.matmul(hflat, self.W_key).view(shp)\n        V = torch.matmul(hflat, self.W_val).view(shp)\n\n        # Calculate compatibility (n_heads, batch_size, n_query, graph_size)\n        compatibility = self.norm_factor * torch.matmul(Q, K.transpose(2, 3))\n\n        # Optionally apply mask to prevent attention\n        if mask is not None:\n            mask = mask.view(1, batch_size, n_query, graph_size).expand_as(compatibility)\n            compatibility[mask] = -np.inf\n\n        attn = torch.softmax(compatibility, dim=-1)\n\n        # If there are nodes with no neighbours then softmax returns nan so we fix them to 0\n        if mask is not None:\n            attnc = attn.clone()\n            attnc[mask] = 0\n            attn = attnc\n\n        heads = torch.matmul(attn, V)\n\n        out = torch.mm(\n            heads.permute(1, 2, 0, 3).contiguous().view(-1, self.n_heads * self.val_dim),\n            self.W_out.view(-1, self.embed_dim)\n        ).view(batch_size, n_query, self.embed_dim)\n\n        return out\n\n\nclass Normalization(nn.Module):\n\n    def __init__(self, embed_dim, normalization='batch'):\n        super(Normalization, self).__init__()\n\n        normalizer_class = {\n            'batch': nn.BatchNorm1d,\n            'instance': nn.InstanceNorm1d\n        }.get(normalization, None)\n\n        self.normalizer = normalizer_class(embed_dim, affine=True)\n\n    def forward(self, input):\n\n        if isinstance(self.normalizer, nn.BatchNorm1d):\n            return self.normalizer(input.view(-1, input.size(-1))).view(*input.size())\n        elif isinstance(self.normalizer, nn.InstanceNorm1d):\n            return self.normalizer(input.permute(0, 2, 1)).permute(0, 2, 1)\n        else:\n            assert self.normalizer is None, \"Unknown normalizer type\"\n            return input\n\n\nclass MultiHeadAttentionLayer(nn.Sequential):\n\n    def __init__(\n            self,\n            n_heads,\n            embed_dim,\n            feed_forward_hidden=512,\n            normalization='batch',\n    ):\n        super(MultiHeadAttentionLayer, self).__init__(\n            SkipConnection(\n                MultiHeadAttention(\n                    n_heads,\n                    input_dim=embed_dim,\n                    embed_dim=embed_dim\n                )\n            ),\n            Normalization(embed_dim, normalization),\n            SkipConnection(\n                nn.Sequential(\n                    nn.Linear(embed_dim, feed_forward_hidden),\n                    nn.ReLU(),\n                    nn.Linear(feed_forward_hidden, embed_dim)\n                ) if feed_forward_hidden > 0 else nn.Linear(embed_dim, embed_dim)\n            ),\n            Normalization(embed_dim, normalization)\n        )\n\n\nclass GraphAttentionEncoder(nn.Module):\n    def __init__(\n            self,\n            n_heads=8,\n            embed_dim=128,\n            n_layers=2,\n            node_dim=5+5,  \n            normalization='batch',\n            feed_forward_hidden=512\n    ):\n        super(GraphAttentionEncoder, self).__init__()\n\n        # To map input to embedding space\n        self.init_embed = nn.Linear(node_dim, embed_dim)\n\n        self.layers = nn.Sequential(*(\n            MultiHeadAttentionLayer(n_heads, embed_dim, feed_forward_hidden, normalization)\n            for _ in range(n_layers)\n        ))\n\n        self.classify = Net2()\n\n    def forward(self, x, mask=None):\n        assert mask is None, \"TODO mask not yet supported!\"\n\n        batch_size, node_size, dim_size = x.shape  # (5*500, 10, 5)\n\n        # positional encoding\n        position_enc = position_encoding(node_size, 5).to(DEVICE)  # (10,5)\n        pe = torch.stack([position_enc for _ in range(batch_size)], dim=0).cuda() # (2500, 10, 5)\n        x = torch.cat([x, pe], dim=-1)  # (2500, 10, 5+5) \n\n        # Batch multiply to get initial embeddings of nodes \n        h = self.init_embed(x.view(-1, x.size(-1))).view(*x.size()[:2], -1)\n\n        h = self.layers(h)  # (2500, 10, 128)\n\n        a,b,c = h.shape  # (2500, 10, 128)\n        h = h.reshape(a,b*c) #  (2500, 10*128)\n        h, sig_h = self.classify(h)  #(2500, 1)\n\n        return h, sig_h\n\n\nclass MLPEncoder(nn.Module):\n    def __init__(\n            self,\n            n_heads=8,\n            embed_dim=128,\n            n_layers=2,\n            node_dim=5+5,  \n            normalization='batch',\n            feed_forward_hidden=512\n    ):\n        super(MLPEncoder, self).__init__()\n\n        # To map input to embedding space\n        self.init_embed = nn.Linear(node_dim, embed_dim)\n\n        self.classify = Net2()\n\n    def forward(self, x, mask=None):\n        assert mask is None, \"TODO mask not yet supported!\"\n\n        batch_size, node_size, dim_size = x.shape  # (5*500, 20, 5)\n\n        # positional encoding\n        position_enc = position_encoding(node_size, 5).to(DEVICE)  # (10,5)\n        pe = torch.stack([position_enc for _ in range(batch_size)], dim=0).cuda() # (2500, 10, 5)\n        x = torch.cat([x, pe], dim=-1) # (2500, 10, 5+5) \n\n        # Batch multiply to get initial embeddings of nodes \n        h = self.init_embed(x.view(-1, x.size(-1))).view(*x.size()[:2], -1)\n\n        a,b,c = h.shape  # (2500, 10, 128)\n        h = h.reshape(a,b*c) #  (2500, 10*128)\n        h, sig_h = self.classify(h)  #(2500, 1)\n\n        return h, sig_h\n\n\nclass FixingMLPEncoder(nn.Module):\n    def __init__(\n            self,\n            n_heads=8,\n            embed_dim=128,\n            n_layers=2,\n            node_dim=5+5,  \n            normalization='batch',\n            feed_forward_hidden=512\n    ):\n        super(FixingMLPEncoder, self).__init__()\n\n        # To map input to embedding space\n        self.init_embed = nn.Linear(node_dim, embed_dim)\n\n        self.classify = Net_fixing()\n\n    def forward(self, x, mask=None):\n        assert mask is None, \"TODO mask not yet supported!\"\n\n        batch_size, node_size, dim_size = x.shape  # (5*500, 20, 5)\n\n        # positional encoding\n        position_enc = position_encoding(node_size, 5).to(DEVICE)  # (20,5)\n        pe = torch.stack([position_enc for _ in range(batch_size)], dim=0) # (2500, 20, 5)\n        x = torch.cat([x, pe], dim=-1)  # (2500, 20, 5+5) \n\n        # Batch multiply to get initial embeddings of nodes \n        h = self.init_embed(x.view(-1, x.size(-1))).view(*x.size()[:2], -1)\n        # (2500, 20, 5+5) -> (2500, 20, 128)\n\n        a,b,c = h.shape  # (2500, 20, 128)\n        h = h.reshape(a,b*c) #  (2500, 20*128)\n        h, sig_h = self.classify(h)  #(2500, 1)\n\n        return h, sig_h\n\n# ==========================================\n# File: SparseAttack/SparseAttack/trainer.py\n# Function/Context: PolicyKL\n# ==========================================\nimport torch\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nfrom SparseAttack.common.consts import DEVICE\nfrom SparseAttack.common.utils import max_onehot\nimport math\n\nimport torch.nn as nn  \nimport os\nimport numpy as np \n\n# from Segmentation.cython.src import lpbox # use the Cython from Segmentation \nimport time\nfrom statistics import mean \nimport math \nfrom numpy import linalg as LA\nimport math \n\nimport glob \n\n\ndef MyMSE(x_hat, x):\n    return torch.sum((x_hat - x) ** 2, dim=-1).view(-1)\n\ndef sgm(v, s):\n    l = len(v)\n    # print(l)\n    res = 1\n    for i in range(l):\n        v[i] = max(1, v[i]+s)\n        res = res * v[i]\n    # v = sum(v)\n    # print(v)\n    v = pow(res, 1.0/l)\n    v = v -s \n    return v\n\ndef readFile(file):\n    f = open(file)\n    line = f.readline()\n    xiters = []\n    while line:\n        xiter = line.split(',')\n        xiter = xiter[1:-1]\n        xiter = list(map(float, xiter))\n        xiters.append(xiter)\n        line = f.readline()\n    xiters = np.array(xiters)       # (8000, 500)\n    xiterss = np.transpose(xiters)  # (500. 8000)\n    # print(xiters.shape)\n    # print(f\"Readfile: {file} \",xiterss.shape)\n    return xiterss\n\nimport collections \ndef getLabel(dataset):\n    label = []\n    for i in range(len(dataset)):\n        if(dataset[i,-1]>=0.5):\n            label.append(1.0)\n        else:\n            label.append(0.0)\n    # print(\"Label length: \",len(label))\n    # tmp = collections.Counter(label)\n    # print(tmp)\n    label = np.array(label)\n    label = label[:, np.newaxis]\n    # print(\"Labels: \", label.shape)\n    return label \n\ndef getSubset(data, idx, ws):\n    # idx = 1 # sub set index\n    # r = 10  # remainder\n    # step_size = 25\n    # subdata = data[:,(idx-1)*500+r:idx*500:step_size] # from (idx-1)*500, to idx*500, every 25 pick 1 value. \n    subdata = data[:,(idx-1)*ws:idx*ws]\n    # print(\"subset size: \",subdata.shape)  # (500, 20),  (500,500)\n    return subdata \n\ndef deter_fix_2(sco_sigmoid):\n    data = sco_sigmoid.cpu().detach().numpy() # 500,1 \n    fix_val = []\n    f1 = 0\n    f0 = 0\n    f = 0\n    C = 0.9\n    for i in range(len(data)):\n        if data[i] > C:\n            f1 = f1 + 1\n            fix_val.append(1.0)\n\n        elif data[i] < 1-C:\n            f0 = f0 + 1\n            fix_val.append(0.0)\n        else:\n            fix_val.append(-1.0)\n    fix_val = np.array(fix_val)\n    return fix_val, f1, f0 \n\n\nclass PolicyKL:\n    def __init__(self, args, score_net, optimizer, scheduler):#, fix_net, optimizer2, scheduler2\n        self.args = args\n        self.score_net = score_net\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.epochs = args.num_epochs\n        self.var = self.args.var\n        self.start_epoch = self.args.start_epoch\n        self.ws = self.args.ws \n        # self.fix_net = fix_net\n        # self.optimizer2 = optimizer2\n        # self.scheduler2 = scheduler2\n\n    # Fucntions: get fixing vectors  \n    def _get_fix_vec(self, input):\n        if input.shape[0] <= 20000:\n            print(\"#### this is in IF: input shape < 2w\")\n            pred, pred_sigmoid = self.score_net(input)\n            tmp = pred_sigmoid.cpu().detach().numpy()\n            sigm = pred_sigmoid.cpu().detach().numpy()\n\n            vec_ret, f1_ret, f0_ret = deter_fix_2(pred_sigmoid)\n        else:\n            print(\"#### this is in ELSE: input shape > 2w\")\n            # print(\"#### shape is greater than 20,000.\")\n            batch_size = 10000\n            batches = math.ceil(input.shape[0]/batch_size)\n            for i in range(batches):\n                if i==batches:\n                    end = input.shape[0]\n                else:\n                    end = batch_size * (i+1)\n                start =  batch_size * i\n                input_batch = input[start:end]    \n                pred, pred_sigmoid = self.score_net(input_batch)\n                # pred_sigmoid = pred_sigmoid.cpu().detach().numpy()\n                vec, f1, f0 = deter_fix_2(pred_sigmoid)\n                # print(\"this is one batch shape: \", pred_sigmoid.shape)\n\n                if i == 0: \n                    # prediction = pred_sigmoid\n                    vec_ret = vec \n                    f1_ret = f1\n                    f0_ret = f0 \n                else:\n                    # prediction = np.concatenate((prediction, pred_sigmoid), axis=0)\n                    vec_ret = np.concatenate((vec_ret, vec), 0)\n                    f1_ret += f1\n                    f0_ret += f0 \n\n            print(\"prediction shape:\", vec_ret.shape)\n        return vec_ret, f1_ret, f0_ret \n\n\n    def _train_mha_100(self, epoch):\n        print(\"Epoch:%d, Learning rate: %f\" % (epoch, self.optimizer.param_groups[0]['lr']))\n        # n outputs, n-1 nets\n        self.score_net.train()\n        loss_list = []   \n        files_list = glob.glob('../xiter/*')\n        for it in tqdm(files_list):\n            total_loss = 0.0 \n            best_loss = 0.0\n            # generate path\n            dataset = readFile(it)\n            labels = getLabel(dataset)\n            sa, sb = dataset.shape\n            # print(f\"shape of dataset: {dataset.shape}\")\n            tmp = np.sum(labels)\n            # print(\"tmp: \", tmp)\n\n            labels = torch.from_numpy(labels.astype(np.float32)).to(DEVICE) # (500)\n\n\n            # left_idx = np.arange(500)\n            # rest_idx = left_idx \n            # all_idx = []\n            # all_val = []\n\n            self.optimizer.zero_grad()\n\n            # Method 2: contencate all 10 episodes as 1 batch. \n            cont_set = []\n            weight = [] \n            for i in range(1,4):\n                subset = getSubset(dataset, i, self.ws)\n                cont_set.append(subset)\n                weight_cur = sa * [1.0/i] # column size = 500\n                weight = weight + weight_cur\n            weight = np.array(weight)  \n            weight = weight[:, np.newaxis]\n            cont_set = np.array(cont_set)\n            # cont_set = cont_set[:, np.newaxis]\n            a,b,c = cont_set.shape # (10,500,ws)\n            # print(cont_set.shape)\n            cont_set = cont_set.reshape(a*b, c) # (10*500, ws)\n            # print(cont_set.shape)\n            \n            labels = labels.tile((3,1))  #  (10*500, ws)\n\n            # tmp = np.zeros((a*b, 20, 5))\n            # for i in range(a*b):\n            #     for j in range(20):\n            #         tmp[i, j, :] =  cont_set[i, j:(j+5)]\n            # # print(tmp.shape)\n            # cont_set = tmp \n\n            cont_set = cont_set.reshape(a*b, 10, 5) # new Reshape: (10*500, ws) -> (10*ws, 20, 5)\n            cont_set = torch.from_numpy(cont_set.astype(np.float32)).to(DEVICE)\n            weight = torch.from_numpy(weight.astype(np.float32)).to(DEVICE)\n            # print(f\"Weight shape: {weight.shape}; input data shape: {cont_set.shape}; label shape: {labels.shape}\")\n\n            pred, sco_sigmoid = self.score_net(cont_set)\n            # print(f\"pre shape: {pred.shape}; sco_sigmoid shape: {sco_sigmoid.shape}.\")\n            loss_fn = nn.BCEWithLogitsLoss(weight=weight) \n            loss = loss_fn(pred, labels)\n            loss.backward()\n            self.optimizer.step()\n            self.scheduler.step()\n            total_loss += loss.item()\n\n            # print(f\"Problem [{it}]; Total loss: [{total_loss}];\")\n            loss_list.append(total_loss)       \n            \n        return mean(loss_list)\n\n    def _train_mlp_100(self, epoch):\n        print(\"Epoch:%d, Learning rate: %f\" % (epoch, self.optimizer.param_groups[0]['lr']))\n        # n outputs, n-1 nets\n        self.score_net.train()\n        loss_list = []   \n        files_list = glob.glob('../xiter/*')\n        # print(files_list)\n        for it in tqdm(files_list):\n            total_loss = 0.0 \n            best_loss = 0.0\n            # generate path\n            dataset = readFile(it)\n            labels = getLabel(dataset)\n            sa, sb = dataset.shape\n            labels = torch.from_numpy(labels.astype(np.float32)).to(DEVICE) # (500)\n\n            self.optimizer.zero_grad()\n\n            # Method 2: contencate all 5 episodes as 1 batch. \n            cont_set = []\n            weight = [] \n            for i in range(1,4):\n                subset = getSubset(dataset, i, self.ws)\n                cont_set.append(subset)\n                weight_cur = sa * [1.0/i] # column size = 500\n                weight = weight + weight_cur\n            weight = np.array(weight)  \n            weight = weight[:, np.newaxis]\n            cont_set = np.array(cont_set)\n            a,b,c = cont_set.shape # (10,500,ws)\n            # print(a,b,c)\n            cont_set = cont_set.reshape(a*b, c) # (10*500, ws)\n            labels = labels.tile((3,1))  #  (10*500, ws)\n            \n            cont_set = cont_set.reshape(a*b, 10, 5) # new Reshape: (10*500, ws) -> (10*ws, 20, 5)\n            \n            # tmp = np.zeros((a*b, 20, 5))\n            # for i in range(a*b):\n            #     for j in range(20):\n            #         tmp[i, j, :] =  cont_set[i, j:(j+5)]\n            # # print(tmp.shape)\n            # cont_set = tmp \n            \n            cont_set = torch.from_numpy(cont_set.astype(np.float32)).to(DEVICE)\n            weight = torch.from_numpy(weight.astype(np.float32)).to(DEVICE)\n            # print(f\"Weight shape: {weight.shape}; input data shape: {cont_set.shape}; label shape: {labels.shape}\\n {cont_set[0]}\")\n\n            pred, sco_sigmoid = self.score_net(cont_set)\n            # print(f\"pre shape: {pred.shape}; sco_sigmoid shape: {sco_sigmoid.shape}.\")\n            loss_fn = nn.BCEWithLogitsLoss(weight=weight) \n            loss = loss_fn(pred, labels)\n            loss.backward()\n            self.optimizer.step()\n            self.scheduler.step()\n            total_loss += loss.item()\n            # print(f\"Problem [{it}]; Total loss: [{total_loss}];\")\n            loss_list.append(total_loss)       \n            \n        return mean(loss_list)\n\n    # train init | single net | including: _train_mha_100, _train_mlp_100  \n    def train(self):\n        \"\"\"\n        training logic\n        :return:\n        \"\"\"\n        print(\"This is in training\")\n        best_val_loss = None\n        progress_bar = tqdm(range(self.start_epoch, self.start_epoch+self.epochs))\n\n        f = open(self.args.save_dir + \"log/log.txt\", 'a')\n        for epoch in progress_bar:\n            # train\n            if self.args.net == 'mlp':\n                train_loss = self._train_mlp_100(epoch)\n            else:\n                train_loss = self._train_mha_100(epoch)\n            f.write(f\"For Epoch {epoch}, Training Loss Average=[{train_loss}]\\n\")\n            print(f\"For Epoch {epoch}, Training Loss Average=[{train_loss}]\")\n            # if best_val_loss is None or train_loss < best_val_loss:\n            #     print(\"Save NEW best SeqNet in Training!\")\n            #     best_val_loss = train_loss\n            #     checkpoint =  {\n            #         \"net\": self.score_net.state_dict(),\n            #         \"optimizer\": self.optimizer.state_dict(),\n            #         \"epoch\": epoch \n            #     }\n            #     torch.save(checkpoint, self.args.save_dir + 'checkpoint/best_checkpoint_ws100_weighted_MHA_train.cp')\n\n            # validation\n            # if epoch >= 100:\n            #     f.write(f\"In validation epoch [{epoch}]\\n\")",
  "description": "Combined Analysis:\n- [LinerProgramming/LinearProgramming/cython_solver/LPboxADMMsolver.cpp]: This file implements the core Lp-Box ADMM solver for Integer Programming problems. It contains: 1) The LPboxADMMsolver class with projection operators (box and Lp-ball), 2) Objective computation for quadratic programming (x^T A x + b^T x), 3) Conjugate Gradient linear system solver with preconditioning, 4) ADMM parameter initialization, 5) Early fixing infrastructure via consistency and fix_threshold parameters. The code implements the mathematical model: maximize x^T A x + b^T x subject to x âˆˆ {0,1}^n through ADMM iterations with Lp-ball relaxations. The solver uses Eigen for sparse linear algebra and implements optimization techniques like noalias() for efficient matrix operations.\n- [LinerProgramming/LinearProgramming/cython_solver/LPboxADMMsolver.h]: This header file implements the core LP-Box ADMM solver with early fixing capabilities for integer programming problems. It provides the mathematical framework for solving binary quadratic programming (BQP) problems with various constraint types (unconstrained, equality, inequality, mixed). The class contains the complete ADMM iteration logic, projection operations, and early fixing mechanisms described in the paper. Key components include: 1) ADMM update variables (x_sol, y1, y2, z1, z2), 2) Early fixing data structures (x_iters, fix_idx, fix_threshold), 3) Specialized methods for different problem types, 4) Learning-based early fixing interface (ADMM_lp_iters_l2f). The implementation uses Eigen for sparse/dense matrix operations and follows the â„“p-box ADMM algorithm with extensions for variable fixing.\n- [LinerProgramming/LinearProgramming/trainer.py]: This file implements the core early fixing algorithm described in the paper. The PolicyKL._valid_1 method demonstrates the iterative early fixing process: 1) It runs the Lp-Box ADMM solver in windows of iterations (window_size), 2) At each window, it extracts the variable trajectories (xiters), 3) Passes them through the trained policy network (score_net) to get predictions, 4) Uses deter_fix_4 to decide which variables to fix (to 0 or 1) based on a confidence threshold (C=0.9), 5) Passes the fixing decisions back to the solver via solve_iter_l2f. This matches the paper's description of learning to early fix variables during optimization to accelerate convergence.\n- [Segmentation/Segmentation/cython/src/LPboxADMMsolver.cpp]: This file implements the core Lp-Box ADMM solver for integer programming problems, specifically for image segmentation applications. The code contains: 1) Conjugate Gradient solver for ADMM subproblems (optimizing quadratic objectives), 2) Projection operators for enforcing box constraints [0,1] (critical for binary variables), 3) Problem formulation functions that convert image segmentation costs into quadratic IP form (A, b, c). While this file doesn't show the complete ADMM iteration loop or the early fixing mechanism (which would be in a higher layer), it implements the fundamental mathematical optimization components that the early fixing framework accelerates.\n- [Segmentation/Segmentation/cython/src/LPboxADMMsolver.h]: This header file implements the core Lp-Box ADMM solver for binary quadratic programming (BQP) with early fixing capabilities. The class LPboxADMMsolver provides the mathematical optimization framework for solving problems of the form min_x (x^T A x + b^T x) subject to x âˆˆ {0,1}^n. Key aspects include: 1) ADMM iterations with â„“p-box projections (project_shifted_Lp_ball), 2) Early fixing infrastructure through members like fix_idx, non_fix_idx, and x_iters to track variable evolution, 3) The ADMM_bqp_unconstrained_l2f method specifically implements the learning-to-fix logic by accepting fixation instructions (vec, fix_num) over iteration windows (iter_start, iter_end). The solver uses Eigen for sparse/dense linear algebra and implements standard ADMM steps with convergence monitoring via objective value history (obj_list, compute_std_obj).\n- [SparseAttack/SparseAttack/mha.py]: This file implements the core policy network architectures for the early fixing framework described in the paper. It contains three main encoder classes: GraphAttentionEncoder (with attention layers), MLPEncoder (without attention), and FixingMLPEncoder (for fixing decisions). These networks process the state of variables during ADMM iterations (with positional encoding) and output decisions about early fixing. The MultiHeadAttention and related classes implement the attention mechanism that allows the network to capture dependencies between variables. This directly implements the 'policy network with attention layers' mentioned in the algorithm steps, which is trained via imitation learning to decide when to fix variables early during the optimization process.\n- [SparseAttack/SparseAttack/trainer.py]: This file implements the core training logic for the early fixing policy network described in the paper. The PolicyKL class trains a neural network (either MLP or MHA with attention layers) using imitation learning (behavior cloning) to predict which variables can be fixed early during the Lp-Box ADMM optimization process. Key components include: 1) Reading ADMM iteration trajectories from files, 2) Creating training data by extracting subsets of iteration windows, 3) Training the network with weighted BCE loss to predict final binary values from intermediate ADMM states, 4) Implementing early fixing decisions through the _get_fix_vec method which uses threshold-based fixing (0.9 for 1, 0.1 for 0). The implementation directly corresponds to the paper's Markov decision process formulation and imitation learning approach for accelerating integer programming solvers.",
  "dependencies": [
    "Eigen/Sparse",
    "cmath",
    "torch.stack",
    "nn.BatchNorm1d",
    "string",
    "deter_fix_4",
    "nn.InstanceNorm1d",
    "readFile",
    "numpy.linalg",
    "get_binary_cost",
    "nn.Module",
    "fstream",
    "Eigen/IterativeLinearSolvers",
    "Standard C++ libraries (vector, string)",
    "generate_pixel_pairs",
    "time",
    "unsupported/Eigen/IterativeSolvers",
    "torch.cat",
    "Eigen4/Sparse",
    "opencv2/core/eigen.hpp",
    "torch.nn.functional",
    "Custom structures (Solution, MatrixInfo, SolverInstruction)",
    "SparseAttack.common.consts",
    "vector",
    "LPboxADMMsolver.h",
    "get_unary_cost",
    "Eigen4/Dense",
    "math",
    "tqdm",
    "get_lpbox_info",
    "Eigen/Dense",
    "vectorize",
    "mat_mul_vec",
    "nn.functional.relu",
    "Eigen library (Sparse and Dense modules)",
    "nn.Sequential",
    "nn.Linear",
    "torch.matmul",
    "torch.softmax",
    "os",
    "Helper functions (vec_add, vec_sub, vec_set)",
    "statistics",
    "calculate_mat_expr_multiplication",
    "SparseAttack.common.utils.position_encoding",
    "chrono",
    "getLabel",
    "torch.sigmoid",
    "LinearProgramming.cython_solver.lpbox",
    "numpy",
    "torch.Tensor",
    "nn.Parameter",
    "glob",
    "collections",
    "torch.nn",
    "SparseAttack.common.consts.DEVICE",
    "iostream",
    "opencv2/opencv.hpp",
    "nn.ReLU",
    "torch",
    "SparseAttack.common.utils"
  ]
}