{
  "paper_id": "Controlling_Continuous_Relaxation_for_Combinatorial_Optimiza",
  "title": "Controlling Continuous Relaxation for Combinatorial Optimization",
  "abstract": "Unsupervised learning (UL)-based solvers for combinatorial optimization (CO) train a neural network that generates a soft solution by directly optimizing the CO objective using a continuous relaxation strategy. These solvers offer several advantages over traditional methods and other learning-based methods, particularly for large-scale CO problems. However, UL-based solvers face two practical issues: (I) an optimization issue, where UL-based solvers are easily trapped at local optima, and (II) a rounding issue, where UL-based solvers require artificial post-learning rounding from the continuous space back to the original discrete space, undermining the robustness of the results. This study proposes a Continuous Relaxation Annealing (CRA) strategy, an effective rounding-free learning method for UL-based solvers. CRA introduces a penalty term that dynamically shifts from prioritizing continuous solutions, to enforcing discreteness, eliminating artificial rounding. Experimental results demonstrate that CRA significantly enhances the performance of UL-based solvers, outperforming existing UL-based solvers and greedy algorithms in complex CO problems. Additionally, CRA effectively eliminates artificial rounding and accelerates the learning process.",
  "problem_description_natural": "The paper addresses combinatorial optimization (CO) problems, which involve finding an optimal solution from a discrete (typically binary) search space under given constraints. Examples include the Maximum Independent Set (MIS), MaxCut, and Diverse Bipartite Matching (DBM) problems on graphs. The goal is to minimize (or maximize) a cost function defined over binary decision variables subject to feasibility constraints. The authors focus on unsupervised learning approaches that relax the binary variables into continuous ones in [0,1], optimize a differentiable surrogate loss, and then round the result back to discrete valuesâ€”a process that introduces ambiguity and suboptimality. Their proposed method, Continuous Relaxation Annealing (CRA), avoids post-hoc rounding by gradually enforcing discreteness during training through an adaptive penalty term.",
  "problem_type": "combinatorial optimization",
  "datasets": [
    "Random Regular Graphs (RRG)",
    "Gset"
  ],
  "performance_metrics": [
    "Approximation Ratio (ApR)",
    "Independent Set Density",
    "Cut Ratio"
  ],
  "lp_model": {
    "objective": "$\\min f(\\boldsymbol{x}; C)$",
    "constraints": [
      "$g_i(\\boldsymbol{x}; C) \\leq 0$ for all $i \\in [I]$",
      "$h_j(\\boldsymbol{x}; C) = 0$ for all $j \\in [J]$"
    ],
    "variables": [
      "$\\boldsymbol{x} \\in \\{0,1\\}^N$: binary decision vector"
    ]
  },
  "raw_latex_model": "$$\\min_{\\boldsymbol{x} \\in \\{0,1\\}^N} f(\\boldsymbol{x}; C) \\quad \\text{s.t.} \\quad \\boldsymbol{x} \\in \\mathcal{X}(C),$$ where $\\mathcal{X}(C) = \\{\\boldsymbol{x} \\in \\{0,1\\}^N \\mid \\forall i \\in [I],\\ g_i(\\boldsymbol{x}; C) \\leq 0,\\ \\forall j \\in [J],\\ h_j(\\boldsymbol{x}; C) = 0\\}$.",
  "algorithm_description": "Continuous Relaxation Annealing (CRA) for Unsupervised Learning-based solvers, specifically applied to the PI-GNN solver using Graph Neural Networks. CRA introduces a penalty term to control discreteness and continuity of relaxed variables, with annealing of the penalty parameter, enabling optimization without labeled data or artificial rounding."
}