{
  "paper_id": "Controlling_Continuous_Relaxation_for_Combinatorial_Optimiza",
  "title": "Controlling Continuous Relaxation for Combinatorial Optimization",
  "abstract": "Unsupervised learning (UL)-based solvers for combinatorial optimization (CO) train a neural network that generates a soft solution by directly optimizing the CO objective using a continuous relaxation strategy. These solvers offer several advantages over traditional methods and other learning-based methods, particularly for large-scale CO problems. However, UL-based solvers face two practical issues: (I) an optimization issue, where UL-based solvers are easily trapped at local optima, and (II) a rounding issue, where UL-based solvers require artificial post-learning rounding from the continuous space back to the original discrete space, undermining the robustness of the results. This study proposes a Continuous Relaxation Annealing (CRA) strategy, an effective rounding-free learning method for UL-based solvers. CRA introduces a penalty term that dynamically shifts from prioritizing continuous solutions, to enforcing discreteness, eliminating artificial rounding. Experimental results demonstrate that CRA significantly enhances the performance of UL-based solvers, outperforming existing UL-based solvers and greedy algorithms in complex CO problems. Additionally, CRA effectively eliminates artificial rounding and accelerates the learning process.",
  "problem_description_natural": "The paper addresses combinatorial optimization (CO) problems, which involve finding an optimal solution from a discrete (typically binary) search space under given constraints. Examples include the Maximum Independent Set (MIS), MaxCut, and Diverse Bipartite Matching (DBM) problems on graphs. The goal is to minimize (or maximize) a cost function defined over binary decision variables subject to feasibility constraints. The authors focus on unsupervised learning approaches that relax the binary variables into continuous ones in [0,1], optimize a differentiable surrogate loss, and then round the result back to discrete values—a process that introduces ambiguity and suboptimality. Their proposed method, Continuous Relaxation Annealing (CRA), avoids post-hoc rounding by gradually enforcing discreteness during training through an adaptive penalty term.",
  "problem_type": "combinatorial optimization",
  "datasets": [
    "Random Regular Graphs (RRG)",
    "Gset"
  ],
  "performance_metrics": [
    "Approximation Ratio (ApR)",
    "Independent Set Density",
    "Cut Ratio"
  ],
  "lp_model": {
    "objective": "\\min_{\\boldsymbol{p} \\in [0,1]^N} \\hat{r}(\\boldsymbol{p}; C, \\boldsymbol{\\lambda}, \\gamma) = \\hat{f}(\\boldsymbol{p}; C) + \\sum_{i=1}^{I+J} \\lambda_i \\hat{v}_i(\\boldsymbol{p}; C) + \\gamma \\sum_{i=1}^N (1 - (2p_i - 1)^\\alpha)",
    "constraints": [
      "0 \\leq p_i \\leq 1 \\quad \\forall i \\in [N]"
    ],
    "variables": [
      "\\boldsymbol{p} = (p_1, \\dots, p_N) \\in [0,1]^N - relaxed continuous variables"
    ]
  },
  "raw_latex_model": "$$\\hat{r}(\\boldsymbol{p}; C, \\boldsymbol{\\lambda}, \\gamma) = \\hat{l}(\\boldsymbol{p}; C, \\boldsymbol{\\lambda}) + \\gamma \\Phi(\\boldsymbol{p}), \\quad \\Phi(\\boldsymbol{p}) \\triangleq \\sum_{i=1}^N (1 - (2p_i - 1)^\\alpha), \\quad \\alpha \\in \\{2n \\mid n \\in \\mathbb{N}_+\\},$$",
  "algorithm_description": "The Continuous Relaxation Annealing (CRA) algorithm for UL-based solvers, specifically applied to the PI-GNN solver (CRA-PI-GNN), proceeds as follows:\n1. Initialize the graph neural network (GNN) parameters θ.\n2. Set the initial penalty parameter γ to a negative value (e.g., γ(0) = -20 for MIS problems) and define the annealing rate ε > 0 (e.g., ε = 10^{-3}).\n3. For each training iteration τ:\n   a. Compute the relaxed solution p_θ = p_θ(C) using the GNN forward pass.\n   b. Compute the loss function: Ĵ(θ; C, λ, γ) = Ĵ(p_θ; C, λ) + γ Φ(p_θ), where Ĵ(p_θ; C, λ) = f̂(p_θ; C) + Σ_{i=1}^{I+J} λ_i v̂_i(p_θ; C) and Φ(p_θ) = Σ_{i=1}^N (1 - (2p_{θ,i} - 1)^α).\n   c. Update θ using gradient descent (e.g., AdamW optimizer) based on the gradients of Ĵ with respect to θ.\n   d. Update γ: γ(τ+1) = γ(τ) + ε.\n   e. Check for early stopping: if Φ(p_θ) ≈ 0 (within a tolerance), stop training.\n4. After training, the output p_θ is nearly binary due to the annealing process. Apply a simple projection if needed: for each i, set x_i = 1 if p_{θ,i} > 0.5, else 0, to obtain the discrete solution."
}