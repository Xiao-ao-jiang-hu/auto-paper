{
  "file_path": "MatNet/ATSP-ROCO/ATSPModel.py, MatNet/ATSP-ROCO/ATSPTrainer.py, MatNet/ATSP-ROCO/ATSProblemDef.py, MatNet/ATSP-ROCO/attacker_env.py, MatNet/ATSP-ROCO/attacker_model.py, POMO/CVRP/generate_adv.py, POMO/TSP/TSProblemDef.py, POMO/TSP/generate_adv.py",
  "function_name": "ATSPModel, ATSPTrainer, beam_search, generate_x_adv, AttackerEnv, ActorCritic, generate_adv_dataset, generate_x_adv, generate_adv_dataset",
  "code_snippet": "\n\n# ==========================================\n# File: MatNet/ATSP-ROCO/ATSPModel.py\n# Function/Context: ATSPModel\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom ATSPModel_LIB import AddAndInstanceNormalization, FeedForward, MixedScore_MultiHeadAttention\n\n\nclass ATSPModel(nn.Module):\n\n    def __init__(self, **model_params):\n        super().__init__()\n        self.model_params = model_params\n        self.eval_type = self.model_params['eval_type']\n\n        self.encoder = ATSP_Encoder(**model_params)\n        self.decoder = ATSP_Decoder(**model_params)\n\n        self.encoded_row = None\n        self.encoded_col = None\n        self.device = torch.device('cuda', torch.cuda.current_device()) if 'device' not in model_params.keys() else model_params['device']\n        # shape: (batch, node, embedding)\n\n    def pre_forward(self, reset_state):\n\n        problems = reset_state.problems\n        # problems.shape: (batch, node, node)\n\n        batch_size = problems.size(0)\n        node_cnt = problems.size(1)\n        embedding_dim = self.model_params['embedding_dim']\n\n        row_emb = torch.zeros(size=(batch_size, node_cnt, embedding_dim))\n        # emb.shape: (batch, node, embedding)\n        col_emb = torch.zeros(size=(batch_size, node_cnt, embedding_dim))\n        # shape: (batch, node, embedding)\n\n        seed_cnt = self.model_params['one_hot_seed_cnt']\n        rand = torch.rand(batch_size, seed_cnt)\n        batch_rand_perm = rand.argsort(dim=1)\n        rand_idx = batch_rand_perm[:, :node_cnt]\n\n        b_idx = torch.arange(batch_size)[:, None].expand(batch_size, node_cnt)\n        n_idx = torch.arange(node_cnt)[None, :].expand(batch_size, node_cnt)\n        col_emb[b_idx, n_idx, rand_idx] = 1\n        # shape: (batch, node, embedding)\n\n        self.encoded_row, self.encoded_col = self.encoder(row_emb, col_emb, problems)\n        # encoded_nodes.shape: (batch, node, embedding)\n\n        self.decoder.set_kv(self.encoded_col)\n\n    def set_eval_type(self, eval_type):\n        self.eval_type = eval_type\n\n    def forward(self, state):\n\n        batch_size = state.BATCH_IDX.size(0)\n        pomo_size = state.BATCH_IDX.size(1)\n\n        if state.current_node is None:\n            selected = torch.arange(pomo_size)[None, :].expand(batch_size, pomo_size)\n            prob = torch.ones(size=(batch_size, pomo_size))\n\n            # encoded_rows_mean = self.encoded_row.mean(dim=1, keepdim=True)\n            # encoded_cols_mean = self.encoded_col.mean(dim=1, keepdim=True)\n            # # shape: (batch, 1, embedding)\n            encoded_first_row = _get_encoding(self.encoded_row, selected)\n            # shape: (batch, pomo, embedding)\n            self.decoder.set_q1(encoded_first_row)\n\n        else:\n            encoded_current_row = _get_encoding(self.encoded_row, state.current_node)\n            # shape: (batch, pomo, embedding)\n            all_job_probs = self.decoder(encoded_current_row, ninf_mask=state.ninf_mask)\n            # shape: (batch, pomo, job)\n\n            if self.training or self.eval_type == 'softmax':\n                while True:  # to fix pytorch.multinomial bug on selecting 0 probability elements\n                    with torch.no_grad():\n                        selected = all_job_probs.reshape(batch_size * pomo_size, -1).multinomial(1) \\\n                            .squeeze(dim=1).reshape(batch_size, pomo_size)\n                        # shape: (batch, pomo)\n\n                    prob = all_job_probs[state.BATCH_IDX, state.POMO_IDX, selected] \\\n                        .reshape(batch_size, pomo_size)\n                    # shape: (batch, pomo)\n\n                    if (prob != 0).all():\n                        break\n            else:\n                selected = all_job_probs.argmax(dim=2)\n                # shape: (batch, pomo)\n                prob = None\n\n        return selected, prob\n\n\ndef _get_encoding(encoded_nodes, node_index_to_pick):\n    # encoded_nodes.shape: (batch, problem, embedding)\n    # node_index_to_pick.shape: (batch, pomo)\n\n    batch_size = node_index_to_pick.size(0)\n    pomo_size = node_index_to_pick.size(1)\n    embedding_dim = encoded_nodes.size(2)\n\n    gathering_index = node_index_to_pick[:, :, None].expand(batch_size, pomo_size, embedding_dim)\n    # shape: (batch, pomo, embedding)\n\n    picked_nodes = encoded_nodes.gather(dim=1, index=gathering_index)\n    # shape: (batch, pomo, embedding)\n\n    return picked_nodes\n\n\n########################################\n# ENCODER\n########################################\nclass ATSP_Encoder(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n        encoder_layer_num = model_params['encoder_layer_num']\n        self.layers = nn.ModuleList([EncoderLayer(**model_params) for _ in range(encoder_layer_num)])\n\n    def forward(self, row_emb, col_emb, cost_mat):\n        # col_emb.shape: (batch, col_cnt, embedding)\n        # row_emb.shape: (batch, row_cnt, embedding)\n        # cost_mat.shape: (batch, row_cnt, col_cnt)\n\n        for layer in self.layers:\n            row_emb, col_emb = layer(row_emb, col_emb, cost_mat)\n\n        return row_emb, col_emb\n\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n        self.row_encoding_block = EncodingBlock(**model_params)\n        self.col_encoding_block = EncodingBlock(**model_params)\n\n    def forward(self, row_emb, col_emb, cost_mat):\n        # row_emb.shape: (batch, row_cnt, embedding)\n        # col_emb.shape: (batch, col_cnt, embedding)\n        # cost_mat.shape: (batch, row_cnt, col_cnt)\n        row_emb_out = self.row_encoding_block(row_emb, col_emb, cost_mat)\n        col_emb_out = self.col_encoding_block(col_emb, row_emb, cost_mat.transpose(1, 2))\n\n        return row_emb_out, col_emb_out\n\n\nclass EncodingBlock(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n        self.model_params = model_params\n        embedding_dim = self.model_params['embedding_dim']\n        head_num = self.model_params['head_num']\n        qkv_dim = self.model_params['qkv_dim']\n\n        self.Wq = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.Wk = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.Wv = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.mixed_score_MHA = MixedScore_MultiHeadAttention(**model_params)\n        self.multi_head_combine = nn.Linear(head_num * qkv_dim, embedding_dim)\n\n        self.add_n_normalization_1 = AddAndInstanceNormalization(**model_params)\n        self.feed_forward = FeedForward(**model_params)\n        self.add_n_normalization_2 = AddAndInstanceNormalization(**model_params)\n\n    def forward(self, row_emb, col_emb, cost_mat):\n        # NOTE: row and col can be exchanged, if cost_mat.transpose(1,2) is used\n        # input1.shape: (batch, row_cnt, embedding)\n        # input2.shape: (batch, col_cnt, embedding)\n        # cost_mat.shape: (batch, row_cnt, col_cnt)\n        head_num = self.model_params['head_num']\n\n        q = reshape_by_heads(self.Wq(row_emb), head_num=head_num)\n        # q shape: (batch, head_num, row_cnt, qkv_dim)\n        k = reshape_by_heads(self.Wk(col_emb), head_num=head_num)\n        v = reshape_by_heads(self.Wv(col_emb), head_num=head_num)\n        # kv shape: (batch, head_num, col_cnt, qkv_dim)\n\n        out_concat = self.mixed_score_MHA(q, k, v, cost_mat)\n        # shape: (batch, row_cnt, head_num*qkv_dim)\n\n        multi_head_out = self.multi_head_combine(out_concat)\n        # shape: (batch, row_cnt, embedding)\n\n        out1 = self.add_n_normalization_1(row_emb, multi_head_out)\n        out2 = self.feed_forward(out1)\n        out3 = self.add_n_normalization_2(out1, out2)\n\n        return out3\n        # shape: (batch, row_cnt, embedding)\n\n\n########################################\n# Decoder\n########################################\n\nclass ATSP_Decoder(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n        self.model_params = model_params\n        embedding_dim = self.model_params['embedding_dim']\n        head_num = self.model_params['head_num']\n        qkv_dim = self.model_params['qkv_dim']\n\n        self.Wq_0 = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.Wq_1 = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.Wk = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.Wv = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n\n        self.multi_head_combine = nn.Linear(head_num * qkv_dim, embedding_dim)\n\n        self.k = None  # saved key, for multi-head attention\n        self.v = None  # saved value, for multi-head_attention\n        self.single_head_key = None  # saved key, for single-head attention\n        self.q1 = None  # saved q1, for multi-head attention\n\n    def set_kv(self, encoded_jobs):\n        # encoded_jobs.shape: (batch, job, embedding)\n        head_num = self.model_params['head_num']\n\n        self.k = reshape_by_heads(self.Wk(encoded_jobs), head_num=head_num)\n        self.v = reshape_by_heads(self.Wv(encoded_jobs), head_num=head_num)\n        # shape: (batch, head_num, job, qkv_dim)\n        self.single_head_key = encoded_jobs.transpose(1, 2)\n        # shape: (batch, embedding, job)\n\n    def set_q1(self, encoded_q1):\n        # encoded_q.shape: (batch, n, embedding)  # n can be 1 or pomo\n        head_num = self.model_params['head_num']\n\n        self.q1 = reshape_by_heads(self.Wq_1(encoded_q1), head_num=head_num)\n        # shape: (batch, head_num, n, qkv_dim)\n\n    def forward(self, encoded_q0, ninf_mask):\n        # encoded_q4.shape: (batch, pomo, embedding)\n        # ninf_mask.shape: (batch, pomo, job)\n\n        head_num = self.model_params['head_num']\n\n        #  Multi-Head Attention\n        #######################################################\n        q0 = reshape_by_heads(self.Wq_0(encoded_q0), head_num=head_num)\n        # shape: (batch, head_num, pomo, qkv_dim)\n\n        q = self.q1 + q0\n        # shape: (batch, head_num, pomo, qkv_dim)\n\n        out_concat = self._multi_head_attention(q, self.k, self.v, rank3_ninf_mask=ninf_mask)\n        # shape: (batch, pomo, head_num*qkv_dim)\n\n        mh_atten_out = self.multi_head_combine(out_concat)\n        # shape: (batch, pomo, embedding)\n\n        #  Single-Head Attention, for probability calculation\n        #######################################################\n        score = torch.matmul(mh_atten_out, self.single_head_key)\n        # shape: (batch, pomo, job)\n\n        sqrt_embedding_dim = self.model_params['sqrt_embedding_dim']\n        logit_clipping = self.model_params['logit_clipping']\n\n        score_scaled = score / sqrt_embedding_dim\n        # shape: (batch, pomo, job)\n\n        score_clipped = logit_clipping * torch.tanh(score_scaled)\n\n        score_masked = score_clipped + ninf_mask\n\n        probs = F.softmax(score_masked, dim=2)\n        # shape: (batch, pomo, job)\n\n        return probs\n\n    def _multi_head_attention(self, q, k, v, rank2_ninf_mask=None, rank3_ninf_mask=None):\n        # q shape: (batch, head_num, n, key_dim)   : n can be either 1 or pomo\n        # k,v shape: (batch, head_num, node, key_dim)\n        # rank2_ninf_mask.shape: (batch, node)\n        # rank3_ninf_mask.shape: (batch, group, node)\n\n        batch_s = q.size(0)\n        n = q.size(2)\n        node_cnt = k.size(2)\n\n        head_num = self.model_params['head_num']\n        qkv_dim = self.model_params['qkv_dim']\n        sqrt_qkv_dim = self.model_params['sqrt_qkv_dim']\n\n        score = torch.matmul(q, k.transpose(2, 3))\n        # shape: (batch, head_num, n, node)\n\n        score_scaled = score / sqrt_qkv_dim\n        if rank2_ninf_mask is not None:\n            score_scaled = score_scaled + rank2_ninf_mask[:, None, None, :].expand(batch_s, head_num, n, node_cnt)\n        if rank3_ninf_mask is not None:\n            score_scaled = score_scaled + rank3_ninf_mask[:, None, :, :].expand(batch_s, head_num, n, node_cnt)\n\n        weights = nn.Softmax(dim=3)(score_scaled)\n        # shape: (batch, head_num, n, node)\n\n        out = torch.matmul(weights, v)\n        # shape: (batch, head_num, n, key_dim)\n\n        out_transposed = out.transpose(1, 2)\n        # shape: (batch, n, head_num, key_dim)\n\n        out_concat = out_transposed.reshape(batch_s, n, head_num * qkv_dim)\n        # shape: (batch, n, head_num*key_dim)\n\n        return out_concat\n\n\n########################################\n# NN SUB FUNCTIONS\n########################################\n\ndef reshape_by_heads(qkv, head_num):\n    # q.shape: (batch, n, head_num*key_dim)   : n can be either 1 or PROBLEM_SIZE\n\n    batch_s = qkv.size(0)\n    n = qkv.size(1)\n\n    q_reshaped = qkv.reshape(batch_s, n, head_num, -1)\n    # shape: (batch, n, head_num, key_dim)\n\n    q_transposed = q_reshaped.transpose(1, 2)\n    # shape: (batch, head_num, n, key_dim)\n\n    return q_transposed\n\n\nclass ATSP_Routing(nn.Module):\n\n    def __init__(self, embedding_dim=128, num_expert=1, logit_clipping=10, n=20):\n        super().__init__()\n        self.num_expert = num_expert\n        self.embedding_dim = embedding_dim\n        self.logit_clipping = logit_clipping\n\n        self.expert = nn.Embedding(num_expert, embedding_dim)\n        self.W1 = nn.Linear(n**2, embedding_dim)\n        self.W2 = nn.Linear(num_expert, embedding_dim)\n        # self.W3 = nn.Linear(embedding_dim*2, embedding_dim)\n        self.Wq = nn.Linear(embedding_dim * 2, embedding_dim, bias=False)\n        self.Wk = nn.Linear(embedding_dim, embedding_dim, bias=False)\n\n    def forward(self, x, state):\n        \"\"\"\n            input - x: (batch_size, problem, problem); state: (batch_size, num_expert)\n            output - logits: (batch_size, num_expert)\n        \"\"\"\n        bs = x.size(0)\n        h = self.W1(x.view(bs, -1))  # (batch_size, embedding_dim)\n        state = self.W2(state)  # (batch_size, embedding_dim)\n        q = self.Wq(torch.cat((h, state), dim=1))  # (batch_size, embedding_dim)\n        k = self.Wk(self.expert(torch.arange(self.num_expert)))  # (num_expert, embedding_dim)\n        score = torch.matmul(q, k.transpose(0, 1)) / torch.sqrt(torch.tensor(self.embedding_dim, dtype=torch.float))\n        logits = self.logit_clipping * torch.tanh(score)  # (batch_size, num_expert)\n\n        return logits\n\n# ==========================================\n# File: MatNet/ATSP-ROCO/ATSPTrainer.py\n# Function/Context: ATSPTrainer\n# ==========================================\nimport os, glob\nimport torch\nfrom logging import getLogger\nfrom ATSPEnv import ATSPEnv as Env\nfrom ATSPModel import ATSPModel as Model\nfrom ATSPModel import ATSP_Routing\nfrom attacker_model import ActorCritic\nfrom ATSPTester import ATSPTester as Tester\nfrom ATSProblemDef import get_random_problems, load_single_problem_from_file, generate_x_adv\nfrom torch.optim import Adam as Optimizer\nfrom torch.optim.lr_scheduler import MultiStepLR as Scheduler\nfrom utils.utils import *\nfrom utils.functions import load_dataset, save_dataset\n\n\nclass ATSPTrainer:\n    def __init__(self, env_params, model_params, optimizer_params, trainer_params, adv_params):\n\n        # save arguments\n        self.env_params = env_params\n        self.model_params = model_params\n        self.optimizer_params = optimizer_params\n        self.trainer_params = trainer_params\n        self.adv_params = adv_params\n\n        # result folder, logger\n        self.logger = getLogger(name='trainer')\n        self.result_folder = get_result_folder()\n        self.result_log = LogData()\n\n        # cuda\n        USE_CUDA = self.trainer_params['use_cuda']\n        if USE_CUDA:\n            cuda_device_num = self.trainer_params['cuda_device_num']\n            torch.cuda.set_device(cuda_device_num)\n            device = torch.device('cuda', cuda_device_num)\n            torch.set_default_tensor_type('torch.cuda.FloatTensor')\n        else:\n            device = torch.device('cpu')\n            torch.set_default_tensor_type('torch.FloatTensor')\n        self.device = device\n        self.env_params['device'] = device\n        self.model_params['device'] = device\n\n        # Main Components\n        self.num_expert = self.trainer_params['num_expert']\n        self.env = Env(**self.env_params)\n        # pretraining for phase 1\n        self.pre_model = Model(**self.model_params)\n        self.pre_optimizer = Optimizer(self.pre_model.parameters(), **self.optimizer_params['optimizer'])\n        # several experts for phase 2\n        self.models = [Model(**self.model_params) for _ in range(self.num_expert)]\n        self.optimizers = [Optimizer(model.parameters(), **self.optimizer_params['optimizer']) for model in self.models]\n        self.schedulers = [Scheduler(optimizer, **self.optimizer_params['scheduler']) for optimizer in self.optimizers]\n        self.routing_model = ATSP_Routing(embedding_dim=model_params['embedding_dim'], num_expert=self.num_expert, n=self.env_params['node_cnt']) if self.trainer_params['routing_model'] else None\n        self.routing_optimizer = Optimizer(self.routing_model.parameters(), **self.optimizer_params['optimizer']) if self.routing_model else None\n\n        # load data\n        if self.trainer_params[\"fixed_dataset\"]:\n            nat_path, local_adv_path = \"./data/train_n20\", \"./data/CNF_train/train_adv_n20_local_0\"\n            local_adv_path_1, local_adv_path_2, global_adv_path = None, None, None\n            # local_adv_path_1, local_adv_path_2 = \"./data/CNF_train/train_adv_n20_local_1\", \"./data/CNF_train/train_adv_n20_local_2\"\n            # global_adv_path = \"./data/CNF_train/train_adv_n20_global\"\n            self.nat_data = torch.zeros(0, self.env_params['node_cnt'], self.env_params['node_cnt'])\n            self.local_adv_data = torch.zeros(0, self.env_params['node_cnt'], self.env_params['node_cnt'])\n            self.global_adv_data = torch.zeros(0, self.env_params['node_cnt'], self.env_params['node_cnt'])\n            for fp in sorted(glob.iglob(os.path.join(nat_path, \"*.atsp\"))):\n                data = load_single_problem_from_file(fp, node_cnt=self.env_params['node_cnt'], scaler=1000 * 1000)\n                self.nat_data = torch.cat((self.nat_data, data.unsqueeze(0)), dim=0)\n            for fp in sorted(glob.iglob(os.path.join(local_adv_path, \"*.atsp\"))):\n                data = load_single_problem_from_file(fp, node_cnt=self.env_params['node_cnt'], scaler=1000 * 1000)\n                self.local_adv_data = torch.cat((self.local_adv_data, data.unsqueeze(0)), dim=0)\n            if local_adv_path_1 is not None:\n                for fp in sorted(glob.iglob(os.path.join(local_adv_path_1, \"*.atsp\"))):\n                    data = load_single_problem_from_file(fp, node_cnt=self.env_params['node_cnt'], scaler=1000 * 1000)\n                    self.local_adv_data = torch.cat((self.local_adv_data, data.unsqueeze(0)), dim=0)\n            if local_adv_path_2 is not None:\n                for fp in sorted(glob.iglob(os.path.join(local_adv_path_2, \"*.atsp\"))):\n                    data = load_single_problem_from_file(fp, node_cnt=self.env_params['node_cnt'], scaler=1000 * 1000)\n                    self.local_adv_data = torch.cat((self.local_adv_data, data.unsqueeze(0)), dim=0)\n            if global_adv_path is not None:\n                for fp in sorted(glob.iglob(os.path.join(global_adv_path, \"*.atsp\"))):\n                    data = load_single_problem_from_file(fp, node_cnt=self.env_params['node_cnt'], scaler=1000 * 1000)\n                    self.global_adv_data = torch.cat((self.global_adv_data, data.unsqueeze(0)), dim=0)\n            if global_adv_path is None:\n                self.training_data = torch.cat((self.nat_data, self.local_adv_data), dim=0)\n            else:\n                self.training_data = torch.cat((self.nat_data, self.local_adv_data, self.global_adv_data), dim=0)\n            print(self.training_data.size())\n\n        # load attacker\n        self.attacker = None\n        if not self.trainer_params[\"fixed_dataset\"]:\n            attack_params = self.adv_params[\"node_feature_dim\"], self.adv_params[\"node_output_size\"], self.adv_params[\"batch_norm\"], self.adv_params[\"one_hot_degree\"], self.adv_params[\"gnn_layers\"]\n            self.attacker = [ActorCritic(*attack_params).to(self.device) for _ in range(self.num_expert)]\n            for i in range(self.num_expert):\n                checkpoint = torch.load(self.adv_params['path_{}'.format(i)], map_location=self.device)\n                attacker = self.attacker[i]\n                attacker.load_state_dict(checkpoint, strict=True)\n            print(\">> Load attacker from {}\".format(self.adv_params['path']))\n\n        # Restore\n        self.start_epoch = 1\n        model_load = trainer_params['model_load']\n        pretrain_load = trainer_params['pretrain_load']\n        if model_load['enable']:\n            checkpoint_fullname = '{path}/checkpoint-{n}-{epoch}.pt'.format(**model_load)\n            checkpoint = torch.load(checkpoint_fullname, map_location=device)\n            model_state_dict = checkpoint['model_state_dict']\n            optimizer_state_dict = checkpoint['optimizer_state_dict']\n            for i in range(self.num_expert):\n                model, optimizer, scheduler = self.models[i], self.optimizers[i], self.schedulers[i]\n                model.load_state_dict(model_state_dict[i])\n                optimizer.load_state_dict(optimizer_state_dict[i])\n                scheduler.last_epoch = model_load['epoch'] - 1\n            self.start_epoch = 1 + model_load['epoch']\n            self.result_log.set_raw_data(checkpoint['result_log'])\n            self.logger.info('Checkpoint loaded successfully from {}'.format(checkpoint_fullname))\n\n        elif pretrain_load['enable']:  # (Only) Load pretrain model\n            checkpoint_fullname = '{path}/checkpoint-{n}-{epoch}.pt'.format(**pretrain_load)\n            checkpoint = torch.load(checkpoint_fullname, map_location=device)\n            for i in range(self.num_expert):\n                self.models[i].load_state_dict(checkpoint['model_state_dict'])\n            self.logger.info('Pretrain model loaded successfully from {}'.format(checkpoint_fullname))\n\n        else:  # pretrain (phase 1) from scratch\n            self.logger.info('No pretrain model found! Pretraining from scratch.')\n            for epoch in range(self.trainer_params['pretrain_epochs']+1):\n                self._train_one_epoch(epoch, mode=\"nat\")\n            model_state_dict = self.pre_model.state_dict()\n            for i in range(self.num_expert):\n                self.models[i].load_state_dict(model_state_dict)\n            del self.pre_model\n            self.logger.info('Pretraining finished.')\n\n        # utility\n        self.time_estimator = TimeEstimator()\n\n    def run(self):\n        self.time_estimator.reset(self.start_epoch)\n        for epoch in range(self.start_epoch, self.trainer_params['epochs']+1):\n            self.logger.info('=================================================================')\n\n            # Train\n            self._train_one_epoch(epoch, mode=\"adv\")\n            for i in range(self.num_expert):\n                self.schedulers[i].step()\n\n            # Validation\n            dirs = [\"./data/test_n20\", \"./data/test_adv_n20\"]\n            for dir in dirs:\n                self._val_and_stat(dir, batch_size=100, val_episodes=1000)\n\n            # Logs & Checkpoint\n            train_score, train_loss = 0, 0\n            self.result_log.append('train_score', epoch, train_score)\n            self.result_log.append('train_loss', epoch, train_loss)\n            elapsed_time_str, remain_time_str = self.time_estimator.get_est_string(epoch, self.trainer_params['epochs'])\n            self.logger.info(\"Epoch {:3d}/{:3d}: Time Est.: Elapsed[{}], Remain[{}]\".format(epoch, self.trainer_params['epochs'], elapsed_time_str, remain_time_str))\n\n            all_done = (epoch == self.trainer_params['epochs'])\n            model_save_interval = self.trainer_params['logging']['model_save_interval']\n            img_save_interval = self.trainer_params['logging']['img_save_interval']\n\n            if epoch > 1:  # save latest images, every epoch\n                self.logger.info(\"Saving log_image\")\n                image_prefix = '{}/latest'.format(self.result_folder)\n                util_save_log_image_with_label(image_prefix, self.trainer_params['logging']['log_image_params_1'], self.result_log, labels=['train_score'])\n                util_save_log_image_with_label(image_prefix, self.trainer_params['logging']['log_image_params_2'], self.result_log, labels=['train_loss'])\n\n            if all_done or (epoch % model_save_interval) == 0:\n                self.logger.info(\"Saving trained_model\")\n                checkpoint_dict = {\n                    'epoch': epoch,\n                    'model_state_dict': [model.state_dict() for model in self.models],\n                    'optimizer_state_dict': [optimizer.state_dict() for optimizer in self.optimizers],\n                    'scheduler_state_dict': [scheduler.state_dict() for scheduler in self.schedulers],\n                    'routing_model_state_dict': self.routing_model.state_dict() if self.routing_model else None,\n                    'routing_optimizer_state_dict': self.routing_optimizer.state_dict() if self.routing_model else None,\n                    'result_log': self.result_log.get_raw_data()\n                }\n                torch.save(checkpoint_dict, '{}/checkpoint-{}.pt'.format(self.result_folder, epoch))\n\n            if all_done or (epoch % img_save_interval) == 0:\n                image_prefix = '{}/img/checkpoint-{}'.format(self.result_folder, epoch)\n                util_save_log_image_with_label(image_prefix, self.trainer_params['logging']['log_image_params_1'],\n                                    self.result_log, labels=['train_score'])\n                util_save_log_image_with_label(image_prefix, self.trainer_params['logging']['log_image_params_2'],\n                                    self.result_log, labels=['train_loss'])\n\n            if all_done:\n                self.logger.info(\" *** Training Done *** \")\n                self.logger.info(\"Now, printing log array...\")\n                util_print_log_array(self.logger, self.result_log)\n\n    def _train_one_epoch(self, epoch, mode=\"nat\"):\n\n        episode = 0\n        train_num_episode = self.trainer_params['train_episodes']\n        if self.trainer_params[\"fixed_dataset\"]:\n            # train_num_episode = self.training_data.size(0)\n            self.nat_data = self.nat_data[torch.randperm(self.nat_data.size(0))]\n            self.local_adv_data = self.local_adv_data[torch.randperm(self.nat_data.size(0))]\n            self.global_adv_data = self.global_adv_data[torch.randperm(self.nat_data.size(0))] if self.global_adv_data.size(0) != 0 else self.global_adv_data\n\n        while episode < train_num_episode:\n            remaining = train_num_episode - episode\n            batch_size = min(self.trainer_params['train_batch_size'], remaining)\n            nat_data = get_random_problems(batch_size, self.env_params[\"node_cnt\"], self.env_params[\"problem_gen_params\"])\n\n            if mode == \"nat\":\n                score, loss = self._train_one_batch(self.pre_model, nat_data)\n                avg_score, avg_loss = score.mean().item(), loss.mean()\n                self.pre_optimizer.zero_grad()\n                avg_loss.backward()\n                self.pre_optimizer.step()\n            elif mode == \"adv\":\n                # The first two steps are completed in advance for training efficiency\n                # 1. generate adversarial examples by each expert (local)   # adv_data = generate_x_adv(self.models[i], self.attacker[i], nat_data, global=False)\n                # 2. collaborate to generate adversarial examples (global)  # adv_data = generate_x_adv(self.models, self.attacker, nat_data, global=True)\n                # 3. forward to neural router\n                assert self.trainer_params[\"fixed_dataset\"]\n                nat_data = self.nat_data[episode: episode + batch_size]\n                if epoch <= 10:\n                    local_adv_data = self.local_adv_data[episode: (episode + batch_size)]\n                    all_data = torch.cat((nat_data, local_adv_data), dim=0)\n                    # print(all_data.size(), episode, train_num_episode)\n                else:\n                    local_adv_data = self.local_adv_data[episode * 3: (episode + batch_size) * 3]\n                    global_adv_data = self.global_adv_data[episode: episode + batch_size]\n                    all_data = torch.cat((nat_data, local_adv_data, global_adv_data), dim=0)\n                scores = torch.zeros(all_data.size(0), 0)\n                for k in range(self.num_expert):\n                    _, score = self._fast_val(self.models[k], data=all_data, aug_factor=1, eval_type=\"softmax\")\n                    scores = torch.cat((scores, score.unsqueeze(1)), dim=1)\n                if self.routing_model:\n                    self._update_model_routing(all_data, scores, type=\"exp_choice_with_best\")\n                else:\n                    self._update_model_heuristic(all_data, scores, type=\"ins_exp_choice\")\n            else:\n                raise NotImplementedError\n\n            episode += batch_size\n\n        # Log Once, for each epoch\n        self.logger.info('Epoch {:3d}: Train ({:3.0f}%)'.format(epoch, 100. * episode / train_num_episode))\n\n# ==========================================\n# File: MatNet/ATSP-ROCO/ATSProblemDef.py\n# Function/Context: beam_search, generate_x_adv\n# ==========================================\nimport os, time, copy\nimport itertools\nimport torch\nimport numpy as np\nfrom attacker_env import AttackerEnv\n\ndef repeat_interleave(inp_list, repeat_num):\n    return list(itertools.chain.from_iterable(zip(*itertools.repeat(inp_list, repeat_num))))\n\ndef beam_search_step_kernel(idx, act_n_sel,acts1, acts2, probs1, probs2, ready_nodes1, ready_nodes2_flat, graph_list, act_list, prob_list, orig_greedy, atsp_env, defense=False):\n    beam_idx = idx // act_n_sel ** 2\n    act1_idx = idx // act_n_sel % act_n_sel\n    act2_idx = idx % act_n_sel\n    act1, prob1 = acts1[beam_idx, act1_idx].item(), probs1[beam_idx, act1_idx].item()\n    act2, prob2 = acts2[beam_idx, act1_idx, act2_idx].item(), probs2[beam_idx, act1_idx, act2_idx].item()\n    ready_nodes_1 = ready_nodes1[beam_idx]\n    ready_nodes_2 = ready_nodes2_flat[beam_idx * act_n_sel + act1_idx]\n\n    if act1 in ready_nodes_1 and act2 in ready_nodes_2:\n        assert prob1 > 0\n        assert prob2 > 0\n        reward, new_lower_matrix, edge_candidates, new_greedy, done = atsp_env.step(graph_list[beam_idx], (act1, act2), orig_greedy, defense)\n        return (\n                new_lower_matrix,\n                edge_candidates,\n                reward,\n                act_list[beam_idx] + [(act1, act2)],\n                prob_list[beam_idx] + [(prob1, prob2)],\n                done\n        )\n    else:\n        return None\n\ndef beam_search(base_model, policy_model, atsp_env, inp_lower_matrix, edge_candidates, greedy_cost, max_actions, beam_size=5, attack=True, defense=False, global_adv=False):\n    start_time = time.time()\n\n    if not global_adv:\n        state_encoder = policy_model.state_encoder\n        actor_net = policy_model.actor_net\n\n    orig_greedy = greedy_cost\n    best_tuple = (\n        copy.deepcopy(inp_lower_matrix),  # input lower-left adjacency matrix\n        edge_candidates,  # edge candidates\n        -100,  # accumulated reward\n        [],  # actions\n        [],  # probabilities\n        False,\n    )\n    topk_graphs = [best_tuple]\n\n    act_n_sel = beam_size\n    best_reward_each_step = np.zeros(max_actions + 1)\n    for step in range(1, max_actions + 1):\n        if global_adv:\n            from ATSPTester import ATSPTester as Tester\n            env_params = {'node_cnt': atsp_env.node_dimension, 'problem_gen_params': {'int_min': 0, 'int_max': 1000 * 1000, 'scaler': 1000 * 1000}, 'pomo_size': atsp_env.node_dimension}\n            tester_params = {\"augmentation_enable\": True, \"aug_factor\": 16, \"use_cuda\": True, \"cuda_device_num\": torch.cuda.current_device()}\n            # eval on M models\n            best_model, best_attacker, best_length = None, None, 10**8\n            for i in range(len(policy_model)):\n                tester = Tester(env_params, None, tester_params, model=base_model[i])\n                tour, length = tester.attacker_run((torch.tensor(inp_lower_matrix) / 1e4).to(torch.float32).to(device))\n                if length < best_length:\n                    best_length = length\n                    best_model = base_model[i]\n                    best_attacker = policy_model[i]\n            # attack best model\n            state_encoder = best_attacker.state_encoder\n            actor_net = best_attacker.actor_net\n            # recreate env\n            tester = Tester(env_params, None, tester_params, model=best_model)\n            atsp_env = AttackerEnv(solver_type=\"MatNet\", node_dimension=atsp_env.node_dimension, is_attack=True, tester=tester, path=\"./data/train_n20\", printinfo=False)\n\n        lower_matrix_list, edge_cand_list, reward_list, act_list, prob_list = [], [], [], [], []\n        for lower_matrix, edge_cand, reward, acts, probs, done in topk_graphs:\n            lower_matrix_list.append(lower_matrix)\n            edge_cand_list.append(edge_cand)\n            reward_list.append(reward)\n            act_list.append(acts)\n            prob_list.append(probs)\n            if done:\n                ret_solution = orig_greedy + reward if (attack and not defense) else orig_greedy - reward\n                return {\n                    'reward': reward,\n                    'solution': ret_solution,\n                    'acts': acts,\n                    'probs': probs,\n                    'time': time.time() - start_time,\n                }\n\n        state_feat = state_encoder(lower_matrix_list)\n\n        # mask1: (beam_size, max_num_nodes)\n        mask1, ready_nodes1 = actor_net._get_mask1(state_feat.shape[0], state_feat.shape[1], edge_cand_list)\n        # acts1, probs1: (beam_size, act_n_sel)\n        acts1, probs1 = actor_net._select_node(state_feat, mask1, greedy_sel_num=act_n_sel)\n        # acts1_flat, probs1_flat: (beam_size x act_n_sel,)\n        acts1_flat, probs1_flat = acts1.reshape(-1), probs1.reshape(-1)\n        # mask2_flat: (beam_size x act_n_sel, max_num_nodes)\n        mask2_flat, ready_nodes2_flat = actor_net._get_mask2(state_feat.shape[0] * act_n_sel, state_feat.shape[1], repeat_interleave(edge_cand_list, act_n_sel), acts1_flat)\n        # acts2_flat, probs2_flat: (beam_size x act_n_sel, act_n_sel)\n        acts2_flat, probs2_flat = actor_net._select_node(state_feat.repeat_interleave(act_n_sel, dim=0), mask2_flat, prev_act=acts1_flat, greedy_sel_num=act_n_sel)\n        # acts2, probs2: (beam_size, act_n_sel, act_n_sel)\n        acts2, probs2 = acts2_flat.reshape(-1, act_n_sel, act_n_sel), probs2_flat.reshape(-1, act_n_sel, act_n_sel)\n\n        acts1, acts2, probs1, probs2 = acts1.cpu(), acts2.cpu(), probs1.cpu(), probs2.cpu()\n\n        def kernel_func_feeder(max_idx):\n            for idx in range(max_idx):\n                yield (\n                    idx, act_n_sel,\n                    acts1, acts2, probs1, probs2, ready_nodes1, ready_nodes2_flat,\n                    lower_matrix_list, act_list, prob_list,\n                    orig_greedy, atsp_env, defense\n                )\n\n        tmp_graphs = [beam_search_step_kernel(*x) for x in kernel_func_feeder(len(lower_matrix_list) * act_n_sel ** 2)]\n        searched_graphs = []\n        for graph_tuple in tmp_graphs:\n            if graph_tuple is not None:\n                searched_graphs.append(graph_tuple)\n\n        # find the best action\n        searched_graphs.sort(key=lambda x: x[2], reverse=True)\n        if searched_graphs[0][2] > best_tuple[2]:\n            best_tuple = searched_graphs[0]\n        # print(searched_graphs[0], '\\n\\n')\n        best_reward_each_step[step] = best_tuple[2]\n        # find the topk expandable actions\n        topk_graphs = searched_graphs[:beam_size]\n\n    ret_solution = orig_greedy + best_tuple[2] if (attack and not defense) else orig_greedy - best_tuple[2]\n    best_solution_each_step = orig_greedy + best_reward_each_step if (attack and not defense) else orig_greedy - best_reward_each_step\n\n    return {\n        'inp_lower_matrix': best_tuple[0],\n        'reward': best_tuple[2],\n        'solution': ret_solution,\n        'acts': best_tuple[3],\n        'probs': best_tuple[4],\n        'time': time.time() - start_time,\n        'best_reward_each_step': best_reward_each_step,\n        'best_solution_each_step': best_solution_each_step,\n    }\n\ndef generate_x_adv(model, attacker, nat_data, global_adv=False):\n    \"\"\"\n    Generate adversarial data based on the attacker model.\n    See also: \"ROCO: A General Framework for Evaluating Robustness of Combinatorial Optimization Solvers on Graphs\" in ICLR 2023.\n    \"\"\"\n    from ATSPTester import ATSPTester as Tester\n    env_params = {'node_cnt': nat_data.size(-1), 'problem_gen_params': {'int_min': 0, 'int_max': 1000 * 1000, 'scaler': 1000 * 1000}, 'pomo_size': nat_data.size(-1)}\n    tester_params = {\"augmentation_enable\": True, \"aug_factor\": 16, \"use_cuda\": True, \"cuda_device_num\": torch.cuda.current_device()}\n    if global_adv:\n        for i in range(len(attacker)):\n            model[i].eval()\n            attacker[i].eval()\n        tester = Tester(env_params, None, tester_params, model=model[0])\n    else:\n        model.eval()\n        attacker.eval()\n        tester = Tester(env_params, None, tester_params, model=model)\n\n    atsp_env = AttackerEnv(solver_type=\"MatNet\", node_dimension=nat_data.size(-1), is_attack=True, tester=tester, path=\"./data/train_n20\")\n    _, transform_data = atsp_env.generate_tuples(0, nat_data.size(0), rand_id=0, defense=True)\n    print(len(transform_data))\n    adv_data = nat_data.clone().detach()\n\n    for id, (inp_lower_matrix, edge_candidates, ori_greedy, baselines, _, tsp_path) in enumerate(transform_data):\n        print(id)\n        bs_result = beam_search(model, attacker, atsp_env, inp_lower_matrix, edge_candidates, ori_greedy, max_actions=10, beam_size=3, attack=True, defense=False, global_adv=global_adv)\n        actions = bs_result[\"acts\"]\n        # generate adv instances - half the cost of selected edges\n        for action in actions:\n            adv_data[id][action[0], action[1]] /= 2\n\n    return adv_data\n\n# ==========================================\n# File: MatNet/ATSP-ROCO/attacker_env.py\n# Function/Context: AttackerEnv\n# ==========================================\nimport torch\nimport random\nimport os\nimport glob\nfrom copy import deepcopy\nimport time\nimport numpy as np\nimport tsplib95\nfrom ATSP_algorithms import calc_lkh_tour_len, calc_nearest_neighbor_tour_len, solveFarthestInsertion, get_adj, calc_MatNet_tour_len, calc_furthest_insertion_tour_len\n\nVERY_LARGE_INT = 10  # 65536\n\n\ndef parse_tsp(list_m, dim=None, name='unknown'):\n    if dim is None:\n        dim = len(list_m)\n    outstr = ''\n    outstr += 'TYPE: ATSP\\n'\n    outstr += 'DIMENSION: %d\\n' % dim\n    outstr += 'EDGE_WEIGHT_TYPE: EXPLICIT\\n'\n    outstr += 'EDGE_WEIGHT_FORMAT: FULL_MATRIX\\n'\n    outstr += 'EDGE_WEIGHT_SECTION:\\n'\n    for l in list_m:\n        listToStr = ' '.join([str(elem) for elem in l])\n        outstr += ' %s\\n' % listToStr\n    return outstr\n\n\nclass AttackerEnv(object):\n    def __init__(self, solver_type='nn', node_dimension=20, is_attack=False, tester=None, path=\"./tmp\", printinfo=True):\n        self.solver_type = solver_type\n        self.node_dimension = node_dimension\n        self.process_dataset(path, printinfo)\n        self.available_solvers = ('nn', 'furthest', 'lkh-5', 'MatNet')\n        self.is_attack = is_attack\n        self.tester = tester\n        self.device = torch.device(\"cuda:{}\".format(torch.cuda.current_device()) if torch.cuda.is_available() else \"cpu\")\n        assert solver_type in self.available_solvers\n\n    def process_dataset(self, dirpath=\"./tmp\", printinfo=True):\n        self.tspfiles = []\n        for fp in sorted(glob.iglob(os.path.join(dirpath, \"*.atsp\"))):\n            self.tspfiles.append(fp)\n            if printinfo:\n                print(fp)\n        if printinfo:\n            print(f'Total file num {len(self.tspfiles)}')\n\n    def step(self, list_lower_matrix, act, prev_solution, defense=False):\n        if self.is_attack and not defense:\n            return self.step_Attack(list_lower_matrix, act, prev_solution)\n\n        new_list_lower_matrix = deepcopy(list_lower_matrix)\n        if isinstance(act, torch.Tensor):\n            act = (act[0].item(), act[1].item())\n        new_list_lower_matrix[act[0]][act[1]] *= 2\n        new_tour, new_solution, _ = self.solve_feasible_tsp(new_list_lower_matrix, self.solver_type)\n        new_edge_candidate = self.edge_candidate_from_tour(new_tour, len(new_list_lower_matrix))\n        reward = prev_solution - new_solution\n        done = new_solution == 0\n        return reward, new_list_lower_matrix, new_edge_candidate, new_solution, done\n\n    def step_Attack(self, list_lower_matrix, act, prev_solution):\n        new_list_lower_matrix = deepcopy(list_lower_matrix)\n        if isinstance(act, torch.Tensor):\n            act = (act[0].item(), act[1].item())\n        new_list_lower_matrix[act[0]][act[1]] /= 2\n\n        new_tour, new_solution, _ = self.solve_feasible_tsp(new_list_lower_matrix, self.solver_type)\n        new_edge_candidate = self.edge_candidate_Attack(new_tour, new_list_lower_matrix)\n        reward = new_solution - prev_solution\n        done = new_solution == 0\n        return reward, new_list_lower_matrix, new_edge_candidate, new_solution, done\n\n    def solve_feasible_tsp(self, lower_left_matrix, solver_type):\n        prev_time = time.time()\n        tsp_inst = tsplib95.parse(parse_tsp(lower_left_matrix))\n        if solver_type == 'nn':\n            tour, length = calc_nearest_neighbor_tour_len(tsp_inst)\n        elif solver_type == 'furthest':\n            tour, length = solveFarthestInsertion(tsp_inst)\n        elif solver_type == 'lkh-5':\n            tour, length = calc_lkh_tour_len(tsp_inst)\n        elif 'lkh-' in solver_type:\n            num_moves = int(solver_type.strip('lkh-'))\n            tour, length = calc_lkh_tour_len(tsp_inst, move_type=num_moves, runs=1)\n        elif solver_type == 'MatNet':\n            tour, length = calc_MatNet_tour_len((torch.tensor(lower_left_matrix) / 1e4).to(torch.float32).to(self.device), self.tester)\n        else:\n            raise ValueError(f'{solver_type} is not implemented.')\n        comp_time = time.time() - prev_time\n        return tour, length, comp_time\n\n    @staticmethod\n    def edge_candidate_from_tour(tour, num_nodes):\n        assert tour[0] == tour[-1]\n        edge_candidate = {x: set() for x in range(num_nodes)}\n        iter_obj = iter(tour)\n        last_node = next(iter_obj)\n        for node in iter_obj:\n            edge_candidate[last_node].add(node)\n            edge_candidate[node].add(last_node)\n            last_node = node\n        return edge_candidate\n\n    def edge_candidate_Attack(self, tour, list_lower_matrix):\n        num_nodes = len(list_lower_matrix)\n        edge_candidate = {x: set() for x in range(num_nodes)}\n        for i in range(num_nodes):\n            for j in range(i):\n                if list_lower_matrix[i][j] > 1:\n                    edge_candidate[i].add(j)\n                    edge_candidate[j].add(i)\n        iter_obj = iter(tour)\n        last_node = next(iter_obj)\n        for node in iter_obj:\n            if node in edge_candidate[last_node]:\n                edge_candidate[last_node].remove(node)\n            if last_node in edge_candidate[node]:\n                edge_candidate[node].remove(last_node)\n            last_node = node\n        return edge_candidate\n\n# ==========================================\n# File: MatNet/ATSP-ROCO/attacker_model.py\n# Function/Context: ActorCritic\n# ==========================================\nimport sys\nimport os\nsys.path.append(os.pardir)\n\nimport torch\nfrom torch import nn\nfrom utils.utils_func import construct_graph_batch\nfrom torch_geometric.utils import to_dense_batch\nfrom torch.distributions import Categorical\nimport torch_geometric as pyg\nfrom torch_scatter import scatter\nfrom itertools import chain\n\n\ndef matrix_list_to_graphs(lower_left_matrices, device):\n    graphs = []\n    #edge_candidates = []\n    for b, lower_left_m in enumerate(lower_left_matrices):\n        edge_indices = [[], []]\n        edge_attrs = [] ###############################\n        x = torch.ones(len(lower_left_m), 1)\n        #edge_cand = {x: set() for x in range(len(lower_left_m))}\n        for row, cols in enumerate(lower_left_m):\n            for col, weight in enumerate(cols):\n                # if weight == 0 or weight >= 2:\n                #     pass\n                # else:\n                #     edge_indices[0].append(row)\n                #     edge_indices[1].append(col)\n                #     edge_attrs.append(weight)\n                #     x[row] += weight\n                #     x[col] += weight\n                #     #edge_cand[row].add(col)\n                #     #edge_cand[col].add(row)\n                edge_indices[0].append(row)\n                edge_indices[1].append(col)\n                edge_attrs.append(weight)\n                x[row] += weight\n                x[col] += weight\n        edge_indices = torch.tensor(edge_indices)\n        edge_attrs = torch.Tensor(edge_attrs).to(device)  ####################\n        #x = (x) / torch.std(x)\n        # graphs.append(pyg.data.Data(x=x, edge_index=edge_indices)) #, edge_attrs=edge_attrs)) ############################\n        graphs.append(pyg.data.Data(x=x, edge_index=edge_indices, edge_attrs=edge_attrs))  ##########################\n        #edge_candidates.append(edge_cand)\n    return graphs #, edge_candidates\n\n\nclass GCN(nn.Module):\n    def __init__(self, num_in_feats, num_out_feats, num_layers=3, batch_norm=True):\n        super(GCN, self).__init__()\n        self.num_in_feats = num_in_feats\n        self.num_out_feats = num_out_feats\n        self.num_layers = num_layers\n\n        for l in range(self.num_layers):\n            if l == 0:\n                conv = pyg.nn.GCNConv(self.num_in_feats, self.num_out_feats)\n            else:\n                conv = pyg.nn.GCNConv(self.num_out_feats, self.num_out_feats)\n            if batch_norm:\n                norm = nn.BatchNorm1d(self.num_out_feats)\n            else:\n                norm = nn.Identity()\n            self.add_module('conv_{}'.format(l), conv)\n            self.add_module('norm_{}'.format(l), norm)\n\n        self.init_parameters()\n\n    def init_parameters(self):\n        for l in range(self.num_layers):\n            # nn.init.xavier_uniform_(getattr(self, 'conv_{}'.format(l)).weight)\n            getattr(self, 'conv_{}'.format(l)).reset_parameters()\n\n    def forward(self, *args):\n        if len(args) == 1 and isinstance(args[0], (pyg.data.Data, pyg.data.Batch)):\n            x, edge_index = args[0].x, args[0].edge_index\n        elif len(args) == 2 and isinstance(args[0], torch.Tensor) and isinstance(args[1], torch.Tensor):\n            x, edge_index = args\n        else:\n            raise ValueError('Unknown combination of data types: {}'.format(','.join([type(x) for x in args])))\n        for l in range(self.num_layers):\n            conv = getattr(self, 'conv_{}'.format(l))\n            norm = getattr(self, 'norm_{}'.format(l))\n            x = conv(x, edge_index)\n            x = nn.functional.relu(norm(x))\n        return x\n\n\nclass GraphAttentionPooling(nn.Module):\n    \"\"\"\n    Attention module to extract global feature of a graph.\n    \"\"\"\n    def __init__(self, feat_dim):\n        \"\"\"\n        :param feat_dim: number dimensions of input features.\n        \"\"\"\n        super(GraphAttentionPooling, self).__init__()\n        self.feat_dim = feat_dim\n        self.setup_weights()\n        self.init_parameters()\n\n    def setup_weights(self):\n        \"\"\"\n        Defining weights.\n        \"\"\"\n        self.weight_matrix = torch.nn.Parameter(torch.Tensor(self.feat_dim, self.feat_dim))\n\n    def init_parameters(self):\n        \"\"\"\n        Initializing weights.\n        \"\"\"\n        torch.nn.init.xavier_uniform_(self.weight_matrix)\n\n    def forward(self, x, batch, size=None):\n        \"\"\"\n        Making a forward propagation pass to create a graph level representation.\n        :param x: Result of the GNN.\n        :param batch: Batch vector, which assigns each node to a specific example\n        :return representation: A graph level representation matrix.\n        \"\"\"\n        size = batch[-1].item() + 1 if size is None else size\n        mean = scatter(x, batch, dim=0, dim_size=size, reduce='mean')\n        transformed_global = torch.tanh(torch.mm(mean, self.weight_matrix))\n\n        coefs = torch.sigmoid((x * transformed_global[batch] * 10).sum(dim=1))\n        weighted = coefs.unsqueeze(-1) * x\n\n        return scatter(weighted, batch, dim=0, dim_size=size, reduce='add')\n\n    def get_coefs(self, x):\n        mean = x.mean(dim=0)\n        transformed_global = torch.tanh(torch.matmul(mean, self.weight_matrix))\n\n        return torch.sigmoid(torch.matmul(x, transformed_global))\n\n\nclass ResNetBlock(nn.Module):\n    def __init__(self, num_in_feats, num_out_feats, num_layers=3, batch_norm=True):\n        super(ResNetBlock, self).__init__()\n        self.num_in_feats = num_in_feats\n        self.num_out_feats = num_out_feats\n        self.num_layers = num_layers\n\n        self.first_linear = None\n        self.last_linear = None\n        self.sequential = []\n        self.output_seq = []\n\n        for l in range(self.num_layers):\n            if l == 0:\n                self.first_linear = nn.Linear(self.num_in_feats, self.num_out_feats)\n                if batch_norm: self.sequential.append(nn.BatchNorm1d(self.num_out_feats))\n                self.sequential.append(nn.ReLU())\n            elif l == self.num_layers - 1:\n                self.last_linear = nn.Linear(self.num_out_feats, self.num_out_feats)\n                if batch_norm: self.output_seq.append(nn.BatchNorm1d(self.num_out_feats))\n            else:\n                self.sequential.append(nn.Linear(self.num_out_feats, self.num_out_feats))\n                if batch_norm: self.sequential.append(nn.BatchNorm1d(self.num_out_feats))\n                self.sequential.append(nn.ReLU())\n\n        self.sequential = nn.Sequential(*self.sequential)\n        self.output_seq = nn.Sequential(*self.output_seq)\n\n        self.init_parameters()\n\n    def init_parameters(self):\n        for mod in chain(self.sequential, self.output_seq):\n            if isinstance(mod, nn.Linear):\n                nn.init.xavier_uniform_(mod.weight)\n\n    def forward(self, inp):\n        x1 = self.first_linear(inp)\n        x2 = self.sequential(x1) + x1\n        return self.output_seq(x2)\n\n\nclass GraphEncoder(torch.nn.Module):\n    def __init__(\n            self,\n            node_feature_dim,\n            node_output_size,\n            batch_norm,\n            one_hot_degree,\n            num_layers=10\n    ):\n        super(GraphEncoder, self).__init__()\n        self.node_feature_dim = node_feature_dim\n        self.node_output_size = node_output_size\n        self.one_hot_degree = one_hot_degree\n        self.batch_norm = batch_norm\n        self.num_layers = num_layers\n\n        one_hot_dim = self.one_hot_degree + 1 if self.one_hot_degree > 0 else 0\n        self.siamese_gcn = GCN(self.node_feature_dim + one_hot_dim, self.node_output_size, num_layers=self.num_layers, batch_norm=self.batch_norm)\n        self.att = GraphAttentionPooling(self.node_output_size)\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n    def forward(self, inp_lower_matrix):\n        # construct graph batches\n        batched_graphs = construct_graph_batch(matrix_list_to_graphs(inp_lower_matrix, self.device), self.one_hot_degree, self.device)\n\n        # forward pass\n        batched_node_feat = self.siamese_gcn(batched_graphs)\n        node_feat_reshape, _ = to_dense_batch(batched_node_feat, batched_graphs.batch)\n        graph_feat = self.att(batched_node_feat, batched_graphs.batch)\n        state_feat = torch.cat(\n            (node_feat_reshape, graph_feat.unsqueeze(1).expand(-1, node_feat_reshape.shape[1], -1)), dim=-1)\n\n        return state_feat\n\n\nclass ActorNet(torch.nn.Module):\n    def __init__(\n            self,\n            state_feature_size,\n            batch_norm,\n    ):\n        super(ActorNet, self).__init__()\n        self.state_feature_size = state_feature_size\n        self.batch_norm = batch_norm\n\n        self.act1_resnet = ResNetBlock(self.state_feature_size, 1, batch_norm=self.batch_norm)\n        #self.act2_resnet = ResNetBlock(self.state_feature_size * 2, 1, batch_norm=self.batch_norm)\n        self.act2_query = nn.Linear(self.state_feature_size, self.state_feature_size, bias=False)\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n    def forward(self, input_feat, edge_candidates, known_action=None):\n        return self._act(input_feat, edge_candidates, known_action)\n\n    def _act(self, input_feat, edge_candidates, known_action=None):\n        if known_action is None:\n            known_action = (None, None)\n        # roll-out 2 acts\n        mask1, ready_nodes1 = self._get_mask1(input_feat.shape[0], input_feat.shape[1], edge_candidates)\n        act1, log_prob1, entropy1 = self._select_node(input_feat, mask1, known_action[0])\n        mask2, ready_nodes2 = self._get_mask2(input_feat.shape[0], input_feat.shape[1], edge_candidates, act1)\n        act2, log_prob2, entropy2 = self._select_node(input_feat, mask2, known_action[1], act1)\n        return torch.stack((act1, act2)), torch.stack((log_prob1, log_prob2)), entropy1 + entropy2\n\n    def _select_node(self, state_feat, mask, known_cur_act=None, prev_act=None, greedy_sel_num=0):\n        # neural net prediction\n        if prev_act is None:  # for act 1\n            act_scores = self.act1_resnet(state_feat).squeeze(-1)\n        else:  # for act 2\n            prev_node_feat = state_feat[torch.arange(len(prev_act)), prev_act, :]\n            #state_feat = torch.cat(\n            #    (state_feat, prev_node_feat.unsqueeze(1).expand(-1, state_feat.shape[1], -1)), dim=-1)\n            #act_scores = self.act2_resnet(state_feat).squeeze(-1)\n            act_query = torch.tanh(self.act2_query(prev_node_feat))\n            act_scores = (act_query.unsqueeze(1) * state_feat).sum(dim=-1)\n\n        # select action\n        if greedy_sel_num > 0:\n            act_probs = nn.functional.softmax(act_scores + mask, dim=1)\n            argsort_prob = torch.argsort(act_probs, dim=-1, descending=True)\n            acts = argsort_prob[:, :greedy_sel_num]\n            return acts, act_probs[torch.arange(acts.shape[0]).unsqueeze(-1), acts]\n        else:\n            act_log_probs = nn.functional.log_softmax(act_scores + mask, dim=1)\n            dist = Categorical(logits=act_log_probs)\n            if known_cur_act is None:\n                act = dist.sample()\n                return act, dist.log_prob(act), dist.entropy()\n            else:\n                return known_cur_act, dist.log_prob(known_cur_act), dist.entropy()\n\n    def _get_mask1(self, batch_size, num_nodes, edge_candidates):\n        masks = torch.full((batch_size, num_nodes), -float('inf'), device=self.device)\n        ready_nodes = []\n        for b in range(batch_size):\n            ready_nodes.append([])\n            for node, candidates in edge_candidates[b].items():\n                if len(candidates) == 0:\n                    pass\n                else:\n                    masks[b, node] = 0\n                    ready_nodes[b].append(node)\n        return masks, ready_nodes\n\n    def _get_mask2(self, batch_size, num_nodes, edge_candidates, act1):\n        masks = torch.full((batch_size, num_nodes), -float('inf'), device=self.device)\n        ready_nodes = []\n        for b in range(batch_size):\n            ready_nodes.append([])\n            candidates = edge_candidates[b][act1[b].item()]\n            for index in candidates:\n                masks[b, index] = 0.0\n                ready_nodes[b].append(index)\n        return masks, ready_nodes\n\n\nclass CriticNet(torch.nn.Module):\n    def __init__(\n            self,\n            state_feature_size,\n            batch_norm,\n    ):\n        super(CriticNet, self).__init__()\n        self.state_feature_size = state_feature_size\n        self.batch_norm = batch_norm\n\n        self.critic_resnet = ResNetBlock(self.state_feature_size, 1, batch_norm=self.batch_norm)\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n    def forward(self, state_feat):\n        return self._eval(state_feat)\n\n    def _eval(self, state_feat):\n        # get global features\n        state_feat = torch.max(state_feat, dim=1).values\n        state_value = self.critic_resnet(state_feat).squeeze(-1)\n        return state_value\n\n\nclass ActorCritic(nn.Module):\n    def __init__(self, node_feature_dim, node_output_size, batch_norm, one_hot_degree, gnn_layers):\n        super(ActorCritic, self).__init__()\n        self.state_encoder = GraphEncoder(node_feature_dim, node_output_size, batch_norm, one_hot_degree, gnn_layers)\n        self.actor_net = ActorNet(node_output_size * 2, batch_norm)\n        self.value_net = CriticNet(node_output_size * 2, batch_norm)\n\n    def forward(self):\n        raise NotImplementedError\n\n    def act(self, inp_lower_matrix, edge_candidates, memory):\n        state_feat = self.state_encoder(inp_lower_matrix)\n        actions, action_logits, entropy = self.actor_net(state_feat, edge_candidates)\n\n        memory.states.append(inp_lower_matrix)\n        memory.edge_candidates.append(edge_candidates)\n        memory.actions.append(actions)\n        memory.logprobs.append(action_logits)\n\n        return actions\n\n    def evaluate(self, inp_lower_matrix, edge_candidates, action):\n        state_feat = self.state_encoder(inp_lower_matrix)\n        _, action_logits, entropy = self.actor_net(state_feat, edge_candidates, action)\n        state_value = self.value_net(state_feat)\n        return action_logits, state_value, entropy\n\n# ==========================================\n# File: POMO/CVRP/generate_adv.py\n# Function/Context: generate_adv_dataset\n# ==========================================\nimport os, sys\nimport time\nimport pickle\nimport argparse\nos.chdir(os.path.dirname(os.path.abspath(__file__)))\nsys.path.insert(0, \"..\")  # for utils\nimport torch\nimport numpy as np\nfrom datetime import timedelta\n\nfrom CVRPModel import CVRPModel as Model\nfrom CVRProblemDef import generate_x_adv\nfrom CVRP_baseline import *\nfrom utils.utils import *\nfrom utils.functions import *\n\n\ndef generate_adv_dataset(model, data, eps_min=1, eps_max=100, num_steps=1, perturb_demand=False):\n    \"\"\"\n        generate adversarial dataset (ins and sol).\n        Note: data should include depot_xy, node_xy, normalized node_demand.\n    \"\"\"\n    eps = iter([i for i in range(eps_min, eps_max+1, 1)])\n    depot_xy, node_xy, node_demand = data\n    episode, batch_size, test_num_episode = 0, 10, depot_xy.size(0)\n    # adv_depot_xy = torch.zeros(0, 1, 2)\n    adv_node_xy = torch.zeros(0, node_xy.size(1), 2)\n    adv_node_demand = torch.zeros(0, node_xy.size(1))\n    while episode < test_num_episode:\n        remaining = test_num_episode - episode\n        batch_size = min(batch_size, remaining)\n        nat_data = (depot_xy[episode: episode + batch_size], node_xy[episode: episode + batch_size], node_demand[episode: episode + batch_size])\n        _, node, demand = generate_x_adv(model, nat_data, eps=next(eps), num_steps=num_steps, perturb_demand=perturb_demand)\n        # adv_depot_xy = torch.cat((adv_depot_xy, depot), dim=0)\n        adv_node_xy = torch.cat((adv_node_xy, node), dim=0)\n        adv_node_demand = torch.cat((adv_node_demand, demand), dim=0)\n        episode += batch_size\n\n    return depot_xy, adv_node_xy, adv_node_demand\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--num_expert\", type=int, default=1, help=\"Number of experts\")\n    parser.add_argument(\"--model_path\", type=str, default='../../pretrained/POMO-CVRP100/checkpoint-30500.pt', help=\"Path of the checkpoint to load\")\n    parser.add_argument(\"--test_set_path\", type=str, default='../../data/CVRP/cvrp100_uniform.pkl', help=\"Filename of the dataset(s) to evaluate\")\n    parser.add_argument('--test_episodes', type=int, default=1000, help=\"Number of instances to process\")\n    parser.add_argument('--eps_min', type=int, default=1, help=\"Min attack budget\")\n    parser.add_argument('--eps_max', type=int, default=100, help=\"Max attack budget\")\n    parser.add_argument('--num_steps', type=int, default=1, help=\"Number of steps to generate adversarial examples\")\n    parser.add_argument('--perturb_demand', action='store_true', help=\"whether to perturb node demands or not for CVRP\")\n    parser.add_argument('--gpu_id', type=int, default=0, help=\"GPU ID\")\n    opts = parser.parse_args()\n\n    model_params = {\n        'embedding_dim': 128,\n        'sqrt_embedding_dim': 128 ** (1 / 2),\n        'encoder_layer_num': 6,\n        'qkv_dim': 16,\n        'head_num': 8,\n        'logit_clipping': 10,\n        'ff_hidden_dim': 512,\n        'eval_type': 'softmax',\n        'norm': 'instance',\n    }\n    torch.cuda.set_device(opts.gpu_id)\n    device = torch.device('cuda', opts.gpu_id)\n    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n    checkpoint = torch.load(opts.model_path, map_location=device)\n\n    # load data & preprocessing\n    data = load_dataset(opts.test_set_path)[: opts.test_episodes]\n    depot_xy, node_xy, ori_node_demand, capacity = [i[0] for i in data], [i[1] for i in data], [i[2] for i in data], [i[3] for i in data]\n    depot_xy, node_xy, ori_node_demand, capacity = torch.Tensor(depot_xy), torch.Tensor(node_xy), torch.Tensor(ori_node_demand), torch.Tensor(capacity)\n    node_demand = ori_node_demand / capacity.view(-1, 1)\n    test_data = (depot_xy, node_xy, node_demand)  # [batch_size, 1, 2], [batch_size, problems, 2], [batch_size, problems]\n\n    if opts.num_expert == 1:\n        model = Model(**model_params)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        models = [model]\n    else:\n        models = [Model(**model_params) for _ in range(opts.num_expert)]\n        model_state_dict = checkpoint['model_state_dict']\n        for i in range(opts.num_expert):\n            models[i].load_state_dict(model_state_dict[i])\n\n    # generate adversarial examples (only coordinates of nods are adversarially updated)\n    start_time = time.time()\n    # adv_depot_xy = torch.zeros(0, 1, 2)\n    adv_node_xy = torch.zeros(0, test_data[1].size(1), 2)\n    adv_node_demand = torch.zeros(0, test_data[1].size(1))\n    for i in range(opts.num_expert):\n        _, node, demand = generate_adv_dataset(models[i], test_data, eps_min=opts.eps_min, eps_max=opts.eps_max, num_steps=opts.num_steps, perturb_demand=opts.perturb_demand)\n        # adv_depot_xy = torch.cat((adv_depot_xy, depot), dim=0)\n        adv_node_xy = torch.cat((adv_node_xy, node), dim=0)\n        adv_node_demand = torch.cat((adv_node_demand, demand), dim=0)\n    dir, filename = os.path.split(opts.test_set_path)\n\n    demand_scaler = {20: 30, 50: 40, 100: 50, 200: 70}\n    adv_data = (torch.cat([depot_xy] * opts.num_expert, dim=0), adv_node_xy, torch.clamp(torch.ceil(adv_node_demand * demand_scaler[adv_node_xy.size(1)]), min=1, max=9), torch.cat([capacity] * opts.num_expert, dim=0))\n    # adv_data = (torch.cat([depot_xy] * opts.num_expert, dim=0), adv_node_xy, torch.cat([ori_node_demand] * opts.num_expert, dim=0), torch.cat([capacity] * opts.num_expert, dim=0))\n    # save_dataset(adv_data, \"{}/adv_{}\".format(dir, filename))\n    with open(\"{}/adv_{}\".format(dir, filename), \"wb\") as f:\n        pickle.dump(list(zip(adv_data[0].tolist(), adv_data[1].tolist(), adv_data[2].tolist(), adv_data[3].tolist())), f, pickle.HIGHEST_PROTOCOL)  # [(depot_xy, node_xy, node_demand, capacity), ...]\n    print(\">> Adversarial dataset generation finished within {:.2f}s\".format(time.time()-start_time))\n\n    # obtain (sub-)opt solution using HGS\n    start_time = time.time()\n    params = argparse.ArgumentParser()\n    params.cpus, params.n, params.progress_bar_mininterval = None, None, 0.1\n    dataset = [attr.cpu().tolist() for attr in adv_data]\n    dataset = [(dataset[0][i][0], dataset[1][i], [int(d) for d in dataset[2][i]], int(dataset[3][i])) for i in range(adv_data[0].size(0))]\n    executable = get_hgs_executable()\n    def run_func(args):\n        return solve_hgs_log(executable, *args, runs=1, disable_cache=True)  # otherwise it directly loads data from dir\n\n    results, parallelism = run_all_in_pool(run_func, \"./HGS_result\", dataset, params, use_multiprocessing=False)\n    os.system(\"rm -rf ./HGS_result\")\n\n    costs, tours, durations = zip(*results)\n    print(\">> Solving adversarial dataset finished using HGS within {:.2f}s\".format(time.time()-start_time))\n    print(\"Average cost: {} +- {}\".format(np.mean(costs), 2 * np.std(costs) / np.sqrt(len(costs))))\n    print(\"Average serial duration: {} +- {}\".format(np.mean(durations), 2 * np.std(durations) / np.sqrt(len(durations))))\n    print(\"Average parallel duration: {}\".format(np.mean(durations) / parallelism))\n    print(\"Calculated total duration: {}\".format(timedelta(seconds=int(np.sum(durations) / parallelism))))\n\n    results = [(i[0], i[1]) for i in results]\n    save_dataset(results, \"{}/hgs_adv_{}\".format(dir, filename))\n\n# ==========================================\n# File: POMO/TSP/TSProblemDef.py\n# Function/Context: generate_x_adv\n# ==========================================\nimport torch\nimport numpy as np\n\n\ndef get_random_problems(batch_size, problem_size):\n    problems = torch.rand(size=(batch_size, problem_size, 2))\n    # problems.shape: (batch, problem, 2)\n    return problems\n\n\ndef augment_xy_data_by_8_fold(problems):\n    # problems.shape: (batch, problem, 2)\n\n    x = problems[:, :, [0]]\n    y = problems[:, :, [1]]\n    # x,y shape: (batch, problem, 1)\n\n    dat1 = torch.cat((x, y), dim=2)\n    dat2 = torch.cat((1 - x, y), dim=2)\n    dat3 = torch.cat((x, 1 - y), dim=2)\n    dat4 = torch.cat((1 - x, 1 - y), dim=2)\n    dat5 = torch.cat((y, x), dim=2)\n    dat6 = torch.cat((1 - y, x), dim=2)\n    dat7 = torch.cat((y, 1 - x), dim=2)\n    dat8 = torch.cat((1 - y, 1 - x), dim=2)\n\n    aug_problems = torch.cat((dat1, dat2, dat3, dat4, dat5, dat6, dat7, dat8), dim=0)\n    # shape: (8*batch, problem, 2)\n\n    return aug_problems\n\n\ndef generate_x_adv(model, nat_data, eps=10.0, num_steps=1, return_opt=False):\n    \"\"\"\n        Generate adversarial data based on the current model.\n        See also: \"Learning to Solve Travelling Salesman Problem with Hardness-adaptive Curriculum\" in AAAI 2022.\n    \"\"\"\n    from TSPEnv import TSPEnv as Env\n    from TSP_gurobi import solve_all_gurobi\n    from torch.autograd import Variable\n    def minmax(xy_):\n        # min_max normalization: [b, n, 2]\n        xy_ = (xy_ - xy_.min(dim=1, keepdims=True)[0]) / (xy_.max(dim=1, keepdims=True)[0] - xy_.min(dim=1, keepdims=True)[0])\n        return xy_\n\n    data = nat_data.clone().detach()\n    if eps == 0: return data\n    # generate x_adv\n    model.eval()\n    model.set_eval_type(\"softmax\")\n    aug_factor, batch_size = 1, data.size(0)\n    env = Env(**{'problem_size': data.size(1), 'pomo_size': data.size(1)})\n    with torch.enable_grad():\n        for i in range(num_steps):\n            data.requires_grad_()\n            env.load_problems(batch_size, problems=data, aug_factor=aug_factor)\n            reset_state, _, _ = env.reset()\n            model.pre_forward(reset_state)\n            prob_list = torch.zeros(size=(aug_factor * batch_size, env.pomo_size, 0))\n            state, reward, done = env.pre_step()\n            while not done:\n                selected, prob = model(state)\n                state, reward, done = env.step(selected)\n                prob_list = torch.cat((prob_list, prob[:, :, None]), dim=2)\n\n            aug_reward = reward.reshape(aug_factor, batch_size, env.pomo_size).permute(1, 0, 2).view(batch_size, -1)\n            baseline_reward = aug_reward.float().mean(dim=1, keepdims=True)\n            log_prob = prob_list.log().sum(dim=2).reshape(aug_factor, batch_size, env.pomo_size).permute(1, 0, 2).view(batch_size, -1)\n\n            delta = torch.autograd.grad(eps * ((aug_reward / baseline_reward) * log_prob).mean(), data)[0]  # original with baseline\n            # delta = torch.autograd.grad(eps * (aug_reward * log_prob).mean(), data)[0]  # original without baseline\n            data = data.detach() + delta\n            data = minmax(data)\n            data = Variable(data, requires_grad=False)\n\n    # generate opt sol\n    if return_opt:\n        return data, solve_all_gurobi(data)\n\n    return data\n\n# ==========================================\n# File: POMO/TSP/generate_adv.py\n# Function/Context: generate_adv_dataset\n# ==========================================\nimport os, sys\nimport time\nimport argparse\nos.chdir(os.path.dirname(os.path.abspath(__file__)))\nsys.path.insert(0, \"..\")  # for utils\nimport torch\nimport numpy as np\nfrom datetime import timedelta\n\nfrom TSPModel import TSPModel as Model\nfrom TSProblemDef import generate_x_adv\nfrom TSP_baseline import *\nfrom utils.utils import *\nfrom utils.functions import *\n\n\ndef generate_adv_dataset(model, data, eps_min=1, eps_max=100, num_steps=1):\n    \"\"\"\n        generate adversarial dataset (ins and sol).\n    \"\"\"\n    eps = iter([i for i in range(eps_min, eps_max+1, 1)])\n    episode, batch_size, test_num_episode = 0, 10, data.size(0)\n    adv_data, adv_opt = torch.zeros(0, data.size(1), data.size(2)), []\n    while episode < test_num_episode:\n        remaining = test_num_episode - episode\n        batch_size = min(batch_size, remaining)\n        nat_data = data[episode: episode + batch_size]\n        x_adv = generate_x_adv(model, nat_data, eps=next(eps), num_steps=num_steps, return_opt=False)\n        adv_data = torch.cat((adv_data, x_adv), dim=0)\n        # adv_opt.extend(sol)\n        episode += batch_size\n\n    return adv_data\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--num_expert\", type=int, default=3, help=\"Number of experts\")\n    parser.add_argument(\"--model_path\", type=str, default='../../pretrained/POMO-TSP/checkpoint-3000.pt', help=\"Path of the checkpoint to load\")\n    parser.add_argument(\"--test_set_path\", type=str, default='../../data/TSP/tsp100_uniform.pkl', help=\"Filename of the dataset(s) to evaluate\")\n    parser.add_argument('--test_episodes', type=int, default=1000, help=\"Number of instances to process\")\n    parser.add_argument('--eps_min', type=int, default=1, help=\"Min attack budget\")\n    parser.add_argument('--eps_max', type=int, default=100, help=\"Max attack budget\")\n    parser.add_argument('--num_steps', type=int, default=1, help=\"Number of steps to generate adversarial examples\")\n    parser.add_argument('--gpu_id', type=int, default=0, help=\"GPU ID\")\n    opts = parser.parse_args()\n\n    model_params = {\n        'embedding_dim': 128,\n        'sqrt_embedding_dim': 128 ** (1 / 2),\n        'encoder_layer_num': 6,\n        'qkv_dim': 16,\n        'head_num': 8,\n        'logit_clipping': 10,\n        'ff_hidden_dim': 512,\n        'eval_type': 'softmax',\n        'norm': 'instance',\n    }\n    torch.cuda.set_device(opts.gpu_id)\n    device = torch.device('cuda', opts.gpu_id)\n    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n    test_data = torch.Tensor(load_dataset(opts.test_set_path)[: opts.test_episodes])\n    checkpoint = torch.load(opts.model_path, map_location=device)\n\n    if opts.num_expert == 1:\n        model = Model(**model_params)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        models = [model]\n    else:\n        models = [Model(**model_params) for _ in range(opts.num_expert)]\n        model_state_dict = checkpoint['model_state_dict']\n        for i in range(opts.num_expert):\n            models[i].load_state_dict(model_state_dict[i])\n\n    # generate adversarial examples\n    start_time = time.time()\n    adv_data = torch.zeros(0, test_data.size(1), 2)\n    for i in range(opts.num_expert):\n        data = generate_adv_dataset(models[i], test_data, eps_min=opts.eps_min, eps_max=opts.eps_max, num_steps=opts.num_steps)\n        adv_data = torch.cat((adv_data, data), dim=0)\n    dir, filename = os.path.split(opts.test_set_path)\n    save_dataset(adv_data.tolist(), \"{}/adv_{}\".format(dir, filename))\n    print(\">> Adversarial dataset generation finished within {:.2f}s\".format(time.time() - start_time))\n\n    # obtain (sub-)opt solution using Concorde\n    start_time = time.time()\n    params = argparse.ArgumentParser()\n    params.cpus, params.n, params.progress_bar_mininterval = None, None, 0.1\n    dataset = [(instance.cpu().numpy(),) for instance in adv_data]\n    executable = os.path.abspath(os.path.join('concorde', 'concorde', 'TSP', 'concorde'))\n    def run_func(args):\n        return solve_concorde_log(executable, *args, disable_cache=True)\n    results, parallelism = run_all_in_pool(run_func, \"./Concorde_result\", dataset, params, use_multiprocessing=False)\n    os.system(\"rm -rf ./Concorde_result\")\n\n    costs, tours, durations = zip(*results)\n    print(\">> Solving adversarial dataset finished using Concorde within {:.2f}s\".format(time.time() - start_time))\n    print(\"Average cost: {} +- {}\".format(np.mean(costs), 2 * np.std(costs) / np.sqrt(len(costs))))\n    print(\"Average serial duration: {} +- {}\".format(np.mean(durations), 2 * np.std(durations) / np.sqrt(len(durations))))\n    print(\"Average parallel duration: {}\".format(np.mean(durations) / parallelism))\n    print(\"Calculated total duration: {}\".format(timedelta(seconds=int(np.sum(durations) / parallelism))))\n\n    results = [(i[0], i[1]) for i in results]\n    save_dataset(results, \"{}/concorde_adv_{}\".format(dir, filename))",
  "description": "Combined Analysis:\n- [MatNet/ATSP-ROCO/ATSPModel.py]: This file implements the core neural network architecture for solving Asymmetric Traveling Salesman Problem (ATSP) within the Collaborative Neural Framework (CNF). The ATSPModel class implements a neural solver with encoder-decoder architecture that processes ATSP instances (cost matrices) and generates tour sequences. The ATSP_Routing class implements the neural router component of CNF that selects which expert model to use for each instance. The implementation directly corresponds to Step 2.f of the CNF algorithm where the neural router takes instances and cost matrix as input and outputs logits for model selection. The encoder uses mixed-score multi-head attention with cost matrix integration, while the decoder uses attention mechanisms with masking to enforce ATSP constraints (each node visited exactly once).\n- [MatNet/ATSP-ROCO/ATSPTrainer.py]: This file implements the core training logic of the Collaborative Neural Framework (CNF) for adversarial training of neural ATSP solvers. It matches Algorithm Steps 1-2e and 2f-2j from the paper description. Specifically: 1) Initializes M models from pretrained model (phase 1 pretraining) and neural router (ATSP_Routing). 2) In each training epoch: a) Samples clean instances (nat_data). b) Loads pre-generated local/global adversarial instances (when fixed_dataset=True) or generates them via attacker models. c) Collects clean, local adversarial, and global adversarial instances into all_data. d) Evaluates all models on these instances to obtain cost matrix (scores). e) Routes instances to models via neural router (or heuristic) for training. f) Updates models and router (via _update_model_routing/_update_model_heuristic methods not fully shown). The implementation uses PyTorch for optimization and follows the CNF's two-phase training: pretraining on clean data, then collaborative adversarial training with multiple experts and a routing mechanism.\n- [MatNet/ATSP-ROCO/ATSProblemDef.py]: This file implements the core adversarial attack component of the CNF algorithm for ATSP. The beam_search function performs the inner maximization step (Algorithm step 2.c.i/ii) by searching for edge modifications that maximize the solver's cost. The generate_x_adv function implements the adversarial instance generation (Algorithm step 2.b/c) by halving edge costs of selected actions. The code handles both local attacks (single model) and global attacks (multiple models) as specified in the algorithm. The attack modifies the cost matrix c(|x) by perturbing edge weights, directly implementing the adversarial perturbation aspect of the optimization model.\n- [MatNet/ATSP-ROCO/attacker_env.py]: This file implements the adversarial environment for attacking TSP solvers, which corresponds to the inner maximization step (Step 2.c.i/ii) in the CNF algorithm. The AttackerEnv class provides: 1) Step functions that modify edge weights (doubling for defense, halving for attack) and recompute solutions, 2) TSP solving via multiple algorithms (nearest neighbor, furthest insertion, LKH, MatNet), 3) Edge candidate generation for action space. This directly implements the adversarial perturbation mechanism where the attacker modifies instance costs to degrade solver performance, aligning with the paper's adversarial training framework for robust neural routing methods.\n- [MatNet/ATSP-ROCO/attacker_model.py]: This file implements the attacker model (ActorCritic) for generating adversarial perturbations in the CNF adversarial training framework. It corresponds to the inner maximization step (step 2c.i/ii) in the algorithm, where the attacker maximizes the loss of neural VRP solvers by modifying instance edges. The model uses a Graph Neural Network (GCN-based encoder) to process ATSP instances represented as graphs, and an actor-critic RL architecture to select edge modifications (two-node actions) that degrade solver performance. The act() method generates adversarial actions, while evaluate() computes log probabilities and state values for training the attacker via policy gradient methods.\n- [POMO/CVRP/generate_adv.py]: This file implements the adversarial example generation component of the CNF algorithm's inner maximization step (Step 2.c.i-ii). Specifically, it generates adversarial instances by perturbing node coordinates (and optionally demands) using the generate_x_adv function, which performs gradient-based attacks to maximize model loss. The code supports multiple experts (models) and generates adversarial datasets across a range of attack budgets (eps_min to eps_max). While it doesn't implement the full CNF training loop (neural router, instance selection, or RL training), it directly implements the core adversarial instance generation logic used in the inner maximization phase of adversarial training.\n- [POMO/TSP/TSProblemDef.py]: This file implements the inner maximization step (adversarial instance generation) from the CNF algorithm. The generate_x_adv function performs gradient-based adversarial attacks on TSP instances by maximizing the model's expected reward (negative tour length) through REINFORCE policy gradient. It corresponds to step 2.c.i (local adversarial updates) and 2.c.ii (global adversarial updates) in the paper's algorithm, where the gradient ascent on input coordinates generates adversarial perturbations that degrade model performance. The function uses the REINFORCE objective with baseline normalization and supports multiple attack steps with min-max normalization to maintain valid coordinate ranges.\n- [POMO/TSP/generate_adv.py]: This file implements the adversarial example generation component of the CNF algorithm (step 2.c). The generate_adv_dataset function creates adversarial instances by attacking a neural TSP model with bounded perturbations (eps parameter). The main script orchestrates this for multiple expert models, generating an adversarial dataset that is then solved optimally using Concorde to obtain ground-truth solutions. This corresponds to the inner maximization step in adversarial training, where adversarial instances are crafted to maximize the model's loss.",
  "dependencies": [
    "TSPModel",
    "TSProblemDef.generate_x_adv",
    "ATSP_algorithms.calc_MatNet_tour_len",
    "CVRProblemDef.generate_x_adv",
    "torch.autograd.Variable",
    "torch.optim.lr_scheduler.MultiStepLR",
    "argparse",
    "utils.utils.save_dataset",
    "TSP_baseline.solve_concorde_log",
    "utils.utils",
    "ATSP_algorithms.calc_nearest_neighbor_tour_len",
    "ATSP_algorithms.calc_furthest_insertion_tour_len",
    "utils.functions",
    "ATSProblemDef",
    "ATSPModel_LIB.AddAndInstanceNormalization",
    "beam_search_step_kernel",
    "tsplib95",
    "itertools.chain",
    "TSP_baseline.run_all_in_pool",
    "CVRP_baseline.*",
    "utils.utils.load_dataset",
    "numpy",
    "ATSP_algorithms.solveFarthestInsertion",
    "torch_geometric",
    "torch.optim.Adam",
    "ATSPEnv",
    "utils.functions.*",
    "time",
    "TSPEnv",
    "repeat_interleave",
    "copy.deepcopy",
    "torch",
    "utils.utils_func.construct_graph_batch",
    "glob",
    "ATSPModel_LIB.FeedForward",
    "ATSPModel_LIB.MixedScore_MultiHeadAttention",
    "torch.nn.functional",
    "ATSP_algorithms.get_adj",
    "torch.distributions.Categorical",
    "os",
    "torch_scatter",
    "datetime.timedelta",
    "TSP_gurobi",
    "itertools",
    "torch.nn",
    "ATSPTester",
    "ATSP_algorithms.calc_lkh_tour_len",
    "CVRPModel.CVRPModel",
    "attacker_env.AttackerEnv",
    "logging",
    "pickle",
    "utils.utils.*",
    "sys",
    "random",
    "copy",
    "attacker_model",
    "ATSPModel",
    "ATSPTester.ATSPTester"
  ]
}