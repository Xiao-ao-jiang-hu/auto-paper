{
  "paper_id": "A_reinforcement_learning_approach_to_the_orienteering_proble",
  "title": "A REINFORCEMENT LEARNING APPROACH TO THE ORIENTEERING PROBLEM WITH TIME WINDOWS",
  "abstract": "The Orienteering Problem with Time Windows (OPTW) is a combinatorial optimization problem where the goal is to maximize the total score collected from different visited locations. The application of neural network models to combinatorial optimization has recently shown promising results in dealing with similar problems, like the Travelling Salesman Problem. A neural network allows learning solutions using reinforcement learning or supervised learning, depending on the available data. After the learning stage, it can be generalized and quickly fine-tuned to further improve performance and personalization. The advantages are evident since, for real-world applications, solution quality, personalization, and execution times are all important factors that should be taken into account. This study explores the use of Pointer Network models trained using reinforcement learning to solve the OPTW problem. We propose a modified architecture that leverages Pointer Networks to better address problems related with dynamic time-dependent constraints. Among its various applications, the OPTW can be used to model the Tourist Trip Design Problem (TTDP). We train the Pointer Network with the TTDP problem in mind, by sampling variables that can change across tourists visiting a particular instance-region: starting position, starting time, available time, and the scores given to each point of interest. Once a model-region is trained, it can infer a solution for a particular tourist using beam search. We based the assessment of our approach on several existing benchmark OPTW instances. We show that it generalizes across different tourists that visit each region and that it generally outperforms the most commonly used heuristic, while computing the solution in realistic times.",
  "problem_description_natural": "The Orienteering Problem with Time Windows (OPTW) involves selecting a route through a set of points of interest (nodes), each associated with a score, a time window during which it can be visited, and a required visit duration. The objective is to maximize the total collected score by visiting a subset of nodes, starting from a given location at or after a specified start time and ending at a designated endpoint before a global deadline. The route must respect both individual node time windows (i.e., visits must begin no earlier than the opening time and end no later than the closing time) and the overall time budget. This problem is particularly relevant in applications like the Tourist Trip Design Problem (TTDP), where a tourist aims to plan an optimal itinerary within limited time, considering attraction opening hours and personal preferences (scores).",
  "problem_type": "combinatorial optimization",
  "datasets": [
    "Solomon",
    "Cordeau",
    "Gavalas"
  ],
  "performance_metrics": [
    "Total Score",
    "Score Gap to ILS",
    "Score Gap to Best-Known",
    "Inference Time"
  ],
  "lp_model": {
    "objective": "\\max \\sum_{i=1}^{n} r_i y_i",
    "constraints": [
      "a_j + \\text{wait}_j \\leq c_j \\quad \\forall j \\text{ in the route}",
      "a_j + \\text{wait}_j + d_j + \\Delta_{jn} \\leq T_{end} \\quad \\forall j \\text{ in the route}",
      "\\text{wait}_j = \\max(0, o_j - a_j)",
      "The route must start at v_1 and end at v_n"
    ],
    "variables": "y_i \\in \\{0,1\\} \\text{ for node } i \\text{ visited}, a_i \\in \\mathbb{R} \\text{ arrival time at node } i"
  },
  "raw_latex_model": "The objective is to maximize the total score collected from visited locations. Formally, for a solution route $S = (\\pi_1, \\dots, \\pi_m)$, maximize $\\sum_{v_i \\in S} r_i$. Subject to constraints for each node $v_j$ in the route: $$ a_j + \\text{wait}_j \\leq c_j; $$ and $$ a_j + \\text{wait}_j + d_j + \\Delta_{jn} \\leq T_{end}, $$ where $\\text{wait}_j = \\max(0, o_j - a_j)$.",
  "algorithm_description": "Step-by-step description of the reinforcement learning algorithm using Pointer Network for OPTW:\n1. Model Initialization: Initialize the Pointer Network model with parameters θ. The model consists of a set encoder (Transformer with recursion and graph masked self-attention), a sequence encoder (LSTM), and a pointing mechanism.\n2. Training Loop (REINFORCE Algorithm):\n   a. For each training epoch (e.g., 500,000 epochs):\n      i. Sample an instance φ (a tourist-region instance with generated parameters).\n      ii. For b = 1 to B (batch size, e.g., 32):\n          - Construct a solution S_b by sequentially sampling nodes from the model's policy pθ until the end node v_n is reached.\n          - Compute the total reward R(S_b) as the sum of scores of visited nodes.\n      iii. Compute the average reward R̄ = (1/B) ∑_{b=1}^{B} R(S_b).\n      iv. Compute the gradient estimate: gθ = - (1/B) ∑_{b=1}^{B} (R(S_b) - R̄) ∇θ log pθ(S_b|φ).\n      v. Update model parameters θ using the Adam optimizer with learning rate scheduling.\n3. Inference for New Instances:\n   a. Beam Search: For a given instance, maintain a beam of top-nb partial solutions (e.g., up to 128 beams). At each step, expand all admissible next nodes for each partial solution, compute probabilities using the model, and keep the top-nb sequences with highest cumulative probability. Continue until all sequences reach v_n, then select the solution with the highest total score.\n   b. Active Search (Optional): For a specific instance, fine-tune the model using the REINFORCE algorithm for a fixed number of epochs (e.g., 128), then apply beam search as above."
}