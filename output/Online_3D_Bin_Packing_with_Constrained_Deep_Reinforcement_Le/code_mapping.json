{
  "file_path": "MCTS/monteCarlo.py, MCTS/node.py, acktr/algo/acktr_pipeline.py, acktr/model.py, acktr/reorder.py, envs/bpp0/bin3D.py, envs/bpp0/space.py, user_study/bin3D.py, user_study/space.py",
  "function_name": "MCTree, PutNode.expand, PutNode.roll_out, ACKTR.update, Policy, CNNPro, ReorderTree, PackingGame, Space, PackingGame, AdjustPackingGame, Space",
  "code_snippet": "\n\n# ==========================================\n# File: MCTS/monteCarlo.py\n# Function/Context: MCTree\n# ==========================================\nfrom node import PutNode\nimport copy, time\nimport numpy as np\n\n\ndef softmax(x):\n    probs = np.exp(x - np.max(x))\n    probs /= np.sum(probs)\n    return probs\n\n\nclass MCTree(object):\n    def __init__(self, environment, observation, size_seq, nmodel = None, search_depth=None, rollout_length=-1, credit=1):\n        self.sim_env = environment\n        self.root = PutNode(None, 1.0)\n        self.observation = observation\n\n        self.known_size_seq = size_seq\n        self.rollout_length = rollout_length\n        self.credit = credit\n        self.c = 1\n\n        self.subrt = 0\n        self.reached_depth = -1\n        self.nmodel = nmodel\n        if search_depth is not None:\n            self.max_depth = min(search_depth, len(self.known_size_seq)-1)\n        else:\n            self.max_depth = len(self.known_size_seq)-1\n\n        # if there is only one know box size, we can't do simulation\n        assert len(self.known_size_seq) >= 2\n\n    def select(self):\n        cur_node = self.root\n        cur_depth = 0\n        obs = self.observation\n        sim2_env = copy.deepcopy(self.sim_env)\n\n        while True:\n            # Terminated: back up\n            if cur_node.is_terminated():\n                # without future\n                value = 0\n                break\n            # Not Expanded: expand node\n            if not cur_node.is_expanded():\n                #print('expand:',cur_depth)\n                pointer = cur_depth\n                cur_node.expand(nmodel= self.nmodel,\n                                box_size_list=self.known_size_seq[pointer:],\n                                rollout_length=self.rollout_length,\n                                credit=self.credit,\n                                observation=obs,\n                                sim_env=sim2_env)\n                value = cur_node.value\n                break\n            # reached max depth: back up\n            if cur_depth == self.max_depth:\n                value = cur_node.value\n                break\n            # not leaf node: use tree policy\n            cur_action, next_node = cur_node.choose_best(self.c)\n            # Simulate time: take the action\n            action_idx = cur_action\n            obs, reward, done, _ = sim2_env.step([action_idx])\n            next_node.reward = reward\n            if done:\n                self.subrt += 1\n                if not next_node.is_terminated():\n                    next_node.terminate()\n                cur_node = next_node\n                value = 0\n                break\n            cur_node = next_node\n            cur_depth += 1\n\n        if cur_depth > self.reached_depth:\n            self.reached_depth = cur_depth\n        self.backup(cur_node, value)\n\n    def backup(self, leaf_node, value, gamma=1):\n        cur_node = leaf_node\n        while True:\n            value = cur_node.reward + gamma * value\n            cur_node.update(value)\n            if cur_node.prev_node is not None:\n                cur_node = cur_node.prev_node\n                continue\n            break\n\n    def play(self, zeta):\n        actions_visits = [(a, nd.n) for (a, nd) in self.root.next_nodes.items()]\n        actions, visits = zip(*actions_visits)\n        values = softmax(1.0 / zeta * np.log(np.array(visits) + 1e-10))\n        actions_values = dict(zip(actions, values))\n        return actions_values\n\n    def get_policy(self, sim_times, zeta=1):\n        start = time.clock()\n        for i in range(sim_times):\n            # print('simulation',i+1)\n            self.select()\n        end = time.clock()\n        p = self.play(zeta)\n        print('cost time', end-start)\n        print(\"terminated node:\", self.subrt)\n        print('reached depth:', self.reached_depth)\n        return p\n\n    def sample_action(self, policy):\n        if self.max_depth == 0:\n            def get_p(key):\n                return self.root.next_nodes[key].p\n            max_action = max(self.root.next_nodes, key=get_p)\n            return max_action\n        poss = [pos for _, pos in policy.items()]\n        actions = [key for key in policy.keys()]\n        action = np.random.choice(actions, p=poss)\n        return action\n\n    def succeed(self, put_action, new_box_size, observation):\n        put_action = int(put_action)\n        self.known_size_seq.pop(0)\n        self.known_size_seq.append(new_box_size)\n        new_node = self.root.next_nodes.get(put_action)\n        assert new_node is not None\n        new_node.p = 1.0\n        new_node.prev_node = None\n        print('reused simulation times:', new_node.n)\n        print('children node number:', len(new_node.next_nodes))\n        self.observation = observation\n        self.root = new_node\n        self.reached_depth = -1\n        self.subrt = 0\n\n# ==========================================\n# File: MCTS/node.py\n# Function/Context: PutNode.expand, PutNode.roll_out\n# ==========================================\nimport numpy as np\nimport math, copy, time\n\n\nINF = 1e9+7\n\nclass Node:\n    def __init__(self, prev, p):\n        self.prev_node = prev\n        self.next_nodes = {}\n        self.terminated = False\n        self.value = None\n        self.reward = 0\n\n        self.q = 0\n        self.w = 0\n        self.n = 0\n        self.p = p\n\n    def is_expanded(self):\n        return len(self.next_nodes) > 0\n\n    def is_terminated(self):\n        return self.terminated\n\n    def terminate(self):\n        self.terminated = True\n        self.p = 0\n\n    def update(self, value):\n        self.n += 1\n        self.w += value\n        # normal average\n        self.q = self.w / self.n\n        # moving average\n\n    def get_u_value(self):\n        u_value = self.p * np.sqrt(self.prev_node.n)/(self.n+1)\n        return u_value\n\n    def get_q_value(self):\n        return self.q\n\n    # def check_terminate(self):\n    #     ps = []\n    #     for _, node in self.next_nodes.items():\n    #         ps.append(node.p)\n    #     ps = np.array(ps)\n    #     p_sum = np.sum(ps)\n    #     if math.isclose(p_sum, 1.0, rel_tol=1e-3):\n    #         return False\n    #     ps = ps / p_sum\n    #     ps = ps.tolist()\n    #     for _, node in self.next_nodes.items():\n    #         p_new = ps.pop(0)\n    #         p_old = node.p\n    #         node.p = p_new\n    #         assert p_old >= p_new\n    #     return True\n\n    def choose_best(self, c=1):\n        assert len(self.next_nodes) > 0\n        max_value = -INF\n        max_nodes = []\n\n        for (action, node) in self.next_nodes.items():\n            if node.n > 0:\n                advanced_q = node.get_q_value() - node.prev_node.get_q_value()\n                cur_value = advanced_q + c * node.get_u_value()\n            else:\n                cur_value = 0.0 + c * node.get_u_value()\n            if math.isclose(cur_value, max_value, rel_tol=1e-5):\n                max_nodes.append((action, node))\n            elif cur_value > max_value:\n                max_value = cur_value\n                max_nodes.clear()\n                max_nodes.append((action, node))\n        assert len(max_nodes) > 0\n        idx = np.random.randint(0, len(max_nodes))\n        return max_nodes[idx]\n\n    def expand(self, **kwargs):\n        pass\n\n\nclass PutNode(Node):\n\n    def __init__(self, prev, p):\n        super().__init__(prev, p)\n        self.q = 0\n\n    def expand(self, nmodel, **kwargs):\n\n        credit = kwargs.get('credit')\n        rollout_length = kwargs.get('rollout_length')\n        box_size_list = kwargs.get('box_size_list')\n        observation = kwargs.get('observation')\n        sim_env = kwargs.get('sim_env')\n        \n        assert box_size_list is not None\n        assert len(box_size_list) >= 1\n        assert observation is not None\n        assert sim_env is not None\n\n        if credit is not None:\n            assert credit <=1 and credit >=0\n        else:\n            credit = 1\n\n        if rollout_length is not None:\n            if rollout_length == -1:\n                rollout_length = len(box_size_list)-1\n        else:\n            rollout_length = 0\n        \n        # get valid position\n        action_mask = sim_env.get_possible_position()\n        action_mask = np.reshape(action_mask, newshape=(-1,))\n        \n        # get possibilities using neural network\n        value, pvec = nmodel.evaluate(observation, False)\n\n        valid_action_num = np.sum(action_mask)\n        \n        for i in range(len(action_mask)):\n            action = i\n            if action_mask[i] == 1: # !!! still use mask !!!\n                action_possibility = credit * pvec[action] + (1-credit) * (1/valid_action_num)\n                self.next_nodes[action] = PutNode(self, action_possibility)\n\n        # no give-up action, default action is '0'\n        if len(self.next_nodes) == 0:\n            self.next_nodes[0] = PutNode(self, 1)\n\n        if rollout_length >= 1 and len(box_size_list) >= rollout_length + 1:\n            value = self.roll_out(box_size_list[:rollout_length+1], copy.deepcopy(sim_env), observation, nmodel)\n        self.value = value\n\n    def roll_out(self, box_size_list, sim_env, observation, nmodel, gamma=1):\n        assert box_size_list is not None\n        assert sim_env is not None\n        assert observation is not None\n\n        # 做出的动作数量\n        sim3_env = sim_env\n        obs = observation\n        box_num = len(box_size_list)\n        reward_stack = []\n\n        value = None\n        for i in range(box_num):\n            # box_size = box_size_list[i]\n            value, prev = nmodel.evaluate(obs, False)\n\n            # action_pos = dict(zip(range(prev.shape[0]), prev))\n            # action_max = max(action_pos, key=action_pos.get)\n            # obs, reward, done, _ = sim3_env.step([action_max])\n\n            action_sample = np.random.choice(prev.shape[0], p=prev)\n            obs, reward, done, _ = sim3_env.step([action_sample])\n\n            if not done and i+1 < box_num:\n                reward_stack.append(reward)\n            if done:\n                reward_stack.append(reward)\n                value = 0\n                break\n\n        for i in range(len(reward_stack)-1, -1, -1):\n            value = reward_stack[i] + gamma * value\n        return value\n\n# ==========================================\n# File: acktr/algo/acktr_pipeline.py\n# Function/Context: ACKTR.update\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom acktr.algo.kfac import KFACOptimizer\nimport sys\n\nclass ACKTR():\n    def __init__(self,\n                 actor_critic,\n                 value_loss_coef,\n                 entropy_coef,\n                 invaild_coef,\n                 lr = None,\n                 eps = None,\n                 alpha = None,\n                 max_grad_norm = None,\n                 acktr = False,\n                 args = None):\n\n        self.actor_critic = actor_critic\n        self.acktr = acktr\n\n        self.value_loss_coef = value_loss_coef\n        self.invaild_coef = invaild_coef\n        self.max_grad_norm = max_grad_norm\n\n        self.loss_func = nn.MSELoss(reduce=False, size_average=True)\n        self.entropy_coef = entropy_coef\n        self.args = args\n\n        if acktr:\n            self.optimizer = KFACOptimizer(actor_critic)\n        else:\n            self.optimizer = optim.RMSprop(actor_critic.parameters(), lr, eps=eps, alpha=alpha)\n\n\n    def update(self, rollouts):\n        # check_nan(self.actor_critic, 1)\n        obs_shape = rollouts.obs.size()[2:]\n        action_shape = rollouts.actions.size()[-1]\n        num_steps, num_processes, _ = rollouts.rewards.size()\n        mask_size = rollouts.location_masks.size()[-1]\n\n        values, action_log_probs, dist_entropy, _, bad_prob, pred_mask = self.actor_critic.evaluate_actions(\n            rollouts.obs[:-1].view(-1, *obs_shape),\n            rollouts.recurrent_hidden_states[0].view(-1, self.actor_critic.recurrent_hidden_state_size),\n            rollouts.masks[:-1].view(-1, 1),\n            rollouts.actions.view(-1, action_shape),\n            rollouts.location_masks[:-1].view(-1, mask_size))\n\n        values = values.view(num_steps, num_processes, 1)\n        action_log_probs = action_log_probs.view(num_steps, num_processes, 1)\n\n        advantages = rollouts.returns[:-1] - values\n        value_loss = advantages.pow(2).mean()\n        action_loss = -(advantages.detach() * action_log_probs).mean()\n\n        mask_len = self.args.container_size[0]*self.args.container_size[1]\n        mask_len = mask_len * (1+ self.args.enable_rotation)\n        pred_mask = pred_mask.reshape((num_steps,num_processes,mask_len))\n\n        mask_truth = rollouts.location_masks[0:num_steps] \n        graph_loss = self.loss_func(pred_mask, mask_truth).mean()\n        dist_entropy = dist_entropy.mean()\n        prob_loss = bad_prob.mean()\n\n        if self.acktr and self.optimizer.steps % self.optimizer.Ts == 0:\n            # Sampled fisher, see Martens 2014\n            self.actor_critic.zero_grad()\n            pg_fisher_loss = -action_log_probs.mean()\n\n            value_noise = torch.randn(values.size())\n            if values.is_cuda:\n                value_noise = value_noise.cuda()\n\n            sample_values = values + value_noise\n            vf_fisher_loss = -(values - sample_values.detach()).pow(2).mean() # detach\n\n            fisher_loss = pg_fisher_loss + vf_fisher_loss + graph_loss * 1e-8\n            # fisher_loss = pg_fisher_loss + vf_fisher_loss\n            self.optimizer.acc_stats = True\n            fisher_loss.backward(retain_graph=True)\n            self.optimizer.acc_stats = False\n\n        force = 0.5 * 10\n        self.optimizer.zero_grad()\n        loss = value_loss * self.value_loss_coef\n        loss += action_loss\n        loss += prob_loss * self.invaild_coef\n        loss -= dist_entropy * self.entropy_coef\n        loss += force * graph_loss\n        loss.backward()\n\n        if self.acktr == False:\n            nn.utils.clip_grad_norm_(self.actor_critic.parameters(), self.max_grad_norm)\n\n        self.optimizer.step()\n\n        # return value_loss.item(), action_loss.item(), dist_entropy.item(), prob_loss.item()\n        return value_loss.item(), action_loss.item(), dist_entropy.item(), prob_loss.item(), graph_loss.item()\n\ndef check_nan(model,index):\n    for p in model.parameters():\n        if np.isnan(p.grad.data.mean().item()):\n            print('index '+ str(index) +' happened an error!')\n\n# ==========================================\n# File: acktr/model.py\n# Function/Context: Policy, CNNPro\n# ==========================================\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom acktr.distributions import Bernoulli, Categorical, DiagGaussian\nfrom acktr.utils import init\nimport sys\nsys.path.append('../')\n\nclass Flatten(nn.Module):\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n\n\nclass Policy(nn.Module):\n    def __init__(self, obs_shape, action_space, base=None, base_kwargs=None):\n        super(Policy, self).__init__()\n        if base_kwargs is None:\n            base_kwargs = {}\n        if base is None:\n            if len(obs_shape) == 3:\n                base = CNNBase\n            elif len(obs_shape) == 1:\n                base = CNNPro\n            else:\n                raise NotImplementedError\n        self.base = base(obs_shape[0], **base_kwargs)\n        if action_space.__class__.__name__ == \"Discrete\":\n            num_outputs = action_space.n\n            self.dist = Categorical(self.base.output_size, num_outputs)\n        else:\n            raise NotImplementedError\n\n    @property\n    def is_recurrent(self):\n        return self.base.is_recurrent\n\n    @property\n    def recurrent_hidden_state_size(self):\n        \"\"\"Size of rnn_hx.\"\"\"\n        return self.base.recurrent_hidden_state_size\n\n    def forward(self, inputs, rnn_hxs, masks):\n        raise NotImplementedError\n\n    def binary(self,input):\n        a = torch.ones_like(input)\n        b = torch.zeros_like(input)\n        output = torch.where(input >= 0.5, a, b)\n        return output\n\n    def act(self, inputs, rnn_hxs, masks, location_masks, deterministic=False):\n        value, actor_features, rnn_hxs, graph = self.base(inputs, rnn_hxs, masks)\n        dist, bad_prob, _ = self.dist(actor_features, location_masks)\n\n        if deterministic:\n            action = dist.mode()\n        else:\n            action = dist.sample()\n\n        action_log_probs = dist.log_probs(action)\n\n        return value, action, action_log_probs, rnn_hxs\n\n    def act_indepen(self, inputs, rnn_hxs, masks, deterministic=False):\n        value, actor_features, rnn_hxs, graph = self.base(inputs, rnn_hxs, masks)\n        pred_mask = self.binary(graph)\n        dist,_ = self.dist(actor_features, pred_mask)\n        if deterministic:\n            action = dist.mode()\n        else:\n            action = dist.sample()\n        action_log_probs = dist.log_probs(action)\n        return value, action, action_log_probs, pred_mask\n\n    def get_value(self, inputs, rnn_hxs, masks):\n        value, _, _ ,_= self.base(inputs, rnn_hxs, masks)\n        return value\n\n    def get_policy_distribution(self,inputs, rnn_hxs, masks):\n        value, actor_features, rnn_hxs = self.base(inputs, 0, 0)\n        distribution = self.dist.get_policy_distribution(actor_features)\n        return distribution\n\n    def evaluate_actions(self, inputs, rnn_hxs, masks, action, location_masks):\n        value, actor_features, rnn_hxs, graph = self.base(inputs, rnn_hxs, masks)\n        dist, bad_prob, mask_dist= self.dist(actor_features, location_masks)\n        action_log_probs = dist.log_probs(action)\n        dist_entropy = dist.entropy().mean()\n\n        return value, action_log_probs, dist_entropy, rnn_hxs, bad_prob, graph\n\n    def evaluate_actions_indepen(self, inputs, rnn_hxs, masks, action):\n        value, actor_features, _, graph = self.base(inputs, rnn_hxs, masks)\n        pred_mask = self.binary(graph)\n        dist, _ = self.dist(actor_features, pred_mask)\n        action_log_probs = dist.log_probs(action)\n        dist_entropy = dist.entropy().mean()\n        return value, action_log_probs, dist_entropy, graph\n\n\nclass NNBase(nn.Module):\n    def __init__(self, recurrent, recurrent_input_size, hidden_size, args):\n        super(NNBase, self).__init__()\n\n        self._hidden_size = hidden_size\n        self._recurrent = recurrent\n\n        if recurrent:\n            self.gru = nn.GRU(recurrent_input_size, hidden_size)\n            for name, param in self.gru.named_parameters():\n                if 'bias' in name:\n                    nn.init.constant_(param, 0)\n                elif 'weight' in name:\n                    nn.init.orthogonal_(param)\n\n    @property\n    def is_recurrent(self):\n        return self._recurrent\n\n    @property\n    def recurrent_hidden_state_size(self):\n        if self._recurrent:\n            return self._hidden_size\n        return 1\n\n    @property\n    def output_size(self):\n        return self._hidden_size\n    def _forward_gru(self, x, hxs, masks):\n        if x.size(0) == hxs.size(0):\n            x, hxs = self.gru(x.unsqueeze(0), (hxs * masks).unsqueeze(0))\n            x = x.squeeze(0)\n            hxs = hxs.squeeze(0)\n        else:\n            # x is a (T, N, -1) tensor that has been flatten to (T * N, -1)\n            N = hxs.size(0)\n            T = int(x.size(0) / N)\n\n            # unflatten\n            x = x.view(T, N, x.size(1))\n\n            # Same deal with masks\n            masks = masks.view(T, N)\n\n            # Let's figure out which steps in the sequence have a zero for any agent\n            # We will always assume t=0 has a zero in it as that makes the logic cleaner\n            has_zeros = ((masks[1:] == 0.0) \\\n                            .any(dim=-1)\n                            .nonzero()\n                            .squeeze()\n                            .cpu())\n\n            # +1 to correct the masks[1:]\n            if has_zeros.dim() == 0:\n                # Deal with scalar\n                has_zeros = [has_zeros.item() + 1]\n            else:\n                has_zeros = (has_zeros + 1).numpy().tolist()\n\n            # add t=0 and t=T to the list\n            has_zeros = [0] + has_zeros + [T]\n\n            hxs = hxs.unsqueeze(0)\n            outputs = []\n            for i in range(len(has_zeros) - 1):\n                # We can now process steps that don't have any zeros in masks together!\n                # This is much faster\n                start_idx = has_zeros[i]\n                end_idx = has_zeros[i + 1]\n\n                rnn_scores, hxs = self.gru(\n                    x[start_idx:end_idx],\n                    hxs * masks[start_idx].view(1, -1, 1))\n\n                outputs.append(rnn_scores)\n\n            # assert len(outputs) == T\n            # x is a (T, N, -1) tensor\n            x = torch.cat(outputs, dim=0)\n            # flatten\n            x = x.view(T * N, -1)\n            hxs = hxs.squeeze(0)\n\n        return x, hxs\n\nclass CNNPro(NNBase):\n    def __init__(self, num_inputs, recurrent=False, hidden_size=256, args = None):\n        super(CNNPro, self).__init__(recurrent, num_inputs, hidden_size, args)\n        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.constant_(x, 0), nn.init.calculate_gain('relu'))\n        self.args = args\n        self.share = nn.Sequential(\n            init_(nn.Conv2d(args.channel, 64, 3, stride=1, padding=1)),\n            nn.ReLU(),\n            init_(nn.Conv2d(64, 64, 3, stride=1, padding=1)),\n            nn.ReLU(),\n            init_(nn.Conv2d(64, 64, 3, stride=1, padding=1)),\n            nn.ReLU(),\n            init_(nn.Conv2d(64, 64, 3, stride=1, padding=1)),\n            nn.ReLU(),\n            init_(nn.Conv2d(64, 64, 3, stride=1, padding=1)),\n            nn.ReLU(),\n        )\n        pred_len = args.container_size[0] * args.container_size[1]\n        if args.enable_rotation:\n            pred_len = pred_len * 2\n            \n        self.mask = nn.Sequential(\n            init_(nn.Conv2d(64, 8, 1, stride=1)),\n            nn.ReLU(),\n            Flatten(),\n            init_(nn.Linear(8*args.pallet_size*args.pallet_size, hidden_size)),\n            nn.ReLU(),\n            init_(nn.Linear(hidden_size, pred_len)),\n            nn.ReLU(),\n            # nn.Sigmoid(),\n        )\n\n        self.actor = nn.Sequential(\n            init_(nn.Conv2d(64, 8, 1, stride=1)),\n            nn.ReLU(),\n            Flatten(),\n            init_(nn.Linear(8*args.pallet_size*args.pallet_size, hidden_size)),\n            nn.ReLU(),\n        )\n\n        self.critic = nn.Sequential(\n            init_(nn.Conv2d(64, 4, 1, stride=1)),\n            nn.ReLU(),\n            Flatten(),\n            init_(nn.Linear(4*args.pallet_size*args.pallet_size, hidden_size)),\n            nn.ReLU(),\n        )\n        self.critic_linear = init_(nn.Linear(hidden_size, 1))\n        self.train()\n\n    def forward(self, inputs, rnn_hxs, masks):\n        x = inputs.reshape((-1,self.args.channel,self.args.pallet_size,self.args.pallet_size))\n        assert not self.is_recurrent\n        share = self.share(x)\n        hidden_critic = self.critic(share)\n        hidden_actor = self.actor(share)\n        pred_mask = self.mask(share)\n        cl = self.critic_linear(hidden_critic)\n        return cl, hidden_actor, rnn_hxs, pred_mask\n\n# ==========================================\n# File: acktr/reorder.py\n# Function/Context: ReorderTree\n# ==========================================\nimport numpy as np\nimport copy\nimport gym\nimport math\nimport itertools\n\nclass Node(object):\n    def __init__(self, parent, number, height):\n        self.max_value = None\n        self.action = None  # 记录最好的值和对应的动作\n        self.number = number\n        self.parent = parent\n\n        self.height = height\n        assert self.height is not None\n        self.children = []\n        self.dis_num = height\n\n        self.visit = 0\n        if self.height != -1:\n            self.max_v = math.factorial(self.height)\n        else:\n            self.max_v = 1\n\n    def get_q_value(self): # 直接继承父节点的q值吗？\n        if self.max_value is None:\n            return self.parent.get_q_value()\n        if self.visit >= self.max_v:\n            return -1000\n        if self.dis_num <= 0:\n            return -10000\n        return self.max_value\n\n    def get_u_value(self): # 没有用到policy,直接就展开了\n        c = 0.5\n        return c * np.sqrt(self.parent.visit) / (self.visit + 1)\n\n    def get_value(self):\n        return self.get_q_value() + self.get_u_value()\n\n    def disable(self):\n        self.dis_num = 0\n        if self.parent is None:\n            return\n        self.parent.dis_num -= 1\n        if self.parent.dis_num == 0:\n            self.parent.disable()\n\n    def update(self, value, action):\n        self.visit += 1\n        if self.max_value is None or value > self.max_value:\n            self.max_value = value\n            self.action = action\n        if self.parent is not None:\n            self.parent.update(value, action)\n\n# FOR ENV 'MASK'\nclass ReorderTree(object):\n    def __init__(self, nmodel, box_list, env, encode=True, p_bound=0.8, v_bound=0.1, times=100):\n        self.encode = encode\n        # the box number used for reordering\n        self.box_num = len(box_list)\n        # the network of single step\n        self.nmodel = nmodel\n        # the shape of the action mask\n        self.mask_shape = env.bin_size[:2]\n        self.mask_len = self.mask_shape[0] * self.mask_shape[1]\n        # copy the env and box list\n        self.env = copy.deepcopy(env)\n        self.box_list = copy.deepcopy(box_list)\n        # threshold\n        self.p_bound = p_bound\n        self.v_bound = v_bound\n        self.pos_num = int(1 / self.p_bound)\n        self.times = min(times, math.factorial(self.box_num - 1))\n\n    def get_order_mask(self, smask, box_size):\n        emask = copy.deepcopy(smask)\n        emask = emask.reshape(self.mask_shape)\n        ex = emask.shape[0] - box_size[0] + 1\n        ey = emask.shape[1] - box_size[1] + 1\n        for i in range(ex):\n            for j in range(ey):\n                if emask[i][j] == 1:\n                    if smask.reshape(self.mask_shape)[i:i + box_size[0], j:j + box_size[1]].min() == 0:\n                        emask[i][j] = 0\n        return emask.reshape(-1)\n\n    def get_mixed_mask(self, masks, real_idx, box_size, raw_mask):\n        action_mask = copy.deepcopy(raw_mask).astype(np.int32)\n        ######\n        stacked_mask = np.ones_like(masks[0])\n        for i in range(real_idx + 1, self.box_num):\n            stacked_mask = (stacked_mask == 1) & (masks[i] == 1)\n        stacked_mask = stacked_mask.astype(np.int32)\n        order_mask = self.get_order_mask(stacked_mask, box_size)\n        ######\n        mixed_mask = (action_mask == 1) & (order_mask == 1)\n        mixed_mask = mixed_mask.astype(np.int32)\n        return mixed_mask\n\n    def get_mixed_obs(self, masks, real_idx, raw_obs):\n        max_height = self.env.bin_size[-1]\n        new_obs = copy.deepcopy(raw_obs).reshape(self.env.bin_size[:2])\n        new_obs = new_obs.reshape(-1)\n        for i in range(real_idx+1, self.box_num):\n            new_obs = max_height * (1 - masks[i]) + new_obs * masks[i]\n        return new_obs\n\n    def update_mask(self, mask, box, pos):\n        pos = (pos // self.mask_shape[1], pos % self.mask_shape[1])\n        cmask = copy.deepcopy(mask).reshape(self.mask_shape)\n        cmask[pos[0]:pos[0] + box[0], pos[1]:pos[1] + box[1]] = 0\n        return cmask.reshape(-1)\n\n    def will_terminate(self,  mixed_obs):\n        max_height = self.env.bin_size[-1]\n        # rsum = np.sum(raw_mask)\n        ssum = np.sum(mixed_obs)\n        # print(rsum, ssum)\n        return ssum == self.mask_len * max_height\n        # return rsum > 0.8 * self.mask_len or ssum == self.mask_len * max_height\n\n    def evaluate(self, obs, masks, real_idx):\n        # 4 channels\n        revised_obs = copy.deepcopy(obs).reshape(4,-1)\n        raw_obs = copy.deepcopy(revised_obs[0])\n        new_obs = self.get_mixed_obs(masks, real_idx, raw_obs)\n        revised_obs[0] = new_obs\n        revised_obs = revised_obs.reshape((-1,))\n        val, poss = self.nmodel.evaluate(revised_obs)\n        pos_candidates = list(np.argsort(poss)[-self.pos_num:])\n        wt = self.will_terminate(new_obs)\n        return val, pos_candidates, wt\n\n    def search(self, masks, cur_env, res_idxs, cur_node, cur_value, action):\n        assert cur_node is not None\n        # print('DISNUM: ', cur_node.dis_num)\n        next_eval = 0\n        next_node = None\n        # print(res_idxs)\n\n        if len(cur_node.children) == 0:\n            for idx in res_idxs:\n                if idx == self.box_num - 1 and len(res_idxs) > 1:\n                    continue\n                cur_node.children.append(Node(parent=cur_node, number=idx, height=cur_node.height - 1))\n\n        # find next node with max evaluation\n        for node in cur_node.children:\n            node_eval = node.get_value()\n            assert node.dis_num >= -1\n            # print(node.number, node_eval, node.dis_num)\n            if node_eval > next_eval:\n                next_eval = node_eval\n                next_node = node\n\n        # using single-step model to evaluate future\n        if next_node is None:\n            cur_node.update(-10000, None)\n            return\n        idx = next_node.number\n        cur_box = self.box_list[idx]\n        cur_env.box_creator.box_list = [cur_box, self.env.bin_size]\n        cur_obs = cur_env.cur_observation\n\n        # print(cur_obs[0:100].reshape(10,10))\n        # print(idx, cur_box)\n\n        val, pos_candidates, will_terminate = self.evaluate(cur_obs, masks, idx)\n        pos = pos_candidates[-1]\n        assert len(pos_candidates) == 1\n\n        # will_terminate = False\n        # # if may_terminate:\n        # tmp_env = copy.deepcopy(cur_env)\n        # next_obs, reward, done, _ = tmp_env.step([pos])\n        # if done:\n        #     will_terminate = True\n\n        next_obs, reward, done, _ = cur_env.step([pos])\n\n        if done or will_terminate: \n            fail_flag = False\n            for i in range(self.box_num):\n                if (i in res_idxs and i < idx) or (i not in res_idxs and i >= idx):\n                    fail_flag = True\n                    break  \n            if fail_flag:\n                next_node.disable()\n                cur_node.update(-10000, None)\n                return\n            if idx == 0:\n                cur_node.update(cur_value + 0, 0)\n                return\n            else:\n                cur_node.update(cur_value + 0, action)\n                return\n\n        # reach the evaluation point\n        if len(res_idxs) == 1:\n            assert idx == self.box_num - 1\n            if idx == 0:\n                cur_node.update(cur_value + val, pos)\n                return\n            else:\n                cur_node.update(cur_value + val, action)\n                return\n\n        # copy and update [res_idxs]\n        next_idxs = res_idxs\n        next_idxs.remove(idx)\n        # copy and update [env]\n        # assert not done\n        # copy and update [mask]\n        next_masks = masks\n        next_masks[idx] = self.update_mask(next_masks[idx], cur_box, pos)\n        # next value\n        next_value = cur_value + reward\n        # recursion\n        if action is None and idx == 0:\n            action = pos\n        self.search(next_masks, cur_env, next_idxs, next_node, next_value, action)\n\n    def get_baseline(self):\n        env = copy.deepcopy(self.env)\n        obs = env.cur_observation\n        nor_exp = 0\n        nor_act = None\n        area = self.mask_len\n        for i in range(self.box_num):\n            val, poss = self.nmodel.evaluate(obs)\n            act = np.argmax(poss)\n            if i == 0:\n                nor_act = act\n            obs, reward, done, info = env.step([act])\n            if done:\n                nor_exp += 0\n                return nor_exp, nor_act\n            if i == self.box_num - 1:\n                nor_exp += val\n                return nor_exp, nor_act\n            nor_exp += reward\n\n    def reorder_search(self):\n        nor_exp, nor_act = self.get_baseline()\n        root = Node(None, None, self.box_num - 1)\n        root.max_value = nor_exp \n        root.action = nor_act\n        for i in range(self.times):\n            sim_env = copy.deepcopy(self.env)\n            res_idxs = list(range(self.box_num))\n            masks = np.ones((self.box_num, self.mask_len))\n            self.search(masks, sim_env, res_idxs, root, 0, None)\n        max_exp = root.max_value\n        max_act = root.action\n        if max_act != nor_act and max_exp - nor_exp < self.v_bound:\n            # print('conservative!')\n            max_exp = nor_exp\n            max_act = nor_act\n        default = (max_act == nor_act)\n        return max_act, max_exp, default\n\n# ==========================================\n# File: envs/bpp0/bin3D.py\n# Function/Context: PackingGame\n# ==========================================\nfrom .space import Space\nimport numpy as np\nimport copy\nimport gym\nfrom .cutCreator import CuttingBoxCreator\nfrom .mdCreator  import MDlayerBoxCreator\nfrom .binCreator import RandomBoxCreator, LoadBoxCreator, BoxCreator\n\nclass PackingGame(gym.Env):\n    def __init__(self, box_creator=None, container_size = (20, 20, 20),\n                 box_set = None, data_name = None, test = False,\n                 data_type = 'cut1', enable_rotation=False, **kwags):\n        self.box_creator = box_creator\n        self.bin_size = container_size\n        self.area = int(self.bin_size[0] * self.bin_size[1])\n        self.space = Space(*self.bin_size)\n        self.can_rotate = enable_rotation\n\n        if not test and box_creator is None:\n            assert box_set is not None\n            if data_type == 'rs':\n                print('using random data')\n                self.box_creator = RandomBoxCreator(box_set)\n            elif data_type == 'cut1':\n                low = list(box_set[0])\n                up = list(box_set[-1])\n                low.extend(up)\n                print(low)\n                self.box_creator = CuttingBoxCreator(container_size, low, self.can_rotate)\n            elif data_type == 'cut2':\n                print('using md data')\n                self.box_creator = MDlayerBoxCreator(container_size, [box_set[0][0], box_set[-1][0]])\n            assert isinstance(self.box_creator, BoxCreator)\n\n        if test:\n            self.box_creator = LoadBoxCreator(data_name)\n\n        self.act_len = self.area * (1+self.can_rotate)\n        self.obs_len = self.area * (1+3)\n        self.action_space = gym.spaces.Discrete(self.act_len)\n        self.observation_space = gym.spaces.Box(low=0.0, high=self.space.height, shape=(self.obs_len,))\n        \n\n    def get_box_ratio(self):\n        coming_box = self.next_box\n        return (coming_box[0] * coming_box[1] * coming_box[2]) / (self.space.plain_size[0] * self.space.plain_size[1] * self.space.plain_size[2])\n\n\n    def get_box_plain(self):\n        x_plain = np.ones(self.space.plain_size[:2], dtype=np.int32) * self.next_box[0]\n        y_plain = np.ones(self.space.plain_size[:2], dtype=np.int32) * self.next_box[1]\n        z_plain = np.ones(self.space.plain_size[:2], dtype=np.int32) * self.next_box[2]\n        return (x_plain, y_plain, z_plain)\n\n    def reset(self):\n        self.box_creator.reset()\n        self.space = Space(*self.bin_size)\n        self.box_creator.generate_box_size()\n        return self.cur_observation\n\n    @property\n    def cur_observation(self):\n        hmap = self.space.plain\n        # mask = self.get_possible_position()\n        size = self.get_box_plain()\n        return np.reshape(np.stack((hmap,  *size)), newshape=(-1,))\n\n    @property\n    def next_box(self):\n        return self.box_creator.preview(1)[0]\n\n    def get_possible_position(self, plain=None):\n        x = self.next_box[0]\n        y = self.next_box[1]\n        z = self.next_box[2]\n\n        if plain is None:\n            plain = self.space.plain\n\n        width = self.space.plain_size[0]\n        length = self.space.plain_size[1]\n\n        action_mask = np.zeros(shape=(width, length), dtype=np.int32)\n        \n        for i in range(width-x+1):\n            for j in range(length-y+1):\n                if self.space.check_box(plain, x, y, i, j, z) >= 0:\n                    action_mask[i, j] = 1\n\n        if action_mask.sum() == 0:\n            action_mask[:, :] = 1\n        \n        return action_mask\n\n    def step(self, action):\n        if isinstance(action, np.ndarray) or isinstance(action, list):\n            idx = action[0]\n        else:\n            idx = action\n        flag = False\n        # check whether rotate the box\n        if idx > self.area:\n            assert self.can_rotate\n            idx = idx - self.area\n            flag = True\n        succeeded = self.space.drop_box(self.next_box, idx, flag)\n\n        if not succeeded:\n            reward = 0.0\n            done = True\n            info = {'counter':len(self.space.boxes), 'ratio':self.space.get_ratio(), 'mask':np.ones(shape=self.act_len)}\n            return self.cur_observation, reward, done, info\n\n        box_ratio = self.get_box_ratio()\n\n        self.box_creator.drop_box() # remove current box from the list\n        self.box_creator.generate_box_size() # add a new box to the list\n\n        plain = self.space.plain\n\n        reward = box_ratio * 10\n        done = False\n        info = dict()\n        info['counter'] = len(self.space.boxes)\n        info['ratio'] = self.space.get_ratio()\n        # info['mask'] = self.get_possible_position().reshape((-1,))\n        return self.cur_observation, reward, done, info\n\n# ==========================================\n# File: envs/bpp0/space.py\n# Function/Context: Space\n# ==========================================\nimport numpy as np\nfrom functools import reduce\nimport copy, time\n\n\nclass Box(object):\n    def __init__(self, x, y, z, lx, ly, lz):\n        self.x = x\n        self.y = y\n        self.z = z\n        self.lx = lx\n        self.ly = ly\n        self.lz = lz\n\n    def standardize(self):\n        return tuple([self.x, self.y, self.z, self.lx, self.ly, self.lz])\n\n\nclass Space(object):\n    def __init__(self, width=10, length=10, height=10):\n        self.plain_size = np.array([width, length, height])\n        self.plain = np.zeros(shape=(width, length), dtype=np.int32)\n        self.boxes = []\n        self.flags = [] # record rotation information\n        self.height = height\n\n    def print_height_graph(self):\n        print(self.plain)\n\n    def get_height_graph(self):\n        plain = np.zeros(shape=self.plain_size[:2], dtype=np.int32)\n        for box in self.boxes:\n            plain = self.update_height_graph(plain, box)\n        return plain\n\n    @staticmethod\n    def update_height_graph(plain, box):\n        plain = copy.deepcopy(plain)\n        le = box.lx\n        ri = box.lx + box.x\n        up = box.ly\n        do = box.ly + box.y\n        max_h = np.max(plain[le:ri, up:do])\n        max_h = max(max_h, box.lz + box.z)\n        plain[le:ri, up:do] = max_h\n        return plain\n\n    def get_box_list(self):\n        vec = list()\n        for box in self.boxes:\n            vec += box.standardize()\n        return vec\n\n    def get_plain():\n        return copy.deepcopy(self.plain)\n\n    def get_action_space(self):\n        return self.plain_size[0] * self.plain_size[1]\n\n    def check_box(self, plain, x, y, lx, ly, z):\n        if lx+x > self.plain_size[0] or ly+y > self.plain_size[1]:\n            return -1\n        if lx < 0 or ly < 0:\n            return -1\n\n        rec = plain[lx:lx+x, ly:ly+y]\n        r00 = rec[0,0]\n        r10 = rec[x-1,0]\n        r01 = rec[0,y-1]\n        r11 = rec[x-1,y-1]\n        rm = max(r00,r10,r01,r11)\n        sc = int(r00==rm)+int(r10==rm)+int(r01==rm)+int(r11==rm)\n        if sc < 3:\n            return -1\n        # get the max height\n        max_h = np.max(rec)\n        # check area and corner\n        max_area = np.sum(rec==max_h)\n        area = x * y\n\n        # check boundary\n        assert max_h >= 0\n        if max_h + z > self.height:\n            return -1\n     \n        if max_area/area > 0.95:\n            return max_h\n        if rm == max_h and sc == 3 and max_area/area > 0.85:\n            return max_h\n        if rm == max_h and sc == 4 and max_area/area > 0.50:\n            return max_h\n\n        return -1\n\n    def get_ratio(self):\n        vo = reduce(lambda x, y: x+y, [box.x * box.y * box.z for box in self.boxes], 0.0)\n        mx = self.plain_size[0] * self.plain_size[1] * self.plain_size[2]\n        ratio = vo / mx\n        assert ratio <= 1.0\n        return ratio\n\n    def idx_to_position(self, idx):\n        lx = idx // self.plain_size[1]\n        ly = idx % self.plain_size[1]\n        return lx, ly\n\n    def position_to_index(self, position):\n        assert len(position) == 2\n        assert position[0] >= 0 and position[1] >= 0\n        assert position[0] < self.plain_size[0] and position[1] < self.plain_size[1]\n        return position[0] * self.plain_size[1] + position[1]\n\n    def drop_box(self, box_size, idx, flag):\n        lx, ly = self.idx_to_position(idx)\n        if not flag:\n            x = box_size[0]\n            y = box_size[1]\n        else:\n            x = box_size[1]\n            y = box_size[0]\n        z = box_size[2]\n        plain = self.plain\n        new_h = self.check_box(plain, x, y, lx, ly, z)\n        if new_h != -1:\n            self.boxes.append(Box(x, y, z, lx, ly, new_h)) # record rotated box\n            self.flags.append(flag)\n            self.plain = self.update_height_graph(plain, self.boxes[-1])\n            self.height = max(self.height, new_h + z)\n            return True\n        return False\n\n# ==========================================\n# File: user_study/bin3D.py\n# Function/Context: PackingGame, AdjustPackingGame\n# ==========================================\nfrom space import Space\nfrom binCreator import LoadBoxCreator\nimport numpy as np\nfrom copy import deepcopy\nfrom space import Box\n\nclass PackingGame(object):\n    def __init__(self, box_creator=None, enable_give_up=False):\n        self.box_creator = box_creator\n        self.space = Space(10, 10, 10)\n        self.can_give_up = enable_give_up\n        self.next_box = None\n        self.cur_observation = None\n        self.data_name = 'cut_2.txt'\n        if self.box_creator is None:\n            self.box_creator = LoadBoxCreator(data_name = self.data_name)\n\n    def reset(self):\n        pass\n\n    def step(self, action):\n        idx = action[0]\n        ratio = self.space.get_ratio()\n        if idx >= self.space.get_action_space():\n            if self.can_give_up:\n                observation = self.cur_observation\n                reward = ratio\n                done = True\n                info = dict()\n                info['counter'] = len(self.space.boxes)\n                info['ratio'] = ratio\n                return observation, reward, done, info\n            else:\n                raise Exception('out of the boundary of action space')\n        succeeded = self.space.drop_box(self.next_box, idx)\n\n        if not succeeded:\n            observation = self.cur_observation\n            reward = -1.0\n            done = True\n            info = dict()\n            info['counter'] = len(self.space.boxes)\n            info['ratio'] = ratio\n            return observation, reward, done, info\n\n        ratio = self.space.get_ratio()\n        self.box_creator.get_box_size()\n        self.box_creator.generate_box_size()\n        self.next_box = self.box_creator.preview(1)[0]\n\n        observation = np.array([*self.space.plain.reshape\n        (shape=(self.space.get_action_space(),)),*self.next_box, ratio])\n\n        self.cur_observation = observation\n\n        reward = 0.0\n        done = False\n        info = dict()\n        info['counter'] = len(self.space.boxes)\n        info['ratio'] = ratio\n        return observation, reward, done, info\n\n    def get_possible_position(self):\n        pass\n\n\nclass AdjustPackingGame(PackingGame):\n    def __init__(self, box_creator=None, enable_give_up=False, adjust_grid=0, **kwags):\n        super().__init__(box_creator, enable_give_up)\n        self.adjust_grid = adjust_grid\n        if kwags.get('_adjust_ratio'):\n            self.adjust_grid = kwags.get('_adjust_ratio')\n        self.flip_possibility = kwags.get('flip_possibility')\n\n        self.adjust_flag = kwags.get('adjust')\n        self.container = Box(10,10,10,0,0,0)\n        self.container.set_color('skyblue')\n\n        if self.flip_possibility is not None:\n            self.UD_flip = False\n            self.LR_flip = False\n\n    def reset(self):\n        self.box_creator.reset()\n        self.space = Space(10, 10, 10)\n        self.box_creator.generate_box_size()\n        self.next_box = self.box_creator.preview(1)[0]\n        self.temp_box = deepcopy(self.next_box)\n        self.cur_observation = np.array(\n            [*np.reshape(self.space.plain, newshape=(self.space.get_action_space(),)),\n             *self.next_box, self.space.get_ratio()])\n        return np.append(self.cur_observation, self.get_possible_position(self.adjust_flag))\n\n    def get_possible_position(self, plain=None):\n        x = self.next_box[0]\n        y = self.next_box[1]\n        z = self.next_box[2]\n\n        if plain is None:\n            plain = self.space.plain\n\n        width = self.space.plain_size[0]\n        length = self.space.plain_size[1]\n\n        action_mask = np.zeros(shape=(width, length), dtype=np.int32)\n\n        for i in range(width):\n            for j in range(length):\n                if self.space.check_box(plain, x, y, i, j, z) >= 0:\n                    action_mask[i, j] = 1\n\n        if action_mask.sum() == 0:\n            action_mask[:, :] = 1\n\n        return action_mask\n\n    def get_flip(self):\n        if np.random.random() < self.flip_possibility:\n            print('flip_UD')\n            self.UD_flip = True\n        if np.random.random() < self.flip_possibility:\n            print('flip_LR')\n            self.LR_flip = True\n\n    def augment_observation(self, plain):\n        assert self.flip_possibility is not None\n        if self.UD_flip and self.LR_flip:\n            return np.flip(plain)\n        if self.UD_flip and not self.LR_flip:\n            return np.flipud(plain)\n        if not self.UD_flip and self.LR_flip:\n            return np.fliplr(plain)\n        else:\n            return plain\n\n    def transfer_action(self, action):\n        if not self.UD_flip and not self.LR_flip:\n            return action\n        bin_size = self.space.plain_size\n        box_size = self.next_box\n        lx, ly = self.space.idx_to_position(action)\n        if self.UD_flip and not self.LR_flip:\n            lx = bin_size[0] - lx - box_size[0]\n        if self.LR_flip and not self.UD_flip:\n            ly = bin_size[1] - ly - box_size[1]\n        if self.UD_flip and self.LR_flip:\n            lx = bin_size[0] - lx - box_size[0]\n            ly = bin_size[1] - ly - box_size[1]\n        transfered_action = self.space.position_to_index((lx, ly))\n        self.UD_flip = False\n        self.LR_flip = False\n        return transfered_action\n\n    def try_step(self, action):\n        idx = action[0]\n        succeeded = self.space.try_drop(self.temp_box, idx)\n        return succeeded\n\n    def step(self, action):\n        idx = action[0]\n        if self.flip_possibility is not None:\n            idx = self.transfer_action(idx)\n\n        if idx >= self.space.get_action_space():\n            if self.can_give_up:\n                ratio = self.space.get_ratio()\n                observation = np.append(self.cur_observation, self.get_possible_position(self.adjust_flag))\n                reward = ratio * 10  # todo\n                done = True\n                info = dict()\n                info['mask'] = self.get_possible_position(self.adjust_flag)\n                info['counter'] = len(self.space.boxes)\n                info['ratio'] = ratio\n                return observation, reward, done, info\n            else:\n                raise Exception('out of the boundary of action space')\n\n        succeeded = self.space.drop_box(self.next_box, idx)\n\n        if not succeeded:\n            ratio = self.space.get_ratio()\n            observation = np.append(self.cur_observation, self.get_possible_position(self.adjust_flag))\n            if self.can_give_up:\n                reward = 0.0\n            else:\n                reward = ratio * 10\n            done = True\n            info = dict()\n            info['counter'] = len(self.space.boxes)\n            info['ratio'] = ratio\n            info['mask'] = self.get_possible_position(self.adjust_flag)\n            return observation, reward, done, info\n\n        self.box_creator.get_box_size()\n        self.box_creator.generate_box_size()\n        self.next_box = self.box_creator.preview(1)[0]\n        self.temp_box = deepcopy(self.next_box)\n\n        plain = self.space.plain\n        if self.flip_possibility is not None:\n            self.get_flip()\n            plain = self.augment_observation(plain)\n\n        observation = np.array([*np.reshape(plain, newshape=(-1,)),\n                                *self.next_box, self.space.get_ratio()])\n        self.cur_observation = observation\n\n        mask = self.get_possible_position(self.adjust_flag)\n        observation = np.append(self.cur_observation, mask)\n        reward = 0\n        done = False\n        info = dict()\n        info['counter'] = len(self.space.boxes)\n        info['ratio'] = self.space.get_ratio()\n        # info['dis'] = dis\n        info['mask'] = mask\n        return observation, reward, done, info\n\n    def _get_dis(self, mov):\n        return int(np.linalg.norm(mov, ord=1))\n\n    def _min_mov(self, point, targets, lx, ly):\n        min_dis = 1000\n        min_vec = None\n        for target in targets:\n            target = np.array(target, dtype=np.int32)\n            cur_vec = target - point\n            cur_dis = self._get_dis(cur_vec)\n            if cur_dis <= self.adjust_grid and cur_dis < min_dis:\n                plain = self.space.plain\n                x = self.next_box[0]\n                y = self.next_box[1]\n                z = self.next_box[2]\n                adj_lx = lx + cur_vec[0]\n                adj_ly = ly + cur_vec[1]\n                if self.space.check_box(plain, x, y, adj_lx, adj_ly, z) >= 0:\n                    min_dis = cur_dis\n                    min_vec = cur_vec\n        return min_vec, min_dis\n\n    def adjust(self, idx):\n        movec = np.zeros(shape=2, dtype=np.int32)\n        dis = 1000\n\n        lx, ly = self.space.idx_to_position(idx)\n        x = self.next_box[0]\n        y = self.next_box[1]\n\n        guad = self.space.get_corners()\n        lu = np.array([lx, ly], dtype=np.int32)\n        ld = np.array([lx + x, ly], dtype=np.int32)\n        ru = np.array([lx, ly + y], dtype=np.int32)\n        rd = np.array([lx + x, ly + y], dtype=np.int32)\n        mov_lu, dis_lu = self._min_mov(lu, guad[3], lx, ly)\n        mov_ld, dis_ld = self._min_mov(ld, guad[0], lx, ly)\n        mov_ru, dis_ru = self._min_mov(ru, guad[2], lx, ly)\n        mov_rd, dis_rd = self._min_mov(rd, guad[1], lx, ly)\n\n        def func(cur_mov, cur_dis, mov, min_dis):\n            if cur_dis < min_dis:\n                mov = cur_mov\n                min_dis = cur_dis\n            return mov, min_dis\n\n        movec, dis = func(mov_lu, dis_lu, movec, dis)\n        movec, dis = func(mov_ld, dis_ld, movec, dis)\n        movec, dis = func(mov_ru, dis_ru, movec, dis)\n        movec, dis = func(mov_rd, dis_rd, movec, dis)\n        position = (lx + movec[0], ly + movec[1])\n        return self.space.position_to_index(position), dis\n\n# ==========================================\n# File: user_study/space.py\n# Function/Context: Space\n# ==========================================\nimport numpy as np\nfrom functools import reduce\nfrom copy import deepcopy\n\nclass Box(object):\n    def __init__(self, x, y, z, lx, ly, lz):\n        self.x = x\n        self.y = y\n        self.z = z\n        self.lx = lx\n        self.ly = ly\n        self.lz = lz\n        self.color = None\n        self.vertex = np.zeros((8, 3))\n        self.refresh()\n\n    def set_color(self,color):\n        if self.color is None:\n            self.color = color\n\n    def refresh(self):\n        self.getCorners([self.x, self.y, self.z], [self.lx, self.ly, self.lz])\n\n    def getCorners(self, size, location, quaternion=np.array([1, 0, 0, 0])):\n        for i in range(2):\n            for j in range(2):\n                for k in range(2):\n                    self.vertex[i * 4 + j * 2 + k] = np.array(\n                        [location[0] + k * size[0], location[1] + j * size[1], location[2] + i * size[2]])\n        vertex = np.array(self.vertex, np.float32)\n        return vertex.transpose()\n\n    def standardize(self):\n        return tuple([self.x, self.y, self.z, self.lx, self.ly, self.lz])\n\n\nclass Space(object):\n    def __init__(self, width=10, length=10, height=10):\n        self.plain_size = np.array([width, length, height])\n        self.plain = np.zeros(shape=(width, length), dtype=np.int32)\n        self.boxes = []\n        self.height = height\n        self.try_box = None\n\n    def get_height_graph(self):\n        plain = np.zeros(shape=self.plain_size[:2], dtype=np.int32)\n        for box in self.boxes:\n            plain = self.update_height_graph(plain, box)\n        return plain\n\n    @staticmethod\n    def update_height_graph(plain, box):\n        plain = deepcopy(plain)\n        le = box.lx\n        ri = box.lx + box.x\n        up = box.ly\n        do = box.ly + box.y\n        max_h = np.max(plain[le:ri, up:do])\n        max_h = max(max_h, box.lz + box.z)\n        plain[le:ri, up:do] = max_h\n        return plain\n\n    def get_box_list(self):\n        vec = list()\n        for box in self.boxes:\n            vec += box.standardize()\n        return vec\n\n    def get_plain(self):\n        return deepcopy(self.plain)\n\n    def get_action_space(self):\n        return self.plain_size[0] * self.plain_size[1]\n\n    def check_box(self, plain, x, y, lx, ly, z):\n        if lx + x > self.plain_size[0] or ly + y > self.plain_size[1]:\n            return -1\n        if lx < 0 or ly < 0:\n            return -1\n\n        rec = plain[lx:lx + x, ly:ly + y]\n        max_h = np.max(rec)\n\n        # check boundary\n        assert max_h >= 0\n        if max_h + z > self.height:\n            return -1\n\n        # check area and corner\n        max_area = np.sum(rec == max_h)\n        area = x * y\n\n        LU = int(rec[0, 0] == max_h)\n        LD = int(rec[x - 1, 0] == max_h)\n        RU = int(rec[0, y - 1] == max_h)\n        RD = int(rec[x - 1, y - 1] == max_h)\n\n        if max_area / area > 0.95:\n            return max_h\n        if LU + LD + RU + RD == 3 and max_area / area > 0.85:\n            return max_h\n        if LU + LD + RU + RD == 4 and max_area / area > 0.50:\n            return max_h\n\n        return -1\n\n    def get_ratio(self):\n        vo = reduce(lambda x, y: x + y, [box.x * box.y * box.z for box in self.boxes], 0.0)\n        mx = self.plain_size[0] * self.plain_size[1] * self.plain_size[2]\n        ratio = vo / mx\n        assert ratio <= 1.0\n        return ratio\n\n    def idx_to_position(self, idx):\n        lx = idx // self.plain_size[1]\n        ly = idx % self.plain_size[1]\n        return lx, ly\n\n    def position_to_index(self, position):\n        assert len(position) == 2\n        assert position[0] >= 0 and position[1] >= 0\n        assert position[0] < self.plain_size[0] and position[1] < self.plain_size[1]\n        return position[0] * self.plain_size[1] + position[1]\n\n    def try_drop(self, box_size, idx):\n        self.try_box = None\n        lx, ly = self.idx_to_position(idx)\n        x = box_size[0]\n        y = box_size[1]\n        z = box_size[2]\n        plain = self.plain\n        new_h = self.check_box(plain, x, y, lx, ly, z)\n        if new_h != -1:\n            self.try_box = Box(x, y, z, lx, ly, new_h)\n            return True\n        return False\n\n    def drop_box(self, box_size, idx):\n        self.try_box = None\n        lx, ly = self.idx_to_position(idx)\n        x = box_size[0]\n        y = box_size[1]\n        z = box_size[2]\n        plain = self.plain\n        new_h = self.check_box(plain, x, y, lx, ly, z)\n        if new_h != -1:\n            self.boxes.append(Box(x, y, z, lx, ly, new_h))\n            self.plain = self.update_height_graph(plain, self.boxes[-1])\n            self.height = max(self.height, new_h + z)\n            return True\n        return False",
  "description": "Combined Analysis:\n- [MCTS/monteCarlo.py]: This file implements the Monte Carlo permutation tree search (MCTS) algorithm for lookahead planning in the online 3D bin packing problem. The MCTree class builds a search tree over possible placements of future items (size_seq) using a simulation environment. Key aspects: 1) It explores virtual placements while respecting order dependence constraints through the environment's step function. 2) Uses UCT-like tree policy (choose_best with exploration constant c) for selection. 3) Expands nodes using a neural model (nmodel) to generate actions. 4) Backpropagates cumulative rewards (volume-based) with discount factor gamma. 5) Produces action probabilities via visit-count softmax for decision-making. This directly implements the 'Monte Carlo permutation tree search' mentioned in the paper's algorithm steps for BPP-k with lookahead.\n- [MCTS/node.py]: This file implements the Monte Carlo Tree Search (MCTS) node structure for the online 3D bin packing problem with lookahead (BPP-k). The PutNode class specifically handles the expansion and rollout phases of MCTS, which correspond to the 'Monte Carlo permutation tree search' mentioned in the paper's algorithm steps. Key implementations include: 1) Action selection using UCB formula (choose_best method with advanced_q representing incremental reward), 2) Node expansion that generates child nodes for valid placement positions (constrained by action_mask from environment), 3) Rollout simulation that estimates future rewards by sampling actions from the neural network policy. The code directly supports the lookahead exploration strategy where multiple future items are virtually placed while respecting order dependence constraints through the environment's state transitions.\n- [acktr/algo/acktr_pipeline.py]: This file implements the core training update logic for the constrained deep reinforcement learning algorithm described in the paper. Specifically, it contains the ACKTR (Actor-Critic using Kronecker-Factored Trust Region) optimizer class that performs policy gradient updates with auxiliary losses. The update() method computes: 1) Standard actor-critic losses (value loss, policy gradient loss with advantages), 2) Entropy regularization, 3) Feasibility mask prediction loss (graph_loss) which corresponds to the paper's prediction-and-projection scheme, 4) Invalid action penalty (prob_loss). The mask prediction is trained via MSE loss against ground truth feasibility masks, enforcing the physical constraints described in the optimization model. The force parameter (0.5*10=5) scales the mask prediction loss, aligning with the paper's emphasis on constraint satisfaction. This implements the constrained DRL training loop that learns to maximize packing efficiency while respecting placement constraints.\n- [acktr/model.py]: This file implements the neural network architecture for the constrained deep reinforcement learning agent described in the paper. Specifically, the Policy class uses an actor-critic framework with a CNNPro base network that processes the state (height map and item features) and outputs a value estimate, action features, and a predicted feasibility mask. The mask prediction is a key component of the prediction-and-projection scheme, which modulates action probabilities to satisfy constraints. The CNNPro network is tailored for 3D bin packing with configurable container size and rotation options. The code directly supports the algorithm's core step of generating feasible actions via mask-conditioned distributions, though the exact constraint enforcement (stability, height) is handled elsewhere in the environment.\n- [acktr/reorder.py]: This file implements the Monte Carlo permutation tree search algorithm for lookahead (BPP-k) described in the paper. The ReorderTree class performs tree search over permutations of k lookahead items to determine the optimal current placement action. Key aspects implemented: 1) Tree structure (Node) with UCB-like selection (get_value = Q + U) 2) Order dependence constraint enforcement via mixed masks (get_mixed_mask) that prevent placing earlier items on top of virtually placed later items 3) Virtual placement simulation with termination conditions 4) Baseline comparison to ensure improvement threshold (v_bound). The search explores different permutations of box sequences within the lookahead window to maximize expected cumulative reward.\n- [envs/bpp0/bin3D.py]: This file implements the core environment for the Online 3D Bin Packing Problem as described in the paper. The PackingGame class extends gym.Env and directly implements key aspects of the optimization model:\n\n1. **Objective Implementation**: The reward function in step() calculates reward = box_ratio * 10, where box_ratio = (l*w*h)/(L*W*H), matching the paper's objective of maximizing packed volume.\n\n2. **Constraint Implementation**: \n   - Boundary constraints are enforced in get_possible_position() through range checks (i in range(width-x+1), j in range(length-y+1))\n   - Height constraints are checked via self.space.check_box() which returns >=0 if placement is feasible\n   - Physical stability constraints are likely implemented in the Space.drop_box() method (not shown in this file)\n   - Order dependence is implicitly handled through sequential item arrival via box_creator\n\n3. **State Representation**: The observation includes height map and box dimensions as described in the paper's state architecture.\n\n4. **Action Space**: Actions represent placement positions on the bin floor, with optional rotation, matching the discrete action space described.\n\n5. **Online Nature**: Items arrive sequentially via box_creator.preview(1) and are packed immediately without future knowledge.\n\nThe implementation directly corresponds to the mathematical model's objective function and constraint checking mechanisms, though detailed stability constraint logic resides in the Space class.\n- [envs/bpp0/space.py]: This file implements the core state representation and constraint checking for the 3D Bin Packing Problem (3D-BPP) as described in the paper. The Space class maintains a height map (plain) and list of packed boxes, with methods to check placement feasibility against boundary, height, and physical stability constraints. The check_box method directly implements the paper's stability constraints (corner and area support ratios). The drop_box method handles item placement with rotation, updating the state. This forms the foundation for the DRL environment's state transitions and constraint satisfaction.\n- [user_study/bin3D.py]: This file implements the core environment for the online 3D bin packing problem as described in the paper. The classes PackingGame and AdjustPackingGame define the state representation, action space, reward function, and constraints for sequential item placement. Key implementations include: 1) State representation combining height map (space.plain), current box dimensions, and space utilization ratio; 2) Action space defined by discrete placement positions on the bin floor; 3) Constraint checking via space.check_box() for boundary and height constraints; 4) Reward function that gives 0 for successful placements and final utilization ratio (scaled by 10) upon termination; 5) Feasibility mask generation in get_possible_position() for constrained action selection; 6) Adjustment mechanism in adjust() for optimizing placements near existing corners. The implementation aligns with the paper's mathematical model for online sequential packing with immediate placement decisions and physical constraints.\n- [user_study/space.py]: This file implements the core constraint checking and state management for the online 3D bin packing problem as per the mathematical model. The Space class enforces boundary constraints (0 ≤ x_i ≤ L - l_i, 0 ≤ y_i ≤ W - w_i), height constraints (h_max,i + h_i ≤ H), and physical stability constraints based on supported area and corners (matching the paper's rules with slight numerical variations). It maintains a height map (plain) and box list for state representation, and provides methods for placement validation (check_box), volume ratio calculation (get_ratio), and action space handling, which are essential for the DRL environment. The Box class represents individual items with dimensions and positions.",
  "dependencies": [
    "Flatten",
    "acktr.distributions",
    "copy",
    "nmodel.evaluate()",
    "acktr.algo.kfac.KFACOptimizer",
    "MDlayerBoxCreator",
    "sys",
    "Categorical",
    "torch.optim",
    "space.Box",
    "time",
    "copy.deepcopy",
    "Space",
    "node.PutNode",
    "LoadBoxCreator",
    "env (custom bin packing environment)",
    "functools.reduce",
    "math",
    "gym",
    "CuttingBoxCreator",
    "sim_env.get_possible_position()",
    "space.Space",
    "sim_env.step()",
    "binCreator.LoadBoxCreator",
    "NNBase",
    "acktr.utils",
    "numpy",
    "nmodel (external neural network model)",
    "RandomBoxCreator",
    "torch.nn",
    "BoxCreator",
    "softmax",
    "torch",
    "itertools"
  ]
}