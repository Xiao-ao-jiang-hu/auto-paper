{
  "paper_id": "Controlling_Graph_Dynamics_with_Reinforcement_Learning_and_G",
  "title": "Controlling Graph Dynamics with Reinforcement Learning and Graph Neural Networks",
  "abstract": "We consider the problem of controlling a partially-observed dynamic process on a graph by a limited number of interventions. This problem naturally arises in contexts such as scheduling virus tests to curb an epidemic; targeted marketing in order to promote a product; and manually inspecting posts to detect fake news spreading on social networks. We formulate this setup as a sequential decision problem over a temporal graph process. In face of an exponential state space, combinatorial action space and partial observability, we design a novel tractable scheme to control dynamical processes on temporal graphs. We successfully apply our approach to two popular problems that fall into our framework: prioritizing which nodes should be tested in order to curb the spread of an epidemic, and influence maximization on a graph.",
  "problem_description_natural": "The paper addresses the challenge of controlling diffusive processes (e.g., epidemics or information cascades) on time-evolving graphs through a limited number of nodal interventions. At each time step, the agent observes only partial information about node states and must select a small subset of nodes (e.g., for testing or seeding) to influence the global dynamics. The goal is to optimize a long-term objective—such as minimizing the total number of infected individuals over time or maximizing the number of influenced nodes—despite partial observability, a combinatorial action space (choosing subsets of nodes), and complex spatiotemporal dependencies in the graph dynamics.",
  "problem_type": "combinatorial optimization",
  "datasets": [
    "CA-GrQc",
    "Montreal",
    "Portland",
    "Email",
    "GEMSEC-RO",
    "CA-HEPTh",
    "Preferential attachment (PA) networks",
    "Stochastic Block Model (SBM) networks",
    "Contact-tracing (CT) networks"
  ],
  "performance_metrics": [
    "Percent of healthy nodes",
    "Percent of influenced nodes",
    "Contained epidemic fraction"
  ],
  "lp_model": {
    "objective": "\\max \\sum_t \\gamma^{t-t_0} g(c_1(t), c_2(t), ..)",
    "constraints": [
      "\\sum_i f_i(c_1(t), c_2(t), ..) \\geq z_i(t)"
    ],
    "variables": [
      "ST_v(t) - state of node v at time t",
      "a(t) - subset of k nodes selected for intervention at time t",
      "c_i(t) - total number of nodes in state y_i at time t"
    ]
  },
  "raw_latex_model": "The objective is therefore of the form $\\max \\sum_t \\gamma^{t-t_0} g(c_1(t), c_2(t), ..)$, where future evaluations are weighted by a discount factor $\\gamma \\leq 1$. Additionally, the agent may be subject to constraints written in a similar manner $\\sum_i f_i(c_1(t), c_2(t), ..) \\geq z_i(t)$.",
  "algorithm_description": "Step 1: Input static and dynamic node features, and temporal edge features from the graph. Step 2: Compute local diffusion features for each node using a GNN that aggregates information from direct neighbors based on current interactions and transmission probabilities. Step 3: Compute long-range information features for each node using a multi-layer GNN on a cumulative multi-graph of recent interactions. Step 4: Update the hidden state of each node by combining its previous hidden state, current features, diffusion features, and information features through a neural network (e.g., MLP or GRU). Step 5: Compute a score for each node using another neural network based on the updated hidden state and other inputs. Step 6: Convert scores to a probability distribution using a calibrated function (e.g., subtracting min score and adding epsilon), then sample k nodes without replacement to form the intervention action. Step 7: Train the model using Proximal Policy Optimization (PPO), with a critic module estimating the value function based on aggregated node features."
}