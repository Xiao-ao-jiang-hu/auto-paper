{
  "file_path": "CUTS/cuts_main.py, CUTS/utils/gumbel_softmax.py, CUTS_Plus/cuts_plus.py",
  "function_name": "CUTS.train, gumbel_softmax, MultiCAD.train",
  "code_snippet": "\n\n# ==========================================\n# File: CUTS/cuts_main.py\n# Function/Context: CUTS.train\n# ==========================================\nimport logging\nimport os\nfrom os.path import join as opj\nfrom os.path import dirname as opd\nfrom os.path import basename as opb\nfrom os.path import splitext as ops\n\nimport tqdm\nimport numpy as np\nimport argparse\nfrom omegaconf import OmegaConf\nfrom copy import deepcopy\nimport torch\nfrom torch import dropout, nn\n\nfrom utils.cuts_parts import *\nfrom utils.gumbel_softmax import gumbel_softmax\nfrom utils.misc import plot_causal_matrix, reproduc, plot_causal_matrix_in_training, calc_and_log_metrics, log_time_series, prepross_data\nfrom utils.batch_generater import batch_generater\nfrom utils.opt_type import CUTSopt\nfrom utils.logger import MyLogger\nfrom utils.data_interpolate import interp_multivar_data\nfrom utils.load_data import simulate_var_from_links, simulate_var, simulate_lorenz_96_process, load_netsim_data\n\nfrom datetime import datetime\n\nimport os\nfrom einops import rearrange\n\n\nclass CUTS(object):\n    def __init__(self, args: CUTSopt.CUTSargs, log, device=\"cuda\"):\n        self.log: MyLogger = log\n        self.args = args\n        self.device = device\n\n        if self.args.data_pred.model == \"multi_mlp\":\n            self.fitting_model = MultiMLP(self.args.input_step * self.args.n_nodes * self.args.data_dim,\n                                          self.args.data_pred.mlp_hid,\n                                          self.args.data_dim * self.args.data_pred.pred_step,\n                                          self.args.data_pred.mlp_layers,\n                                          self.args.n_nodes).to(self.device)\n        elif self.args.data_pred.model == \"multi_lstm\":\n            self.fitting_model = MultiLSTM(self.args.n_nodes * self.args.data_dim,\n                                          self.args.data_pred.mlp_hid,\n                                          self.args.data_dim * self.args.data_pred.pred_step,\n                                          self.args.data_pred.mlp_layers,\n                                          self.args.n_nodes).to(self.device)\n        else:\n            raise NotImplementedError\n\n        self.data_pred_loss = nn.MSELoss()\n        self.data_pred_optimizer = torch.optim.Adam(self.fitting_model.parameters(),\n                                                    lr=self.args.data_pred.lr_data_start,\n                                                    weight_decay=self.args.data_pred.weight_decay)\n        \n        \n        if \"every\" in self.args.fill_policy:\n            lr_schedule_length = int(self.args.fill_policy.split(\"_\")[-1])\n        else:\n            lr_schedule_length = self.args.total_epoch\n            \n        gamma = (self.args.data_pred.lr_data_end / self.args.data_pred.lr_data_start) ** (1 / lr_schedule_length)\n        self.data_pred_scheduler = torch.optim.lr_scheduler.StepLR(\n            self.data_pred_optimizer, step_size=1, gamma=gamma)\n        \n        if hasattr(self.args, \"disable_graph\") and self.args.disable_graph:\n            print(\"Using full graph and disable graph discovery...\")\n            self.graph = nn.Parameter(torch.ones([self.args.n_nodes, self.args.n_nodes, self.args.input_step]).to(self.device) * 1000)\n        else:\n            self.graph = nn.Parameter(torch.ones([self.args.n_nodes, self.args.n_nodes, self.args.input_step]).to(self.device) * 0)\n        # self.graph = nn.Parameter(torch.zeros([self.args.n_nodes, self.args.n_nodes, self.args.input_step]).to(self.device))\n        self.graph_optimizer = torch.optim.Adam([self.graph], lr=self.args.graph_discov.lr_graph_start)\n        gamma = (self.args.graph_discov.lr_graph_end / self.args.graph_discov.lr_graph_start) ** (1 / self.args.total_epoch)\n        self.graph_scheduler = torch.optim.lr_scheduler.StepLR(self.graph_optimizer, step_size=1, gamma=gamma)\n\n        end_tau, start_tau = self.args.graph_discov.end_tau, self.args.graph_discov.start_tau\n        self.gumbel_tau_gamma = (end_tau / start_tau) ** (1 / self.args.total_epoch)\n        self.gumbel_tau = start_tau\n        self.start_tau = start_tau\n        \n        end_lmd, start_lmd = self.args.graph_discov.lambda_s_end, self.args.graph_discov.lambda_s_start\n        self.lambda_gamma = (end_lmd / start_lmd) ** (1 / self.args.total_epoch)\n        self.lambda_s = start_lmd\n        \n        \n\n    def latent_data_pred(self, x, y, observ_mask):\n        \n        def sample_graph(sample_matrix, batch_size, prob=True):\n            sample_matrix = torch.sigmoid(\n                sample_matrix[None, :, :, :].expand(batch_size, -1, -1, -1))\n            if prob:\n                return torch.bernoulli(sample_matrix)\n            else:\n                return sample_matrix\n        \n        bs, n, m, t, d = x.shape\n        self.fitting_model.train()\n        self.data_pred_optimizer.zero_grad()\n        \n        # graph_no_self = self.graph.clone()\n        # for i in range(graph_no_self.shape[0]):\n        #     graph_no_self[i,i,:] = torch.ones_like(graph_no_self[i,i,:]) * -1000\n        if hasattr(self.args.data_pred, \"disable_graph\") and \\\n            self.args.data_pred.disable_graph:\n                sampled_graph = torch.ones_like(self.graph)[None].expand(bs, -1, -1, -1)\n        else:\n            sampled_graph = sample_graph(self.graph, bs, self.args.data_pred.prob)\n            \n        y_pred = self.fitting_model(x, sampled_graph)\n\n        loss = self.data_pred_loss(y * observ_mask, y_pred * observ_mask) / torch.mean(observ_mask)\n        loss.backward()\n        self.data_pred_optimizer.step()\n        return y_pred, loss\n\n    def graph_discov(self, x, y, observ_mask):\n\n        def sigmoid_gumbel_sample(graph, batch_size, tau=1):\n            prob = torch.sigmoid(graph[None, :, :, :, None].expand(batch_size, -1, -1, -1, -1))\n            logits = torch.concat([prob, (1-prob)], axis=-1)\n            samples = gumbel_softmax(logits, tau=tau)[:, :, :, :, 0]\n            return samples\n\n        # self.fitting_model.eval()\n        self.graph_optimizer.zero_grad()\n        prob_graph = torch.sigmoid(self.graph[None, :, :])\n        sample_graph = sigmoid_gumbel_sample(self.graph, self.args.batch_size, tau=self.gumbel_tau)\n\n        y_pred = self.fitting_model(x, sample_graph)\n        \n        gs = prob_graph.shape\n        loss_sparsity = torch.norm(prob_graph, p=1) / (gs[0] * gs[1] * gs[2])\n        loss_data = self.data_pred_loss(y * observ_mask, y_pred * observ_mask) / torch.mean(observ_mask)\n        loss = loss_sparsity * self.lambda_s + loss_data\n        loss.backward()\n        self.graph_optimizer.step()\n        return loss, loss_sparsity, loss_data\n\n\n\n    def train(self, data, observ_mask, original_data, true_cm=None):\n\n        original_data = torch.from_numpy(original_data).float().to(self.device)\n        observ_mask = torch.from_numpy(observ_mask).float().to(self.device)\n        data = torch.from_numpy(data).float().to(self.device)\n        \n        if self.args.supervision_policy == \"masked\":\n            print(\"Using masked supervision for data prediction...\")\n        elif self.args.supervision_policy == \"full\":\n            print(\"Using full supervision for data prediction......\")\n            observ_mask = torch.ones_like(observ_mask)\n        elif \"masked_before\" in self.args.supervision_policy:\n            print(f\"Using masked supervision for data prediction ({self.args.supervision_policy:s})......\")\n\n        latent_pred_step = 0\n        graph_discov_step = 0\n        pbar = tqdm.tqdm(total=self.args.total_epoch)\n        data_interp = deepcopy(data)\n        original_mask = deepcopy(observ_mask)\n        auc = 0\n        for epoch_i in range(self.args.total_epoch):\n            if \"every\" in self.args.fill_policy:\n                update_every = int(self.args.fill_policy.split(\"_\")[-1])\n                if (epoch_i+1) % update_every == 0:\n                    data = data_pred\n                    print(\"Update data!\")\n                    # self.graph_optimizer.param_groups[0]['lr'] = self.args.graph_discov.lr_graph_start\n                    self.data_pred_optimizer.param_groups[0]['lr'] = self.args.data_pred.lr_data_start\n                    observ_mask = torch.ones_like(original_mask)\n            elif \"rate\" in self.args.fill_policy:\n                update_rate = float(self.args.fill_policy.split(\"_\")[1])\n                update_after = int(self.args.fill_policy.split(\"_\")[3])\n                if epoch_i+1 > update_after:\n                    if epoch_i == update_after:\n                        print(\"Data update started!\")\n                    data = data * (1 - update_rate) + data_pred * update_rate\n            else:\n                # no data update\n                pass\n            \n            if \"masked_before\" in self.args.supervision_policy:\n                masked_before = int(self.args.supervision_policy.split(\"_\")[2])\n                if epoch_i == masked_before:\n                    print(\"Using full supervision for data prediction......\")\n                    observ_mask = torch.ones_like(original_mask)\n                    self.gumbel_tau = self.start_tau\n            \n            # Data Prediction\n            if hasattr(self.args, \"data_pred\"):\n                if hasattr(self.args, \"sample_period\"):\n                    sample_period = self.args.sample_period\n                else:\n                    sample_period = 1\n                ## \n                batch_gen = batch_generater(data, observ_mask, # !!!!! TO-DO\n                                            bs=self.args.batch_size, \n                                            n_nodes=self.args.n_nodes, \n                                            input_step=self.args.input_step, \n                                            pred_step=self.args.data_pred.pred_step,\n                                            sample_period=sample_period)\n                batch_gen = list(batch_gen)\n                \n                data_pred = deepcopy(data) # masked data points are predicted\n                data_pred_all = deepcopy(data)\n                for x, y, t, mask in batch_gen:\n                    latent_pred_step += self.args.batch_size\n                    y_pred, loss = self.latent_data_pred(x, y, mask)\n                    data_pred[t] = (y_pred*(1-mask) + y*mask).clone().detach()[:,:,0]\n                    data_pred_all[t] = y_pred.clone().detach()[:,:,0]\n                    self.log.log_metrics({\"latent_data_pred/pred_loss\": loss.item()}, latent_pred_step)\n                    pbar.set_postfix_str(f\"S1 loss={loss.item():.2f}, spr=IDLE, auc={auc:.4f}\")\n\n                current_data_pred_lr = self.graph_optimizer.param_groups[0]['lr']\n                self.log.log_metrics({\"graph_discov/lr\": current_data_pred_lr}, latent_pred_step)\n                self.data_pred_scheduler.step()\n                mse_pred_to_original = self.data_pred_loss(original_data, data_pred)\n                mse_interp_to_original = self.data_pred_loss(original_data, data_interp)\n                \n                self.log.log_metrics({\"latent_data_pred/mse_pred_to_original\": mse_pred_to_original,\n                                      \"latent_data_pred/mse_interp_to_original\": mse_interp_to_original}, latent_pred_step)\n            \n            # Graph Discovery\n            if hasattr(self.args, \"graph_discov\"):\n                # batch_gen = batch_generater(data, observ_mask, \n                #                             bs=self.args.batch_size, \n                #                             n_nodes=self.args.n_nodes, \n                #                             input_step=self.args.input_step, \n                #                             pred_step=self.args.data_pred.pred_step, \n                #                             sample_period=period)\n                for x, y, t, mask in batch_gen:\n                    graph_discov_step += self.args.batch_size\n                    if hasattr(self.args, \"disable_graph\") and self.args.disable_graph:\n                        pass\n                    else:\n                        loss, loss_sparsity, loss_data = self.graph_discov(x, y, mask)\n                        self.log.log_metrics({\"graph_discov/sparsity_loss\": loss_sparsity.item(),\n                                            \"graph_discov/data_loss\": loss_data.item(),\n                                            \"graph_discov/total_loss\": loss.item()}, graph_discov_step)\n                        pbar.set_postfix_str(f\"S2 loss={loss_data.item():.2f}, spr={loss_sparsity.item():.2f}, auc={auc:.4f}\")\n                    \n                self.graph_scheduler.step()\n                current_graph_disconv_lr = self.graph_optimizer.param_groups[0]['lr']\n                self.log.log_metrics({\"graph_discov/lr\": current_graph_disconv_lr}, graph_discov_step)\n                self.log.log_metrics({\"graph_discov/tau\": self.gumbel_tau}, graph_discov_step)\n                self.gumbel_tau *= self.gumbel_tau_gamma\n\n            pbar.update(1)\n            self.lambda_s *= self.lambda_gamma\n                     \n            calc, val = self.args.causal_thres.split(\"_\")\n            if calc == \"value\":\n                threshold = float(val)\n            else:\n                raise NotImplementedError\n            \n            time_coef_mat = self.graph.detach().cpu().numpy()\n            plot_roc = False\n            if (epoch_i+1) % self.args.show_graph_every == 0:\n                avg_mask = np.mean(observ_mask.cpu().numpy(), axis=(0,2))\n                if np.min(avg_mask) < 1:\n                    time_series_idx = int(np.argwhere(avg_mask < 1)[0])\n                else:\n                    time_series_idx = 0\n                log_time_series(original_data.cpu()[-100:,time_series_idx], \n                                data_interp.cpu()[-100:,time_series_idx], \n                                data_pred_all.cpu()[-100:,time_series_idx], log=self.log, log_step=latent_pred_step)\n                plot_causal_matrix_in_training(time_coef_mat, self.log, graph_discov_step, threshold=threshold)\n                plot_roc = True\n            \n            # Show TPR FPR AUC ROC\n            if true_cm is not None:\n                time_prob_mat = torch.sigmoid(self.graph).detach().cpu().numpy()      \n                auc = calc_and_log_metrics(time_prob_mat, true_cm, self.log, graph_discov_step, threshold=threshold, plot_roc=plot_roc)\n\n# ==========================================\n# File: CUTS/utils/gumbel_softmax.py\n# Function/Context: gumbel_softmax\n# ==========================================\nimport torch\nfrom torch import Tensor\nimport warnings\n\ndef gumbel_softmax(logits: Tensor, tau: float = 1, hard: bool = False, eps: float = 1e-10, dim: int = -1) -> Tensor:\n    r\"\"\"\n    Samples from the Gumbel-Softmax distribution (`Link 1`_  `Link 2`_) and optionally discretizes.\n\n    Args:\n      logits: `[..., num_features]` unnormalized log probabilities\n      tau: non-negative scalar temperature\n      hard: if ``True``, the returned samples will be discretized as one-hot vectors,\n            but will be differentiated as if it is the soft sample in autograd\n      dim (int): A dimension along which softmax will be computed. Default: -1.\n\n    Returns:\n      Sampled tensor of same shape as `logits` from the Gumbel-Softmax distribution.\n      If ``hard=True``, the returned samples will be one-hot, otherwise they will\n      be probability distributions that sum to 1 across `dim`.\n\n    .. note::\n      This function is here for legacy reasons, may be removed from nn.Functional in the future.\n\n    .. note::\n      The main trick for `hard` is to do  `y_hard - y_soft.detach() + y_soft`\n\n      It achieves two things:\n      - makes the output value exactly one-hot\n      (since we add then subtract y_soft value)\n      - makes the gradient equal to y_soft gradient\n      (since we strip all other gradients)\n\n    Examples::\n        >>> logits = torch.randn(20, 32)\n        >>> # Sample soft categorical using reparametrization trick:\n        >>> F.gumbel_softmax(logits, tau=1, hard=False)\n        >>> # Sample hard categorical using \"Straight-through\" trick:\n        >>> F.gumbel_softmax(logits, tau=1, hard=True)\n\n    .. _Link 1:\n        https://arxiv.org/abs/1611.00712\n    .. _Link 2:\n        https://arxiv.org/abs/1611.01144\n    \"\"\"\n    if eps != 1e-10:\n        warnings.warn(\"`eps` parameter is deprecated and has no effect.\")\n\n    gumbels = (\n        -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format).exponential_().log()\n    )  # ~Gumbel(0,1)\n    gumbels = (logits + gumbels) / tau  # ~Gumbel(logits,tau)\n    y_soft = gumbels.softmax(dim)\n\n    if hard:\n        # Straight through.\n        index = y_soft.max(dim, keepdim=True)[1]\n        y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(dim, index, 1.0)\n        ret = y_hard - y_soft.detach() + y_soft\n    else:\n        # Reparametrization trick.\n        ret = y_soft\n    return ret\n\nif __name__==\"__main__\":\n    a = torch.tensor([[2.0, 0.7]]*10)\n    print(gumbel_softmax(a, tau=10))\n\n# ==========================================\n# File: CUTS_Plus/cuts_plus.py\n# Function/Context: MultiCAD.train\n# ==========================================\nimport logging\nimport os, sys\nfrom os.path import join as opj\nfrom os.path import dirname as opd\nfrom os.path import basename as opb\nfrom os.path import splitext as ops\n\nsys.path.append(opj(opd(__file__), \"..\"))\n\nimport tqdm\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport argparse\nfrom omegaconf import OmegaConf\nfrom copy import deepcopy\nimport torch\nfrom torch import dropout, nn\nimport torch.nn.functional as F\nfrom torch.utils.tensorboard import SummaryWriter\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\nfrom utils.gumbel_softmax import gumbel_softmax\nfrom utils.misc import calc_and_log_metrics, log_time_series, plot_causal_matrix\nfrom utils.opt_type import MultiCADopt\nfrom utils.logger import MyLogger\n\nfrom datetime import datetime\nfrom model.cuts_plus_net import CUTS_Plus_Net\n\nimport os\nfrom einops import rearrange\n\n\ndef generate_indices(input_step, pred_step, t_length, block_size=None):\n    if block_size is None:\n        block_size = t_length\n        \n    offsets_in_block = np.arange(input_step, block_size-pred_step+1)\n    assert t_length % block_size == 0, \"t_length % block_size != 0\"\n    random_t_list = []\n    for block_start in range(0, t_length, block_size):\n        random_t_list += (offsets_in_block + block_start).tolist()\n    \n    np.random.shuffle(random_t_list)\n    return random_t_list\n\n\ndef batch_generater(data, observ_mask, bs, n_nodes, input_step, pred_step, block_size=None):\n    t, n, d = data.shape\n    first_sample_t = input_step\n    random_t_list = generate_indices(input_step, pred_step, t_length=t, block_size=block_size)\n\n    for batch_i in range(len(random_t_list) // bs):\n        x = torch.zeros([bs, n_nodes, input_step, d]).to(data.device)\n        y = torch.zeros([bs, n_nodes, pred_step, d]).to(data.device)\n        t = torch.zeros([bs]).to(data.device).long()\n        mask_x = torch.zeros([bs, n_nodes, input_step, d]).to(data.device)\n        mask_y = torch.zeros([bs, n_nodes, pred_step, d]).to(data.device)\n        for data_i in range(bs):\n            data_t = random_t_list.pop()\n            x[data_i, :, :, :] = rearrange(data[data_t-input_step : data_t, :], \"t n d -> n t d\")\n            y[data_i, :, :, :] = rearrange(data[data_t : data_t+pred_step, :], \"t n d -> n t d\")\n            t[data_i] = data_t\n            mask_x[data_i, :, :, :] = rearrange(observ_mask[data_t-input_step : data_t, :], \"t n d -> n t d\")\n            mask_y[data_i, :, :, :] = rearrange(observ_mask[data_t:data_t+pred_step, :], \"t n d -> n t d\")\n\n        yield x, y, t, mask_x, mask_y\n        \n        \n\n\nclass MultiCAD(object):\n    def __init__(self, args: MultiCADopt.MultiCADargs, log, device=\"cuda\"):\n        self.log: MyLogger = log\n        self.args = args\n        self.device = device\n\n        self.fitting_model = CUTS_Plus_Net(self.args.n_nodes, in_ch=self.args.data_dim,\n                                           n_layers=self.args.data_pred.gru_layers,\n                                           hidden_ch=self.args.data_pred.mlp_hid,\n                                           shared_weights_decoder=self.args.data_pred.shared_weights_decoder,\n                                           concat_h=self.args.data_pred.concat_h,\n                                           ).to(self.device)\n\n        self.data_pred_loss = nn.MSELoss()\n        self.data_pred_optimizer = torch.optim.Adam(self.fitting_model.parameters(),\n                                                    lr=self.args.data_pred.lr_data_start,\n                                                    weight_decay=self.args.data_pred.weight_decay)\n        \n        \n        if \"every\" in self.args.fill_policy:\n            lr_schedule_length = int(self.args.fill_policy.split(\"_\")[-1])\n        else:\n            lr_schedule_length = self.args.total_epoch\n            \n        gamma = (self.args.data_pred.lr_data_end / self.args.data_pred.lr_data_start) ** (1 / lr_schedule_length)\n        self.data_pred_scheduler = torch.optim.lr_scheduler.StepLR(\n            self.data_pred_optimizer, step_size=1, gamma=gamma)\n        \n        self.n_groups = self.args.n_groups\n        print(\"n_groups: \", self.n_groups)\n        if self.args.group_policy == \"None\":\n            self.args.group_policy = None\n\n        end_tau, start_tau = self.args.graph_discov.end_tau, self.args.graph_discov.start_tau\n        self.gumbel_tau_gamma = (end_tau / start_tau) ** (1 / self.args.total_epoch)\n        self.gumbel_tau = start_tau\n        self.start_tau = start_tau\n        \n        end_lmd, start_lmd = self.args.graph_discov.lambda_s_end, self.args.graph_discov.lambda_s_start\n        self.lambda_gamma = (end_lmd / start_lmd) ** (1 / self.args.total_epoch)\n        self.lambda_s = start_lmd\n    \n    def set_graph_optimizer(self, epoch=None):\n        if epoch == None:\n            epoch = 0\n        \n        gamma = (self.args.graph_discov.lr_graph_end / self.args.graph_discov.lr_graph_start) ** (1 / self.args.total_epoch)\n        self.graph_optimizer = torch.optim.Adam([self.GT], lr=self.args.graph_discov.lr_graph_start * gamma ** epoch)\n        self.graph_scheduler = torch.optim.lr_scheduler.StepLR(self.graph_optimizer, step_size=1, gamma=gamma)\n        \n\n    def latent_data_pred(self, x, y, mask_x, mask_y):\n        \n        def sample_bernoulli(sample_matrix, batch_size):\n            sample_matrix = sample_matrix[None].expand(batch_size, -1, -1)\n            return torch.bernoulli(sample_matrix).float()\n        \n        def sample_multinorm(sample_matrix, batch_size):\n            sampled = torch.multinomial(sample_matrix, batch_size, replacement=True).T\n            return F.one_hot(sampled).float()\n            \n        \n        bs, n, t, d = x.shape\n        self.fitting_model.train()\n        self.data_pred_optimizer.zero_grad()\n        \n        GT_prob = self.GT\n        G_prob = self.G\n        \n        Graph = torch.einsum(\"nm,ml->nl\", G_prob, torch.sigmoid(GT_prob))\n        graph_sampled = sample_bernoulli(Graph, self.args.batch_size)\n            \n        y_pred = self.fitting_model(x, mask_x, graph_sampled)\n\n        loss = self.data_pred_loss(y * mask_y, y_pred * mask_y) / torch.mean(mask_y)\n        loss.backward()\n        self.data_pred_optimizer.step()\n        return y_pred, loss\n\n    def graph_discov(self, x, y, mask_x, mask_y):\n\n        def gumbel_sigmoid_sample(graph, batch_size, tau=1):\n            prob = graph[None, :, :, None].expand(batch_size, -1, -1, -1)\n            logits = torch.concat([prob, (1-prob)], axis=-1)\n            samples = gumbel_softmax(logits, tau=tau, hard=True)[:, :, :, 0]\n            return samples\n        \n        gn, n = self.GT.shape\n        self.graph_optimizer.zero_grad()\n        GT_prob = self.GT\n        G_prob = self.G\n        \n        Graph = torch.einsum(\"nm,ml->nl\", G_prob, torch.sigmoid(GT_prob))\n        graph_sampled = gumbel_sigmoid_sample(Graph, self.args.batch_size) \n        \n        loss_sparsity = torch.linalg.norm(Graph.flatten(), ord=1) / (n * n)\n\n        y_pred = self.fitting_model(x, mask_x, graph_sampled)\n        \n        loss_data = self.data_pred_loss(y * mask_y, y_pred * mask_y) / torch.mean(mask_y)\n        loss = loss_sparsity * self.lambda_s + loss_data\n        loss.backward()\n        self.graph_optimizer.step()\n        \n        return loss, loss_sparsity, loss_data\n\n\n    def train(self, data, observ_mask, original_data, true_cm=None):\n        original_data = torch.from_numpy(original_data).float().to(self.device)\n        observ_mask = torch.from_numpy(observ_mask).float().to(self.device)\n        data = torch.from_numpy(data).float().to(self.device)\n        \n        if self.args.supervision_policy == \"masked\":\n            print(\"Using masked supervision for data prediction...\")\n        elif self.args.supervision_policy == \"full\":\n            print(\"Using full supervision for data prediction......\")\n            observ_mask = torch.ones_like(observ_mask)\n        elif \"masked_before\" in self.args.supervision_policy:\n            print(f\"Using masked supervision for data prediction ({self.args.supervision_policy:s})......\")\n\n        latent_pred_step = 0\n        graph_discov_step = 0\n        pbar = tqdm.tqdm(total=self.args.total_epoch)\n        data_interp = deepcopy(data)\n        original_mask = deepcopy(observ_mask)\n        auc = 0\n        for epoch_i in range(self.args.total_epoch):\n            \n            if self.args.group_policy is not None:\n                group_mul = int(self.args.group_policy.split(\"_\")[1])\n                group_every = int(self.args.group_policy.split(\"_\")[3])\n                if epoch_i % group_every == 0 and self.n_groups < self.args.n_nodes:\n                    if epoch_i != 0:\n                        self.n_groups *= group_mul\n                    if self.n_groups > self.args.n_nodes:\n                        self.n_groups = self.args.n_nodes\n                    \n                    self.G = torch.zeros([self.args.n_nodes, self.n_groups]).to(self.device)\n\n                    for i in range(0, self.n_groups):\n                        for j in range(0, self.args.n_nodes // self.n_groups):\n                            self.G[i*(self.args.n_nodes // self.n_groups) + j, i] = 1\n                    for k in range(i*(self.args.n_nodes // self.n_groups) + j, self.args.n_nodes):\n                        self.G[k, i] = 1\n\n                    if hasattr(self, \"GT\"):\n                        GT_init = torch.sigmoid(self.GT).detach().cpu().repeat_interleave(group_mul, 0)[:self.n_groups, :]\n                        GT_init = 1 - (1 - GT_init)**(1 / group_mul)\n                    else:\n                        GT_init = torch.ones((self.n_groups, self.args.n_nodes))*0.5\n\n                    self.GT = nn.Parameter(GT_init.to(self.device))\n                    \n                    self.set_graph_optimizer(epoch_i)\n                elif epoch_i == 0 and self.n_groups == self.args.n_nodes:\n                    self.G = torch.eye(self.args.n_nodes).to(self.device)\n                    GT_init = torch.ones((self.n_groups, self.args.n_nodes))*0.5\n                    self.GT = nn.Parameter(GT_init.to(self.device))\n                    self.set_graph_optimizer(epoch_i)\n                    \n            \n            if \"every\" in self.args.fill_policy:\n                update_every = int(self.args.fill_policy.split(\"_\")[-1])\n                if (epoch_i+1) % update_every == 0:\n                    data = data_pred\n                    print(\"Update data!\")\n                    self.data_pred_optimizer.param_groups[0]['lr'] = self.args.data_pred.lr_data_start\n                    observ_mask = torch.ones_like(original_mask)\n            elif \"rate\" in self.args.fill_policy:\n                update_rate = float(self.args.fill_policy.split(\"_\")[1])\n                update_after = int(self.args.fill_policy.split(\"_\")[3])\n                if epoch_i+1 > update_after:\n                    if epoch_i == update_after:\n                        print(\"Data update started!\")\n                    data = data * (1 - update_rate) + data_pred * update_rate\n            else:\n                pass\n            \n            if \"masked_before\" in self.args.supervision_policy:\n                masked_before = int(self.args.supervision_policy.split(\"_\")[2])\n                if epoch_i == masked_before:\n                    print(\"Using full supervision for data prediction......\")\n                    observ_mask = torch.ones_like(original_mask)\n                    self.gumbel_tau = self.start_tau\n            \n            if hasattr(self.args, \"data_pred\"):\n                if hasattr(self.args, \"block_size\"):\n                    block_size = self.args.block_size\n                else:\n                    block_size = None\n                \n                batch_gen = batch_generater(data, observ_mask,\n                                            bs=self.args.batch_size, \n                                            n_nodes=self.args.n_nodes, \n                                            input_step=self.args.input_step, \n                                            pred_step=self.args.data_pred.pred_step,\n                                            block_size=block_size)\n                batch_gen = list(batch_gen)\n                \n                data_pred = deepcopy(data)\n                data_pred_all = deepcopy(data)\n                for x, y, t, mask_x, mask_y in batch_gen:\n                    latent_pred_step += self.args.batch_size\n                    y_pred, loss = self.latent_data_pred(x, y, mask_x, mask_y)\n                    data_pred[t] = (y_pred*(1-mask_y) + y*mask_y).clone().detach()[:,:,0]\n                    data_pred_all[t] = y_pred.clone().detach()[:,:,0]\n                    self.log.log_metrics({\"latent_data_pred/pred_loss\": loss.item()}, latent_pred_step)\n                    pbar.set_postfix_str(f\"S1 loss={loss.item():.2f}, spr=IDLE, auc={auc:.4f}\")\n\n                current_data_pred_lr = self.data_pred_optimizer.param_groups[0]['lr']\n                self.log.log_metrics({\"graph_discov/lr\": current_data_pred_lr}, latent_pred_step)\n                self.data_pred_scheduler.step()\n                mse_pred_to_original = self.data_pred_loss(original_data, data_pred)\n                mse_interp_to_original = self.data_pred_loss(original_data, data_interp)\n                \n                self.log.log_metrics({\"latent_data_pred/mse_pred_to_original\": mse_pred_to_original,\n                                      \"latent_data_pred/mse_interp_to_original\": mse_interp_to_original}, latent_pred_step)\n                \n            if hasattr(self.args, \"graph_discov\"):\n                for x, y, t, mask_x, mask_y in batch_gen:\n                    graph_discov_step += self.args.batch_size\n                    # Note: The graph discovery implementation continues here\n                    # but was cut off in the provided code snippet",
  "description": "Combined Analysis:\n- [CUTS/cuts_main.py]: This file implements the core CUTS algorithm exactly as described in the paper. The CUTS class contains: 1) Two separate optimizers for data prediction (DSGNN) and causal graph parameters (CPG), 2) The alternating training loop with configurable fill policies (data imputation) and supervision policies (masked/full), 3) Gumbel-softmax sampling for differentiable causal graph discovery, 4) Combined loss function L_graph = L_pred + λ||σ(θ)||₁ where L_pred is masked MSE loss normalized by observation rate, 5) Iterative data imputation via linear combination (Eq. 6) controlled by fill_policy parameters. The implementation matches the three-phase algorithm: initial masked training, iterative data imputation/graph refinement, and final fine-tuning (via supervision policy switching).\n- [CUTS/utils/gumbel_softmax.py]: This file implements the Gumbel-Softmax sampling technique, which is a critical component for differentiable sampling of binary causal masks (S) from the Causal Probability Graph (CPG) parameters θ in the CUTS algorithm. The function enables:\n1. Differentiable sampling via the reparameterization trick (when hard=False) for gradient flow through θ\n2. Straight-through gradient estimation (when hard=True) for discrete one-hot samples while maintaining differentiability\n3. Temperature parameter (τ) control for annealing between exploration and exploitation during training\n\nThis directly corresponds to the sampling of causal masks S in the optimization model, where the CPG parameters θ are transformed into binary adjacency matrices through Gumbel-Softmax sampling, allowing gradient-based optimization of the sparsity-regularized objective ℒ_graph.\n- [CUTS_Plus/cuts_plus.py]: This file implements the core CUTS+ optimization algorithm for neural causal discovery from irregular time-series data. The MultiCAD class performs alternating optimization between: 1) Data imputation using a graph neural network (CUTS_Plus_Net) conditioned on the current causal graph estimate, and 2) Causal graph learning with L1 sparsity regularization. The implementation includes grouping mechanisms for scalability, various supervision policies, and iterative data imputation updates that match the EM-style optimization described in the paper.",
  "dependencies": [
    "utils.logger",
    "utils.misc",
    "einops",
    "omegaconf",
    "utils.gumbel_softmax",
    "utils.opt_type",
    "tqdm",
    "utils.load_data",
    "numpy",
    "model.cuts_plus_net",
    "utils.data_interpolate",
    "warnings",
    "matplotlib",
    "sklearn.metrics",
    "utils.batch_generater",
    "utils.cuts_parts",
    "torch"
  ]
}