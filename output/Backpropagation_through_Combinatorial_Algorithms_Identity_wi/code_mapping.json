{
  "file_path": "discrete-VAE-experiments-neurips-identity.ipynb",
  "function_name": "IdentitySubsetLayer",
  "code_snippet": "\n\n# ==========================================\n# File: discrete-VAE-experiments-neurips-identity.ipynb\n# Function/Context: IdentitySubsetLayer\n# ==========================================\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras import Model\nimport tensorflow_probability as tfp\nimport numpy as np\n\nclass IdentitySubsetLayer(tf.keras.layers.Layer):\n  \n    def __init__(self, _k=10, _tau=10.0, _lambda=10.0):\n        super(IdentitySubsetLayer, self).__init__()\n        \n        self.k = _k\n        self._tau = _tau\n        self._lambda = _lambda\n        self.samples = None\n        self.gumbel_dist = tfp.distributions.Gumbel(loc=0.0, scale=1.0)\n        \n    @tf.function\n    def sample_gumbel(self, shape, eps=1e-20):\n        return self.gumbel_dist.sample(shape)\n    \n    @tf.function\n    def sample_gumbel_k(self, shape):\n        \n        s = tf.map_fn(fn=lambda t: tf.random.gamma(shape, 1.0/self.k,  self.k/t), \n                  elems=tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]))\n        # now add the samples\n        s = tf.reduce_sum(s, 0)\n        # the log(m) term\n        s = s - tf.math.log(10.0)\n        # divide by k --> each s[c] has k samples whose sum is distributed as Gumbel(0, 1)\n        s = self._tau * (s / self.k)\n\n        return s\n       \n    @tf.custom_gradient\n    def identity_layer(self, logits, hard=False):\n\n        # toggle between Gumbel(0, 1) and sum-of-Gamma\n        #forward_sample = self.sample_gumbel(tf.shape(logits))\n        threshold = tf.expand_dims(tf.nn.top_k(logits, self.k, sorted=True)[0][:,-1], -1)\n        y_map = tf.cast(tf.greater_equal(logits, threshold), tf.float32)\n\n        def custom_grad(dy):\n            return dy, hard\n\n        return y_map, custom_grad\n\n    def call(self, logits, hard=False):\n        logits = logits + self.sample_gumbel_k(tf.shape(logits))\n        # Project onto the hyperplane\n        logits = logits - tf.reduce_mean(logits, 1)[:, None]\n        # Project onto the sphere\n        logits = logits / tf.norm(logits, axis=1)[:, None]\n        return self.identity_layer(logits, hard)",
  "description": "Combined Analysis:\n- [discrete-VAE-experiments-neurips-identity.ipynb]: This file implements the core identity method from the paper for k-hot subset selection. The IdentitySubsetLayer class implements: 1) Forward pass: Adds Gumbel noise to logits, applies projection (mean subtraction and normalization) to handle solver invariants, then performs k-hot selection via top-k thresholding. 2) Backward pass: Uses @tf.custom_gradient to replace the true gradient with the identity (returning dy unchanged), implementing the negative identity gradient replacement. The projection steps correspond to the paper's standardization projection P_std(ω) = (ω - μ)/σ, adapted for the sphere. The noise addition provides regularization to prevent cost collapse. This matches the paper's optimization model for subset selection where Y is the set of k-hot vectors.",
  "dependencies": [
    "tensorflow-probability==0.7.0",
    "scikit-learn",
    "numpy",
    "matplotlib",
    "tensorflow==2.3.0"
  ]
}