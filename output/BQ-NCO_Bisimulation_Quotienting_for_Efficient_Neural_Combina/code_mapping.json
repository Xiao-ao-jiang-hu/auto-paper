{
  "file_path": "learning/cvrp/dataloading/dataset.py, learning/cvrp/decoding.py, learning/cvrp/traj_learner.py, learning/kp/decoding.py, learning/kp/traj_learner.py, learning/op/decoding.py, learning/op/traj_learner.py, learning/tsp/decoding.py, learning/tsp/traj_learner.py, model/model.py",
  "function_name": "collate_func_with_sample, decode, TrajectoryLearner, decode, greedy_decoding, beam_search_decoding, TrajectoryLearner, TrajectoryLearner, decode, beam_search_decoding_loop, greedy_decoding_loop, beam_search_decoding_step, greedy_decoding_step, prepare_input_and_forward_pass, reformat_subproblem_for_next_step, TrajectoryLearner, BQModel",
  "code_snippet": "\n\n# ==========================================\n# File: learning/cvrp/dataloading/dataset.py\n# Function/Context: collate_func_with_sample\n# ==========================================\nimport numpy\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom scipy.spatial.distance import pdist, squareform\nimport numpy as np\nfrom torch.utils.data.dataloader import default_collate\n\n\nclass DotDict(dict):\n    def __init__(self, **kwds):\n        self.update(kwds)\n        self.__dict__ = self\n\n\nclass DataSet(Dataset):\n\n    def __init__(self, node_coords, demands, capacities, remaining_capacities, tour_lens=None, via_depots=None):\n        self.node_coords = node_coords\n        self.demands = demands\n        self.capacities = capacities\n        self.remaining_capacities = remaining_capacities\n        self.via_depots = via_depots\n        self.tour_lens = tour_lens\n\n    def __len__(self):\n        return len(self.node_coords)\n\n    def __getitem__(self, item):\n        node_coords = self.node_coords[item]\n        demands = self.demands[item]\n        capacity = self.capacities[item]\n        if self.tour_lens is not None:\n            tour_len = self.tour_lens[item]\n        else:\n            tour_len = numpy.array([])\n\n        if self.remaining_capacities is not None:\n            via_depots = self.via_depots[item]\n            current_capacities = self.remaining_capacities[item]\n        else:\n            via_depots = numpy.array([])\n            current_capacities = numpy.array([])\n\n        distance_matrix = squareform(pdist(node_coords, metric='euclidean'))\n\n        # From list to tensors as a DotDict\n        item_dict = DotDict()\n        item_dict.dist_matrices = torch.Tensor(distance_matrix)\n        item_dict.node_coords = torch.Tensor(node_coords)\n        item_dict.demands = torch.Tensor(demands)\n        item_dict.capacities = torch.tensor(capacity).float()\n        item_dict.remaining_capacities = torch.Tensor(current_capacities)\n        item_dict.tour_len = torch.tensor(tour_len)\n        item_dict.via_depots = torch.Tensor(via_depots).long()\n        return item_dict\n\n\ndef load_dataset(filename, batch_size, shuffle=False, what=\"test\"):\n    data = np.load(filename)\n\n    if what == \"train\":\n        assert data[\"reorder\"]\n\n    node_coords = data[\"coords\"]\n    demands = data[\"demands\"]\n    capacities = data[\"capacities\"]\n\n\n    # in training dataset we have via_depots and remaining capacities but not tour lens\n    tour_lens = data[\"tour_lens\"] if \"tour_lens\" in data.keys() else None\n    remaining_capacities = data[\"remaining_capacities\"] if \"remaining_capacities\" in data.keys() else None\n    via_depots = data[\"via_depots\"] if \"via_depots\" in data.keys() else None\n\n    collate_fn = collate_func_with_sample if what == \"train\" else None\n\n    dataset = DataLoader(DataSet(node_coords, demands, capacities,\n                                 remaining_capacities=remaining_capacities,\n                                 tour_lens=tour_lens,\n                                 via_depots=via_depots), batch_size=batch_size,\n                         drop_last=False, shuffle=shuffle, collate_fn=collate_fn)\n    return dataset\n\n\ndef collate_func_with_sample(l_dataset_items):\n    \"\"\"\n    assemble minibatch out of dataset examples.\n    For instances of TOUR-CVRP of graph size N (i.e. nb_nodes=N+1 including return to beginning node),\n    this function also takes care of sampling a SUB-problem (PATH-TSP) of size 3 to N+1\n    \"\"\"\n    nb_nodes = len(l_dataset_items[0].dist_matrices)\n    begin_idx = np.random.randint(0, nb_nodes - 3)  # between _ included and nb_nodes + 1 excluded\n\n    l_dataset_items_new = []\n    for d in l_dataset_items:\n        d_new = {}\n        for k, v in d.items():\n            if k == \"dist_matrices\":\n                v_ = v[begin_idx:, begin_idx:]\n            elif k == \"remaining_capacities\":\n                v_ = v[begin_idx]\n            elif k == \"capacities\":\n                v_ = v\n            else:\n                v_ = v[begin_idx:, ...]\n\n            d_new.update({k + '_s': v_})\n        l_dataset_items_new.append({**d, **d_new})\n\n    return default_collate(l_dataset_items_new)\n\n# ==========================================\n# File: learning/cvrp/decoding.py\n# Function/Context: decode\n# ==========================================\nimport copy\nfrom dataclasses import dataclass\nimport numpy as np\nimport torch\nfrom torch import Tensor\nfrom torch.nn import Module\n\n\n@dataclass\nclass DecodingSubPb:\n    \"\"\"\n    In decoding, we successively apply model on progressively smaller sub-problems.\n    In each sub-problem, we keep track of the indices of each node in the original full-problem.\n    \"\"\"\n    node_coords: Tensor\n    distance_matrices: Tensor\n    demands: Tensor\n    current_capacities: Tensor\n    original_idxs: Tensor\n\n\ndef reconstruct_tours(paths: Tensor, via_depots: Tensor):\n    bs = paths.shape[0]\n    complete_paths = [[0] for _ in range(bs)]\n    for pos in range(1, paths.shape[1]):\n        nodes_to_add = paths[:, pos].tolist()\n        for instance in (via_depots[:, pos] == True).nonzero().squeeze(-1).cpu().numpy():\n            complete_paths[instance].append(0)\n        for instance in range(bs):\n            complete_paths[instance].append(nodes_to_add[instance])\n\n    return complete_paths\n\n\ndef decode(node_coords: Tensor, adj_matrices: Tensor, demands: Tensor, capacities: Tensor, net: Module,\n           beam_size: int, knns: int, make_tours: bool = False) -> Tensor:\n    if beam_size == 1:\n        paths, via_depots, tour_lengths = greedy_decoding_loop(node_coords, adj_matrices, demands, capacities, net,\n                                                               knns)\n    else:\n        paths, via_depots, tour_lengths = beam_search_decoding_loop(node_coords, adj_matrices, demands, capacities, net,\n                                                                    beam_size, knns)\n    num_nodes = node_coords.shape[1]\n    assert paths.sum(axis=1).sum() == paths.shape[0] * .5 * (num_nodes - 1) * num_nodes\n\n    if make_tours:\n        tours = reconstruct_tours(paths, via_depots)\n    else:\n        tours = None\n\n    return tour_lengths, tours\n\n\ndef greedy_decoding_loop(node_coords: Tensor, adj_matrices: Tensor, demands: Tensor, capacities: Tensor,\n                         net: Module, knns: int) -> Tensor:\n    bs, num_nodes, _ = node_coords.shape\n    original_idxs = torch.tensor(list(range(num_nodes)), device=node_coords.device)[None, :].repeat(bs, 1)\n    paths = torch.zeros((bs, num_nodes), dtype=torch.long, device=node_coords.device)\n    via_depots = torch.full((bs, num_nodes), False, dtype=torch.bool, device=node_coords.device)\n    initial_capacities = copy.deepcopy(capacities).unsqueeze(-1)\n    paths[:, -1] = num_nodes - 1\n    lenghts = torch.zeros(bs, device=node_coords.device)\n    sub_problem = DecodingSubPb(node_coords, adj_matrices, demands, initial_capacities, original_idxs)\n    for dec_pos in range(1, num_nodes - 1):\n        idx_selected, via_depot, sub_problem = greedy_decoding_step(capacities, sub_problem, net, knns)\n        paths[:, dec_pos] = idx_selected\n        via_depots[:, dec_pos] = via_depot\n\n        # compute lenghts for direct edges\n        lenghts[~via_depots[:, dec_pos]] += adj_matrices[~via_depots[:, dec_pos],\n                                                         paths[~via_depots[:, dec_pos], dec_pos-1],\n                                                         paths[~via_depots[:, dec_pos], dec_pos]]\n        # compute lenghts for edges via depot\n        lenghts[via_depots[:, dec_pos]] += adj_matrices[via_depots[:, dec_pos],\n                                                        paths[via_depots[:, dec_pos], dec_pos-1],\n                                                        paths[via_depots[:, dec_pos], 0]] +\\\n                                           adj_matrices[via_depots[:, dec_pos],\n                                                        paths[via_depots[:, dec_pos], 0],\n                                                        paths[via_depots[:, dec_pos], dec_pos]]\n    assert torch.count_nonzero(sub_problem.current_capacities < 0) == 0\n    lenghts += adj_matrices[torch.arange(bs), paths[:, -2], paths[:, -1]]\n\n    return paths, via_depots, lenghts\n\n\ndef beam_search_decoding_loop(node_coords: Tensor, adj_matrices: Tensor, demands: Tensor, capacities: Tensor,\n                              net: Module, beam_size: int, knns: int) -> Tensor:\n    bs, num_nodes, _ = node_coords.shape  # (including repetition of begin=end node)\n    original_idxs = torch.tensor(list(range(num_nodes)), device=node_coords.device)[None, :].repeat(bs, 1)\n    paths = torch.zeros((bs * beam_size, num_nodes), dtype=torch.long, device=node_coords.device)\n    via_depots = torch.full((bs * beam_size, num_nodes), False, dtype=torch.bool, device=node_coords.device)\n    paths[:, -1] = num_nodes - 1\n\n    initial_capacities = copy.deepcopy(capacities).unsqueeze(-1)\n    probabilities = torch.zeros((bs, 1), device=node_coords.device)\n    lenghts = torch.zeros(bs * beam_size, device=node_coords.device)\n\n    sub_problem = DecodingSubPb(node_coords, adj_matrices, demands, initial_capacities, original_idxs)\n\n    for dec_pos in range(1, num_nodes - 1):\n        idx_selected, via_depot, batch_in_prev_input, capacities, probabilities, sub_problem =\\\n            beam_search_decoding_step(capacities, sub_problem, net, probabilities, bs, beam_size, knns)\n\n        paths = paths[batch_in_prev_input]\n        via_depots = via_depots[batch_in_prev_input]\n        adj_matrices = adj_matrices[batch_in_prev_input]\n        paths[:, dec_pos] = idx_selected\n        via_depots[:, dec_pos] = via_depot\n        lenghts = lenghts[batch_in_prev_input]\n\n        # compute lenghts for direct edges\n        lenghts[~via_depots[:, dec_pos]] += adj_matrices[~via_depots[:, dec_pos],\n                                                         paths[~via_depots[:, dec_pos], dec_pos - 1],\n                                                         paths[~via_depots[:, dec_pos], dec_pos]]\n        # compute lenghts for edges via depot\n        lenghts[via_depots[:, dec_pos]] += adj_matrices[via_depots[:, dec_pos],\n                                                        paths[via_depots[:, dec_pos], dec_pos - 1],\n                                                        paths[via_depots[:, dec_pos], 0]] + \\\n                                           adj_matrices[via_depots[:, dec_pos],\n                                                        paths[via_depots[:, dec_pos], 0],\n                                                        paths[via_depots[:, dec_pos], dec_pos]]\n\n    lenghts += adj_matrices[torch.arange(bs * beam_size), paths[:, -2], paths[:, -1]]\n\n    lenghts = lenghts.reshape(bs, -1)\n    paths = paths.reshape(bs, -1, num_nodes)\n    via_depots = via_depots.reshape(bs, -1, num_nodes)\n    min_lenghts = torch.argmin(lenghts, dim=1)\n    return paths[torch.arange(bs), min_lenghts], via_depots[torch.arange(bs), min_lenghts],\\\n           lenghts[torch.arange(bs), min_lenghts]\n\n\ndef greedy_decoding_step(capacities: Tensor, sub_problem: DecodingSubPb, net: Module,\n                         knns: int) -> (Tensor, DecodingSubPb):\n    scores = prepare_input_and_forward_pass(capacities, sub_problem, net, knns)\n    selected_nodes = torch.argmax(scores, dim=1, keepdim=True)\n    idx_selected = torch.div(selected_nodes, 2, rounding_mode='trunc')\n    via_depot = (selected_nodes % 2 == 1)\n    idx_selected_original = torch.gather(sub_problem.original_idxs, 1, idx_selected)\n\n    new_subproblem, via_depot = reformat_subproblem_for_next_step(capacities, sub_problem, idx_selected, via_depot, knns)\n    return idx_selected_original.squeeze(1), via_depot.squeeze(1), new_subproblem\n\n\ndef prepare_input_and_forward_pass(capacities: Tensor, sub_problem: DecodingSubPb, net: Module, knns: int) -> Tensor:\n    # find K nearest neighbors of the current node\n    bs, num_nodes, node_dim = sub_problem.node_coords.shape\n    if 0 < knns < num_nodes:\n        bs, num_nodes, node_dim = sub_problem.node_coords.shape\n\n        knn_indices = torch.topk(sub_problem.distance_matrices[:, :-1, 0], k=knns - 1, dim=-1, largest=False).indices\n        # and add it manually\n        knn_indices = torch.cat(\n            [knn_indices, torch.full([bs, 1], num_nodes - 1, device=sub_problem.node_coords.device)],\n            dim=-1)\n        knn_coords = torch.gather(sub_problem.node_coords, 1, knn_indices.unsqueeze(dim=-1).repeat(1, 1, node_dim))\n        knn_demands = torch.gather(sub_problem.demands, 1, knn_indices)\n        current_capacities = sub_problem.current_capacities[:, -1]\n\n        inputs = torch.cat([knn_coords, (knn_demands / capacities.unsqueeze(-1)).unsqueeze(-1),\n                            (current_capacities.unsqueeze(-1) / capacities.unsqueeze(-1)).repeat(1, knns).unsqueeze(-1)\n                            ], dim=-1)\n\n        knn_scores = net(inputs, demands=knn_demands, remaining_capacities=current_capacities)  # (b, seq)\n\n        # create result tensor for scores with all -inf elements\n        scores = torch.full((sub_problem.node_coords.shape[0], 2 * sub_problem.node_coords.shape[1]),\n                            -np.inf, device=knn_coords.device)\n        double_knn_indices = torch.zeros([knn_indices.shape[0], 2 * knn_indices.shape[1]], device=knn_indices.device,\n                                         dtype=torch.int64)\n        double_knn_indices[:, 0::2] = 2 * knn_indices\n        double_knn_indices[:, 1::2] = 2 * knn_indices + 1\n\n        # and put computed scores for KNNs\n        scores = torch.scatter(scores, 1, double_knn_indices, knn_scores)\n\n    else:\n        current_capacities = sub_problem.current_capacities[:, -1]\n        inputs = torch.cat([sub_problem.node_coords,\n                            (sub_problem.demands / capacities.unsqueeze(-1)).unsqueeze(-1),\n                            (current_capacities / capacities).unsqueeze(-1).repeat(1,\n                                num_nodes).unsqueeze(-1)], dim=-1)\n        scores = net(inputs, demands=sub_problem.demands, remaining_capacities=current_capacities)\n    return scores\n\n\ndef beam_search_decoding_step(capacities: Tensor, sub_problem: DecodingSubPb, net: Module, prev_probabilities: Tensor,\n                              test_batch_size: int, beam_size: int, knns: int) -> (Tensor, DecodingSubPb):\n    scores = prepare_input_and_forward_pass(capacities, sub_problem, net, knns)\n    num_nodes = sub_problem.node_coords.shape[1]\n    num_instances = sub_problem.node_coords.shape[0] // test_batch_size\n    candidates = torch.softmax(scores, dim=1)\n\n    # repeat 2*num_nodes -> for each node we have two scores - direct edge and via depot\n    probabilities = (prev_probabilities.repeat(1, 2 * num_nodes) + torch.log(candidates)).reshape(test_batch_size, -1)\n\n    k = min(beam_size, probabilities.shape[1] - 2)\n    topk_values, topk_indexes = torch.topk(probabilities, k, dim=1)\n    batch_in_prev_input = ((num_instances * torch.arange(test_batch_size, device=probabilities.device)).unsqueeze(dim=1) +\\\n                           torch.div(topk_indexes, 2 * num_nodes, rounding_mode=\"floor\")).flatten()\n    topk_values = topk_values.flatten()\n    topk_indexes = topk_indexes.flatten()\n    sub_problem.node_coords = sub_problem.node_coords[batch_in_prev_input]\n    sub_problem.original_idxs = sub_problem.original_idxs[batch_in_prev_input]\n    sub_problem.demands = sub_problem.demands[batch_in_prev_input]\n    sub_problem.current_capacities = sub_problem.current_capacities[batch_in_prev_input]\n    sub_problem.distance_matrices = sub_problem.distance_matrices[batch_in_prev_input]\n    capacities = capacities[batch_in_prev_input]\n\n    selected_nodes = torch.remainder(topk_indexes, 2 * num_nodes).unsqueeze(dim=1)\n    idx_selected = torch.div(selected_nodes, 2, rounding_mode='trunc')\n    via_depot = (selected_nodes % 2 == 1)\n\n    idx_selected_original = torch.gather(sub_problem.original_idxs, 1, idx_selected)\n    new_subproblem, via_depot = reformat_subproblem_for_next_step(capacities, sub_problem, idx_selected, via_depot,\n                                                                  knns)\n\n    return idx_selected_original.squeeze(1), via_depot.squeeze(1), batch_in_prev_input, capacities,\\\n        topk_values.unsqueeze(dim=1), new_subproblem\n\n\ndef reformat_subproblem_for_next_step(capacities: Tensor, sub_problem: DecodingSubPb, idx_selected: Tensor,\n                                      via_depot: Tensor, knns: int) -> DecodingSubPb:\n    # Example: current_subproblem: [a b c d e] => (model selects d) => next_subproblem: [d b c e]\n    subpb_size = sub_problem.node_coords.shape[1]\n    bs = sub_problem.node_coords.shape[0]\n    is_selected = torch.arange(subpb_size, device=sub_problem.node_coords.device).unsqueeze(dim=0).repeat(bs, 1) ==\\\n                  idx_selected.repeat(1, subpb_size)\n\n    # next begin node = just-selected node\n    next_begin_node_coord = sub_problem.node_coords[is_selected].unsqueeze(dim=1)\n    next_begin_demand = sub_problem.demands[is_selected].unsqueeze(dim=1)\n    next_begin_original_idx = sub_problem.original_idxs[is_selected].unsqueeze(dim=1)\n\n    # remaining nodes = the rest, minus current first node\n    next_remaining_node_coords = sub_problem.node_coords[~is_selected].reshape((bs, -1, 2))[:, 1:, :]\n    next_remaining_demands = sub_problem.demands[~is_selected].reshape((bs, -1))[:, 1:]\n    next_remaining_original_idxs = sub_problem.original_idxs[~is_selected].reshape((bs, -1))[:, 1:]\n\n    # concatenate\n    next_node_coords = torch.cat([next_begin_node_coord, next_remaining_node_coords], dim=1)\n    next_demands = torch.cat([next_begin_demand, next_remaining_demands], dim=1)\n    next_original_idxs = torch.cat([next_begin_original_idx, next_remaining_original_idxs], dim=1)\n\n    # update current capacities\n    current_capacities = sub_problem.current_capacities[:, -1].unsqueeze(dim=1) - next_begin_demand\n\n    # recompute capacities\n    current_capacities[via_depot.bool()] = capacities.unsqueeze(-1)[via_depot.bool()] -\\\n                                           next_begin_demand[via_depot.bool()]\n    if torch.count_nonzero(current_capacities < 0) > 0:\n        print(\"stp\")\n\n    next_current_capacities = torch.cat([sub_problem.current_capacities, current_capacities], dim=-1)\n    if knns != -1:\n        num_nodes = sub_problem.distance_matrices.shape[1]\n\n        # select row (=column) of adj matrix for just-selected node\n        next_row_column = sub_problem.distance_matrices[is_selected]\n        # remove distance to the selected node (=0)\n        next_row_column = next_row_column[~is_selected].reshape((bs, -1))[:, 1:]\n\n        # remove rows and columns of selected nodes\n        next_adj_matrices = sub_problem.distance_matrices[~is_selected].reshape(bs, -1, num_nodes)[:, 1:, :]\n        next_adj_matrices = next_adj_matrices.transpose(1, 2)[~is_selected].reshape(bs, num_nodes-1, -1)[:, 1:, :]\n\n        # add new row on the top and remove second (must be done like this, because on dimenstons of the matrix)\n        next_adj_matrices = torch.cat([next_row_column.unsqueeze(dim=1), next_adj_matrices], dim=1)\n\n        # and add it to the beginning-\n        next_row_column = torch.c\n\n# ==========================================\n# File: learning/cvrp/traj_learner.py\n# Function/Context: TrajectoryLearner\n# ==========================================\nimport time\nimport torch\nfrom torch import nn\nfrom learning.cvrp.decoding import decode\nfrom utils.misc import do_lr_decay, EpochMetrics, get_opt_gap\n\nDEBUG_NUM_BATCHES = 3\n\nclass TrajectoryLearner:\n\n    def __init__(self, args, net, module, device, data_iterator, optimizer=None, checkpointer=None):\n        # same supervisor is used for training and testing, during testing we do not have optimizer, mlflow etc.\n\n        self.net = net\n        self.module = module\n        self.device = device\n        self.knns = args.knns\n        self.beam_size = args.beam_size\n        self.data_iterator = data_iterator\n        self.optimizer = optimizer\n        self.checkpointer = checkpointer\n\n        self.output_dir = args.output_dir\n        self.test_only = args.test_only\n\n        self.debug = args.debug\n\n        if not args.test_only:\n            try:\n                self.test_every = args.test_every if args.test_every > 0 else None\n            except AttributeError:\n                self.test_every = None\n            self.decay_rate = args.decay_rate\n            self.decay_every = args.decay_every\n            self.loss = nn.CrossEntropyLoss()\n            self.best_current_val_metric = float('inf')\n            self.epoch_done = 0\n            self.nb_epochs = args.nb_total_epochs\n\n    def train(self):\n        assert not self.test_only\n\n        for _ in range(self.nb_epochs):\n            # Train one epoch\n            start = time.time()\n            self.net.train()\n            epoch_metrics_train = EpochMetrics()\n\n            for batch_num, data in enumerate(self.data_iterator.train_trajectories):\n\n                self.optimizer.zero_grad()\n                node_coords, _, demands, capacities, remaining_capacities, via_depots, _ = self.prepare_batch(data)\n                inputs = torch.cat([node_coords, (demands / capacities.unsqueeze(-1)).unsqueeze(-1),\n                                    (remaining_capacities / capacities).unsqueeze(-1).repeat(1, node_coords.shape[1]).unsqueeze(-1)], dim=-1)\n                output_scores = self.net(inputs, demands=demands, remaining_capacities=remaining_capacities)\n\n                ground_truth = torch.full((output_scores.shape[0], ), 2, dtype=torch.long, device=output_scores.device)\n                # update ground truth for edges via depot\n                ground_truth[via_depots[:, 1] == 1.] += 1\n                loss = self.loss(output_scores, ground_truth)\n                epoch_metrics_train.update({\"training loss\": loss})\n                loss.backward()\n                self.optimizer.step()\n                if batch_num == DEBUG_NUM_BATCHES and self.debug:\n                    break\n\n            metrics = {f'{k}_train': v for k, v in epoch_metrics_train.get_means().items()}\n\n            print(\"[EPOCH {:03d}] Time: {:.3f}s \".format(self.epoch_done, time.time() - start))\n            for k, v in metrics.items():\n                print(k, f\"{v:.5f}\")\n\n            # Val test\n            val_metrics = self.val_test(\"val\")\n\n            if val_metrics['opt_gap_val'] < self.best_current_val_metric:\n                self.best_current_val_metric = val_metrics['opt_gap_val']\n                self.checkpointer.save(self.module, None, 'best')  # only model\n\n            # test\n            if self.test_every is not None:\n                if self.epoch_done % self.test_every == 0:\n                    self.save_model(\"current\")\n                    self.load_model(\"best\")\n                    self.val_test(\"test\")\n                    self.load_model(\"current\")\n                    self.remove_model(\"current\")\n\n            # lr decay\n            if self.epoch_done % self.decay_every == 0:\n                do_lr_decay(self.optimizer, self.decay_rate)\n\n            self.epoch_done += 1\n\n    def val_test(self, what=\"test\"):\n        if what == \"test\":\n            dataloader = self.data_iterator.test_trajectories\n        else:\n            dataloader = self.data_iterator.val_trajectories\n\n        self.net.eval()\n        epoch_metrics = EpochMetrics()\n        with torch.no_grad():\n\n            for batch_num, data in enumerate(dataloader):\n                val_test_metrics = self.get_minibatch_val_test_metrics(data)\n                epoch_metrics.update(val_test_metrics)\n                if batch_num == DEBUG_NUM_BATCHES and self.debug:\n                    break\n\n        res = {f'{k}_{what}': v for k, v in epoch_metrics.get_means().items()}\n\n        for k, v in res.items():\n            print(k, f\"{v:.3f}\")\n\n        return res\n\n    def load_model(self, label, allow_not_exist=False):\n        assert label in ['current', 'current_FULL', 'best']\n        self.checkpointer.load(self.module, None, label, allow_not_exist=allow_not_exist)\n\n    def save_model(self, label, complete=False):\n        assert label in ['current', 'best']\n        args = {'module': self.module}\n        if not complete:\n            args_ = {'optimizer': None, 'label': label}\n        else:\n            assert not self.eval_only\n            assert label == 'current'\n            args_ = {'optimizer': self.optimizer,\n                     'label': label+'_FULL',\n                     'other': {\n                         'epoch_done': self.epoch_done,\n                         'best_current_val_metric': self.best_current_val_metric,\n                         'data_iterator': self.data_iterator}\n                     }\n        self.checkpointer.save(**args, **args_)\n\n    def remove_model(self, label):\n        assert label in ['current', 'best']\n        self.checkpointer.delete(label)\n\n    def get_minibatch_val_test_metrics(self, data):\n\n        metrics = {}\n\n        # autoregressive decoding\n        node_coords, dist_matrices, demands, capacities, _, _, tour_lens = self.prepare_batch(data, sample=False)\n\n        decoding_metrics = {}\n        predicted_tour_lens, _ = decode(node_coords, dist_matrices, demands, capacities, self.net, self.beam_size, self.knns)\n\n        opt_gap = get_opt_gap(predicted_tour_lens, tour_lens)\n        decoding_metrics.update({'opt_gap': opt_gap})\n\n        return {**metrics, **decoding_metrics}\n\n    def prepare_batch(self, data, sample=True):\n        ks = '_s' if sample else ''\n        node_coords = data[f\"node_coords{ks}\"].to(self.device)\n        distance_matrices = data[f\"dist_matrices{ks}\"].to(self.device)\n        demands = data[f\"demands{ks}\"].to(self.device)\n        capacities = data[f\"capacities{ks}\"].to(self.device)\n        remaining_capacities = data[f\"remaining_capacities{ks}\"].to(self.device)\n        via_depots = data[f\"via_depots{ks}\"].to(self.device)\n        tour_len = data[f\"tour_len{ks}\"].to(self.device)\n\n        return node_coords, distance_matrices, demands, capacities, remaining_capacities, via_depots, tour_len\n\n# ==========================================\n# File: learning/kp/decoding.py\n# Function/Context: decode, greedy_decoding, beam_search_decoding\n# ==========================================\n\"\"\"\nBQ-NCO\nCopyright (c) 2023-present NAVER Corp.\nCreative Commons Attribution-NonCommercial-ShareAlike 4.0 license\n\"\"\"\n\nfrom dataclasses import dataclass\nimport torch\nimport numpy as np\nfrom torch import Tensor\nfrom torch.nn import Module\n\n@dataclass\nclass DecodingSubPb:\n    \"\"\"\n    In decoding, we successively apply model on progressively smaller sub-problems.\n    In each sub-problem, we keep track of the indices of each node in the original full-problem.\n    \"\"\"\n    values: Tensor\n    weights: Tensor\n    remaining_capacities: Tensor\n    original_idxs: Tensor\n\n\ndef decode(values: Tensor, weights: Tensor, capacities: Tensor, scale: Tensor, net: Module,\n           beam_size: int = 1) -> Tensor:\n\n    if beam_size == 1:\n        selected_items, prices = greedy_decoding(values, weights, capacities, scale, net)\n    else:\n        selected_items, prices = beam_search_decoding(values, weights, capacities, net, beam_size)\n\n    return selected_items, prices\n\n\ndef greedy_decoding(values: Tensor, weights: Tensor, capacities: Tensor, scale: Tensor, net: Module):\n    bs, problem_size = values.shape\n    original_idxs = torch.tensor(list(range(problem_size)), device=values.device)[None, :].repeat(bs, 1)\n\n    sub_problem = DecodingSubPb(values, weights, capacities, original_idxs)\n    selected_items = torch.full((bs, problem_size), -1, dtype=torch.long, device=values.device)\n    steps = list()\n    for dec_pos in range(problem_size):\n        scores, idx_selected, sub_problem = greedy_decoding_step(sub_problem, scale, net)\n        if torch.count_nonzero(idx_selected.flatten() == -1) == bs:\n            # all tours are done!\n            break\n        selected_items[:, dec_pos] = idx_selected\n        steps.append([sub_problem.original_idxs, scores])\n\n    # trick: add 0 at the end of weights and values, for \"selected nodes\" with index -1\n    selected_items[selected_items == -1] = problem_size\n    weights = torch.cat([weights, torch.zeros(bs, 1, device=weights.device)], dim=-1)\n    values = torch.cat([values, torch.zeros(bs, 1, device=values.device)], dim=-1)\n    assert torch.all(torch.gather(weights, 1, selected_items).sum(axis=1) <= capacities)\n\n    rewards = torch.gather(values, 1, selected_items).sum(axis=1)\n\n    return selected_items, rewards\n\n\ndef greedy_decoding_step(sub_problem: DecodingSubPb, scale: Tensor, net: Module) -> (Tensor, DecodingSubPb):\n    values = sub_problem.values / scale.unsqueeze(-1)\n    weights = sub_problem.weights / scale.unsqueeze(-1)\n    current_capacities = sub_problem.remaining_capacities / scale\n\n    inputs = torch.cat([values.unsqueeze(-1), weights.unsqueeze(-1),\n                        current_capacities.unsqueeze(dim=-1).repeat(1, weights.shape[1]).unsqueeze(-1)],\n                       dim=-1)\n    scores = net(inputs, weights=sub_problem.weights, remaining_capacities=sub_problem.remaining_capacities)\n    idx_selected = torch.argmax(scores, dim=1, keepdim=True)\n    # if all possible choices are masked (= -inf), than instance is solved\n    idx_selected_original = torch.gather(sub_problem.original_idxs, 1, idx_selected)\n    idx_selected_original[scores.max(dim=-1)[0] == -np.inf] = -1\n    return scores, idx_selected_original.squeeze(1), reformat_subproblem_for_next_step(sub_problem, idx_selected)\n\n\ndef beam_search_decoding(values: Tensor, weights: Tensor, capacities: Tensor, net: Module, beam_size: int) -> Tensor:\n    bs, problem_size = values.shape\n    assert bs == 1 # this should be improved\n\n    original_idxs = torch.tensor(list(range(problem_size)), device=values.device)[None, :].repeat(bs, 1)\n\n    sub_problem = DecodingSubPb(values, weights, capacities, original_idxs)\n    trajectories = torch.full((bs * beam_size, problem_size), -1, dtype=torch.long, device=values.device)\n    probabilities = torch.zeros((bs, 1), device=values.device)\n\n    # select first elements for all batch instances\n    rewards = torch.zeros(bs * beam_size, device=values.device)\n    possible_trajectories, possible_rewards, possible_capacities = list(), list(), list()\n    for dec_pos in range(problem_size):\n        idx_selected, selected_values, idx_done, pos_in_previous_input, probabilities, sub_problem =\\\n            beam_search_decoding_step(sub_problem, net, probabilities, beam_size, trajectories[:, :dec_pos])\n        if idx_selected is None:\n            break\n            # all tours are done!\n\n        trajectories = trajectories[pos_in_previous_input]\n        rewards = rewards[pos_in_previous_input]\n        trajectories[:, dec_pos] = idx_selected\n\n        rewards += selected_values\n        possible_trajectories.extend(trajectories)\n        possible_rewards.extend(rewards)\n        possible_capacities.extend(sub_problem.capacities)\n\n    possible_rewards = torch.stack(possible_rewards)\n    possible_trajectories = torch.stack(possible_trajectories)\n    possible_capacities = torch.tensor(possible_capacities)\n\n    possible_trajectories = possible_trajectories[possible_capacities > 0]\n    possible_rewards = possible_rewards[possible_capacities > 0]\n\n    return possible_trajectories[torch.argmax(possible_rewards)], torch.max(possible_rewards)\n\n\ndef beam_search_decoding_step(sub_problem: DecodingSubPb, net: Module, prev_probabilities: Tensor,\n                              beam_size: int, trajectories: Tensor) -> (Tensor, DecodingSubPb):\n    num_remaining_items = sub_problem.values.shape[1]\n    inputs = torch.cat([sub_problem.values.unsqueeze(-1), sub_problem.weights.unsqueeze(-1),\n                        sub_problem.capacities.unsqueeze(dim=-1).repeat(1, sub_problem.weights.shape[1]).unsqueeze(-1)],\n                       dim=-1)\n    scores = net(inputs)\n\n    idx_done = (scores.max(dim=-1)[0] == -np.inf).nonzero()\n    if len(idx_done) == len(scores):\n        return None, None, idx_done, None, None, None\n\n    candidates = torch.softmax(scores, dim=-1)\n\n    probabilities = (prev_probabilities.repeat(1, num_remaining_items) +\n                     torch.log(candidates)).reshape(sub_problem.values.shape[0], -1)\n    # set all probabilities in finished trajectories to -inf\n\n    top_probabilities, top_indexes = torch.sort(probabilities.reshape(1, -1), descending=True)\n\n    top_indexes = top_indexes[(~top_probabilities.isnan() & ~top_probabilities.isinf())]\n    top_probabilities = top_probabilities[(~top_probabilities.isnan() & ~top_probabilities.isinf())]\n\n    top_probabilities = top_probabilities.flatten()\n    top_indexes = top_indexes.flatten()\n    pos_in_prev_input = torch.div(top_indexes, num_remaining_items, rounding_mode=\"floor\")\n    idx_selected = torch.remainder(top_indexes, num_remaining_items)\n    sub_problem.values = sub_problem.values[pos_in_prev_input]\n    sub_problem.weights = sub_problem.weights[pos_in_prev_input]\n    sub_problem.capacities = sub_problem.capacities[pos_in_prev_input]\n    sub_problem.original_idxs = sub_problem.original_idxs[pos_in_prev_input]\n\n    selected_values = sub_problem.values[torch.arange(sub_problem.values.shape[0]), idx_selected.squeeze(-1)]\n    idx_selected_original = torch.gather(sub_problem.original_idxs, 1, idx_selected.unsqueeze(-1)).squeeze(-1)\n\n    # select non-intersected trajectories\n    trajectories = trajectories[pos_in_prev_input]\n    trajectories = torch.cat([trajectories, idx_selected_original.unsqueeze(-1)], dim=-1)\n\n    unique_trajectories, selected_idx = list(), list()\n    no_selected = 0\n    for idx in range(len(trajectories)):\n        trajectory = trajectories[idx].cpu().tolist()\n        trajectory.sort()\n        if trajectory not in unique_trajectories:\n            unique_trajectories.append(trajectory)\n            selected_idx.append(idx)\n            no_selected += 1\n        if no_selected == beam_size:\n            break\n\n    idx_selected_original = idx_selected_original[selected_idx]\n    idx_selected = idx_selected[selected_idx].unsqueeze(-1)\n    selected_values = selected_values[selected_idx]\n    pos_in_prev_input = pos_in_prev_input[selected_idx]\n    sub_problem.values = sub_problem.values[selected_idx]\n    sub_problem.weights = sub_problem.weights[selected_idx]\n    sub_problem.capacities = sub_problem.capacities[selected_idx]\n    sub_problem.original_idxs = sub_problem.original_idxs[selected_idx]\n    top_probabilities = top_probabilities[selected_idx].unsqueeze(-1)\n\n    return idx_selected_original, selected_values, idx_done, pos_in_prev_input, top_probabilities,\\\n        reformat_subproblem_for_next_step(sub_problem, idx_selected)\n\n\ndef reformat_subproblem_for_next_step(sub_problem: DecodingSubPb, idx_selected: Tensor) -> DecodingSubPb:\n    # Example: current_subproblem: [a b c d e] => (model selects d) => next_subproblem: [d b c e]\n    subpb_size = sub_problem.weights.shape[1]\n    bs = sub_problem.weights.shape[0]\n    is_selected = torch.arange(\n        subpb_size, device=sub_problem.weights.device).unsqueeze(dim=0).repeat(bs, 1) == idx_selected.repeat(1,\n                                                                                                             subpb_size)\n    # remaining items = the rest\n    next_capacities = sub_problem.remaining_capacities - sub_problem.weights[is_selected]\n    next_remaining_values = sub_problem.values[~is_selected].reshape((bs, -1))\n    next_remaining_weights = sub_problem.weights[~is_selected].reshape((bs, -1))\n    next_original_idxs = sub_problem.original_idxs[~is_selected].reshape((bs, -1))\n    return DecodingSubPb(next_remaining_values, next_remaining_weights, next_capacities, next_original_idxs)\n\n# ==========================================\n# File: learning/kp/traj_learner.py\n# Function/Context: TrajectoryLearner\n# ==========================================\nimport time\nimport torch\nfrom utils.multi_class_loss import CrossEntropyLoss\nfrom learning.kp.decoding import decode\nfrom utils.misc import do_lr_decay, EpochMetrics\n\nDEBUG_NUM_BATCHES = 3\n\nclass TrajectoryLearner:\n\n    def __init__(self, args, net, module, device, data_iterator, optimizer=None, checkpointer=None):\n        # same supervisor is used for training and testing, during testing we do not have optimizer, mlflow etc.\n\n        self.net = net\n        self.module = module\n        self.device = device\n        self.data_iterator = data_iterator\n        self.optimizer = optimizer\n        self.checkpointer = checkpointer\n        self.beam_size = args.beam_size\n\n        self.output_dir = args.output_dir\n        self.test_only = args.test_only\n\n        self.debug = args.debug\n\n        if not args.test_only:\n            try:\n                self.test_every = args.test_every if args.test_every > 0 else None\n            except AttributeError:\n                self.test_every = None\n            self.decay_rate = args.decay_rate\n            self.decay_every = args.decay_every\n            self.loss = CrossEntropyLoss()\n            self.best_current_val_metric = float('inf')\n            self.epoch_done = 0\n            self.nb_epochs = args.nb_total_epochs\n\n    def train(self):\n        assert not self.test_only\n\n        for _ in range(self.nb_epochs):\n            # Train one epoch\n            start = time.time()\n            self.net.train()\n            epoch_metrics_train = EpochMetrics()\n\n            for batch_num, data in enumerate(self.data_iterator.train_trajectories):\n\n                self.optimizer.zero_grad()\n                values, weights, remaining_capacities, solutions_probs, _, scale = self.prepare_batch(data)\n                values_n, weights_n, current_capacities_n = self.normalize_data(values, weights, remaining_capacities,\n                                                                                scale)\n                inputs = torch.cat([values_n.unsqueeze(-1), weights_n.unsqueeze(-1),\n                                   current_capacities_n.unsqueeze(dim=-1).\n                                   repeat(1, weights.shape[1]).unsqueeze(-1)], dim=-1)\n                output_scores = self.net(inputs, remaining_capacities=remaining_capacities, weights=weights)\n\n                loss = self.loss(output_scores, torch.softmax(solutions_probs, dim=-1))\n                loss.backward()\n                epoch_metrics_train.update({\"training loss\": loss})\n                self.optimizer.step()\n\n                if batch_num == DEBUG_NUM_BATCHES and self.debug:\n                    break\n                batch_num += 1\n\n            metrics = {f'{k}_train': v for k, v in epoch_metrics_train.get_means().items()}\n\n            print(\"[EPOCH {:03d}] Time: {:.3f}s \".format(self.epoch_done, time.time() - start))\n            for k, v in metrics.items():\n                print(k, f\"{v:.5f}\")\n\n            # Val test\n            val_metrics = self.val_test(\"val\")\n\n            if val_metrics[\"opt_gap_val\"] < self.best_current_val_metric:\n                # monitoring on current trajectories, in order to see if we are training enough on them or not\n                self.best_current_val_metric = val_metrics[\"opt_gap_val\"]\n                self.checkpointer.save(self.module, None, 'best')  # only model\n\n            print(\"[EPOCH {:03d}] Time: {:.3f}s \".format(self.epoch_done, time.time() - start))\n\n            # test\n            if self.test_every is not None:\n                if self.epoch_done % self.test_every == 0:\n                    self.save_model(\"current\")\n                    self.load_model(\"best\")\n                    self.val_test(\"test\")\n                    self.load_model(\"current\")\n                    self.remove_model(\"current\")\n\n            self.epoch_done += 1\n\n            # lr decay\n            if self.epoch_done % self.decay_every == 0:\n                do_lr_decay(self.optimizer, self.decay_rate)\n\n    def val_test(self, what=\"test\"):\n        if what == \"test\":\n            dataloader = self.data_iterator.test_trajectories\n        else:\n            dataloader = self.data_iterator.val_trajectories\n\n        self.net.eval()\n        epoch_metrics = EpochMetrics()\n        with torch.no_grad():\n\n            for batch_num, data in enumerate(dataloader):\n                val_test_metrics = self.get_minibatch_val_test_metrics(data)\n                epoch_metrics.update(val_test_metrics)\n                if batch_num == DEBUG_NUM_BATCHES and self.debug:\n                    break\n\n        res = {f'{k}_{what}': v for k, v in epoch_metrics.get_means().items()}\n\n        for k, v in res.items():\n            print(k, f\"{v:.3f}\")\n\n        return res\n\n    @staticmethod\n    def normalize_data(values, weights, current_capacities, scale):\n        return values / scale.unsqueeze(-1), weights / scale.unsqueeze(-1), current_capacities / scale\n\n    def load_model(self, label, allow_not_exist=False):\n        assert label in [\"current\", \"current_FULL\", \"best\"]\n        self.checkpointer.load(self.module, None, label, allow_not_exist=allow_not_exist)\n\n    def save_model(self, label, complete=False):\n        assert label in [\"current\", \"best\"]\n        args = {\"module\": self.module}\n        if not complete:\n            args_ = {\"optimizer\": None, 'label': label}\n        else:\n            assert not self.eval_only\n            assert label == \"current\"\n            args_ = {\"optimizer\": self.optimizer,\n                     \"label\": label+\"_FULL\",\n                     \"other\": {\n                         \"epoch_done\": self.epoch_done,\n                         \"best_current_val_metric\": self.best_current_val_metric,\n                         \"data_iterator\": self.data_iterator}\n                     }\n        self.checkpointer.save(**args, **args_)\n\n    def remove_model(self, label):\n        assert label in [\"current\", \"best\"]\n        self.checkpointer.delete(label)\n\n    def get_minibatch_val_test_metrics(self, data):\n        metrics = {}\n        # autoregressive decoding\n        values, weights, total_capacities, _, rewards, scale = self.prepare_batch(data, sample=False)\n        decoding_metrics = {}\n        _, predicted_rewards = decode(values, weights, total_capacities, scale, self.net, self.beam_size)\n        opt_gap = 100 * ((rewards - predicted_rewards) / rewards).mean().item()\n        decoding_metrics.update({\"opt_gap\": opt_gap})\n        decoding_metrics.update({\"predicted_price\": predicted_rewards.mean().item()})\n\n        return {**metrics, **decoding_metrics}\n\n    def prepare_batch(self, data, sample=True):\n        ks = \"_s\" if sample else \"\"\n        weights = data[f\"weights{ks}\"].to(self.device)\n        values = data[f\"values{ks}\"].to(self.device)\n        remaining_capacities = data[f\"remaining_capacities{ks}\"].to(self.device)\n        if sample:\n            solutions_probs = data[f\"solution_probs{ks}\"].to(self.device)\n        else:\n            solutions_probs = None\n        rewards = data[f\"optimal_values{ks}\"].to(self.device)\n        scale = data[\"scale\"].to(self.device)\n\n        return values, weights, remaining_capacities, solutions_probs, rewards, scale\n\n# ==========================================\n# File: learning/op/decoding.py\n# Function/Context: \n# ==========================================\nfrom dataclasses import dataclass\nimport numpy as np\nimport torch\nimport copy\nfrom torch import Tensor\nfrom torch.nn import Module\nfrom utils.misc import compute_tour_lens\n\n@dataclass\nclass DecodingSubPb:\n    \"\"\"\n    In decoding, we successively apply model on progressively smaller sub-problems.\n    In each sub-problem, we keep track of the indices of each node in the original full-problem.\n    \"\"\"\n    node_coords: Tensor\n    node_values: Tensor\n    upper_bounds: Tensor\n    original_idxs: Tensor\n    dist_matrices: Tensor\n\n\ndef decode(node_coords: Tensor, node_values: Tensor, upper_bounds: Tensor, dist_matrices: Tensor, net: Module,\n           beam_size: int, knns: int) -> Tensor:\n    if beam_size == 1:\n        tours, collected_rewards = greedy_decoding_loop(node_coords, node_values, upper_bounds, dist_matrices, net, knns)\n    else:\n        tours, collected_rewards = beam_search_decoding_loop(node_coords, node_values, upper_bounds, dist_matrices, net, beam_size, knns)\n\n    distances = compute_tour_lens(tours, dist_matrices)\n    assert torch.all(distances <= upper_bounds + 1e-4)\n\n    return collected_rewards, tours\n\n\ndef greedy_decoding_loop(node_coords: Tensor, node_values: Tensor, upper_bounds: Tensor, dist_matrices: Tensor,\n                         net: Module, knns: int) -> Tensor:\n    bs, num_nodes, _ = node_coords.shape\n    original_idxs = torch.tensor(list(range(num_nodes)), device=node_coords.device)[None, :].repeat(bs, 1)\n    paths = torch.zeros((bs, num_nodes), dtype=torch.long, device=node_coords.device)\n    collected_rewards = torch.zeros(bs, device=node_coords.device)\n    sub_problem = DecodingSubPb(node_coords, node_values, upper_bounds, original_idxs, dist_matrices)\n    for dec_pos in range(1, num_nodes - 1):\n        idx_selected, sub_problem = greedy_decoding_step(sub_problem, net, knns)\n        paths[:, dec_pos] = idx_selected\n        collected_rewards += node_values[torch.arange(bs), idx_selected]\n        if torch.count_nonzero(idx_selected.flatten() == -1) == bs:\n            # all tours are done!\n            break\n\n    return paths, collected_rewards\n\n\ndef greedy_decoding_step(sub_problem: DecodingSubPb, net: Module, knns: int) -> (Tensor, DecodingSubPb):\n    scores = prepare_input_and_forward_pass(sub_problem, net, knns)\n    idx_selected = torch.argmax(scores, dim=1, keepdim=True)  # (b, 1)\n    idx_selected_original = torch.gather(sub_problem.original_idxs, 1, idx_selected)\n    idx_selected_original[scores.max(dim=-1)[0] == -np.inf] = -1\n    return idx_selected_original.squeeze(1), reformat_subproblem_for_next_step(sub_problem, idx_selected)\n\n\ndef beam_search_decoding_loop(node_coords: Tensor, node_values: Tensor, upper_bounds: Tensor, dist_matrices: Tensor,\n                              net: Module, beam_size: int, knns: int) -> Tensor:\n    bs, num_nodes, _ = node_coords.shape  # (including repetition of begin=end node)\n\n    original_idxs = torch.tensor(list(range(num_nodes)), device=node_coords.device)[None, :].repeat(bs, 1)\n    paths = torch.zeros((bs * beam_size, num_nodes), dtype=torch.long, device=node_coords.device)\n    paths[:, -1] = num_nodes - 1\n\n    probabilities = torch.zeros((bs, 1), device=node_coords.device)\n    paths = torch.zeros((bs, num_nodes), dtype=torch.long, device=node_coords.device)\n    sub_problem = DecodingSubPb(node_coords, node_values, upper_bounds, original_idxs, dist_matrices)\n    solutions_candidates = [None] * bs\n\n    for dec_pos in range(1, num_nodes - 1):\n        idx_selected_original, instances_done, batch_in_prev_input, probabilities, sub_problem = \\\n            beam_search_decoding_step(sub_problem, net, probabilities, bs, beam_size, knns)\n\n        for idx in torch.nonzero(instances_done):\n            instance_idx = torch.div(idx, beam_size, rounding_mode=\"trunc\")\n            if solutions_candidates[instance_idx.item()] is None:\n                solutions_candidates[instance_idx.item()] = list()\n            solutions_candidates[instance_idx.item()].append(copy.deepcopy(paths[idx]))\n        if torch.count_nonzero(instances_done) == bs * beam_size:\n            # all done\n            break\n\n        paths = paths[batch_in_prev_input]\n        paths[:, dec_pos] = idx_selected_original\n\n    paths, collected_rewards = list(), list()\n    for instance_id in range(bs):\n        tours_s = torch.cat(solutions_candidates[instance_id], dim=0)\n        rewards = node_values[instance_id][tours_s].sum(axis=-1)\n        best = torch.argmax(rewards)\n        paths.append(tours_s[best])\n        collected_rewards.append(rewards[best])\n\n    return torch.stack(paths, dim=0), torch.stack(collected_rewards, dim=0)\n\n\ndef beam_search_decoding_step(sub_problem: DecodingSubPb, net: Module, prev_probabilities: Tensor, test_batch_size: int,\n                              beam_size: int, knns: int) -> (Tensor, DecodingSubPb):\n    scores = prepare_input_and_forward_pass(sub_problem, net, knns)\n    instances_done = scores.max(dim=-1)[0] == -np.inf\n    num_nodes = sub_problem.node_coords.shape[1]\n    num_instances = sub_problem.node_coords.shape[0] // test_batch_size\n    candidates = torch.softmax(scores, dim=1)\n\n    probabilities = (prev_probabilities.repeat(1, num_nodes) + torch.log(candidates)).reshape(test_batch_size, -1)\n    # replace nans (scores of already done instances) to -np.inf\n    probabilities[probabilities.isnan()] = -np.inf\n\n    k = min(beam_size, probabilities.shape[1] - 2)\n    topk_values, topk_indexes = torch.topk(probabilities, k, dim=1)\n    batch_in_prev_input = ((num_instances * torch.arange(test_batch_size, device=probabilities.device)).unsqueeze(dim=1) +\\\n                           torch.div(topk_indexes, num_nodes, rounding_mode=\"floor\")).flatten()\n    topk_values = topk_values.flatten()\n    topk_indexes = topk_indexes.flatten()\n    sub_problem.node_coords = sub_problem.node_coords[batch_in_prev_input]\n    sub_problem.node_values = sub_problem.node_values[batch_in_prev_input]\n    sub_problem.original_idxs = sub_problem.original_idxs[batch_in_prev_input]\n    sub_problem.dist_matrices = sub_problem.dist_matrices[batch_in_prev_input]\n    sub_problem.upper_bounds = sub_problem.upper_bounds[batch_in_prev_input]\n    idx_selected = torch.remainder(topk_indexes, num_nodes).unsqueeze(dim=1)\n    idx_selected_original = torch.gather(sub_problem.original_idxs, 1, idx_selected).squeeze(-1)\n\n    idx_selected_original[topk_values == -np.inf] = 0\n    return idx_selected_original, instances_done, batch_in_prev_input, topk_values.unsqueeze(dim=1),\\\n        reformat_subproblem_for_next_step(sub_problem, idx_selected)\n\n\ndef prepare_input_and_forward_pass(sub_problem: DecodingSubPb, net: Module, knns: int) -> Tensor:\n    # find K nearest neighbors of the current node\n    bs, num_nodes, node_dim = sub_problem.node_coords.shape\n    if 0 < knns < num_nodes:\n        # sort node by distance from the origin (ignore the target node)\n        _, sorted_nodes_idx = torch.sort(sub_problem.dist_matrices[:, :-1, 0], dim=-1)\n\n        num_closest = (knns - 1) // 2\n        num_largest = knns - 1 - num_closest\n        # select KNNs\n        indices_closest = sorted_nodes_idx[:, :num_closest]\n        remaining_idx = sorted_nodes_idx[:, num_closest:-1]\n        indices_largest = torch.stack([remaining_idx[i][torch.topk(sub_problem.node_values[i, remaining_idx[i]],\n                           k=num_largest)[1]] for i in range(bs)])\n\n        # concatenate them, together with the destination\n        input_nodes_idx = torch.cat([indices_closest, indices_largest,\n                                     torch.full([bs, 1], num_nodes - 1, device=indices_closest.device)], dim=-1)\n        knn_node_coords = torch.gather(sub_problem.node_coords, 1, input_nodes_idx.unsqueeze(dim=-1).repeat(1, 1,\n                                                                                                            node_dim))\n        knn_node_values = torch.gather(sub_problem.node_values, 1, input_nodes_idx)\n        knn_dist_matrices = torch.gather(sub_problem.dist_matrices, 1,\n                                         input_nodes_idx.unsqueeze(-1).repeat(1, 1, num_nodes))\n        knn_dist_matrices = torch.gather(knn_dist_matrices, 2, input_nodes_idx.unsqueeze(1).repeat(1, knns, 1))\n        inputs = torch.cat([knn_node_coords, knn_node_values.unsqueeze(-1),\n                            sub_problem.upper_bounds.unsqueeze(-1).repeat(1, knns).unsqueeze(-1)], dim=-1)\n        knn_scores = net(inputs, dist_matrices=knn_dist_matrices)  # (b, seq)\n\n        # create result tensor for scores with all -inf elements\n        scores = torch.full(sub_problem.node_coords.shape[:-1], -np.inf, device=knn_node_coords.device)\n        # and put computed scores for KNNs\n        scores = torch.scatter(scores, 1, input_nodes_idx, knn_scores)\n    else:\n        inputs = torch.cat([sub_problem.node_coords, sub_problem.node_values.unsqueeze(-1),\n                            sub_problem.upper_bounds.unsqueeze(-1).repeat(1,\n                                                                          sub_problem.node_coords.shape[1]).unsqueeze(\n                                -1)],\n                           dim=-1)\n        scores = net(inputs, dist_matrices=sub_problem.dist_matrices)\n\n    return scores\n\n\ndef reformat_subproblem_for_next_step(sub_problem: DecodingSubPb, idx_selected: Tensor) -> DecodingSubPb:\n    # Example: current_subproblem: [a b c d e] => (model selects d) => next_subproblem: [d b c e]\n    bs, subpb_size, _ = sub_problem.node_coords.shape\n    is_selected = torch.arange(\n        subpb_size, device=sub_problem.node_coords.device).unsqueeze(dim=0).repeat(bs, 1) == idx_selected.repeat(1,\n                                                                                                                 subpb_size)\n    # next begin node = just-selected node\n    next_begin_node_coord = sub_problem.node_coords[is_selected].unsqueeze(dim=1)\n    next_begin_original_idx = sub_problem.original_idxs[is_selected].unsqueeze(dim=1)\n    next_begin_node_value = sub_problem.node_values[is_selected].unsqueeze(dim=1)\n\n    # remaining nodes = the rest, minus current first node\n    next_remaining_node_coords = sub_problem.node_coords[~is_selected].reshape((bs, -1, 2))[:, 1:, :]\n    next_remaining_original_idxs = sub_problem.original_idxs[~is_selected].reshape((bs, -1))[:, 1:]\n    next_remaining_node_values = sub_problem.node_values[~is_selected].reshape((bs, -1))[:, 1:]\n\n    # concatenate\n    next_node_coords = torch.cat([next_begin_node_coord, next_remaining_node_coords], dim=1)\n    next_original_idxs = torch.cat([next_begin_original_idx, next_remaining_original_idxs], dim=1)\n    next_node_values = torch.cat([next_begin_node_value, next_remaining_node_values], dim=1)\n\n    next_upper_bounds = sub_problem.upper_bounds - sub_problem.dist_matrices[:, 0, :][is_selected]\n\n    # select row (=column) of adj matrix for just-selected node\n    next_row_column = sub_problem.dist_matrices[is_selected]\n    # remove distance to the selected node (=0)\n    next_row_column = next_row_column[~is_selected].reshape((bs, -1))[:, 1:]\n\n    # remove rows and columns of selected nodes\n    next_dist_matrices = sub_problem.dist_matrices[~is_selected].reshape(bs, -1, subpb_size)[:, 1:, :]\n    next_dist_matrices = next_dist_matrices.transpose(1, 2)[~is_selected].reshape(bs, subpb_size-1, -1)[:, 1:, :]\n\n    # add new row on the top and remove second (must be done like this, because on dimenstons of the matrix)\n    next_dist_matrices = torch.cat([next_row_column.unsqueeze(dim=1), next_dist_matrices], dim=1)\n\n    # and add it to the beginning-\n    next_row_column = torch.cat([torch.zeros(idx_selected.shape, device=next_row_column.device), next_row_column], dim=1)\n    next_dist_matrices = torch.cat([next_row_column.unsqueeze(dim=2), next_dist_matrices], dim=2)\n\n    return DecodingSubPb(next_node_coords, next_node_values, next_upper_bounds, next_original_idxs, next_dist_matrices)\n\n# ==========================================\n# File: learning/op/traj_learner.py\n# Function/Context: TrajectoryLearner\n# ==========================================\nimport time\nimport torch\nfrom torch import nn\nfrom learning.op.decoding import decode\nfrom utils.misc import do_lr_decay, EpochMetrics\n\nDEBUG_NUM_BATCHES = 3\n\nclass TrajectoryLearner:\n\n    def __init__(self, args, net, module, device, data_iterator, optimizer=None, checkpointer=None):\n        # same supervisor is used for training and testing, during testing we do not have optimizer, mlflow etc.\n\n        self.net = net\n        self.module = module\n        self.device = device\n        self.data_iterator = data_iterator\n        self.optimizer = optimizer\n        self.checkpointer = checkpointer\n        self.beam_size = args.beam_size\n        self.knns = args.knns\n\n        self.output_dir = args.output_dir\n        self.test_only = args.test_only\n\n        self.debug = args.debug\n\n        if not args.test_only:\n            try:\n                self.test_every = args.test_every if args.test_every > 0 else None\n            except AttributeError:\n                self.test_every = None\n            self.decay_rate = args.decay_rate\n            self.decay_every = args.decay_every\n            self.loss = nn.CrossEntropyLoss()\n            self.best_current_val_metric = float('inf')\n            self.epoch_done = 0\n            self.nb_epochs = args.nb_total_epochs\n\n    def train(self):\n        assert not self.test_only\n\n        for _ in range(self.nb_epochs):\n            # Train one epoch\n            start = time.time()\n            self.net.train()\n            epoch_metrics_train = EpochMetrics()\n\n            batch_num = 0\n            for data in self.data_iterator.train_trajectories:\n\n                self.optimizer.zero_grad()\n                node_coords, node_values, upper_bounds, dist_matrices, _ = self.prepare_batch(data)\n                inputs = torch.cat([node_coords, node_values.unsqueeze(-1),\n                                    upper_bounds.unsqueeze(-1).repeat(1, node_coords.shape[1]).unsqueeze(-1)],\n                                   dim=-1)\n                output_scores = self.net(inputs, dist_matrices=dist_matrices)\n                loss = self.loss(output_scores,\n                                 torch.ones((output_scores.shape[0]), dtype=torch.long,\n                                            device=output_scores.device))\n\n                loss.backward()\n                epoch_metrics_train.update({\"training loss\": loss})\n                self.optimizer.step()\n\n                if batch_num == DEBUG_NUM_BATCHES and self.debug:\n                    break\n                batch_num += 1\n\n            metrics = {f'{k}_train': v for k, v in epoch_metrics_train.get_means().items()}\n            print(\"[EPOCH {:03d}] Time: {:.3f}s \".format(self.epoch_done, time.time() - start))\n            for k, v in metrics.items():\n                print(k, f\"{v:.5f}\")\n\n            # Val test\n            val_metrics = self.val_test(\"val\")\n\n            if val_metrics[\"opt_gap_val\"] < self.best_current_val_metric:\n                # monitoring on current trajectories, in order to see if we are training enough on them or not\n                self.best_current_val_metric = val_metrics[\"opt_gap_val\"]\n                self.checkpointer.save(self.module, None, 'best')  # only model\n\n            print(\"[EPOCH {:03d}] Time: {:.3f}s \".format(self.epoch_done, time.time() - start))\n\n            # test\n            if self.test_every is not None:\n                if self.epoch_done % self.test_every == 0:\n                    self.save_model(\"current\")\n                    self.load_model(\"best\")\n                    self.val_test(\"test\")\n                    self.load_model(\"current\")\n                    self.remove_model(\"current\")\n\n            self.epoch_done += 1\n\n            # lr decay\n            if self.epoch_done % self.decay_every == 0:\n                do_lr_decay(self.optimizer, self.decay_rate)\n\n    def val_test(self, what=\"test\"):\n        if what == \"test\":\n            dataloader = self.data_iterator.test_trajectories\n        else:\n            dataloader = self.data_iterator.val_trajectories\n\n        self.net.eval()\n        epoch_metrics = EpochMetrics()\n        with torch.no_grad():\n\n            for batch_num, data in enumerate(dataloader):\n                val_test_metrics = self.get_minibatch_val_test_metrics(data)\n                epoch_metrics.update(val_test_metrics)\n                if batch_num == DEBUG_NUM_BATCHES and self.debug:\n                    break\n\n        res = {f'{k}_{what}': v for k, v in epoch_metrics.get_means().items()}\n\n        for k, v in res.items():\n            print(k, f\"{v:.3f}\")\n\n        return res\n\n    def get_minibatch_val_test_metrics(self, data):\n        metrics = {}\n        # autoregressive decoding\n        node_coords, node_values, upper_bounds, dist_matrices, opt_values = self.prepare_batch(data, sample=False)\n        decoding_metrics = {}\n        predicted_values, _ = decode(node_coords, node_values, upper_bounds, dist_matrices, self.net,\n                                     self.beam_size, self.knns)\n\n        opt_gap = 100 * ((opt_values - predicted_values) / opt_values).mean().item()\n\n        decoding_metrics.update({\"opt_gap\": opt_gap}) \n        return {**metrics, **decoding_metrics}\n\n    def load_model(self, label, allow_not_exist=False):\n        assert label in [\"current\", \"current_FULL\", \"best\"]\n        self.checkpointer.load(self.module, None, label, allow_not_exist=allow_not_exist)\n\n    def save_model(self, label, complete=False):\n        assert label in [\"current\", \"best\"]\n        args = {\"module\": self.module}\n        if not complete:\n            args_ = {\"optimizer\": None, 'label': label}\n        else:\n            assert not self.eval_only\n            assert label == \"current\"\n            args_ = {\"optimizer\": self.optimizer,\n                     \"label\": label+\"_FULL\",\n                     \"other\": {\n                         \"epoch_done\": self.epoch_done,\n                         \"best_current_val_metric\": self.best_current_val_metric,\n                         \"data_iterator\": self.data_iterator}\n                     }\n        self.checkpointer.save(**args, **args_)\n\n    def remove_model(self, label):\n        assert label in [\"current\", \"best\"]\n        self.checkpointer.delete(label)\n\n    def prepare_batch(self, data, sample=True):\n        ks = \"_s\" if sample else \"\"\n        node_coords = data[f\"node_coords{ks}\"].to(self.device)\n        node_values = data[f\"node_values{ks}\"].to(self.device)\n        upper_bounds = data[f\"upper_bounds{ks}\"].to(self.device)\n\n        dist_matrices = data[f\"dist_matrices{ks}\"].to(self.device)\n        collected_rewards = data[f\"collected_rewards{ks}\"].to(self.device)\n\n        return node_coords, node_values, upper_bounds, dist_matrices, collected_rewards\n\n# ==========================================\n# File: learning/tsp/decoding.py\n# Function/Context: decode, beam_search_decoding_loop, greedy_decoding_loop, beam_search_decoding_step, greedy_decoding_step, prepare_input_and_forward_pass, reformat_subproblem_for_next_step\n# ==========================================\nfrom dataclasses import dataclass\nimport numpy as np\nimport torch\nfrom torch import Tensor\nfrom torch.nn import Module\nfrom utils.misc import compute_tour_lens\n\n@dataclass\nclass DecodingSubPb:\n    \"\"\"\n    In decoding, we successively apply model on progressively smaller sub-problems.\n    In each sub-problem, we keep track of the indices of each node in the original full-problem.\n    \"\"\"\n    node_coords: Tensor\n    original_idxs: Tensor\n    dist_matrices: Tensor\n\n\ndef decode(node_coords: Tensor, dist_matrices: Tensor, net: Module, beam_size: int, knns: int) -> Tensor:\n    if beam_size == 1:\n        tours = greedy_decoding_loop(node_coords, dist_matrices, net, knns)\n    else:\n        tours = beam_search_decoding_loop(node_coords, dist_matrices, net, beam_size, knns)\n\n    tours = tours[:, :-1]\n\n    num_nodes = tours.shape[1]\n    assert tours.sum(axis=1).sum() == tours.shape[0] * .5 * (num_nodes - 1) * num_nodes\n    # compute distances by using (original) distance matrix\n    distances = compute_tour_lens(tours, dist_matrices)\n\n    return tours, distances\n\n\ndef beam_search_decoding_loop(node_coords: Tensor, dist_matrices: Tensor, net: Module, beam_size: int,\n                              knns: int) -> Tensor:\n    bs, num_nodes, _ = node_coords.shape  # (including repetition of begin=end node)\n\n    original_idxs = torch.tensor(list(range(num_nodes)), device=node_coords.device)[None, :].repeat(bs, 1)\n    paths = torch.zeros((bs * beam_size, num_nodes), dtype=torch.long, device=node_coords.device)\n    paths[:, -1] = num_nodes - 1\n\n    probabilities = torch.zeros((bs, 1), device=node_coords.device)\n    distances = torch.zeros(bs * beam_size, 1, device=node_coords.device)\n\n    sub_problem = DecodingSubPb(node_coords, original_idxs, dist_matrices)\n    for dec_pos in range(1, num_nodes - 1):\n        origin_coords = sub_problem.node_coords[:, 0]\n\n        idx_selected_original, batch_in_prev_input, probabilities, sub_problem =\\\n            beam_search_decoding_step(sub_problem, net, probabilities, bs, beam_size, knns)\n\n        paths = paths[batch_in_prev_input]\n        paths[:, dec_pos] = idx_selected_original\n        distances = distances[batch_in_prev_input]\n        # these are distances between normalized! coordinates (!= real tour lengths)\n        distances += torch.cdist(origin_coords[batch_in_prev_input].unsqueeze(dim=1),\n                                 sub_problem.node_coords[:, 0].unsqueeze(dim=1)).squeeze(-1)\n    distances += torch.cdist(sub_problem.node_coords[:, 0].unsqueeze(dim=1),\n                             sub_problem.node_coords[:, -1].unsqueeze(dim=1)).squeeze(-1)\n\n    distances = distances.reshape(bs, -1)\n    paths = paths.reshape(bs, -1, num_nodes)\n    return paths[torch.arange(bs), torch.argmin(distances, dim=1)]\n\n\ndef greedy_decoding_loop(node_coords: Tensor, dist_matrices: Tensor, net: Module, knns: int) -> Tensor:\n    num_nodes = node_coords.shape[1]  # (including repetition of begin=end node)\n    bs = node_coords.shape[0]\n    original_idxs = torch.tensor(list(range(num_nodes)), device=node_coords.device)[None, :].repeat(bs, 1)\n    paths = torch.zeros((bs, num_nodes), dtype=torch.long, device=node_coords.device)\n    paths[:, -1] = num_nodes - 1\n\n    sub_problem = DecodingSubPb(node_coords, original_idxs, dist_matrices)\n    for dec_pos in range(1, num_nodes - 1):\n        idx_selected, sub_problem = greedy_decoding_step(sub_problem, net, knns)\n        paths[:, dec_pos] = idx_selected\n\n    return paths\n\n\ndef prepare_input_and_forward_pass(sub_problem: DecodingSubPb, net: Module, knns: int) -> Tensor:\n    # find K nearest neighbors of the current node\n    bs, num_nodes, node_dim = sub_problem.node_coords.shape\n    if 0 < knns < num_nodes:\n        # sort node by distance from the origin (ignore the target node)\n        _, sorted_nodes_idx = torch.sort(sub_problem.dist_matrices[:, :-1, 0], dim=-1)\n\n        # select KNNs\n        knn_indices = sorted_nodes_idx[:, :knns-1]\n        # and add the target at the end\n        input_nodes = torch.cat([knn_indices, torch.full([bs, 1], num_nodes - 1, device=knn_indices.device)], dim=-1)\n\n        node_coords = torch.gather(sub_problem.node_coords, 1, input_nodes.unsqueeze(dim=-1).repeat(1, 1, node_dim))\n        knn_scores = net(node_coords)  # (b, seq)\n\n        # create result tensor for scores with all -inf elements\n        scores = torch.full(sub_problem.node_coords.shape[:-1], -np.inf, device=node_coords.device)\n        # and put computed scores for KNNs\n        scores = torch.scatter(scores, 1, knn_indices, knn_scores)\n    else:\n        scores = net(sub_problem.node_coords)\n\n    return scores\n\n\ndef beam_search_decoding_step(sub_problem: DecodingSubPb, net: Module, prev_probabilities: Tensor, test_batch_size: int,\n                              beam_size: int, knns: int) -> (Tensor, DecodingSubPb):\n    scores = prepare_input_and_forward_pass(sub_problem, net, knns)\n    num_nodes = sub_problem.node_coords.shape[1]\n    num_instances = sub_problem.node_coords.shape[0] // test_batch_size\n    candidates = torch.softmax(scores, dim=1)\n\n    probabilities = (prev_probabilities.repeat(1, num_nodes) + torch.log(candidates)).reshape(test_batch_size, -1)\n\n    k = min(beam_size, probabilities.shape[1] - 2)\n    topk_values, topk_indexes = torch.topk(probabilities, k, dim=1)\n    batch_in_prev_input = ((num_instances * torch.arange(test_batch_size, device=probabilities.device)).unsqueeze(dim=1) +\\\n                           torch.div(topk_indexes, num_nodes, rounding_mode=\"floor\")).flatten()\n    topk_values = topk_values.flatten()\n    topk_indexes = topk_indexes.flatten()\n    sub_problem.node_coords = sub_problem.node_coords[batch_in_prev_input]\n    sub_problem.original_idxs = sub_problem.original_idxs[batch_in_prev_input]\n    sub_problem.dist_matrices = sub_problem.dist_matrices[batch_in_prev_input]\n    idx_selected = torch.remainder(topk_indexes, num_nodes).unsqueeze(dim=1)\n    idx_selected_original = torch.gather(sub_problem.original_idxs, 1, idx_selected).squeeze(-1)\n\n    return idx_selected_original, batch_in_prev_input, topk_values.unsqueeze(dim=1), \\\n           reformat_subproblem_for_next_step(sub_problem, idx_selected, knns)\n\n\ndef greedy_decoding_step(sub_problem: DecodingSubPb, net: Module, knns: int) -> (Tensor, DecodingSubPb):\n    scores = prepare_input_and_forward_pass(sub_problem, net, knns)\n    idx_selected = torch.argmax(scores, dim=1, keepdim=True)  # (b, 1)\n    idx_selected_original = torch.gather(sub_problem.original_idxs, 1, idx_selected)\n    return idx_selected_original.squeeze(1), reformat_subproblem_for_next_step(sub_problem, idx_selected, knns)\n\n\ndef reformat_subproblem_for_next_step(sub_problem: DecodingSubPb, idx_selected: Tensor, knns: int) -> DecodingSubPb:\n    # Example: current_subproblem: [a b c d e] => (model selects d) => next_subproblem: [d b c e]\n    subpb_size = sub_problem.node_coords.shape[1]\n    bs = sub_problem.node_coords.shape[0]\n    is_selected = torch.arange(\n        subpb_size, device=sub_problem.node_coords.device).unsqueeze(dim=0).repeat(bs, 1) == idx_selected.repeat(1,\n                                                                                                                 subpb_size)\n    # next begin node = just-selected node\n    next_begin_node_coord = sub_problem.node_coords[is_selected].unsqueeze(dim=1)\n    next_begin_original_idx = sub_problem.original_idxs[is_selected].unsqueeze(dim=1)\n\n    # remaining nodes = the rest, minus current first node\n    next_remaining_node_coords = sub_problem.node_coords[~is_selected].reshape((bs, -1, 2))[:, 1:, :]\n    next_remaining_original_idxs = sub_problem.original_idxs[~is_selected].reshape((bs, -1))[:, 1:]\n\n    # concatenate\n    next_node_coords = torch.cat([next_begin_node_coord, next_remaining_node_coords], dim=1)\n    next_original_idxs = torch.cat([next_begin_original_idx, next_remaining_original_idxs], dim=1)\n\n    if knns != -1:\n        num_nodes = sub_problem.dist_matrices.shape[1]\n\n        # select row (=column) of adj matrix for just-selected node\n        next_row_column = sub_problem.dist_matrices[is_selected]\n        # remove distance to the selected node (=0)\n        next_row_column = next_row_column[~is_selected].reshape((bs, -1))[:, 1:]\n\n        # remove rows and columns of selected nodes\n        next_dist_matrices = sub_problem.dist_matrices[~is_selected].reshape(bs, -1, num_nodes)[:, 1:, :]\n        next_dist_matrices = next_dist_matrices.transpose(1, 2)[~is_selected].reshape(bs, num_nodes-1, -1)[:, 1:, :]\n\n        # add new row on the top and remove second (must be done like this, because on dimenstons of the matrix)\n        next_dist_matrices = torch.cat([next_row_column.unsqueeze(dim=1), next_dist_matrices], dim=1)\n\n        # and add it to the beginning-\n        next_row_column = torch.cat([torch.zeros(idx_selected.shape, device=next_row_column.device),\n                                     next_row_column], dim=1)\n        next_dist_matrices = torch.cat([next_row_column.unsqueeze(dim=2), next_dist_matrices], dim=2)\n    else:\n        next_dist_matrices = sub_problem.dist_matrices\n\n    return DecodingSubPb(next_node_coords, next_original_idxs, next_dist_matrices)\n\n# ==========================================\n# File: learning/tsp/traj_learner.py\n# Function/Context: TrajectoryLearner\n# ==========================================\nimport time\nimport torch\nfrom torch import nn\nfrom learning.tsp.decoding import decode\nfrom utils.misc import do_lr_decay, EpochMetrics, get_opt_gap\n\nDEBUG_NUM_BATCHES = 2\n\nclass TrajectoryLearner:\n\n    def __init__(self, args, net, module, device, data_iterator, optimizer=None, checkpointer=None):\n        # same supervisor is used for training and testing, during testing we do not have optimizer etc.\n\n        self.net = net\n        self.module = module\n        self.device = device\n        self.knns = args.knns\n        self.beam_size = args.beam_size\n        self.data_iterator = data_iterator\n        self.optimizer = optimizer\n        self.checkpointer = checkpointer\n\n        self.output_dir = args.output_dir\n        self.test_only = args.test_only\n\n        self.debug = args.debug\n\n        if not args.test_only:\n            try:\n                self.test_every = args.test_every if args.test_every > 0 else None\n            except AttributeError:\n                self.test_every = None\n            self.decay_rate = args.decay_rate\n            self.decay_every = args.decay_every\n            self.loss = nn.CrossEntropyLoss()\n            self.best_current_val_metric = float('inf')\n            self.epoch_done = 0\n            self.nb_epochs = args.nb_total_epochs\n\n    def train(self):\n        assert not self.test_only\n\n        for _ in range(self.nb_epochs):\n            # Train one epoch\n            start = time.time()\n            self.net.train()\n            epoch_metrics_train = EpochMetrics()\n\n            for batch_num, data in enumerate(self.data_iterator.train_trajectories):\n\n                self.optimizer.zero_grad()\n                node_coords_ordered, _, _ = self.prepare_batch(data)\n                output_scores = self.net(node_coords_ordered)\n                loss = self.loss(output_scores,\n                                 torch.ones((output_scores.shape[0]), dtype=torch.long, device=output_scores.device))\n                epoch_metrics_train.update({\"training loss\": loss})\n                loss.backward()\n                self.optimizer.step()\n                if batch_num == DEBUG_NUM_BATCHES and self.debug:\n                    break\n\n            metrics = {f'{k}_train': v for k, v in epoch_metrics_train.get_means().items()}\n\n            print(\"[EPOCH {:03d}] Time: {:.3f}s \".format(self.epoch_done, time.time() - start))\n            for k, v in metrics.items():\n                print(k, f\"{v:.5f}\")\n\n            # Val test\n            val_metrics = self.val_test(\"val\")\n\n            if val_metrics[\"opt_gap_val\"] < self.best_current_val_metric:\n                # monitoring on current trajectories, in order to see if we are training enough on them or not\n                self.best_current_val_metric = val_metrics[\"opt_gap_val\"]\n                self.checkpointer.save(self.module, None, 'best')  # only model\n\n            # test\n            if self.test_every is not None:\n                if self.epoch_done % self.test_every == 0:\n                    self.save_model(\"current\")\n                    self.load_model(\"best\")\n                    self.val_test(\"test\")\n                    self.load_model(\"current\")\n                    self.remove_model(\"current\")\n\n            # lr decay\n            if self.epoch_done % self.decay_every == 0:\n                do_lr_decay(self.optimizer, self.decay_rate)\n\n            self.epoch_done += 1\n\n    def val_test(self, what=\"test\"):\n        if what == \"test\":\n            dataloader = self.data_iterator.test_trajectories\n        else:\n            dataloader = self.data_iterator.val_trajectories\n\n        self.net.eval()\n        epoch_metrics = EpochMetrics()\n        with torch.no_grad():\n\n            for batch_num, data in enumerate(dataloader):\n                val_test_metrics = self.get_minibatch_val_test_metrics(data)\n                epoch_metrics.update(val_test_metrics)\n                if batch_num == DEBUG_NUM_BATCHES and self.debug:\n                    break\n\n        res = {f'{k}_{what}': v for k, v in epoch_metrics.get_means().items()}\n\n        for k, v in res.items():\n            print(k, f\"{v:.3f}\")\n\n        return res\n\n    def load_model(self, label, allow_not_exist=False):\n        assert label in [\"current\", \"current_FULL\", \"best\"]\n        self.checkpointer.load(self.module, None, label, allow_not_exist=allow_not_exist)\n\n    def save_model(self, label, complete=False):\n        assert label in [\"current\", \"best\"]\n        args = {\"module\": self.module}\n        if not complete:\n            args_ = {\"optimizer\": None, 'label': label}\n        else:\n            assert not self.eval_only\n            assert label == \"current\"\n            args_ = {\"optimizer\": self.optimizer,\n                     \"label\": label+\"_FULL\",\n                     \"other\": {\n                         \"epoch_done\": self.epoch_done,\n                         \"best_current_val_metric\": self.best_current_val_metric,\n                         \"data_iterator\": self.data_iterator}\n                     }\n        self.checkpointer.save(**args, **args_)\n\n    def remove_model(self, label):\n        assert label in [\"current\", \"best\"]\n        self.checkpointer.delete(label)\n\n    def get_minibatch_val_test_metrics(self, data):\n        metrics = {}\n        # autoregressive decoding\n        node_coords, dist_matrices, opt_value = self.prepare_batch(data, sample=False)\n        decoding_metrics = {}\n        predicted_tours, predicted_tour_lens =\\\n            decode(node_coords, dist_matrices, self.net, self.beam_size, self.knns)\n        opt_gap = get_opt_gap(predicted_tour_lens, opt_value)\n        decoding_metrics.update({\"opt_gap\": opt_gap})\n\n        return {**metrics, **decoding_metrics}\n\n    def prepare_batch(self, data, sample=True):\n        ks = \"_s\" if sample else \"\"\n        node_coords = data[f\"nodes_coord{ks}\"].to(self.device)\n        dist_matrices = data[f\"dist_matrices{ks}\"].to(self.device)\n        tour_len = data[f\"tour_len{ks}\"].to(self.device)\n\n        return node_coords, dist_matrices, tour_len\n\n# ==========================================\n# File: model/model.py\n# Function/Context: BQModel\n# ==========================================\n\"\"\"\nBQ-NCO\nCopyright (c) 2023-present NAVER Corp.\nCreative Commons Attribution-NonCommercial-ShareAlike 4.0 license\n\"\"\"\n\nimport numpy as np\nimport torch\nfrom torch.nn import Module, Linear, Embedding\nfrom torch.nn.modules import ModuleList\nfrom model.encoder import EncoderLayer\n\n\nclass BQModel(Module):\n\n    def __init__(self, dim_input_nodes, emb_size, dim_ff, activation_ff, nb_layers_encoder, nb_heads,\n                 activation_attention, dropout, batchnorm, problem=\"tsp\"):\n        assert problem == \"tsp\" or problem == \"cvrp\" or problem == \"kp\" or problem == \"op\"\n        super().__init__()\n        self.problem = problem\n        self.input_emb = Linear(dim_input_nodes, emb_size)\n        if problem != \"kp\":\n            self.begin_end_tokens = Embedding(2, emb_size)\n\n        self.nb_layers_encoder = nb_layers_encoder\n        self.encoder = ModuleList([EncoderLayer(nb_heads, activation_attention, emb_size, dim_ff, activation_ff,\n                                                dropout, batchnorm) for _ in range(nb_layers_encoder)])\n\n        if problem == \"cvrp\":\n            output_dim = 2\n        else:\n            output_dim = 1\n        self.scores_projection = Linear(emb_size, output_dim)\n\n    def forward(self, inputs, **problem_data):\n        # inputs\n        #     TSP [batch_size, seq_len, (x_coord, y_coord)]\n        #    CVRP [batch_size, seq_len, (x_coord, y_coord, demand, current_capacity)]\n        #      OP [batch_size, seq_len, (x_coord, y_coord, node_value, upper_bound)]\n        #      KP [batch_size, seq_len, (weight, value, remaining_capacity)]\n        if self.problem == \"op\":\n            assert \"dist_matrices\" in problem_data\n\n        input_emb = self.input_emb(inputs)  # [batch_size, seq_len, emb_size]\n        if self.problem != \"kp\":\n            input_emb[:, 0, :] += self.begin_end_tokens(torch.tensor([[0]], device=inputs.device)).squeeze(1)\n            input_emb[:, -1, :] += self.begin_end_tokens(torch.tensor([[1]], device=inputs.device)).squeeze(1)\n\n        state = input_emb\n        for layer in self.encoder:\n            state = layer(state)  # [batch_size, seq_len, emb_size]\n\n        scores = self.scores_projection(state).squeeze(-1) # [batch_size, seq_len]\n\n        if self.problem == \"tsp\":\n            # mask origin and destination\n            scores[:, 0] = scores[:, -1] = -np.inf\n        elif self.problem == \"cvrp\":\n            # mask origin and destination (x2 - direct edge and via depot)\n            scores[:, 0, :] = scores[:, -1, :] = -np.inf\n            # exclude all impossible edges (direct edges to nodes with capacity larger than available demand)\n            scores[..., 0][problem_data[\"demands\"] > problem_data[\"remaining_capacities\"].unsqueeze(-1)] = -np.inf\n        elif self.problem == \"op\":\n            scores[:, 0] = scores[:, -1] = -np.inf\n            # op - mask all nodes with cost to go there and back to depot > current upperbound\n            # todo: update with real values\n            scores[problem_data[\"dist_matrices\"][:, 0] +\n                   problem_data[\"dist_matrices\"][:, -1] - inputs[..., 3] > 1e-6] = -np.inf\n        elif self.problem == \"kp\":\n            # kp - mask all nodes with weights > current capacity\n            scores[problem_data[\"weights\"] > problem_data[\"remaining_capacities\"].unsqueeze(-1)] = -np.inf\n\n        return scores.reshape(scores.shape[0], -1)",
  "description": "Combined Analysis:\n- [learning/cvrp/dataloading/dataset.py]: This file implements the data loading and preprocessing pipeline for the CVRP problem, directly supporting Algorithm Step 6 (training data augmentation via subproblem sampling). The key function collate_func_with_sample operationalizes the paper's strategy of sampling sub-paths from full trajectories to create varied training instances. By randomly selecting a starting index and extracting a contiguous subsequence of nodes (with adjusted distance matrices, demands, and capacities), it generates induced subproblems that mirror the recursive structure exploited by bisimulation quotienting. This enables the policy network to learn from diverse partial solution states, which is essential for generalizing the bisimulation mapping _{(f,X)}(x) = (f * x, X * x) during imitation learning.\n- [learning/cvrp/decoding.py]: This file implements Step 7 of the algorithm: 'At inference, use the trained policy to construct solutions greedily (single rollout) or with beam search for improved performance'. It contains the complete decoding logic for CVRP including:\n1. Main decode() function that dispatches to greedy or beam search decoding\n2. Greedy decoding loop that sequentially builds solutions using policy network\n3. Beam search decoding loop that maintains multiple candidate solutions\n4. Helper functions for preparing subproblems and computing scores via the neural network\n5. The DecodingSubPb dataclass that represents the bisimulation-quotiented subproblem state (tail subproblem)\nThe code directly implements the sequential decision-making process described in the paper where partial solutions are incrementally constructed by applying the policy to progressively smaller subproblems.\n- [learning/cvrp/traj_learner.py]: This file implements the core training and inference logic for the BQ-NCO algorithm for CVRP. It directly corresponds to Algorithm Steps 6-7 from the paper: (6) Training the policy by imitation learning using cross-entropy loss on expert trajectories, and (7) Inference using autoregressive decoding with beam search. The TrajectoryLearner class handles the training loop with expert trajectory supervision, validation/testing with optimality gap computation, and model checkpointing. The key implementation matches the paper's description of learning from (near-)optimal trajectories and using the trained policy for sequential decision-making.\n- [learning/kp/decoding.py]: This file implements the core inference algorithm (Step 7) from the paper's optimization model. It provides both greedy decoding (single rollout) and beam search decoding for the Knapsack Problem. The code maintains a subproblem representation (DecodingSubPb) that tracks remaining items and capacity, which aligns with the bisimulation quotienting concept where states are tail subproblems. The neural network (net) acts as the policy network from the BQ-MDP, scoring available actions (items) at each step. The implementation directly corresponds to sequential solution construction using the trained policy network, with beam search providing improved performance through multiple trajectory exploration.\n- [learning/kp/traj_learner.py]: This file implements the core training and evaluation logic for the Knapsack Problem (KP) using the BQ-NCO framework. It directly corresponds to Algorithm Steps 6 (training by imitation learning) and 7 (inference with beam search) from the paper. The TrajectoryLearner class trains a policy network via cross-entropy loss on expert trajectories (imitation learning), using normalized instance data (values, weights, capacities) as input states. During evaluation, it uses autoregressive decoding with beam search to construct solutions and computes the optimality gap. The code handles the complete training loop, learning rate decay, model checkpointing, and validation/testing procedures.\n- [learning/op/decoding.py]: This file implements Algorithm Step 7 from the paper: 'At inference, use the trained policy to construct solutions greedily (single rollout) or with beam search for improved performance.' It contains the complete decoding logic for the Orienteering Problem (OP) using the BQ-MDP framework. The DecodingSubPb dataclass represents the tail subproblem (induced instance) after partial solutions, directly implementing the bisimulation quotienting concept where states are mapped to equivalent remaining subproblems. The decode() function orchestrates either greedy or beam search decoding, while prepare_input_and_forward_pass() calls the policy network (net) on the current subproblem instance. The reformat_subproblem_for_next_step() function updates the subproblem after each action, implementing the transition dynamics of the BQ-MDP. This matches the paper's description of sequential decision-making where partial solutions are incrementally constructed using the trained policy on progressively smaller subproblems.\n- [learning/op/traj_learner.py]: This file implements the core training and evaluation loop for the imitation learning approach described in the paper. Specifically, it corresponds to Algorithm Step 6 (training by imitation learning using cross-entropy loss on expert trajectories) and Step 7 (inference via autoregressive decoding with beam search). The TrajectoryLearner class trains a policy network (self.net) using expert trajectories from the data iterator, minimizing cross-entropy loss between the network's output scores and the expert actions (represented as a tensor of ones, indicating the expert's chosen action). During validation/testing, it uses the decode function to perform autoregressive construction of solutions with beam search and optional knns, then computes the optimality gap (opt_gap) against the optimal values. The prepare_batch method formats the input data (node coordinates, values, upper bounds, distance matrices) as required by the policy network, which aligns with the BQ-MDP state representation described in the paper.\n- [learning/tsp/decoding.py]: This file implements the core inference logic (Algorithm Step 7) from the paper, specifically the sequential decision-making process for constructing solutions using the trained policy network. It provides both greedy decoding (single rollout) and beam search decoding for the Traveling Salesman Problem (TSP). The DecodingSubPb dataclass represents the bisimulation-quotiented subproblem state (tail subproblem), and the decoding functions sequentially apply the policy network to progressively smaller subproblems, selecting nodes and reformatting the subproblem for the next step. This directly corresponds to the paper's description of using the trained policy to construct solutions greedily or with beam search.\n- [learning/tsp/traj_learner.py]: This file implements the training and inference pipeline for the TSP policy network as described in Algorithm Steps 6-7. The TrajectoryLearner class performs imitation learning using cross-entropy loss on expert trajectories (Step 6) and uses autoregressive decoding with beam search for inference (Step 7). The prepare_batch method handles data formatting for the BQ-MDP states (partial solutions), while decode implements the sequential construction of solutions using the trained policy network. The code directly maps to the paper's training methodology (imitation learning on optimal trajectories) and inference strategy (beam search decoding).\n- [model/model.py]: This file implements the core policy network (Algorithm Step 5) for the BQ-MDP framework. The BQModel class is a transformer-based architecture that takes the current problem instance (tail subproblem) as input and outputs action scores (logits) for each possible step. It handles four COPs (TSP, CVRP, OP, KP) with problem-specific input representations and constraint masking, directly corresponding to the policy network design in the paper. The forward method encodes the current state (partial solution representation), processes it through encoder layers, projects to scores, and applies constraint masks to invalid actions, implementing the policy  for the bisimulation-quotiented MDP.",
  "dependencies": [
    "beam_search_decoding_step",
    "scipy.spatial.distance",
    "torch.nn.Embedding",
    "collate_func_with_sample",
    "torch.nn.Module",
    "decode",
    "torch.nn.modules.ModuleList",
    "dataclasses.dataclass",
    "utils.misc.compute_tour_lens",
    "torch.nn.Linear",
    "utils.multi_class_loss.CrossEntropyLoss",
    "greedy_decoding_step",
    "reformat_subproblem_for_next_step",
    "DecodingSubPb",
    "beam_search_decoding_loop",
    "numpy",
    "torch.utils.data",
    "torch.Tensor",
    "learning.cvrp.decoding.decode",
    "learning.kp.decoding.decode",
    "utils.misc.EpochMetrics",
    "torch",
    "Module (from torch.nn)",
    "DotDict",
    "greedy_decoding_loop",
    "utils.misc.get_opt_gap",
    "dataclasses",
    "model.encoder.EncoderLayer",
    "compute_tour_lens (from utils.misc)",
    "prepare_input_and_forward_pass",
    "DataSet",
    "learning.tsp.decoding.decode",
    "Tensor (from torch)",
    "learning.op.decoding.decode",
    "load_dataset",
    "copy",
    "nn.CrossEntropyLoss",
    "utils.misc.do_lr_decay"
  ]
}