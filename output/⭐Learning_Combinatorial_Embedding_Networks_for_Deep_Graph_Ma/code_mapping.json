{
  "file_path": "models/BBGM/affinity_layer.py, models/BBGM/model.py, models/GMN/affinity_layer.py, models/GMN/model.py, models/NGM/model.py, models/PCA/affinity_layer.py, models/PCA/model.py, src/gconv.py, src/lap_solvers/hungarian.py, src/lap_solvers/sinkhorn.py",
  "function_name": "InnerProductWithWeightsAffinity, Net.forward, InnerpAffinity, GaussianAffinity, Net.forward, Net.forward, Affinity, AffinityInp, AffinityLR, AffinityMah, AffinityFC, AffinityBiFC, Net.forward, Gconv, hungarian, Sinkhorn.forward_ori",
  "code_snippet": "\n\n# ==========================================\n# File: models/BBGM/affinity_layer.py\n# Function/Context: InnerProductWithWeightsAffinity\n# ==========================================\nimport torch\nimport torch.nn as nn\n\n\nclass InnerProductWithWeightsAffinity(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(InnerProductWithWeightsAffinity, self).__init__()\n        self.d = output_dim\n        self.A = torch.nn.Linear(input_dim, output_dim)\n\n    def _forward(self, X, Y, weights, use_global):\n        assert X.shape[1] == Y.shape[1] == self.d, (X.shape[1], Y.shape[1], self.d)\n        coefficients = torch.tanh(self.A(weights))\n        if use_global:\n            res = torch.matmul(X * coefficients, Y.transpose(0, 1))\n        else:\n            res = torch.matmul(X, Y.transpose(0, 1))\n        res = torch.nn.functional.softplus(res) - 0.5\n        return res\n\n    def forward(self, Xs, Ys, Ws, use_global=True):\n        return [self._forward(X, Y, W, use_global) for X, Y, W in zip(Xs, Ys, Ws)]\n\n# ==========================================\n# File: models/BBGM/model.py\n# Function/Context: Net.forward\n# ==========================================\nimport torch\nimport itertools\n\nfrom models.BBGM.affinity_layer import InnerProductWithWeightsAffinity\nfrom models.BBGM.sconv_archs import SiameseSConvOnNodes, SiameseNodeFeaturesToEdgeFeatures\nfrom lpmp_py import GraphMatchingModule\nfrom lpmp_py import MultiGraphMatchingModule\nfrom src.feature_align import feature_align\n\nfrom src.utils.config import cfg\n\nfrom src.backbone import *\nCNN = eval(cfg.BACKBONE)\n\n\ndef lexico_iter(lex):\n    return itertools.combinations(lex, 2)\n\ndef normalize_over_channels(x):\n    channel_norms = torch.norm(x, dim=1, keepdim=True)\n    return x / channel_norms\n\n\ndef concat_features(embeddings, num_vertices):\n    res = torch.cat([embedding[:, :num_v] for embedding, num_v in zip(embeddings, num_vertices)], dim=-1)\n    return res.transpose(0, 1)\n\n\nclass Net(CNN):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.message_pass_node_features = SiameseSConvOnNodes(input_node_dim=cfg.BBGM.FEATURE_CHANNEL)\n        self.build_edge_features_from_node_features = SiameseNodeFeaturesToEdgeFeatures(\n            total_num_nodes=self.message_pass_node_features.num_node_features\n        )\n        self.global_state_dim = cfg.BBGM.FEATURE_CHANNEL\n        self.vertex_affinity = InnerProductWithWeightsAffinity(\n            self.global_state_dim, self.message_pass_node_features.num_node_features)\n        self.edge_affinity = InnerProductWithWeightsAffinity(\n            self.global_state_dim,\n            self.build_edge_features_from_node_features.num_edge_features)\n        self.rescale = cfg.PROBLEM.RESCALE\n\n    def forward(\n        self,\n        data_dict,\n    ):\n        images = data_dict['images']\n        points = data_dict['Ps']\n        n_points = data_dict['ns']\n        graphs = data_dict['pyg_graphs']\n        num_graphs = len(images)\n\n        if cfg.PROBLEM.TYPE == '2GM' and 'gt_perm_mat' in data_dict:\n            gt_perm_mats = [data_dict['gt_perm_mat']]\n        elif cfg.PROBLEM.TYPE == 'MGM' and 'gt_perm_mat' in data_dict:\n            gt_perm_mats = data_dict['gt_perm_mat'].values()\n        else:\n            raise ValueError('Ground truth information is required during training.')\n\n        global_list = []\n        orig_graph_list = []\n        for image, p, n_p, graph in zip(images, points, n_points, graphs):\n            # extract feature\n            nodes = self.node_layers(image)\n            edges = self.edge_layers(nodes)\n\n            global_list.append(self.final_layers(edges).reshape((nodes.shape[0], -1)))\n            nodes = normalize_over_channels(nodes)\n            edges = normalize_over_channels(edges)\n\n            # arrange features\n            U = concat_features(feature_align(nodes, p, n_p, self.rescale), n_p)\n            F = concat_features(feature_align(edges, p, n_p, self.rescale), n_p)\n            node_features = torch.cat((U, F), dim=1)\n            graph.x = node_features\n\n            graph = self.message_pass_node_features(graph)\n            orig_graph = self.build_edge_features_from_node_features(graph)\n            orig_graph_list.append(orig_graph)\n\n        global_weights_list = [\n            torch.cat([global_src, global_tgt], axis=-1) for global_src, global_tgt in lexico_iter(global_list)\n        ]\n\n        global_weights_list = [normalize_over_channels(g) for g in global_weights_list]\n\n        unary_costs_list = [\n            self.vertex_affinity([item.x for item in g_1], [item.x for item in g_2], global_weights)\n            for (g_1, g_2), global_weights in zip(lexico_iter(orig_graph_list), global_weights_list)\n        ]\n\n        # Similarities to costs\n        unary_costs_list = [[-x for x in unary_costs] for unary_costs in unary_costs_list]\n\n        if self.training:\n            unary_costs_list = [\n                [\n                    x + 1.0*gt[:dim_src, :dim_tgt]  # Add margin with alpha = 1.0\n                    for x, gt, dim_src, dim_tgt in zip(unary_costs, gt_perm_mat, ns_src, ns_tgt)\n                ]\n                for unary_costs, gt_perm_mat, (ns_src, ns_tgt) in zip(unary_costs_list, gt_perm_mats, lexico_iter(n_points))\n            ]\n\n        quadratic_costs_list = [\n            self.edge_affinity([item.edge_attr for item in g_1], [item.edge_attr for item in g_2], global_weights)\n            for (g_1, g_2), global_weights in zip(lexico_iter(orig_graph_list), global_weights_list)\n        ]\n\n        # Similarities to costs\n        quadratic_costs_list = [[-0.5 * x for x in quadratic_costs] for quadratic_costs in quadratic_costs_list]\n\n        if cfg.BBGM.SOLVER_NAME == \"LPMP\":\n            all_edges = [[item.edge_index for item in graph] for graph in orig_graph_list]\n            gm_solvers = [\n                GraphMatchingModule(\n                    all_left_edges,\n                    all_right_edges,\n                    ns_src,\n                    ns_tgt,\n                    cfg.BBGM.LAMBDA_VAL,\n                    cfg.BBGM.SOLVER_PARAMS,\n                )\n                for (all_left_edges, all_right_edges), (ns_src, ns_tgt) in zip(\n                    lexico_iter(all_edges), lexico_iter(n_points)\n                )\n            ]\n            matchings = [\n                gm_solver(unary_costs, quadratic_costs)\n                for gm_solver, unary_costs, quadratic_costs in zip(gm_solvers, unary_costs_list, quadratic_costs_list)\n            ]\n        elif cfg.BBGM.SOLVER_NAME == \"LPMP_MGM\":\n            all_edges = [[item.edge_index for item in graph] for graph in orig_graph_list]\n            gm_solver = MultiGraphMatchingModule(\n                all_edges, n_points, cfg.BBGM.LAMBDA_VAL, cfg.BBGM.SOLVER_PARAMS)\n            matchings = gm_solver(unary_costs_list, quadratic_costs_list)\n        else:\n            raise ValueError(\"Unknown solver {}\".format(cfg.BBGM.SOLVER_NAME))\n\n\n        if cfg.PROBLEM.TYPE == '2GM':\n            data_dict.update({\n                'ds_mat': None,\n                'perm_mat': matchings[0]\n            })\n        elif cfg.PROBLEM.TYPE == 'MGM':\n            indices = list(lexico_iter(range(num_graphs)))\n            data_dict.update({\n                'perm_mat_list': matchings,\n                'graph_indices': indices,\n            })\n\n        return data_dict\n\n# ==========================================\n# File: models/GMN/affinity_layer.py\n# Function/Context: InnerpAffinity, GaussianAffinity\n# ==========================================\nimport torch\nimport torch.nn as nn\nfrom torch.nn.parameter import Parameter\nfrom torch import Tensor\nimport math\n\n\nclass InnerpAffinity(nn.Module):\n    \"\"\"\n    Affinity Layer to compute the affinity matrix via inner product from feature space.\n    Me = X * Lambda * Y^T\n    Mp = Ux * Uy^T\n    Parameter: scale of weight d\n    Input: edgewise (pairwise) feature X, Y\n           pointwise (unary) feature Ux, Uy\n    Output: edgewise affinity matrix Me\n            pointwise affinity matrix Mp\n    Weight: weight matrix Lambda = [[Lambda1, Lambda2],\n                                    [Lambda2, Lambda1]]\n            where Lambda1, Lambda2 > 0\n    \"\"\"\n    def __init__(self, d):\n        super(InnerpAffinity, self).__init__()\n        self.d = d\n        self.lambda1 = Parameter(Tensor(self.d, self.d))\n        self.lambda2 = Parameter(Tensor(self.d, self.d))\n        self.relu = nn.ReLU()  # problem: if weight<0, then always grad=0. So this parameter is never updated!\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.lambda1.size(1) * 2)\n        self.lambda1.data.uniform_(-stdv, stdv)\n        self.lambda2.data.uniform_(-stdv, stdv)\n        self.lambda1.data += torch.eye(self.d) / 2\n        self.lambda2.data += torch.eye(self.d) / 2\n\n    def forward(self, X, Y, Ux, Uy, w1=1, w2=1):\n        assert X.shape[1] == Y.shape[1] == 2 * self.d\n        lambda1 = self.relu(self.lambda1 + self.lambda1.transpose(0, 1)) * w1\n        lambda2 = self.relu(self.lambda2 + self.lambda2.transpose(0, 1)) * w2\n        weight = torch.cat((torch.cat((lambda1, lambda2)),\n                            torch.cat((lambda2, lambda1))), 1)\n        Me = torch.matmul(X.transpose(1, 2), weight)\n        Me = torch.matmul(Me, Y)\n        Mp = torch.matmul(Ux.transpose(1, 2), Uy)\n\n        return Me, Mp\n\n\nclass GaussianAffinity(nn.Module):\n    \"\"\"\n    Affinity Layer to compute the affinity matrix via gaussian kernel from feature space.\n    Me = exp(- L2(X, Y) / sigma)\n    Mp = Ux * Uy^T\n    Parameter: scale of weight d, gaussian kernel sigma\n    Input: edgewise (pairwise) feature X, Y\n           pointwise (unary) feature Ux, Uy\n    Output: edgewise affinity matrix Me\n            pointwise affinity matrix Mp\n    \"\"\"\n\n    def __init__(self, d, sigma):\n        super(GaussianAffinity, self).__init__()\n        self.d = d\n        self.sigma = sigma\n\n    def forward(self, X, Y, Ux=None, Uy=None, ae=1., ap=1.):\n        assert X.shape[1] == Y.shape[1] == self.d\n\n        X = X.unsqueeze(-1).expand(*X.shape, Y.shape[2])\n        Y = Y.unsqueeze(-2).expand(*Y.shape[:2], X.shape[2], Y.shape[2])\n        # dist = torch.sum(torch.pow(torch.mul(X - Y, self.w.unsqueeze(0).unsqueeze(-1).unsqueeze(-1)), 2), dim=1)\n        dist = torch.sum(torch.pow(X - Y, 2), dim=1)\n        dist[torch.isnan(dist)] = float(\"Inf\")\n        Me = torch.exp(- dist / self.sigma) * ae\n\n        if Ux is None or Uy is None:\n            return Me\n        else:\n            Mp = torch.matmul(Ux.transpose(1, 2), Uy) * ap\n            return Me, Mp\n\n# ==========================================\n# File: models/GMN/model.py\n# Function/Context: Net.forward\n# ==========================================\nimport torch.nn as nn\n\nfrom models.GMN.affinity_layer import InnerpAffinity as Affinity\nfrom src.qap_solvers.spectral_matching import SpectralMatching\nfrom src.qap_solvers.rrwm import RRWM\nfrom src.lap_solvers.sinkhorn import Sinkhorn\nfrom src.lap_solvers.hungarian import hungarian\nfrom src.build_graphs import reshape_edge_feature\nfrom src.feature_align import feature_align\nfrom src.factorize_graph_matching import construct_aff_mat\n\nfrom src.utils.config import cfg\n\nfrom src.backbone import *\nCNN = eval(cfg.BACKBONE)\n\n\nclass Net(CNN):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.affinity_layer = Affinity(cfg.GMN.FEATURE_CHANNEL)\n        if cfg.GMN.GM_SOLVER == 'SM':\n            self.gm_solver = SpectralMatching(max_iter=cfg.GMN.PI_ITER_NUM, stop_thresh=cfg.GMN.PI_STOP_THRESH)\n        elif cfg.GMN.GM_SOLVER == 'RRWM':\n            self.gm_solver = RRWM()\n        self.sinkhorn = Sinkhorn(max_iter=cfg.GMN.BS_ITER_NUM, tau=1/cfg.GMN.VOTING_ALPHA, epsilon=cfg.GMN.BS_EPSILON, log_forward=False)\n        self.l2norm = nn.LocalResponseNorm(cfg.GMN.FEATURE_CHANNEL * 2, alpha=cfg.GMN.FEATURE_CHANNEL * 2, beta=0.5, k=0)\n        self.rescale = cfg.PROBLEM.RESCALE\n\n    def forward(self, data_dict, **kwargs):\n        if 'images' in data_dict:\n            # real image data\n            src, tgt = data_dict['images']\n            P_src, P_tgt = data_dict['Ps']\n            ns_src, ns_tgt = data_dict['ns']\n            G_src, G_tgt = data_dict['Gs']\n            H_src, H_tgt = data_dict['Hs']\n            K_G, K_H = data_dict['KGHs']\n\n            # extract feature\n            src_node = self.node_layers(src)\n            src_edge = self.edge_layers(src_node)\n            tgt_node = self.node_layers(tgt)\n            tgt_edge = self.edge_layers(tgt_node)\n\n            # feature normalization\n            src_node = self.l2norm(src_node)\n            src_edge = self.l2norm(src_edge)\n            tgt_node = self.l2norm(tgt_node)\n            tgt_edge = self.l2norm(tgt_edge)\n\n            # arrange features\n            U_src = feature_align(src_node, P_src, ns_src, self.rescale)\n            F_src = feature_align(src_edge, P_src, ns_src, self.rescale)\n            U_tgt = feature_align(tgt_node, P_tgt, ns_tgt, self.rescale)\n            F_tgt = feature_align(tgt_edge, P_tgt, ns_tgt, self.rescale)\n        elif 'features' in data_dict:\n            # synthetic data\n            src, tgt = data_dict['features']\n            P_src, P_tgt = data_dict['Ps']\n            ns_src, ns_tgt = data_dict['ns']\n            G_src, G_tgt = data_dict['Gs']\n            H_src, H_tgt = data_dict['Hs']\n            K_G, K_H = data_dict['KGHs']\n\n            U_src = src[:, :src.shape[1] // 2, :]\n            F_src = src[:, src.shape[1] // 2:, :]\n            U_tgt = tgt[:, :tgt.shape[1] // 2, :]\n            F_tgt = tgt[:, tgt.shape[1] // 2:, :]\n        else:\n            raise ValueError('Unknown data type for this model.')\n\n        X = reshape_edge_feature(F_src, G_src, H_src)\n        Y = reshape_edge_feature(F_tgt, G_tgt, H_tgt)\n\n        # affinity layer\n        Me, Mp = self.affinity_layer(X, Y, U_src, U_tgt)\n\n        M = construct_aff_mat(Me, Mp, K_G, K_H)\n\n        v = self.gm_solver(M, num_src=P_src.shape[1], ns_src=ns_src, ns_tgt=ns_tgt)\n        s = v.view(v.shape[0], P_tgt.shape[1], -1).transpose(1, 2)\n\n        s = self.sinkhorn(s, ns_src, ns_tgt)\n\n        data_dict.update({\n            'ds_mat': s,\n            'perm_mat': hungarian(s, ns_src, ns_tgt),\n            'aff_mat': M\n        })\n        return data_dict\n\n# ==========================================\n# File: models/NGM/model.py\n# Function/Context: Net.forward\n# ==========================================\nimport torch\nimport torch.nn as nn\n\nfrom src.lap_solvers.sinkhorn import Sinkhorn, GumbelSinkhorn\nfrom src.build_graphs import reshape_edge_feature\nfrom src.feature_align import feature_align\nfrom src.factorize_graph_matching import construct_aff_mat\nfrom models.NGM.gnn import GNNLayer\nfrom models.NGM.geo_edge_feature import geo_edge_feature\nfrom models.GMN.affinity_layer import InnerpAffinity, GaussianAffinity\nfrom src.evaluation_metric import objective_score\nfrom src.lap_solvers.hungarian import hungarian\nimport math\nfrom src.utils.gpu_memory import gpu_free_memory\n\nfrom src.utils.config import cfg\nfrom src.backbone import *\nCNN = eval(cfg.BACKBONE)\n\n\nclass Net(CNN):\n    def __init__(self):\n        super(Net, self).__init__()\n        if cfg.NGM.EDGE_FEATURE == 'cat':\n            self.affinity_layer = InnerpAffinity(cfg.NGM.FEATURE_CHANNEL)\n        elif cfg.NGM.EDGE_FEATURE == 'geo':\n            self.affinity_layer = GaussianAffinity(1, cfg.NGM.GAUSSIAN_SIGMA)\n        else:\n            raise ValueError('Unknown edge feature type {}'.format(cfg.NGM.EDGE_FEATURE))\n        self.tau = cfg.NGM.SK_TAU\n        self.rescale = cfg.PROBLEM.RESCALE\n        self.sinkhorn = Sinkhorn(max_iter=cfg.NGM.SK_ITER_NUM, tau=self.tau, epsilon=cfg.NGM.SK_EPSILON)\n        self.gumbel_sinkhorn = GumbelSinkhorn(max_iter=cfg.NGM.SK_ITER_NUM, tau=self.tau * 10, epsilon=cfg.NGM.SK_EPSILON, batched_operation=True)\n        self.l2norm = nn.LocalResponseNorm(cfg.NGM.FEATURE_CHANNEL * 2, alpha=cfg.NGM.FEATURE_CHANNEL * 2, beta=0.5, k=0)\n\n        self.gnn_layer = cfg.NGM.GNN_LAYER\n        for i in range(self.gnn_layer):\n            tau = cfg.NGM.SK_TAU\n            if i == 0:\n                gnn_layer = GNNLayer(1, 1, cfg.NGM.GNN_FEAT[i] + (1 if cfg.NGM.SK_EMB else 0), cfg.NGM.GNN_FEAT[i],\n                                     sk_channel=cfg.NGM.SK_EMB, sk_tau=tau, edge_emb=cfg.NGM.EDGE_EMB)\n            else:\n                gnn_layer = GNNLayer(cfg.NGM.GNN_FEAT[i - 1] + (1 if cfg.NGM.SK_EMB else 0), cfg.NGM.GNN_FEAT[i - 1],\n                                     cfg.NGM.GNN_FEAT[i] + (1 if cfg.NGM.SK_EMB else 0), cfg.NGM.GNN_FEAT[i],\n                                     sk_channel=cfg.NGM.SK_EMB, sk_tau=tau, edge_emb=cfg.NGM.EDGE_EMB)\n            self.add_module('gnn_layer_{}'.format(i), gnn_layer)\n\n        self.classifier = nn.Linear(cfg.NGM.GNN_FEAT[-1] + (1 if cfg.NGM.SK_EMB else 0), 1)\n\n    def forward(self, data_dict, **kwargs):\n        batch_size = data_dict['batch_size']\n        if 'images' in data_dict:\n            src, tgt = data_dict['images']\n            P_src, P_tgt = data_dict['Ps']\n            ns_src, ns_tgt = data_dict['ns']\n            G_src, G_tgt = data_dict['Gs']\n            H_src, H_tgt = data_dict['Hs']\n            K_G, K_H = data_dict['KGHs']\n\n            src_node = self.node_layers(src)\n            src_edge = self.edge_layers(src_node)\n            tgt_node = self.node_layers(tgt)\n            tgt_edge = self.edge_layers(tgt_node)\n\n            src_node = self.l2norm(src_node)\n            src_edge = self.l2norm(src_edge)\n            tgt_node = self.l2norm(tgt_node)\n            tgt_edge = self.l2norm(tgt_edge)\n\n            U_src = feature_align(src_node, P_src, ns_src, self.rescale)\n            F_src = feature_align(src_edge, P_src, ns_src, self.rescale)\n            U_tgt = feature_align(tgt_node, P_tgt, ns_tgt, self.rescale)\n            F_tgt = feature_align(tgt_edge, P_tgt, ns_tgt, self.rescale)\n        elif 'features' in data_dict:\n            src, tgt = data_dict['features']\n            P_src, P_tgt = data_dict['Ps']\n            ns_src, ns_tgt = data_dict['ns']\n            G_src, G_tgt = data_dict['Gs']\n            H_src, H_tgt = data_dict['Hs']\n            K_G, K_H = data_dict['KGHs']\n\n            U_src = src[:, :src.shape[1] // 2, :]\n            F_src = src[:, src.shape[1] // 2:, :]\n            U_tgt = tgt[:, :tgt.shape[1] // 2, :]\n            F_tgt = tgt[:, tgt.shape[1] // 2:, :]\n        elif 'aff_mat' in data_dict:\n            K = data_dict['aff_mat']\n            ns_src, ns_tgt = data_dict['ns']\n        else:\n            raise ValueError('Unknown data type for this model.')\n\n        if 'images' in data_dict or 'features' in data_dict:\n            tgt_len = P_tgt.shape[1]\n            if cfg.NGM.EDGE_FEATURE == 'cat':\n                X = reshape_edge_feature(F_src, G_src, H_src)\n                Y = reshape_edge_feature(F_tgt, G_tgt, H_tgt)\n            elif cfg.NGM.EDGE_FEATURE == 'geo':\n                X = geo_edge_feature(P_src, G_src, H_src)[:, :1, :]\n                Y = geo_edge_feature(P_tgt, G_tgt, H_tgt)[:, :1, :]\n            else:\n                raise ValueError('Unknown edge feature type {}'.format(cfg.NGM.EDGE_FEATURE))\n\n            Ke, Kp = self.affinity_layer(X, Y, U_src, U_tgt)\n            K = construct_aff_mat(Ke, torch.zeros_like(Kp), K_G, K_H)\n            A = (K > 0).to(K.dtype)\n\n            if cfg.NGM.FIRST_ORDER:\n                emb = Kp.transpose(1, 2).contiguous().view(Kp.shape[0], -1, 1)\n            else:\n                emb = torch.ones(K.shape[0], K.shape[1], 1, device=K.device)\n        else:\n            tgt_len = int(math.sqrt(K.shape[2]))\n            dmax = (torch.max(torch.sum(K, dim=2, keepdim=True), dim=1, keepdim=True).values + 1e-5)\n            K = K / dmax * 1000\n            A = (K > 0).to(K.dtype)\n            emb = torch.ones(K.shape[0], K.shape[1], 1, device=K.device)\n\n        emb_K = K.unsqueeze(-1)\n\n        for i in range(self.gnn_layer):\n            gnn_layer = getattr(self, 'gnn_layer_{}'.format(i))\n            emb_K, emb = gnn_layer(A, emb_K, emb, ns_src, ns_tgt)\n\n        v = self.classifier(emb)\n        s = v.view(v.shape[0], tgt_len, -1).transpose(1, 2)\n\n        if self.training or cfg.NGM.GUMBEL_SK <= 0:\n            ss = self.sinkhorn(s, ns_src, ns_tgt, dummy_row=True)\n            x = hungarian(ss, ns_src, ns_tgt)\n        else:\n            gumbel_sample_num = cfg.NGM.GUMBEL_SK\n            if self.training:\n                gumbel_sample_num //= 10\n            ss_gumbel = self.gumbel_sinkhorn(s, ns_src, ns_tgt, sample_num=gumbel_sample_num, dummy_row=True)\n\n            repeat = lambda x, rep_num=gumbel_sample_num: torch.repeat_interleave(x, rep_num, dim=0)\n            if not self.training:\n                ss_gumbel = hungarian(ss_gumbel, repeat(ns_src), repeat(ns_tgt))\n            ss_gumbel = ss_gumbel.reshape(batch_size, gumbel_sample_num, ss_gumbel.shape[-2], ss_gumbel.shape[-1])\n\n            if ss_gumbel.device.type == 'cuda':\n                dev_idx = ss_gumbel.device.index\n                free_mem = gpu_free_memory(dev_idx) - 100 * 1024 ** 2\n                K_mem_size = K.element_size() * K.nelement()\n                max_repeats = free_mem // K_mem_size\n                if max_repeats <= 0:\n                    print('Warning: GPU may not have enough memory')\n                    max_repeats = 1\n            else:\n                max_repeats = gumbel_sample_num\n\n            obj_score = []\n            for idx in range(0, gumbel_sample_num, max_repeats):\n                if idx + max_repeats > gumbel_sample_num:\n                    rep_num = gumbel_sample_num - idx\n                else:\n                    rep_num = max_repeats\n                obj_score.append(\n                    objective_score(\n                        ss_gumbel[:, idx:(idx+rep_num), :, :].reshape(-1, ss_gumbel.shape[-2], ss_gumbel.shape[-1]),\n                        repeat(K, rep_num)\n                    ).reshape(batch_size, -1)\n                )\n            obj_score = torch.cat(obj_score, dim=1)\n            min_obj_score = obj_score.min(dim=1)\n            ss = ss_gumbel[torch.arange(batch_size), min_obj_score.indices.cpu(), :, :]\n            x = hungarian(ss, repeat(ns_src), repeat(ns_tgt))\n\n        data_dict.update({\n            'ds_mat': ss,\n            'perm_mat': x,\n            'aff_mat': K\n        })\n        return data_dict\n\n# ==========================================\n# File: models/PCA/affinity_layer.py\n# Function/Context: Affinity, AffinityInp, AffinityLR, AffinityMah, AffinityFC, AffinityBiFC\n# ==========================================\nimport torch\nimport torch.nn as nn\nfrom torch.nn.parameter import Parameter\nfrom torch import Tensor\nimport math\n\n\nclass Affinity(nn.Module):\n    \"\"\"\n    Affinity Layer to compute the affinity matrix from feature space.\n    M = X * A * Y^T\n    Parameter: scale of weight d\n    Input: feature X, Y\n    Output: affinity matrix M\n    \"\"\"\n    def __init__(self, d):\n        super(Affinity, self).__init__()\n        self.d = d\n        self.A = Parameter(Tensor(self.d, self.d))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.d)\n        self.A.data.uniform_(-stdv, stdv)\n        self.A.data += torch.eye(self.d)\n\n    def forward(self, X, Y):\n        assert X.shape[2] == Y.shape[2] == self.d\n        M = torch.matmul(X, self.A)\n        #M = torch.matmul(X, (self.A + self.A.transpose(0, 1)) / 2)\n        M = torch.matmul(M, Y.transpose(1, 2))\n        return M\n\nclass AffinityInp(nn.Module):\n    \"\"\"\n    Affinity Layer to compute inner product affinity matrix from feature space.\n    M = X * A * Y^T\n    Parameter: scale of weight d\n    Input: feature X, Y\n    Output: affinity matrix M\n    \"\"\"\n    def __init__(self, d):\n        super(AffinityInp, self).__init__()\n        self.d = d\n\n    def forward(self, X, Y):\n        assert X.shape[2] == Y.shape[2] == self.d\n        M = torch.matmul(X, Y.transpose(1, 2))\n        return M\n\n\n\nclass AffinityLR(nn.Module):\n    def __init__(self, d, k=512):\n        super(AffinityLR, self).__init__()\n        self.d = d\n        self.k = k\n        self.A = Parameter(Tensor(self.d, self.k))\n        self.relu = nn.ReLU()\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.d)\n        self.A.data.uniform_(-stdv, stdv)\n\n    def forward(self, X, Y):\n        assert X.shape[2] == Y.shape[2] == self.d\n        M = torch.matmul(self.A, self.A.transpose(0, 1))\n        M = torch.matmul(X, M)\n        M = torch.matmul(M, Y.transpose(1, 2))\n\n        return self.relu(M.squeeze())\n\nclass AffinityMah(nn.Module):\n    def __init__(self, d, k=100):\n        super(AffinityMah, self).__init__()\n        self.d = d\n        self.k = k\n        self.A = Parameter(Tensor(self.d, self.k))\n        self.relu = nn.ReLU()\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.d)\n        self.A.data.uniform_(-stdv, stdv)\n\n    def forward(self, X, Y):\n        assert X.shape[2] == Y.shape[2] == self.d\n        X = X.unsqueeze(1)\n        Y = Y.unsqueeze(2)\n        dxy = X - Y\n        M = torch.matmul(self.A, self.A.transpose(0, 1))\n        M = torch.matmul(dxy.unsqueeze(-2), M)\n        M = torch.matmul(M, dxy.unsqueeze(-1))\n\n        return self.relu(M.squeeze())\n\n\nclass AffinityFC(nn.Module):\n    \"\"\"\n    Affinity Layer to compute the affinity matrix from feature space.\n    Affinity score is modeled by a fc neural network.\n    Parameter: input dimension d, list of hidden layer dimension hds\n    Input: feature X, Y\n    Output: affinity matrix M\n    \"\"\"\n    def __init__(self, d, hds=None):\n        super(AffinityFC, self).__init__()\n        self.d = d\n        if hds is None:\n            self.hds = [1024,]\n        else:\n            self.hds = hds\n        self.hds.append(1)\n        fc_lst = []\n        last_hd = self.d * 2\n        for hd in self.hds:\n            fc_lst.append(nn.Linear(last_hd, hd))\n            fc_lst.append(nn.ReLU())\n            last_hd = hd\n\n        self.fc = nn.Sequential(*fc_lst[:-1])  # last relu omitted\n\n    def forward(self, X, Y):\n        assert X.shape[2] == Y.shape[2] == self.d\n        cat_feat = torch.cat((X.unsqueeze(-2).expand(X.shape[0], X.shape[1], Y.shape[1], X.shape[2]),\n                              Y.unsqueeze(-3).expand(Y.shape[0], X.shape[1], Y.shape[1], Y.shape[2])), dim=-1)\n        result = self.fc(cat_feat).squeeze(-1)\n        return result\n\n\nclass AffinityBiFC(nn.Module):\n    \"\"\"\n    Affinity Layer to compute the affinity matrix from feature space.\n    Affinity score is modeled by a bilinear layer followed by a fc neural network.\n    Parameter: input dimension d, biliear dimension bd, list of hidden layer dimension hds\n    Input: feature X, Y\n    Output: affinity matrix M\n    \"\"\"\n    def __init__(self, d, bd=1024, hds=None):\n        super(AffinityBiFC, self).__init__()\n        self.d = d\n        self.bd = bd\n        if hds is None:\n            self.hds = []\n        self.hds.append(1)\n\n        self.A = Parameter(Tensor(self.d, self.d, self.bd))\n        self.reset_parameters()\n\n        fc_lst = []\n        last_hd = self.bd\n        for hd in self.hds:\n            fc_lst.append(nn.Linear(last_hd, hd))\n            fc_lst.append(nn.ReLU())\n            last_hd = hd\n        self.fc = nn.Sequential(*fc_lst[:-1])  # last relu omitted\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.d)\n        self.A.data.uniform_(-stdv, stdv)\n\n    def forward(self, X, Y):\n        device = X.device\n        assert X.shape[2] == Y.shape[2] == self.d\n        bi_result = torch.empty(X.shape[0], X.shape [1], Y.shape[1], self.bd, device=device)\n        for i in range(self.bd):\n            tmp = torch.matmul(X, self.A[:, :, i])\n            tmp = torch.matmul(tmp, Y.transpose(1, 2))\n            bi_result[:, :, :, i] = tmp\n        S = self.fc(bi_result)\n        assert len(S.shape) == 3\n        return S\n\n# ==========================================\n# File: models/PCA/model.py\n# Function/Context: Net.forward\n# ==========================================\nimport torch\nimport torch.nn as nn\n\nfrom src.lap_solvers.sinkhorn import Sinkhorn\nfrom src.feature_align import feature_align\nfrom src.gconv import Siamese_Gconv\nfrom models.PCA.affinity_layer import Affinity\nfrom src.lap_solvers.hungarian import hungarian\n\nfrom src.utils.config import cfg\nfrom models.PCA.model_config import model_cfg\n\nfrom src.backbone import *\nCNN = eval(cfg.BACKBONE)\n\n\nclass Net(CNN):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.sinkhorn = Sinkhorn(max_iter=cfg.PCA.SK_ITER_NUM, epsilon=cfg.PCA.SK_EPSILON, tau=cfg.PCA.SK_TAU)\n        self.l2norm = nn.LocalResponseNorm(cfg.PCA.FEATURE_CHANNEL * 2, alpha=cfg.PCA.FEATURE_CHANNEL * 2, beta=0.5, k=0)\n        self.gnn_layer = cfg.PCA.GNN_LAYER\n        #self.pointer_net = PointerNet(cfg.PCA.GNN_FEAT, cfg.PCA.GNN_FEAT // 2, alpha=cfg.PCA.VOTING_ALPHA)\n        for i in range(self.gnn_layer):\n            if i == 0:\n                gnn_layer = Siamese_Gconv(cfg.PCA.FEATURE_CHANNEL * 2, cfg.PCA.GNN_FEAT)\n            else:\n                gnn_layer = Siamese_Gconv(cfg.PCA.GNN_FEAT, cfg.PCA.GNN_FEAT)\n            self.add_module('gnn_layer_{}'.format(i), gnn_layer)\n            self.add_module('affinity_{}'.format(i), Affinity(cfg.PCA.GNN_FEAT))\n            if i == self.gnn_layer - 2:  # only second last layer will have cross-graph module\n                self.add_module('cross_graph_{}'.format(i), nn.Linear(cfg.PCA.GNN_FEAT * 2, cfg.PCA.GNN_FEAT))\n        self.cross_iter = cfg.PCA.CROSS_ITER\n        self.cross_iter_num = cfg.PCA.CROSS_ITER_NUM\n        self.rescale = cfg.PROBLEM.RESCALE\n\n    def reload_backbone(self):\n        self.node_layers, self.edge_layers = self.get_backbone(True)\n\n\n    def forward(self, data_dict, **kwargs):\n        if 'images' in data_dict:\n            # real image data\n            src, tgt = data_dict['images']\n            P_src, P_tgt = data_dict['Ps']\n            ns_src, ns_tgt = data_dict['ns']\n            A_src, A_tgt = data_dict['As']\n\n            # extract feature\n            src_node = self.node_layers(src)\n            src_edge = self.edge_layers(src_node)\n            tgt_node = self.node_layers(tgt)\n            tgt_edge = self.edge_layers(tgt_node)\n\n            # feature normalization\n            src_node = self.l2norm(src_node)\n            src_edge = self.l2norm(src_edge)\n            tgt_node = self.l2norm(tgt_node)\n            tgt_edge = self.l2norm(tgt_edge)\n\n            # arrange features\n            U_src = feature_align(src_node, P_src, ns_src, self.rescale)\n            F_src = feature_align(src_edge, P_src, ns_src, self.rescale)\n            U_tgt = feature_align(tgt_node, P_tgt, ns_tgt, self.rescale)\n            F_tgt = feature_align(tgt_edge, P_tgt, ns_tgt, self.rescale)\n        elif 'features' in data_dict:\n            # synthetic data\n            src, tgt = data_dict['features']\n            ns_src, ns_tgt = data_dict['ns']\n            A_src, A_tgt = data_dict['As']\n\n            U_src = src[:, :src.shape[1] // 2, :]\n            F_src = src[:, src.shape[1] // 2:, :]\n            U_tgt = tgt[:, :tgt.shape[1] // 2, :]\n            F_tgt = tgt[:, tgt.shape[1] // 2:, :]\n        else:\n            raise ValueError('Unknown data type for this model.')\n\n        emb1, emb2 = torch.cat((U_src, F_src), dim=1).transpose(1, 2), torch.cat((U_tgt, F_tgt), dim=1).transpose(1, 2)\n        ss = []\n\n        if not self.cross_iter:\n            # Vanilla PCA-GM\n            for i in range(self.gnn_layer):\n                gnn_layer = getattr(self, 'gnn_layer_{}'.format(i))\n                emb1, emb2 = gnn_layer([A_src, emb1], [A_tgt, emb2]) \n                affinity = getattr(self, 'affinity_{}'.format(i))\n                s = affinity(emb1, emb2)\n                s = self.sinkhorn(s, ns_src, ns_tgt, dummy_row=True)\n\n                ss.append(s)\n\n                if i == self.gnn_layer - 2:\n                    cross_graph = getattr(self, 'cross_graph_{}'.format(i))\n                    new_emb1 = cross_graph(torch.cat((emb1, torch.bmm(s, emb2)), dim=-1))\n                    new_emb2 = cross_graph(torch.cat((emb2, torch.bmm(s.transpose(1, 2), emb1)), dim=-1))\n                    emb1 = new_emb1\n                    emb2 = new_emb2\n        else:\n            # IPCA-GM\n            for i in range(self.gnn_layer - 1):\n                gnn_layer = getattr(self, 'gnn_layer_{}'.format(i))\n                emb1, emb2 = gnn_layer([A_src, emb1], [A_tgt, emb2])\n\n            emb1_0, emb2_0 = emb1, emb2\n            s = torch.zeros(emb1.shape[0], emb1.shape[1], emb2.shape[1], device=emb1.device)\n\n            for x in range(self.cross_iter_num):\n                i = self.gnn_layer - 2\n                cross_graph = getattr(self, 'cross_graph_{}'.format(i))\n                emb1 = cross_graph(torch.cat((emb1_0, torch.bmm(s, emb2_0)), dim=-1))\n                emb2 = cross_graph(torch.cat((emb2_0, torch.bmm(s.transpose(1, 2), emb1_0)), dim=-1))\n\n                i = self.gnn_layer - 1\n                gnn_layer = getattr(self, 'gnn_layer_{}'.format(i))\n                emb1, emb2 = gnn_layer([A_src, emb1], [A_tgt, emb2])\n                affinity = getattr(self, 'affinity_{}'.format(i))\n                s = affinity(emb1, emb2)\n                s = self.sinkhorn(s, ns_src, ns_tgt, dummy_row=True)\n                ss.append(s)\n\n        data_dict.update({\n            'ds_mat': ss[-1],\n            'perm_mat': hungarian(ss[-1], ns_src, ns_tgt)\n        })\n        return data_dict\n\n# ==========================================\n# File: src/gconv.py\n# Function/Context: Gconv\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Tuple, Optional, List, Union\n\n\nclass Gconv(nn.Module):\n    r\"\"\"\n    Graph Convolutional Layer which is inspired and developed based on Graph Convolutional Network (GCN).\n    Inspired by `Kipf and Welling. Semi-Supervised Classification with Graph Convolutional Networks. ICLR 2017.\n    <https://arxiv.org/abs/1609.02907>`_\n\n    :param in_features: the dimension of input node features\n    :param out_features: the dimension of output node features\n    \"\"\"\n    def __init__(self, in_features: int, out_features: int):\n        super(Gconv, self).__init__()\n        self.num_inputs = in_features\n        self.num_outputs = out_features\n        self.a_fc = nn.Linear(self.num_inputs, self.num_outputs)\n        self.u_fc = nn.Linear(self.num_inputs, self.num_outputs)\n\n    def forward(self, A: Tensor, x: Tensor, norm: bool=True) -> Tensor:\n        r\"\"\"\n        Forward computation of graph convolution network.\n\n        :param A: :math:`(b\\times n\\times n)` {0,1} adjacency matrix. :math:`b`: batch size, :math:`n`: number of nodes\n        :param x: :math:`(b\\times n\\times d)` input node embedding. :math:`d`: feature dimension\n        :param norm: normalize connectivity matrix or not\n        :return: :math:`(b\\times n\\times d^\\prime)` new node embedding\n        \"\"\"\n        if norm is True:\n            A = F.normalize(A, p=1, dim=-2)\n        ax = self.a_fc(x)\n        ux = self.u_fc(x)\n        x = torch.bmm(A, F.relu(ax)) + F.relu(ux) # has size (bs, N, num_outputs)\n        return x\n\n# ==========================================\n# File: src/lap_solvers/hungarian.py\n# Function/Context: hungarian\n# ==========================================\nimport torch\nimport scipy.optimize as opt\nimport numpy as np\nfrom multiprocessing import Pool\nfrom torch import Tensor\n\n\ndef hungarian(s: Tensor, n1: Tensor=None, n2: Tensor=None, nproc: int=1) -> Tensor:\n    r\"\"\"\n    Solve optimal LAP permutation by hungarian algorithm. The time cost is :math:`O(n^3)`.\n\n    :param s: :math:`(b\\times n_1 \\times n_2)` input 3d tensor. :math:`b`: batch size\n    :param n1: :math:`(b)` number of objects in dim1\n    :param n2: :math:`(b)` number of objects in dim2\n    :param nproc: number of parallel processes (default: ``nproc=1`` for no parallel)\n    :return: :math:`(b\\times n_1 \\times n_2)` optimal permutation matrix\n\n    .. note::\n        We support batched instances with different number of nodes, therefore ``n1`` and ``n2`` are\n        required to specify the exact number of objects of each dimension in the batch. If not specified, we assume\n        the batched matrices are not padded.\n    \"\"\"\n    if len(s.shape) == 2:\n        s = s.unsqueeze(0)\n        matrix_input = True\n    elif len(s.shape) == 3:\n        matrix_input = False\n    else:\n        raise ValueError('input data shape not understood: {}'.format(s.shape))\n\n    device = s.device\n    batch_num = s.shape[0]\n\n    perm_mat = s.cpu().detach().numpy() * -1\n    if n1 is not None:\n        n1 = n1.cpu().numpy()\n    else:\n        n1 = [None] * batch_num\n    if n2 is not None:\n        n2 = n2.cpu().numpy()\n    else:\n        n2 = [None] * batch_num\n\n    if nproc > 1:\n        with Pool(processes=nproc) as pool:\n            mapresult = pool.starmap_async(_hung_kernel, zip(perm_mat, n1, n2))\n            perm_mat = np.stack(mapresult.get())\n    else:\n        perm_mat = np.stack([_hung_kernel(perm_mat[b], n1[b], n2[b]) for b in range(batch_num)])\n\n    perm_mat = torch.from_numpy(perm_mat).to(device)\n\n    if matrix_input:\n        perm_mat.squeeze_(0)\n\n    return perm_mat\n\ndef _hung_kernel(s: torch.Tensor, n1=None, n2=None):\n    if n1 is None:\n        n1 = s.shape[0]\n    if n2 is None:\n        n2 = s.shape[1]\n    row, col = opt.linear_sum_assignment(s[:n1, :n2])\n    perm_mat = np.zeros_like(s)\n    perm_mat[row, col] = 1\n    return perm_mat\n\n# ==========================================\n# File: src/lap_solvers/sinkhorn.py\n# Function/Context: Sinkhorn.forward_ori\n# ==========================================\nimport torch\nimport torch.nn as nn\nfrom torch import Tensor\nimport pygmtools as pygm\n\nclass Sinkhorn(nn.Module):\n    r\"\"\"\n    Sinkhorn algorithm turns the input matrix into a bi-stochastic matrix.\n\n    Sinkhorn algorithm firstly applies an ``exp`` function with temperature :math:`\\tau`:\n\n    .. math::\n        \\mathbf{S}_{i,j} = \\exp \\left(\\frac{\\mathbf{s}_{i,j}}{\\tau}\\right)\n\n    And then turns the matrix into doubly-stochastic matrix by iterative row- and column-wise normalization:\n\n    .. math::\n        \\mathbf{S} &= \\mathbf{S} \\oslash (\\mathbf{1}_{n_2} \\mathbf{1}_{n_2}^\\top \\cdot \\mathbf{S}) \\\\\n        \\mathbf{S} &= \\mathbf{S} \\oslash (\\mathbf{S} \\cdot \\mathbf{1}_{n_2} \\mathbf{1}_{n_2}^\\top)\n\n    where :math:`\\oslash` means element-wise division, :math:`\\mathbf{1}_n` means a column-vector with length :math:`n`\n    whose elements are all :math:`1`\\ s.\n\n    :param max_iter: maximum iterations (default: ``10``)\n    :param tau: the hyper parameter :math:`\\tau` controlling the temperature (default: ``1``)\n    :param epsilon: a small number for numerical stability (default: ``1e-4``)\n    :param log_forward: apply log-scale computation for better numerical stability (default: ``True``)\n    :param batched_operation: apply batched_operation for better efficiency (but may cause issues for back-propagation,\n     default: ``False``)\n\n    .. note::\n        ``tau`` is an important hyper parameter to be set for Sinkhorn algorithm. ``tau`` controls the distance between\n        the predicted doubly-stochastic matrix, and the discrete permutation matrix computed by Hungarian algorithm (see\n        :func:`~src.lap_solvers.hungarian.hungarian`). Given a small ``tau``, Sinkhorn performs more closely to\n        Hungarian, at the cost of slower convergence speed and reduced numerical stability.\n\n    .. note::\n        We recommend setting ``log_forward=True`` because it is more numerically stable. It provides more precise\n        gradient in back propagation and helps the model to converge better and faster.\n\n    .. note::\n        Setting ``batched_operation=True`` may be preferred when you are doing inference with this module and do not\n        need the gradient.\n    \"\"\"\n    def __init__(self, max_iter: int=10, tau: float=1., epsilon: float=1e-4,\n                 log_forward: bool=True, batched_operation: bool=False):\n        super(Sinkhorn, self).__init__()\n        self.max_iter = max_iter\n        self.tau = tau\n        self.epsilon = epsilon\n        self.log_forward = log_forward\n        if not log_forward:\n            print('Warning: Sinkhorn algorithm without log forward is deprecated because log_forward is more stable.')\n        self.batched_operation = batched_operation # batched operation may cause instability in backward computation,\n                                                   # but will boost computation.\n\n    def forward(self, s: Tensor, nrows: Tensor=None, ncols: Tensor=None, dummy_row: bool=False) -> Tensor:\n        r\"\"\"\n        :param s: :math:`(b\\times n_1 \\times n_2)` input 3d tensor. :math:`b`: batch size\n        :param nrows: :math:`(b)` number of objects in dim1\n        :param ncols: :math:`(b)` number of objects in dim2\n        :param dummy_row: whether to add dummy rows (rows whose elements are all 0) to pad the matrix to square matrix.\n         default: ``False``\n        :return: :math:`(b\\times n_1 \\times n_2)` the computed doubly-stochastic matrix\n\n        .. note::\n            We support batched instances with different number of nodes, therefore ``nrows`` and ``ncols`` are\n            required to specify the exact number of objects of each dimension in the batch. If not specified, we assume\n            the batched matrices are not padded.\n\n        .. note::\n            The original Sinkhorn algorithm only works for square matrices. To handle cases where the graphs to be\n            matched have different number of nodes, it is a common practice to add dummy rows to construct a square\n            matrix. After the row and column normalizations, the padded rows are discarded.\n\n        .. note::\n            We assume row number <= column number. If not, the input matrix will be transposed.\n        \"\"\"\n        if self.log_forward:\n            return self.forward_log(s, nrows, ncols, dummy_row)\n        else:\n            return self.forward_ori(s, nrows, ncols, dummy_row) # deprecated\n\n    def forward_log(self, s, nrows=None, ncols=None, dummy_row=False):\n        \"\"\"Compute sinkhorn with row/column normalization in the log space.\"\"\"\n        return pygm.sinkhorn(s, n1=nrows, n2=ncols, dummy_row=dummy_row, max_iter=self.max_iter, tau=self.tau, batched_operation=self.batched_operation, backend='pytorch')\n\n    def forward_ori(self, s, nrows=None, ncols=None, dummy_row=False):\n        r\"\"\"\n        Computing sinkhorn with row/column normalization.\n\n        .. warning::\n            This function is deprecated because :meth:`~src.lap_solvers.sinkhorn.Sinkhorn.forward_log` is more\n            numerically stable.\n        \"\"\"\n        if len(s.shape) == 2:\n            s = s.unsqueeze(0)\n            matrix_input = True\n        elif len(s.shape) == 3:\n            matrix_input = False\n        else:\n            raise ValueError('input data shape not understood.')\n\n        batch_size = s.shape[0]\n\n        #s = s.to(dtype=dtype)\n\n        if nrows is None:\n            nrows = [s.shape[1] for _ in range(batch_size)]\n        if ncols is None:\n            ncols = [s.shape[2] for _ in range(batch_size)]\n\n        # tau scaling\n        ret_s = torch.zeros_like(s)\n        for b, n in enumerate(nrows):\n            ret_s[b, 0:n, 0:ncols[b]] = \\\n                nn.functional.softmax(s[b, 0:n, 0:ncols[b]] / self.tau, dim=-1)\n        s = ret_s\n\n        # add dummy elements\n        if dummy_row:\n            dummy_shape = list(s.shape)\n            dummy_shape[1] = s.shape[2] - s.shape[1]\n            #s = torch.cat((s, torch.full(dummy_shape, self.epsilon * 10).to(s.device)), dim=1)\n            #nrows = nrows + dummy_shape[1] # non in-place\n            s = torch.cat((s, torch.full(dummy_shape, 0.).to(s.device)), dim=1)\n            ori_nrows = nrows\n            nrows = ncols\n            for b in range(batch_size):\n                s[b, ori_nrows[b]:nrows[b], :ncols[b]] = self.epsilon\n\n        row_norm_ones = torch.zeros(batch_size, s.shape[1], s.shape[1], device=s.device, dtype=s.dtype)  # size: row x row\n        col_norm_ones = torch.zeros(batch_size, s.shape[2], s.shape[2], device=s.device, dtype=s.dtype)  # size: col x col\n        for b in range(batch_size):\n            row_slice = slice(0, nrows[b])\n            col_slice = slice(0, ncols[b])\n            row_norm_ones[b, row_slice, row_slice] = 1\n            col_norm_ones[b, col_slice, col_slice] = 1\n\n        s += self.epsilon\n\n        for i in range(self.max_iter):\n            if i % 2 == 0:\n                # column norm\n                #ones = torch.ones(batch_size, s.shape[1], s.shape[1], device=s.device)\n                sum = torch.sum(torch.mul(s.unsqueeze(3), col_norm_ones.unsqueeze(1)), dim=2)\n            else:\n                # row norm\n                # ones = torch.ones(batch_size, s.shape[2], s.shape[2], device=s.device)\n                sum = torch.sum(torch.mul(row_norm_ones.unsqueeze(3), s.unsqueeze(1)), dim=2)\n\n            tmp = torch.zeros_like(s)\n            for b in range(batch_size):\n                row_slice = slice(0, nrows[b] if nrows is not None else s.shape[2])\n                col_slice = slice(0, ncols[b] if ncols is not None else s.shape[1])\n                tmp[b, row_slice, col_slice] = 1 / sum[b, row_slice, col_slice]\n            s = s * tmp\n\n        if dummy_row:\n            if dummy_shape[1] > 0:\n                s = s[:, :-dummy_shape[1]]\n            for b in range(batch_size):\n                s[b, ori_nrows[b]:nrows[b], :ncols[b]] = 0\n\n        if matrix_input:\n            s.squeeze_(0)\n\n        return s",
  "description": "Combined Analysis:\n- [models/BBGM/affinity_layer.py]: This file implements the affinity computation layer that corresponds to the Quadratic Assignment Problem (QAP) formulation in the paper. Specifically, it computes the node-to-node affinity matrix K (or sub-blocks of it) through learned inner products between node embeddings. The class InnerProductWithWeightsAffinity takes node embeddings X and Y (from two graphs) and computes their affinity via matrix multiplication (X * Y^T), which aligns with constructing the affinity matrix K in the objective vec(X)^T K vec(X). The optional weight transformation via a linear layer and tanh activation allows learning a global scaling factor for the embeddings, enhancing the model's ability to adapt the affinity metric. The softplus activation ensures non-negative affinities, consistent with the QAP formulation where K is typically non-negative. This layer is a core component in building the end-to-end differentiable graph matching pipeline, as it computes the compatibility scores between nodes that are then optimized by subsequent layers (e.g., Sinkhorn).\n- [models/BBGM/model.py]: This file implements the core optimization logic of the paper 'Learning Combinatorial Embedding Networks for Deep Graph Matching'. The Net class performs: 1) CNN-based feature extraction, 2) Graph neural network message passing for intra-graph structural embeddings via SiameseSConvOnNodes, 3) Cross-graph affinity computation through InnerProductWithWeightsAffinity layers for both node (unary) and edge (quadratic) similarities, 4) Solves the Quadratic Assignment Problem (QAP) using differentiable graph matching solvers (GraphMatchingModule/MultiGraphMatchingModule) that handle the combinatorial constraints. The implementation directly corresponds to maximizing vec(X)ᵀK vec(X) subject to permutation matrix constraints, with the solver providing differentiable permutation predictions during training and discrete matchings during inference.\n- [models/GMN/affinity_layer.py]: This file implements the core affinity computation logic for graph matching. Both classes compute edgewise (Me) and pointwise (Mp) affinity matrices, which directly correspond to the quadratic assignment objective vec(X)ᵀK vec(X) where K is constructed from these affinity matrices. InnerpAffinity uses learnable positive-definite weight matrices (Lambda1, Lambda2) with inner product formulation Me = XΛYᵀ, while GaussianAffinity uses Gaussian kernel Me = exp(-||X-Y||²/σ). These affinity layers are essential components that compute the similarity measures between graph features before optimization via Sinkhorn/Hungarian algorithms.\n- [models/GMN/model.py]: This file implements the core optimization logic of the GMN paper. It constructs the affinity matrix M (equivalent to K in the QAP formulation) using learned node (Mp) and edge (Me) affinities via the affinity_layer. The optimization objective is implicitly addressed by solving the QAP using either SpectralMatching or RRWM (gm_solver) to obtain an initial solution v, which is then refined into a doubly-stochastic matrix s via the differentiable Sinkhorn layer. Finally, the Hungarian algorithm discretizes s into a permutation matrix, satisfying the constraints X ∈ {0,1}^{N×N}, X1 = 1, X^T1 ≤ 1. The pipeline aligns with the paper's end-to-end learning approach, integrating feature extraction, affinity learning, and differentiable permutation prediction.\n- [models/NGM/model.py]: This file implements the core optimization logic of the paper 'Learning Combinatorial Embedding Networks for Deep Graph Matching'. The Net class encapsulates the end-to-end differentiable pipeline: (1) CNN-based feature extraction for node and edge features, (2) graph neural network layers (GNNLayer) for intra-graph and cross-graph embedding to encode higher-order affinities, (3) affinity metric layer (InnerpAffinity/GaussianAffinity) to compute the affinity matrix K, (4) Sinkhorn layer for differentiable permutation prediction (relaxing the discrete assignment to continuous), and (5) Hungarian algorithm for discretization. The forward method maps the quadratic assignment problem (QAP) objective to a deep learning pipeline, using the Sinkhorn algorithm as a differentiable surrogate for the permutation matrix constraints. The implementation supports both training (with Gumbel-Sinkhorn sampling) and inference modes.\n- [models/PCA/affinity_layer.py]: This file implements the core affinity computation component from the paper 'Learning Combinatorial Embedding Networks for Deep Graph Matching'. The paper's optimization model requires computing node-to-node affinity matrices (M) from learned feature embeddings (X, Y). The file provides multiple implementations of this affinity function: 1) Affinity: M = X * A * Y^T with learnable weight matrix A (most directly corresponds to the paper's formulation), 2) AffinityInp: Inner product affinity, 3) AffinityLR: Low-rank approximation with ReLU activation, 4) AffinityMah: Mahalanobis distance-based affinity, 5) AffinityFC: Fully-connected neural network for affinity scoring, 6) AffinityBiFC: Bilinear layer followed by FC network. These implementations directly correspond to the paper's 'affinity metric layer' that computes node-to-node similarities from feature space, which is then used in the quadratic assignment formulation. The code handles batch processing and different feature dimensions as specified in the paper's architecture.\n- [models/PCA/model.py]: This file implements the core optimization logic of the paper 'Learning Combinatorial Embedding Networks for Deep Graph Matching'. The Net class encapsulates the end-to-end differentiable pipeline: (1) CNN backbone for feature extraction (node/edge features), (2) Siamese graph convolutional networks (GCN) for intra-graph and cross-graph embedding, (3) affinity layers to compute node-to-node similarities, (4) Sinkhorn algorithm for differentiable permutation prediction (relaxed assignment), and (5) Hungarian algorithm for final discretization. The forward method handles both vanilla PCA-GM and iterative IPCA-GM variants, aligning with the paper's proposed algorithm steps. The code directly maps to the optimization model's objective (maximizing affinity via learned embeddings) and constraints (handled by Sinkhorn/Hungarian).\n- [src/gconv.py]: This file implements the GCN-based node embedding component from the paper. The Gconv class performs graph convolution operations to update node embeddings based on adjacency matrices, which is exactly the intra-graph structural embedding step described in the paper. However, this is just one component of the complete pipeline - it doesn't implement the cross-graph affinity computation, Sinkhorn layer, or optimization objective.\n- [src/lap_solvers/hungarian.py]: This file implements the Hungarian algorithm (also known as Munkres or linear sum assignment algorithm) for solving the Linear Assignment Problem (LAP), which is a key component in the inference stage of the described graph matching pipeline. While the paper's core optimization model is a Quadratic Assignment Problem (QAP) solved via differentiable Sinkhorn relaxation during training, this file provides the discrete optimization solver used at inference time to convert continuous Sinkhorn outputs into exact permutation matrices. The implementation handles batched inputs with variable-sized graphs and supports parallel processing. It directly corresponds to the algorithm step: 'during inference, the Hungarian algorithm is applied to discretize the Sinkhorn output into a permutation matrix.' The code wraps SciPy's linear_sum_assignment function to find optimal permutations that maximize the assignment score (achieved by negating input scores).\n- [src/lap_solvers/sinkhorn.py]: This file implements the Sinkhorn algorithm, which is a core component of the differentiable permutation prediction step in the paper 'Learning Combinatorial Embedding Networks for Deep Graph Matching'. The Sinkhorn layer relaxes the discrete permutation matrix constraint (X ∈ {0,1}^{N×N}, X1 = 1, X^T1 ≤ 1) into a continuous doubly-stochastic matrix through iterative row/column normalization. This enables gradient-based optimization during training. The implementation handles batched inputs with variable graph sizes, temperature scaling (τ), and dummy row padding for non-square matrices. The forward_ori method explicitly implements the iterative normalization process described in the paper's algorithm steps.",
  "dependencies": [
    "concat_features",
    "Affinity",
    "pygmtools",
    "multiprocessing.Pool",
    "src.utils.config.cfg",
    "src.feature_align.feature_align",
    "nn.Module",
    "models.BBGM.sconv_archs",
    "torch.nn.functional.softplus",
    "src.backbone",
    "torch.tanh",
    "models.GMN.affinity_layer.GaussianAffinity",
    "normalize_over_channels",
    "src.feature_align",
    "lpmp_py",
    "torch.nn.functional",
    "src.qap_solvers.spectral_matching.SpectralMatching",
    "src.utils.config",
    "math",
    "src.qap_solvers.rrwm.RRWM",
    "torch.nn.parameter.Parameter",
    "models.BBGM.affinity_layer",
    "src.backbone.CNN",
    "models.GMN.affinity_layer.InnerpAffinity",
    "src.build_graphs.reshape_edge_feature",
    "model_cfg",
    "nn.functional.softmax",
    "nn.Linear",
    "nn.Sequential",
    "torch.matmul",
    "cfg",
    "src.lap_solvers.sinkhorn.Sinkhorn",
    "models.NGM.gnn.GNNLayer",
    "CNN backbone",
    "torch.Tensor",
    "numpy",
    "Siamese_Gconv",
    "src.lap_solvers.sinkhorn.GumbelSinkhorn",
    "lexico_iter",
    "feature_align",
    "hungarian",
    "scipy.optimize",
    "torch.nn",
    "Sinkhorn",
    "src.evaluation_metric.objective_score",
    "nn.ReLU",
    "torch",
    "src.factorize_graph_matching.construct_aff_mat",
    "models.NGM.geo_edge_feature.geo_edge_feature",
    "src.lap_solvers.hungarian.hungarian",
    "itertools"
  ]
}