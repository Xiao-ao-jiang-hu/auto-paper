{
  "file_path": "BipartiteMatchingExperiment/PO_model_matching.py, BipartiteMatchingExperiment/bipartite.py, BipartiteMatchingExperiment/test_matching_Combinedregression.py, BipartiteMatchingExperiment/test_matching_Listwise.py, EnergyExperiment/ICON_solving.py, EnergyExperiment/PO_model_energy.py, EnergyExperiment/test_energy_Combinedregression.py, EnergyExperiment/test_energy_NCE.py, ShortestPathExperiment/PO_modelsSP.py",
  "function_name": "CachingPO, bmatching, bmatching_diverse, CombinedPO, CachingPO, SolveICON, CachingPO, shortestpath_solver",
  "code_snippet": "\n\n# ==========================================\n# File: BipartiteMatchingExperiment/PO_model_matching.py\n# Function/Context: CachingPO\n# ==========================================\nfrom bipartite import get_cora, bmatching_diverse, get_qpt_matrices\nimport os\nimport torch \nfrom torch import nn, optim\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom torchmetrics.functional import auc\nfrom torch.utils.data import DataLoader\nimport pytorch_lightning as pl\nimport numpy as np\n\ndef batch_solve(param,y,m,relaxation =False):\n    sol = []\n    for i in range(len(y)):\n        sol.append(  solver(y[i], m[i], relaxation=relaxation,**param) )\n    return np.array(sol)\n\ndef growcache(cache, y_hat, m, param):\n    '''\n    cache is torch array [currentpoolsize,48]\n    y_hat is  torch array [batch_size,48]\n    '''\n    sol =  batch_solve(param,y_hat.detach().numpy(),m.numpy(),relaxation =False) \n    cache_np = cache.detach().numpy()\n    cache_np = np.unique(np.append(cache_np,sol,axis=0),axis=0)\n    # torch has no unique function, so we have to do this\n    return torch.from_numpy(cache_np).float()\n\nclass CachingPO(twostage_regression):\n    def __init__(self, param, loss_fn, init_cache,growth=0.1, tau=1., lr=1e-1, seed=2, max_epochs=50):\n        super().__init__(param, lr, seed, max_epochs)\n        # self.save_hyperparameters()\n        self.loss_fn = loss_fn\n        ### The cache\n        init_cache_np = init_cache.detach().numpy()\n        init_cache_np = np.unique(init_cache_np,axis=0)\n        # torch has no unique function, so we have to do this\n        self.cache = torch.from_numpy(init_cache_np).float()\n        self.growth = growth\n        self.tau = tau\n        self.save_hyperparameters(\"lr\",\"growth\",\"tau\")\n\n    def training_step(self, batch, batch_idx):\n        x,y,sol,m = batch\n        y_hat =  self(x).squeeze()\n        if (np.random.random(1)[0]< self.growth) or len(self.cache)==0:\n            self.cache = growcache(self.cache, y_hat,m, self.param)\n\n        loss = self.loss_fn(y_hat,y,sol,self.cache,tau = self.tau)\n        self.log(\"train_loss\",loss, prog_bar=True, on_step=True, on_epoch=True, )\n        return loss\n\ndef Listnet_loss(y_hat,y_true,sol,cache, tau=1.,minimize=False,*wd,**kwd):\n    mm = 1 if minimize else -1 \n    loss = 0\n    for ii in range(len(y_true)):\n         loss += -(F.log_softmax((-mm*y_hat[ii]*cache/tau).sum(dim=1),\n                dim=0)*F.softmax((-mm*y_true[ii]*cache/tau).sum(dim=1),dim=0)).mean()\n    return loss\n\n# ==========================================\n# File: BipartiteMatchingExperiment/bipartite.py\n# Function/Context: bmatching, bmatching_diverse\n# ==========================================\nimport gurobipy as gp\nimport time\nimport numpy as np\nimport pickle\nimport copy\nfrom tqdm.auto import tqdm\nimport sys \nfrom ortools.graph import pywrapgraph\nfrom ortools.linear_solver import pywraplp\n\n\ndef linearobj(x,v, **params):\n    return \n\ndef bmatching(preds, mult=1000, **kwargs):\n    assignment = pywrapgraph.LinearSumAssignment()\n    cost = -preds.reshape(50,50)*mult\n    n1 = len(cost)\n    n2 = len(cost[0])\n    for i in range(n1):\n        for j in range(n2):\n          assignment.AddArcWithCost(i, j, int(cost[i,j]))\n    solve_status = assignment.Solve()\n    solution = np.zeros((50,50))\n    for i in range(assignment.NumNodes()):\n        mate = assignment.RightMate(i)\n        solution[i,mate] = 1\n    return solution.reshape(-1)\n\nsolver = pywraplp.Solver.CreateSolver('GLOP')\n# solver.SuppressOutput()\ndef bmatching_diverse(preds, match_subs, p=0.25, q=0.25, relaxation=False, **kwargs):\n    \n    solver.Clear()\n    mult=1000\n    cost = -preds.reshape(50,50)*mult\n    m = match_subs.reshape(50,50)\n    n1 = len(cost)\n    n2 = len(cost[0])\n    x = {}\n    for i in range(n1):\n        for j in range(n2):\n            x[i,j] = solver.NumVar(0,1,'') if relaxation else solver.IntVar(0,1,'')\n\n    for i in range(n1):\n        solver.Add(solver.Sum([x[i, j] for j in range(n2)]) <= 1)\n\n    for j in range(n2):\n        solver.Add(solver.Sum([x[i, j] for i in range(n1)]) <= 1)\n\n    # pairing in same field\n    pairing_same = []\n    allvars = []\n    for i in range(n1):\n        for j in range(n2):\n            pairing_same.append(x[i,j] * m[i,j])\n            allvars.append(x[i,j])\n    solver.Add(solver.Sum(pairing_same) >= p*solver.Sum(allvars))\n\n    # pairing in distinct field\n    pairing_dis = []\n    for i in range(n1):\n        for j in range(n2):\n            pairing_dis.append(x[i,j] * (1-m[i,j]))\n    solver.Add(solver.Sum(pairing_dis) >= q*solver.Sum(allvars))\n\n    obj = []\n    for i in range(n1):\n        for j in range(n2):\n            obj.append(cost[i,j] * x[i,j]) \n    solver.Minimize(solver.Sum(obj))\n\n    status = solver.Solve()\n    solution = np.zeros((50,50))\n\n    if status == pywraplp.Solver.OPTIMAL:\n        for i in range(n1):\n            for j in range(n2):\n                solution[i,j] = x[i,j].solution_value()\n    #solver.Clear()\n    return solution.reshape(-1)\n\ndef get_qpt_matrices(match_subs, p=0.25, q=0.25, **kwargs):\n    # we only have G * x <= h\n    \n    # Matching\n    N1 = np.zeros((50,2500))\n    N2 = np.zeros_like(N1)\n    b1 = np.ones(50)\n    b2 = np.ones_like(b1)\n    \n    for i in range(50):\n        rowmask = np.zeros((50,50))\n        colmask = np.zeros_like(rowmask)\n        rowmask[i,:] = 1 \n        colmask[:,i] = 1\n        N1[i] = rowmask.flatten()\n        N2[i] = colmask.flatten() \n    \n    # Similarity constraint\n    Sim = p - match_subs \n    bsim = np.zeros(1)\n    \n    # Diversity constraint \n    Div = q - 1 + match_subs \n    bdiv = np.zeros_like(bsim)\n\n    G = np.vstack((N1, N2, Sim, Div))\n    h = np.concatenate((b1, b2, bsim, bdiv))\n    A = None \n    b = None \n    return A,b, G,h\n\n# ==========================================\n# File: BipartiteMatchingExperiment/test_matching_Combinedregression.py\n# Function/Context: CombinedPO\n# ==========================================\nimport shutil\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nimport pytorch_lightning as pl\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import DataLoader\n\n# Note: matching_models module contains the CombinedPO class and other components\nfrom matching_models import *\n\n# Core logic implementation in CombinedPO class (from matching_models import *)\n# The CombinedPO class implements the caching algorithm with these key methods:\n# 1. forward() - computes predicted cost vector ĉ = m(ω, x)\n# 2. training_step() - implements the algorithm steps for each training instance\n# 3. growpool_fn() - manages solution cache growth\n\n# Algorithm implementation in CombinedPO.training_step():\n# a. Compute predicted cost vector ĉ = self.forward(x)\n# b. Transform ĉ to č (if needed by loss function)\n# c. With probability determined by growth parameter, call solver to find optimal solution\n# d. Otherwise, find solution by argmin over cache S\n# e. Compute gradient of loss function L^v with respect to č\n# f. Update ω via backpropagation\n\n# Initialization of solution cache S with true optimal solutions\nsolpool_np = np.array([solver(y_train[i], m_train[i], relaxation=False) for i in range(len(y_train))])\nsolpool_np = np.unique(solpool_np, axis=0)\nsolpool = torch.from_numpy(solpool_np).float()\n\n# Model instantiation with caching parameters\nmodel = CombinedPO(\n    alpha=alpha,                    # probability parameter for cache usage\n    loss_fn=Listnet_loss,           # loss function L^v\n    solpool=solpool,                # initial solution cache S\n    growpool_fn=growpool_fn,        # function to grow cache\n    lr=lr,                          # learning rate\n    growth=growth                   # growth parameter controlling cache updates\n)\n\n# Training loop managed by PyTorch Lightning\n# Each epoch executes the algorithm steps for each training instance\n# The CombinedPO.training_step() implements steps 2a-2e of the algorithm\ntrainer = pl.Trainer(max_epochs=40, callbacks=[checkpoint_callback], min_epochs=5)\ntrainer.fit(model, train_dl, valid_dl)\n\n# ==========================================\n# File: BipartiteMatchingExperiment/test_matching_Listwise.py\n# Function/Context: CachingPO\n# ==========================================\nfrom PO_model_matching import *\nimport shutil\nimport pandas as pd\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning import loggers as pl_loggers\nimport random\nimport torch\nimport numpy as np\nimport pytorch_lightning as pl\nfrom torch.utils.data import DataLoader\nimport os\nfrom tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n\ntorch.use_deterministic_algorithms(True)\n\nexp = \"Matching2\"\nparam_dict = {\"Matching1\":{'p':0.25, 'q':0.25},\"Matching2\":{'p':0.5, 'q':0.5}}\nparam = param_dict[exp]\n\ndef seed_all(seed):\n    print(\"[ Using Seed : \", seed, \" ]\")\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\noutputfile = \"Rslt/Listwise_{}_rslt.csv\".format(exp)\nregretfile = \"Rslt/Listwise_{}_Regret.csv\".format(exp)\nckpt_dir = \"ckpt_dir/Listwise_{}/\".format(exp)\nlog_dir = \"lightning_logs/Listwise_{}/\".format(exp)\nshutil.rmtree(log_dir, ignore_errors=True)\n\n###################################### Hyperparams #########################################\nlr = 0.1\nbatchsize = 32\nmax_epochs = 30\ngrowth = 0.1\ntemperature = 1.\n\n###################################### Data Reading #########################################\nx, y, m = get_cora()\nx_train, x_test = x[:22], x[22:]\ny_train, y_test = y[:22], y[22:]\nm_train, m_test = m[:22], m[22:]\ntrain_df = CoraDataset(x_train, y_train, m_train, param=param)\nvalid_df = CoraDataset(x_test, y_test, m_test, param=param)\n\ntrain_dl = DataLoader(train_df, batch_size=batchsize)\nvalid_dl = DataLoader(valid_df, batch_size=5)\nprint(y_train.shape, y_test.shape)\n\ncache_np = batch_solve(param, y_train, m, relaxation=False)\ncache_np = np.unique(cache_np, axis=0)\ncache = torch.from_numpy(cache_np).float()\n\nfor seed in range(10):\n    seed_all(seed)\n    g = torch.Generator()\n    g.manual_seed(seed)\n    train_dl = DataLoader(train_df, batch_size=batchsize, generator=g, num_workers=8)\n    valid_dl = DataLoader(valid_df, batch_size=5)\n\n    shutil.rmtree(ckpt_dir, ignore_errors=True)\n    checkpoint_callback = ModelCheckpoint(\n        monitor=\"val_regret\",\n        dirpath=ckpt_dir,\n        filename=\"model-{epoch:02d}-{val_loss:.2f}\",\n        save_top_k=1,\n        save_last=True,\n        mode=\"min\"\n    )\n    tb_logger = pl_loggers.TensorBoardLogger(save_dir=log_dir, version=seed)\n\n    trainer = pl.Trainer(max_epochs=max_epochs, log_every_n_steps=1, callbacks=[checkpoint_callback], min_epochs=5, logger=tb_logger)\n\n    model = CachingPO(param=param, loss_fn=Listnet_loss, init_cache=cache, growth=growth,\n                      tau=temperature, lr=lr, seed=seed, max_epochs=max_epochs)\n    trainer.fit(model, train_dl, valid_dl)\n    best_model_path = checkpoint_callback.best_model_path\n    model = CachingPO.load_from_checkpoint(best_model_path,\n                                           param=param, loss_fn=Listnet_loss, init_cache=cache, growth=growth,\n                                           tau=temperature, lr=lr, seed=seed, max_epochs=max_epochs)\n    y_pred = model(torch.from_numpy(x_test).float()).squeeze()\n    sol_test = batch_solve(param, y_test, m_test)\n    regret_list = regret_aslist(y_pred, torch.from_numpy(y_test).float(),\n                                torch.from_numpy(sol_test).float(), torch.from_numpy(m_test).float(), param)\n    df = pd.DataFrame({\"regret\": regret_list})\n    df.index.name = 'instance'\n    df['model'] = 'Listwise'\n    df['seed'] = seed\n    df['batchsize'] = batchsize\n    df['growth'] = growth\n    df['temperature'] = temperature\n    df['lr'] = lr\n    with open(regretfile, 'a') as f:\n        df.to_csv(f, header=f.tell() == 0)\n\n    ##### Summary\n    result = trainer.test(model, dataloaders=valid_dl)\n    df = pd.DataFrame(result)\n    df['model'] = 'Listwise'\n    df['seed'] = seed\n    df['batchsize'] = batchsize\n    df['growth'] = growth\n    df['temperature'] = temperature\n    df['lr'] = lr\n    with open(outputfile, 'a') as f:\n        df.to_csv(f, header=f.tell() == 0)\n\n############################### Save Learning Curve Data ########\nparent_dir = log_dir + \"default/\"\nversion_dirs = [os.path.join(parent_dir, v) for v in os.listdir(parent_dir)]\n\nwalltimes = []\nsteps = []\nregrets = []\nmses = []\naucs = []\nfor logs in version_dirs:\n    event_accumulator = EventAccumulator(logs)\n    event_accumulator.Reload()\n\n    events = event_accumulator.Scalars(\"val_regret_epoch\")\n    walltimes.extend([x.wall_time for x in events])\n    steps.extend([x.step for x in events])\n    regrets.extend([x.value for x in events])\n    events = event_accumulator.Scalars(\"val_mse_epoch\")\n    mses.extend([x.value for x in events])\n    events = event_accumulator.Scalars(\"val_auc_epoch\")\n    aucs.extend([x.value for x in events])\ndf = pd.DataFrame({\"step\": steps, 'wall_time': walltimes, \"val_regret\": regrets,\n                   \"val_mse\": mses, \"val_auc\": aucs})\ndf['model'] = 'Listwise'\ndf.to_csv(\"LearningCurve/Listwise_{}_growth{}_temp{}_lr{}.csv\".format(exp, growth, temperature, lr))\n\n# ==========================================\n# File: EnergyExperiment/ICON_solving.py\n# Function/Context: SolveICON\n# ==========================================\nimport math\nimport numpy as np\nfrom gurobipy import *\nimport logging\n\ndef ICONSolutionPool(nbMachines,nbTasks,nbResources,MC,U,D,E,L,P,idle,up,down,q,\n    verbose=False,method=-1,**h):\n\n\n    Machines = range(nbMachines)\n    Tasks = range(nbTasks)\n    Resources = range(nbResources)\n\n\n    N = 1440//q\n\n    M = Model(\"icon\")\n    if not verbose:\n        M.setParam('OutputFlag', 0)\n   \n    x = M.addVars(Tasks, Machines, range(N), vtype=GRB.BINARY, name=\"x\")\n\n\n    M.addConstrs( x.sum(f,'*',range(E[f])) == 0 for f in Tasks)\n    M.addConstrs( x.sum(f,'*',range(L[f]-D[f]+1,N)) == 0 for f in Tasks)\n    M.addConstrs(( quicksum(x[(f,m,t)] for t in range(N) for m in Machines) == 1  for f in Tasks))\n\n    # capacity requirement\n    for r in Resources:\n        for m in Machines:\n            for t in range(N):\n                M.addConstr( quicksum( quicksum(x[(f,m,t1)]  for t1 in range(max(0,t-D[f]+1),t+1) )*\n                               U[f][r] for f in Tasks) <= MC[m][r]) \n    M.setObjective(0, GRB.MINIMIZE)\n    M.setParam('PoolSearchMode', 2)\n    M.setParam('PoolSolutions', 100)\n#     M = M.presolve()\n#     M.update()\n    M.optimize()\n    \n    batch_sol_spos = []\n\n    if M.status in [GRB.Status.OPTIMAL]:\n        try:\n            for i in range(M.SolCount):\n                M.setParam('SolutionNumber', i)\n                sol = np.zeros(N)\n\n                task_on = np.zeros( (nbTasks,nbMachines,N) )\n                for ((f,m,t),var) in x.items():\n                    try:\n                        task_on[f,m,t] = var.Xn\n                    except AttributeError:\n                        raise\n\n                for t in range(N):        \n                    sol[t] = np.sum( np.sum(task_on[f,:,max(0,t-D[f]+1):t+1])*P[f] for f in Tasks )  \n                sol = sol*q/60 \n                batch_sol_spos.append(sol)\n            return batch_sol_spos\n        except NameError:\n                print(\"\\n__________Something wrong_______ \\n \")\n                raise\n\n\nclass SolveICON:\n    # nbMachines: number of machine\n    # nbTasks: number of task\n    # nb resources: number of resources\n    # MC[m][r] resource capacity of machine m for resource r \n    # U[f][r] resource use of task f for resource r\n    # D[f] duration of tasks f\n    # E[f] earliest start of task f\n    # L[f] latest end of task f\n    # P[f] power use of tasks f\n    # idle[m] idle cost of server m\n    # up[m] startup cost of server m\n    # down[m] shut-down cost of server m\n    # q time resolution\n    # timelimit in seconds\n    def __init__(self,nbMachines,nbTasks,nbResources,MC,U,D,E,L,P,idle,up,down,q,\n        relax=True,\n        verbose=False,method=-1,**h):\n        self.nbMachines  = nbMachines\n        self.nbTasks = nbTasks\n        self.nbResources = nbResources\n        self.MC = MC\n        self.U =  U\n        self.D = D\n        self.E = E\n        self.L = L\n        self.P = P\n        self.idle = idle\n        self.up = up\n        self.down = down\n        self.q= q\n        self.relax = relax\n        self.verbose = verbose\n        self.method = method\n\n       \n        \n    def make_model(self):\n        Machines = range(self.nbMachines)\n        Tasks = range(self.nbTasks)\n        Resources = range(self.nbResources)\n\n        MC = self.MC\n        U =  self.U\n        D = self.D\n        E = self.E\n        L = self.L\n        P = self.P\n        idle = self.idle\n        up = self.up\n        down = self.down\n        relax = self.relax\n        q= self.q\n        N = 1440//q\n\n        M = Model(\"icon\")\n        if not self.verbose:\n            M.setParam('OutputFlag', 0)\n        if relax:\n            x = M.addVars(Tasks, Machines, range(N), lb=0., ub=1., vtype=GRB.CONTINUOUS, name=\"x\")\n        else:\n            x = M.addVars(Tasks, Machines, range(N), vtype=GRB.BINARY, name=\"x\")\n\n\n        M.addConstrs( x.sum(f,'*',range(E[f])) == 0 for f in Tasks)\n        M.addConstrs( x.sum(f,'*',range(L[f]-D[f]+1,N)) == 0 for f in Tasks)\n        M.addConstrs(( quicksum(x[(f,m,t)] for t in range(N) for m in Machines) == 1  for f in Tasks))\n\n        # capacity requirement\n        for r in Resources:\n            for m in Machines:\n                for t in range(N):\n                    M.addConstr( quicksum( quicksum(x[(f,m,t1)]  for t1 in range(max(0,t-D[f]+1),t+1) )*\n                                   U[f][r] for f in Tasks) <= MC[m][r])   \n        # M = M.presolve()\n        M.update()\n        self.model = M\n\n        self.x = dict()\n        for var in M.getVars():\n            name = var.varName\n            if name.startswith('x['):\n                (f,m,t) = map(int, name[2:-1].split(','))\n                self.x[(f,m,t)] = var\n\n    def solve_model(self,price,timelimit=None):\n        Model = self.model\n        MC = self.MC\n        U =  self.U\n        D = self.D\n        E = self.E\n        L = self.L\n        P = self.P\n        idle = self.idle\n        up = self.up\n        down = self.down\n        q= self.q\n        N = 1440//q  \n\n        verbose = self.verbose\n        x =  self.x\n        nbMachines = self.nbMachines\n        nbTasks = self.nbTasks\n        nbResources = self.nbResources\n        Machines = range(nbMachines)\n        Tasks = range(nbTasks)\n        Resources = range(nbResources)\n        obj_expr = quicksum( [x[(f,m,t)]*sum(price[t:t+D[f]])*P[f]*q/60 \n            for f in Tasks for t in range(N-D[f]+1) for m in Machines if (f,m,t) in x] )\n        \n        Model.setObjective(obj_expr, GRB.MINIMIZE)\n        #Model.setObjective( quicksum( (x[(f,m,t)]*P[f]*quicksum([price[t+i] for i in range(D[f])])*q/60) for f in Tasks\n        #                for m in Machines for t in range(N-D[f]+1)), GRB.MINIMIZE)\n        if timelimit:\n            Model.setParam('TimeLimit', timelimit)\n        #if relax:\n        #    Model = Model.relax()\n        Model.setParam('Method', self.method)\n        #logging.info(\"Number of constraints%d\",Model.NumConstrs)\n        Model.optimize()\n        \n        solver = np.zeros(N)\n        if Model.status in [GRB.Status.OPTIMAL]:\n            try:\n                #task_on = Model.getAttr('x',x)\n                task_on = np.zeros( (nbTasks,nbMachines,N) )\n                for ((f,m,t),var) in x.items():\n                    try:\n                        task_on[f,m,t] = var.X\n                    except AttributeError:\n                        task_on[f,m,t] = 0.\n                        print(\"AttributeError: b' Unable to retrieve attribute 'X'\")\n                        print(\"__________Something WRONG___________________________\")\n\n\n                if verbose:\n                    \n                    print('\\nCost: %g' % Model.objVal)\n                    print('\\nExecution Time: %f' %Model.Runtime)\n                    print(\"where the variables is one: \",np.argwhere(task_on==1))\n                for t in range(N):        \n                    solver[t] = sum( np.sum(task_on[f,:,max(0,t-D[f]+1):t+1])*P[f] for f in Tasks ) \n                solver = solver*q/60\n                self.model.reset(0)  \n                return solver\n            except NameError:\n                print(\"\\n__________Something wrong_______ \\n \")\n                # make sure cut is removed! (modifies model)\n                self.model.reset(0)\n                \n                return solver\n\n        elif Model.status == GRB.Status.INF_OR_UNBD:\n            print('Model is infeasible or unbounded')\n        elif Model.status == GRB.Status.INFEASIBLE:\n            print('Model is infeasible')\n        elif Model.status == GRB.Status.UNBOUNDED:\n            print('Model is unbounded')\n        else:\n            print('Optimization ended with status %d' % Model.status)\n        self.model.reset(0)\n\n        return solver\n\n# ==========================================\n# File: EnergyExperiment/PO_model_energy.py\n# Function/Context: CachingPO\n# ==========================================\nimport os\nimport torch \nfrom torch import nn, optim\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nimport numpy as np\nimport gurobipy as gp\nfrom ICON_solving import *\nfrom get_energy import get_energy\n\ndef MakeLpMat(nbMachines,nbTasks,nbResources,MC,U,D,E,L,P,idle,up,down,q,**h):\n    # nbMachines: number of machine\n    # nbTasks: number of task\n    # nb resources: number of resources\n    # MC[m][r] resource capacity of machine m for resource r \n    # U[f][r] resource use of task f for resource r\n    # D[f] duration of tasks f\n    # E[f] earliest start of task f\n    # L[f] latest end of task f\n    # P[f] power use of tasks f\n    # idle[m] idle cost of server m\n    # up[m] startup cost of server m\n    # down[m] shut-down cost of server m\n    # q time resolution\n    # timelimit in seconds\n    \"\"\"\n    G1: rows: n_machine * Time; cols: n_task*n_machine* Time\n        first T row for machine1, next T: (2T) for machine 2 and so on\n        first n_task column for task 1 of machine 1 in time slot 0 then for task 1 machine 2 and so on\n    x: decisiion variable-vector of n_task*n_machine* Time. x[  f*(n_task*n_machine* Time)+m*(n_machine* Time)+Time ]=1 if task f starts at time t on machine m.\n    A1: To ensure each task is scheduled only once.\n    A2: To respect early start time\n    A3: To respect late start time\n    F: rows:Time , cols: n_task*n_machine* Time, bookkeping for power power use for each time unit\n    Code is written assuming nb resources=1\n    \"\"\"\n    Machines = range(nbMachines)\n    Tasks = range(nbTasks)\n    Resources = range(nbResources)\n    N = 1440//q\n\n    ### G and h\n    G1 = torch.zeros((nbMachines*N,nbTasks*nbMachines*N)).float()\n    h1 = torch.zeros(nbMachines*N).float()\n    F = torch.zeros((N,nbTasks*nbMachines*N)).float()\n    for m in Machines:\n        for t in range(N):\n            ## in all of our problem, we have only one resource\n            h1[m*N+t] = MC[m][0]\n            for f in Tasks:\n                c_index = (f*nbMachines+m)*N \n                G1[t + m*N, (c_index+max(0,t-D[f]+1)):(c_index+(t+1))] = U[f][0]\n                F [t,(c_index+max(0,t-D[f]+1)):(c_index+(t+1))  ] = P[f]\n\n    G2 = torch.eye((nbTasks*nbMachines*N))\n    G3 = -1*torch.eye((nbTasks*nbMachines*N))\n    h2 = torch.ones(nbTasks*nbMachines*N)\n    h3 = torch.zeros(nbTasks*nbMachines*N)\n\n    G = G1 # torch.cat((G1,G2,G3)) \n    h = h1 # torch.cat((h1,h2,h3))\n    ### A and b\n    A1 = torch.zeros((nbTasks, nbTasks*nbMachines*N)).float()\n    A2 = torch.zeros((nbTasks, nbTasks*nbMachines*N)).float()\n    A3 = torch.zeros((nbTasks, nbTasks*nbMachines*N)).float()\n\n    for f in Tasks:\n        A1 [f,(f*N*nbMachines):((f+1)*N*nbMachines) ] = 1\n        for m in Machines:\n            start_index = f*N*nbMachines + m*N # Time 0 for task f machine m\n            ## early start time\n            A2 [f,start_index:( start_index + E[f]) ] = 1\n            ## latest end time\n            A3 [f,(start_index+L[f]-D[f]+1):(start_index+N) ] = 1\n    b = torch.cat((torch.ones(nbTasks),torch.zeros(2*nbTasks)))\n    A = torch.cat((A1,A2,A3))    \n    return A,b,G,h,torch.transpose(F, 0, 1)\n\ndef IconMatrixsolver(A,b,G,h,F,y):\n    '''\n    A,b,G,h define the problem\n    y: the price of each hour\n    Multiply y with F to reach the granularity of x\n    x is the solution vector for each hour for each machine for each task \n    '''\n    n = A.shape[1]\n    m = gp.Model(\"matrix1\")\n    x = m.addMVar(shape=n, vtype=GRB.BINARY, name=\"x\")\n\n    m.addConstr(A @ x == b, name=\"eq\")\n    m.addConstr(G @ x <= h, name=\"ineq\")\n    c  = np.matmul(F,y).squeeze()\n    m.setObjective(c @ x, GRB.MINIMIZE)\n    m.optimize()\n    if m.status==2:\n        return x.X\n\n\ndef batch_solve(param,y,relax=False):\n    '''\n    wrapper around te solver to return solution of a vector of cost coefficients\n    '''\n    clf =  SolveICON(relax=relax,**param)\n    clf.make_model()\n    sol = []\n    for i in range(len(y)):\n        sol.append( clf.solve_model(y[i]))\n    return np.array(sol)\n\ndef regret_fn(y_hat,y, sol_true,param, minimize=True):\n    '''\n    computes average regret given a predicted cost vector and the true solution vector and the true cost vector\n    y_hat,y, sol_true are torch tensors\n    '''\n    mm = 1 if minimize else -1    \n    sol_hat = torch.from_numpy(batch_solve(param,y_hat.detach().numpy()))\n    return  ((mm*(sol_hat - sol_true)*y).sum(1)/(sol_true*y).sum(1)).mean()\n\ndef regret_aslist(y_hat,y, sol_true,param, minimize=True): \n    '''\n    computes regret of more than one cost vectors\n    ''' \n    mm = 1 if minimize else -1    \n    sol_hat = torch.from_numpy(batch_solve(param,y_hat.detach().numpy()))\n    return  ((mm*(sol_hat - sol_true)*y).sum(1)/(sol_true*y).sum(1))\n\n### function to grow the cache\ndef growcache(cache, y_hat,param):\n    '''\n    cache is torch array [currentpoolsize,48]\n    y_hat is  torch array [batch_size,48]\n    '''\n    sol = batch_solve(param,y_hat)\n    cache_np = cache.detach().numpy()\n    cache_np = np.unique(np.append(cache_np,sol,axis=0),axis=0)\n    # torch has no unique function, so we have to do this\n    return torch.from_numpy(cache_np).float()\n\n\nclass CachingPO(twostage_regression):\n\n# ==========================================\n# File: EnergyExperiment/test_energy_Combinedregression.py\n# Function/Context: \n# ==========================================\nimport pytorch_lightning as pl\nfrom PO_model import *\nfrom run_energy import *\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom get_energy import get_energy\nimport shutil\n###################################### Hyperparams #########################################\nlr, margin = 0.1, 100\ngrowth =1.\n\nif __name__ == \"__main__\":\n    ### training data prep\n    x_train, y_train, x_test, y_test = get_energy(fname= 'prices2013.dat')\n    x_train = x_train[:,1:]\n    x_test = x_test[:,1:]\n    scaler = StandardScaler()\n    x_train = scaler.fit_transform(x_train)\n    x_test = scaler.transform(x_test)\n    x_train = x_train.reshape(-1,48,x_train.shape[1])\n    y_train = y_train.reshape(-1,48)\n    x_test = x_test.reshape(-1,48,x_test.shape[1])\n    y_test = y_test.reshape(-1,48)\n    x = np.concatenate((x_train, x_test), axis=0)\n    y = np.concatenate((y_train,y_test), axis=0)\n    x,y = sklearn.utils.shuffle(x,y,random_state=0)\n    x_train, y_train = x[:552], y[:552]\n    x_valid, y_valid = x[552:652], y[552:652]\n    x_test, y_test = x[652:], y[652:]\n    print(x_train.shape, x_valid.shape, x_test.shape)\n    test_df = EnergyDatasetWrapper( x_test,y_test,param, relax=False)\n    test_dl = data_utils.DataLoader(test_df, batch_size= 24)\n\n    train_df = EnergyDatasetWrapper( x_train,y_train,param, relax=False)\n\n    train_dl = data_utils.DataLoader(train_df, batch_size=len(y))\n    for x_train,y_train,sol_train in train_dl:\n        solpool_np = np.unique(sol_train.detach().numpy(), axis=0)\n        solpool =  torch.from_numpy(solpool_np).float()\n\n    train_dl = DataLoader(train_df, batch_size= 24)\n\n    valid_df = EnergyDatasetWrapper( x_valid,y_valid,param, relax=False)\n    valid_dl = data_utils.DataLoader(valid_df, batch_size= 24)\n\n    test_df = EnergyDatasetWrapper( x_test,y_test,param, relax=False)\n    test_dl = data_utils.DataLoader(test_df, batch_size= 24)\n\n\n\n    \n    for repeat in range(1):\n        for alpha in [0.2,0.4,0.6,0.8]:\n            shutil.rmtree('ckpt_dir/CO/Energy1/',ignore_errors=True)\n            checkpoint_callback = ModelCheckpoint(\n                monitor=\"val_regret\",\n                dirpath=\"ckpt_dir/CO/Energy1/\",\n                filename=\"model-{epoch:02d}-{val_loss:.2f}\",\n                save_top_k=2,save_last = True,\n                mode=\"min\",\n            )\n            trainer = pl.Trainer(max_epochs= 20, callbacks=[checkpoint_callback], min_epochs=4)\n            model = CombinedPO(alpha=alpha,loss_fn =pairwise_loss,regret_fn= regret_fn,\n            solpool=solpool,growpool_fn =growpool_fn,growth=growth,lr= lr,margin = margin)\n\n\n            trainer.fit(model, train_dl, valid_dl)\n            best_model_path = checkpoint_callback.best_model_path\n            model =CombinedPO.load_from_checkpoint(best_model_path ,\n               alpha=alpha,loss_fn =pairwise_loss,regret_fn= regret_fn,\n            solpool=solpool,growpool_fn =growpool_fn,growth=growth,lr= lr,margin = margin)\n\n\n            result = trainer.test(dataloaders=test_dl)\n            df = pd.DataFrame(result)\n            df ['model'] = 'Pairwise(diff)'\n            df['growth'] = growth\n            df['lr'] = lr\n            df['margin'] = margin\n            df['alpha'] = alpha\n            \n            with open(\"Energy2COwiseRslt.csv\", 'a') as f:\n                df.to_csv(f, header=f.tell()==0)\n\n# ==========================================\n# File: EnergyExperiment/test_energy_NCE.py\n# Function/Context: \n# ==========================================\nimport torch\nimport torch.nn as nn\nimport pytorch_lightning as pl\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import DataLoader\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning import loggers as pl_loggers\nfrom sklearn.preprocessing import StandardScaler\nimport shutil\nimport random\nimport os\nfrom tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n\n# Key imports for optimization logic\nfrom PO_model_energy import *\nfrom get_energy import get_energy\n\n# Optimization model parameters\nload = 2\nparam = data_reading(\"EnergyCost/load{}/day01.txt\".format(load))\n\n# Seed setup for reproducibility\ndef seed_all(seed):\n    print(\"[ Using Seed : \", seed, \" ]\")\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\n# Data preparation\nx_train, y_train, x_test, y_test = get_energy(fname='prices2013.dat')\nx_train = x_train[:,1:]\nx_test = x_test[:,1:]\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\nx_train = x_train.reshape(-1,48,x_train.shape[1])\ny_train = y_train.reshape(-1,48)\nsol_train = batch_solve(param, y_train, relax=False)\nx_test = x_test.reshape(-1,48,x_test.shape[1])\ny_test = y_test.reshape(-1,48)\nsol_test = batch_solve(param, y_test, relax=False)\n\n# Cache initialization with true optimal solutions (Algorithm Step 1)\ncache_np = np.unique(sol_train, axis=0)\ncache = torch.from_numpy(cache_np).float()\n\n# Model initialization with caching mechanism\nmodel = CachingPO(\n    param=param,\n    net=nn.Linear(8,1),\n    loss_fn=NCE_hatc_c,\n    init_cache=cache,\n    growth=0.1,\n    lr=0.5,\n    seed=seed,\n    max_epochs=30\n)\n\n# Training setup with PyTorch Lightning\ntrainer = pl.Trainer(\n    max_epochs=30,\n    callbacks=[checkpoint_callback],\n    min_epochs=1,\n    logger=tb_logger\n)\n\n# Training execution (implements Algorithm Steps 2-3)\ntrainer.fit(model, train_dl, valid_dl)\n\n# Prediction and regret computation\ny_pred = model(torch.from_numpy(x_test).float()).squeeze()\nregret_list = regret_aslist(\n    y_pred,\n    torch.from_numpy(y_test).float(),\n    torch.from_numpy(sol_test).float(),\n    param,\n    minimize=True\n)\n\n# ==========================================\n# File: ShortestPathExperiment/PO_modelsSP.py\n# Function/Context: shortestpath_solver\n# ==========================================\nimport os\nimport torch \nfrom torch import nn, optim\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport pytorch_lightning as pl\nimport numpy as np\nimport networkx as nx\nimport gurobipy as gp\n\n##################################### Graph Structure ###################################################\nV = range(25)\nE = []\n\nfor i in V:\n    if (i+1)%5 !=0:\n        E.append((i,i+1))\n    if i+5<25:\n            E.append((i,i+5))\n\nG = nx.DiGraph()\nG.add_nodes_from(V)\nG.add_edges_from(E)\n\n###################################### Gurobi Shortest path Solver #########################################\nclass shortestpath_solver:\n    def __init__(self,G= G):\n        self.G = G\n    \n    def shortest_pathsolution(self, y):\n        '''\n        y the vector of  edge weight\n        '''\n        A = nx.incidence_matrix(G,oriented=True).todense()\n        b =  np.zeros(len(A))\n        b[0] = -1\n        b[-1] =1\n        model = gp.Model()\n        model.setParam('OutputFlag', 0)\n        x = model.addMVar(shape=A.shape[1], vtype=gp.GRB.BINARY, name=\"x\")\n        model.setObjective(y @x, gp.GRB.MINIMIZE)\n        model.addConstr(A @ x == b, name=\"eq\")\n        model.optimize()\n        if model.status==2:\n            return x.x\n    def solution_fromtorch(self,y_torch):\n        return torch.from_numpy(self.shortest_pathsolution( y_torch.detach().numpy())).float()\nspsolver =  shortestpath_solver()",
  "description": "Combined Analysis:\n- [BipartiteMatchingExperiment/PO_model_matching.py]: This file implements the core caching algorithm from the paper. The CachingPO class maintains a solution cache that grows probabilistically during training (growth parameter controls p_solve). In each training step, with probability 'growth', it solves the optimization problem using predicted parameters y_hat and adds new solutions to the cache via growcache(). The loss function (e.g., Listnet_loss) computes gradients using cached solutions rather than solving the optimization problem directly. This matches the algorithm's key steps: 1) Initialize cache with true solutions (via init_cache), 2) Probabilistic cache growth, 3) Loss computation over cached solutions. The implementation supports various contrastive losses (pointwise, pairwise, listwise) as specified in the paper.\n- [BipartiteMatchingExperiment/bipartite.py]: This file directly implements the core optimization model for bipartite matching with diversity constraints. The bmatching() function solves the standard bipartite matching problem using OR-Tools' LinearSumAssignment, while bmatching_diverse() implements the constrained version with similarity and diversity requirements using OR-Tools' linear solver. The get_qpt_matrices() function provides constraint matrices for QP transformations. These functions correspond to the optimization solver component in the algorithm steps (step 2c), where the solver finds optimal solutions v for given cost vectors č.\n- [BipartiteMatchingExperiment/test_matching_Combinedregression.py]: This file implements the core caching algorithm from the paper. The CombinedPO class (imported from matching_models) implements all key algorithm steps: 1) Initializes solution cache S with true optimal solutions from training data (via solpool initialization). 2) For each training instance, computes predicted cost vector ĉ via forward pass. 3) Uses loss_fn (Listnet_loss) to compute gradients. 4) Manages cache growth via growpool_fn with probability controlled by growth parameter. 5) Updates model parameters via backpropagation. The training loop is managed by PyTorch Lightning, where each training_step in CombinedPO executes steps 2a-2e of the algorithm. The implementation matches the paper's description of caching solutions and using them during training to avoid frequent solver calls.\n- [BipartiteMatchingExperiment/test_matching_Listwise.py]: This file implements the core algorithm steps from the paper. It initializes a solution cache using batch_solve() on training data (Step 1), then trains a CachingPO model which implements the caching mechanism (Steps 2a-e). The model uses Listnet_loss (listwise ranking loss) and grows the cache during training with probability 'growth'. The training loop is managed by PyTorch Lightning's Trainer, which calls the CachingPO's training_step method (not shown here but imported from PO_model_matching) that implements the core logic of: 1) predicting costs, 2) optionally transforming them, 3) solving with probability p_solve or using cache, 4) computing gradient of loss. The file also evaluates regret using regret_aslist() which computes the difference between predicted and true optimal solutions.\n- [EnergyExperiment/ICON_solving.py]: This file implements the core optimization model for the ICON (Integrated Container) scheduling problem, which is a specific instance of the predict-and-optimize framework. The SolveICON class directly corresponds to the optimization model min_{v∈V} f(v,c) where v represents the scheduling decisions (x variables for task assignments) and c is the price vector. The ICONSolutionPool function generates multiple feasible solutions for caching purposes, aligning with the solution caching strategy described in the algorithm. The mathematical model includes constraints for task scheduling windows, machine capacity requirements, and an objective function that minimizes total cost based on predicted electricity prices. This implementation serves as the optimization oracle required in step 2c of the algorithm.\n- [EnergyExperiment/PO_model_energy.py]: This file implements the core optimization model and caching algorithm for the energy scheduling problem described in the paper. The key components include: 1) MakeLpMat function that constructs the linear programming matrices for the energy scheduling optimization problem (mapping to the objective min_{v∈V} f(v,c)), 2) IconMatrixsolver that solves the optimization problem using Gurobi, 3) batch_solve wrapper for solving multiple instances, 4) regret computation functions, 5) growcache function that implements the solution caching mechanism (Algorithm Step 2c), and 6) CachingPO class that inherits from the base training framework. The code directly implements the mathematical optimization model for energy scheduling with constraints on machine capacities, task durations, and time windows, and provides the infrastructure for the caching-based training algorithm.\n- [EnergyExperiment/test_energy_Combinedregression.py]: This file implements the solution caching approach from the paper. Key aspects: 1) Initializes solution cache (solpool) from training data optimal solutions (Algorithm Step 1). 2) Uses CombinedPO model with solpool parameter for cached solutions. 3) Includes growpool_fn for dynamic cache expansion during training. 4) Uses pairwise ranking loss (pairwise_loss) for training. 5) Monitors validation regret during training. The code implements the core predict-and-optimize framework with solution caching to avoid repeated solving of NP-hard optimization problems.\n- [EnergyExperiment/test_energy_NCE.py]: This file implements the core predict-and-optimize pipeline with solution caching as described in the paper. Key matches: 1) Initializes solution cache with true optimal solutions from training data (sol_train via batch_solve). 2) Uses CachingPO model which implements the caching mechanism during training. 3) Employs NCE_hatc_c loss function (contrastive loss variant). 4) Computes regret between predicted and true optimal solutions. The CachingPO class (imported from PO_model_energy) contains the actual algorithm steps: predicting costs, cache lookup/solver calls, and gradient updates. The file orchestrates the complete experimental pipeline including data preparation, model training, and evaluation.\n- [ShortestPathExperiment/PO_modelsSP.py]: This file implements the core optimization model for the shortest path problem as described in the paper. The optimization model is defined as a linear program with binary variables representing edge selections, minimizing the total cost subject to flow conservation constraints. The `shortestpath_solver` class encapsulates the Gurobi solver to find the optimal path given edge weights. The graph structure is a 5x5 grid, and the incidence matrix formulation ensures a path from node 0 to node 24. This directly corresponds to the optimization model in the paper: minimizing f(v, c) = c^T v subject to v ∈ V (feasible paths). The solver is integrated with PyTorch for gradient-based learning, enabling the predict-and-optimize framework.",
  "dependencies": [
    "CoraDataset",
    "twostage_regression",
    "get_energy.get_energy",
    "growpool_fn",
    "DataLoader",
    "ICON_solving",
    "PO_model",
    "batch_solve",
    "tqdm.auto.tqdm",
    "sklearn",
    "run_energy",
    "ortools.graph.pywrapgraph",
    "solver (from matching_models)",
    "PO_model_matching",
    "pandas",
    "PO_model_energy",
    "get_energy",
    "pytorch_lightning",
    "matching_models (custom module containing CombinedPO class)",
    "bipartite.get_qpt_matrices",
    "NCE_hatc_c",
    "numpy",
    "EnergyDatasetWrapper",
    "CachingPO",
    "torch.utils.data",
    "torch",
    "torch.autograd",
    "pairwise_loss",
    "torch.nn.functional",
    "os",
    "growcache",
    "sklearn.preprocessing.StandardScaler",
    "math",
    "ModelCheckpoint",
    "Listnet_loss (from matching_models)",
    "bipartite.bmatching_diverse",
    "torch.nn",
    "CombinedPO",
    "growpool_fn (from matching_models)",
    "bipartite.get_cora",
    "regret_fn",
    "logging",
    "ortools.linear_solver.pywraplp",
    "pickle",
    "tensorboard",
    "sys",
    "networkx",
    "random",
    "copy",
    "regret_aslist",
    "Listnet_loss",
    "gurobipy",
    "shutil"
  ]
}