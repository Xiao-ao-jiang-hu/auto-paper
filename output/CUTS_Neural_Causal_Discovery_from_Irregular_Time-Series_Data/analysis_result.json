{
  "paper_id": "CUTS_Neural_Causal_Discovery_from_Irregular_Time-Series_Data",
  "title": "CUTS: Neural Causal Discovery from Irregular Time-Series Data",
  "abstract": "Causal discovery from time-series data has been a central task in machine learning. Recently, Granger causality inference is gaining momentum due to its good explainability and high compatibility with emerging deep neural networks. However, most existing methods assume structured input data and degenerate greatly when encountering data with randomly missing entries or non-uniform sampling frequencies, which hampers their applications in real scenarios. To address this issue, here we present CUTS, a neural Granger causal discovery algorithm to jointly impute unobserved data points and build causal graphs, via plugging in two mutually boosting modules in an iterative framework: (i) Latent data prediction stage: designs a Delayed Supervision Graph Neural Network (DSGNN) to hallucinate and register irregular data which might be of high dimension and with complex distribution; (ii) Causal graph fitting stage: builds a causal adjacency matrix with imputed data under sparse penalty. Experiments show that CUTS effectively infers causal graphs from irregular time-series data, with significantly superior performance to existing methods. Our approach constitutes a promising step towards applying causal discovery to real applications with non-ideal observations.",
  "problem_description_natural": "The paper addresses the problem of discovering causal relationships (in the form of a directed acyclic graph) from irregularly sampled or partially observed time-series data. Traditional Granger causality methods assume complete, uniformly sampled data, but real-world data often suffer from random missingness or non-uniform sampling frequencies. The authors propose an iterative framework that alternates between (1) imputing missing data using a graph neural network conditioned on the current estimate of the causal graph, and (2) refining the causal graph using the imputed data under a sparsity constraint. The goal is to simultaneously recover missing observations and infer a sparse, accurate causal structure that reflects true Granger-causal dependencies among time-series variables.",
  "problem_type": "Causal discovery / structural learning (non-convex optimization with sparsity constraints)",
  "datasets": [
    "VAR",
    "Lorenz-96",
    "NetSim"
  ],
  "performance_metrics": [
    "AUROC"
  ],
  "lp_model": {
    "objective": "$\\min_{\\boldsymbol{\\theta}} \\mathcal{L}_{pred}\\left(\\tilde{\\mathcal{X}}, \\hat{\\mathcal{X}}, \\mathcal{O}\\right) + \\lambda ||\\sigma(\\boldsymbol{\\theta})||_1$",
    "constraints": [],
    "variables": [
      "$\\theta_{\\tau,ij}$: parameters for causal probability from variable $i$ to $j$ at lag $\\tau$, with $m_{\\tau,ij} = \\sigma(\\theta_{\\tau,ij})$ representing the probability that $x_{t-\\tau,i}$ Granger causes $x_{t,j}$"
    ]
  },
  "raw_latex_model": "$$\\mathcal{L}_{pred}\\left(\\tilde{\\mathcal{X}}, \\hat{\\mathcal{X}}, \\mathcal{O}\\right) = \\sum_{i=1}^{N} \\frac{ \\langle \\mathcal{L}_2\\left(\\hat{\\boldsymbol{x}}_{1:L,i}, \\tilde{\\boldsymbol{x}}_{1:L,i}\\right), \\boldsymbol{o}_{1:L,i} \\rangle }{ \\frac{1}{L} \\langle \\boldsymbol{o}_{1:L,i}, \\boldsymbol{o}_{1:L,i} \\rangle }$$ $$\\mathcal{L}_{graph}\\left(\\tilde{\\mathcal{X}}, \\hat{\\mathcal{X}}, \\mathcal{O}, \\boldsymbol{\\theta}\\right) = \\mathcal{L}_{pred}\\left(\\tilde{\\mathcal{X}}, \\hat{\\mathcal{X}}, \\mathcal{O}\\right) + \\lambda ||\\sigma(\\boldsymbol{\\theta})||_1$$",
  "algorithm_description": "CUTS is a neural Granger causal discovery algorithm that iteratively alternates between two stages: (i) Latent data prediction stage using a Delayed Supervision Graph Neural Network (DSGNN) to impute missing data points, and (ii) Causal graph fitting stage that optimizes causal probability graphs with a sparsity penalty (L1 regularization). The process includes a fine-tuning stage where imputed data are used for supervision. The algorithm jointly learns data imputation and causal graph inference in a mutually boosting manner."
}