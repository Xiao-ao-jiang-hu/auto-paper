{
  "paper_id": "A_Deep_Instance_Generative_Framework_for_MILP_Solvers_Under_",
  "title": "A Deep Instance Generative Framework for MILP Solvers Under Limited Data Availability",
  "abstract": "In the past few years, there has been an explosive surge in the use of machine learning (ML) techniques to address combinatorial optimization (CO) problems, especially mixed-integer linear programs (MILPs). Despite the achievements, the limited availability of real-world instances often leads to sub-optimal decisions and biased solver assessments, which motivates a suite of synthetic MILP instance generation techniques. However, existing methods either rely heavily on expert-designed formulations or struggle to capture the rich features of real-world instances. To tackle this problem, we propose G2MILP, the first deep generative framework for MILP instances. Specifically, G2MILP represents MILP instances as bipartite graphs, and applies a masked variational autoencoder to iteratively corrupt and replace parts of the original graphs to generate new ones. The appealing feature of G2MILP is that it can learn to generate novel and realistic MILP instances without prior expert-designed formulations, while preserving the structures and computational hardness of real-world datasets, simultaneously. Thus the generated instances can facilitate downstream tasks for enhancing MILP solvers under limited data availability. We design a suite of benchmarks to evaluate the quality of the generated MILP instances. Experiments demonstrate that our method can produce instances that closely resemble real-world datasets in terms of both structures and computational hardness. The deliverables are released at https://miralab-ustc.github.io/L20-G2MILP.",
  "problem_description_natural": "The paper addresses the challenge of limited availability of real-world Mixed-Integer Linear Programming (MILP) instances, which hinders the development and evaluation of ML-enhanced MILP solvers. To overcome this, the authors propose a deep generative model, G2MILP, that learns from existing real-world MILP instances and generates new, realistic instances without requiring expert-designed problem formulations. The generated instances preserve both structural properties and computational hardness of the original data, enabling better training and testing of MILP solvers when real data is scarce.",
  "problem_type": "Mixed-Integer Linear Programming (MILP)",
  "datasets": [
    "Maximum Independent Set (MIS)",
    "Set Covering",
    "Mixed-integer Knapsack (MIK)",
    "MIPLIB 2017"
  ],
  "performance_metrics": [
    "Structural distributional similarity (Jensen-Shannon divergence)",
    "Solving time (seconds)",
    "Number of branching nodes",
    "Mean squared error (MSE) for optimal value prediction",
    "Relative MSE",
    "Performance improvement (%)"
  ],
  "lp_model": {
    "objective": "\\min_{\\boldsymbol{x} \\in \\mathbb{R}^n} \\boldsymbol{c}^\\top \\boldsymbol{x}",
    "constraints": [
      "\\boldsymbol{A}\\boldsymbol{x} \\leq \\boldsymbol{b}",
      "\\boldsymbol{l} \\leq \\boldsymbol{x} \\leq \\boldsymbol{u}",
      "x_j \\in \\mathbb{Z}, \\quad \\forall j \\in \\mathcal{I}"
    ],
    "variables": [
      "\\boldsymbol{x} \\in \\mathbb{R}^n \\text{ with } x_j \\in \\mathbb{Z} \\text{ for } j \\in \\mathcal{I}, \\text{ where } \\mathcal{I} \\subset \\{1, 2, \\cdots, n\\}"
    ]
  },
  "raw_latex_model": "\\min_{\\boldsymbol{x} \\in \\mathbb{R}^n} \\boldsymbol{c}^\\top \\boldsymbol{x}, \\quad \\text{s.t.} \\, \\boldsymbol{A}\\boldsymbol{x} \\leq \\boldsymbol{b}, \\, \\boldsymbol{l} \\leq \\boldsymbol{x} \\leq \\boldsymbol{u}, \\, x_j \\in \\mathbb{Z}, \\, \\forall j \\in \\mathcal{I},",
  "algorithm_description": "G2MILP is a deep generative framework for MILP instances based on a masked variational autoencoder (VAE) paradigm. The algorithm proceeds as follows: 1. Data Representation: Represent each MILP instance as a weighted bipartite graph where constraint vertices correspond to constraints, variable vertices correspond to variables, and edges represent non-zero coefficients with weights. 2. Masking Process: During training, uniformly sample a constraint vertex, mask it and its adjacent edges by replacing it with a [mask] token and adding virtual edges to all variable vertices. 3. Encoding: Use a graph neural network (GNN) to obtain node representations, then apply networks to compute mean and log variance for each vertex, and sample latent vectors from a Gaussian distribution. 4. Decoding: Reconstruct the masked constraint using four cooperative modules: (a) Bias Predictor predicts the bias term (right-hand side) from the masked vertex's representation and latent vector. (b) Degree Predictor predicts the degree (number of connected variables) similarly. (c) Logits Predictor predicts connection logits for each variable vertex using their representations and latent vectors, then selects top-k based on predicted degree. (d) Weights Predictor predicts edge weights (coefficients) for connected variables. 5. Training: Minimize the loss function comprising reconstruction loss (sum of losses from the four predictors) and KL divergence to regularize the latent space, using hyperparameters to balance terms. 6. Inference: Discard the encoder, sample latent vectors from a standard Gaussian distribution, and iteratively mask and replace constraint vertices in the graph for a specified number of iterations (controlled by masking ratio Î·) to generate new MILP instances."
}