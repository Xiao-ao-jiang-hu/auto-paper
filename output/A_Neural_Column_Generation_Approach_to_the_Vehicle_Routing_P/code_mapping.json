{
  "file_path": "bpp/model.py",
  "function_name": "RNNBinPacking",
  "code_snippet": "\n\n# ==========================================\n# File: bpp/model.py\n# Function/Context: RNNBinPacking\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef scaled_dot_product_attention(query, key, value, mask=None):\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) / d_k**0.5\n    \n    if mask is not None:\n        scores = scores.masked_fill(mask.unsqueeze(1), -1e9)\n        \n    attention_weights = torch.nn.functional.softmax(scores, dim=-1)\n    output = torch.matmul(attention_weights, value)\n    return output\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n        assert d_model % self.num_heads == 0\n        self.depth = d_model // self.num_heads\n\n        self.wq = nn.Linear(d_model, d_model)\n        self.wk = nn.Linear(d_model, d_model)\n        self.wv = nn.Linear(d_model, d_model)\n\n        self.fc = nn.Linear(d_model, d_model)\n\n    def split_heads(self, x, batch_size):\n        x = x.view(batch_size, -1, self.num_heads, self.depth)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n\n        query = self.wq(query)\n        key = self.wk(key)\n        value = self.wv(value)\n\n        query = self.split_heads(query, batch_size)\n        key = self.split_heads(key, batch_size)\n        value = self.split_heads(value, batch_size)\n\n        output = scaled_dot_product_attention(query, key, value, mask)\n        \n        output = output.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n        output = self.fc(output)\n        \n        return output\n\nclass PositionwiseFeedforward(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super(PositionwiseFeedforward, self).__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n\n    def forward(self, x):\n        x = F.relu(self.linear1(x))\n        x = self.linear2(x)\n        return x\n\nclass TransformerEncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff):\n        super(TransformerEncoderLayer, self).__init__()\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = PositionwiseFeedforward(d_model, d_ff)\n\n        self.ln1 = nn.LayerNorm(d_model)\n        self.ln2 = nn.LayerNorm(d_model)\n\n    def forward(self, x, mask=None):\n        attn_output = self.mha(x, x, x, mask)\n        x = self.ln1(x + attn_output)\n        ffn_output = self.ffn(x)\n        x = self.ln2(x + ffn_output)\n        return x\n    \nclass TransformerEncoder(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, num_layers):\n        super(TransformerEncoder, self).__init__()\n        self.layers = nn.ModuleList([\n            TransformerEncoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)\n        ])\n\n    def forward(self, x, mask=None):\n        for layer in self.layers:\n            x = layer(x, mask)\n        return x\n    \nclass RNNBinPacking(nn.Module):\n    def __init__(self, hidden_size, head_size, num_transformer_layers, num_gru_layers, num_fc_neurons, d_ff):\n        super(RNNBinPacking, self).__init__()\n        \n        self.embed = nn.Linear(2, hidden_size)\n\n        self.transformer_encoder = TransformerEncoder(\n            d_model=hidden_size, \n            num_heads=head_size, \n            d_ff=d_ff, \n            num_layers=num_transformer_layers\n        )\n\n        self.gru = nn.GRU(hidden_size, hidden_size, num_gru_layers, batch_first=True)\n        \n        self.fc = nn.Sequential(\n            nn.Linear(hidden_size, num_fc_neurons),\n            nn.ReLU(),\n            nn.Linear(num_fc_neurons, 1)\n        )\n        \n    def forward(self, x):\n        mask = (x[:, :, 2] == -1).to(x.device)\n\n        order = x[:, :, 2].long()\n        order = order.unsqueeze(1).expand(-1, order.size(-1), -1)  # Shape: (batch_size, sequence_length, sequence_length)\n        order_mask = (order != order.transpose(1, 2)).to(x.device)\n\n        x = self.embed(x[:, :, :2])\n\n        x = self.transformer_encoder(x, order_mask)\n\n        outputs, hn = self.gru(x)\n\n        # Fetching the last relevant output using the mask\n        lengths = (~mask).sum(1).long() - 1  # Minus 1 to get to 0-based index\n        last_outputs = outputs[torch.arange(x.size(0), device=x.device), lengths]\n\n        x = self.fc(last_outputs)\n\n        return x",
  "description": "Combined Analysis:\n- [bpp/model.py]: This file implements the machine learning model for predicting 2D bin packing feasibility with LIFO constraints, which is a critical component of Step 4 in the Neural Column Generation algorithm. The model uses attention mechanisms (TransformerEncoder) to handle homogeneous items within the same customer and recurrence mechanisms (GRU) to process heterogeneous items across customers. The forward method processes input sequences of items with dimensions and customer order information, generating a feasibility prediction. This directly corresponds to the ML model used for filtering candidate columns in the column generation process.",
  "dependencies": [
    "torch.nn",
    "torch.nn.functional",
    "PositionwiseFeedforward",
    "TransformerEncoderLayer",
    "TransformerEncoder",
    "MultiHeadAttention",
    "scaled_dot_product_attention",
    "torch"
  ]
}