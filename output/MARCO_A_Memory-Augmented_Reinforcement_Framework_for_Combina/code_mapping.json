{
  "file_path": "MIS/env/Env.py, MIS/env/memory.py, TSP/TSPTrainer.py, TSP/env/TSPEnv.py, TSP/env/TSPMemory.py, TSP/nets/TSPModel.py",
  "function_name": "Env, Memory, OperationMemory, select_memory, TSPTrainer, TSPEnv, Memory, TSPModel",
  "code_snippet": "\n\n# ==========================================\n# File: MIS/env/Env.py\n# Function/Context: Env\n# ==========================================\nfrom dataclasses import dataclass\nimport networkx as nx\nimport torch\nimport random\nfrom env.memory import select_memory\nfrom env.generators import RandomErdosRenyiGraphGenerator as Generator\n\n\n@dataclass\nclass State:\n    adj_matrix: torch.Tensor\n    solutions: torch.Tensor\n    masks: torch.Tensor\n    mem_info: torch.Tensor\n\n\nclass Env:\n    def __init__(self, env_params, memory_params, device):\n        self.env_params = env_params\n        self.memory_params = memory_params\n\n        # Env Params\n        self.n = env_params['problem_size']\n        self.initialization = env_params['initialization']\n        self.device = device\n        self.float_dtype = torch.float32\n        self.int_dtype = torch.int32\n        self.testing = False\n\n        self.batch_size = None\n        self.patience = None\n        self.patience_count = 0\n        self.iteration = 0\n        self.max_iterations = None\n\n        # Env Modules\n        self.generator = Generator(p_connection=0.15)\n        self.memory = None\n        self.state = None\n\n        # Env Records\n        self.fitness = None\n        self.reward = None\n        self.best_fitness_values = None\n        self.best_mean_fitness = None\n        self.non_improving_steps = None\n\n    def reset(self, batch_size, graphs=None, init_solution_seed=None):\n        \"\"\"\n        :param batch_size: (int) Num of instances in each batch (for training) or num of initializations (for testing)\n        :param graphs: (torch.Tensor) Adjacency matrix of the test graph to be solved (for testing). Shape: (batch_size, n, n)\n        :param init_solution_seed: (int) Seed for initializing solutions (for testing)\n        \"\"\"\n        self.batch_size = batch_size\n\n        if graphs is None:\n            self.testing = False\n            adj_matrix = self.generator.generate_graphs(self.n, batch_size)\n            adj_matrix = torch.tensor(adj_matrix, dtype=self.float_dtype).to(self.device)\n        else:\n            self.testing = True\n            self.n = graphs.shape[1]\n            adj_matrix = graphs.unsqueeze(0).to(self.device)\n\n        # Initialize memory\n        self.memory = select_memory(memory_type=self.memory_params['memory_type'],\n                                    mem_aggr=self.memory_params['mem_aggr'],\n                                    state_dim=self.n,\n                                    batch_size=self.batch_size,\n                                    testing=self.testing,\n                                    device=self.device)\n\n        # Initialize solutions\n        solutions = self.generate_batch_of_solutions(adj_matrix, init_solution_seed)\n\n        masks = self.create_action_mask(adj_matrix, solutions)\n\n        # Initialize memory info\n        mem_info = torch.zeros(batch_size, self.n, 2, dtype=self.float_dtype).to(self.device) if self.memory_params['memory_type'] != 'none' else None\n\n        self.state = State(adj_matrix=adj_matrix, solutions=solutions, masks=masks, mem_info=mem_info)\n\n        # Initialize environment records\n        self.fitness = self.compute_fitness()\n        self.best_fitness_values = self.fitness.clone()\n        self.best_mean_fitness = self.fitness.mean().item()\n        self.non_improving_steps = torch.zeros(batch_size, dtype=self.int_dtype).to(self.device)\n        self.iteration = 0\n\n        return self.state, False\n\n    def step(self, action):\n        \"\"\"\n        :param action: (torch.Tensor) Action(s) to be executed. Shape: (batch_size,)\n        \"\"\"\n        self.iteration += 1\n        # Save state(key) and action(value) in memory\n        sol = 2 * self.state.solutions - 1\n        self.memory.save_in_memory(sol, action)\n\n        # Update solutions based on actions\n        self.state.solutions[torch.arange(self.batch_size), action] = 1 - self.state.solutions[torch.arange(self.batch_size), action]\n\n        # Update masks\n        self.state.masks = self.create_action_mask(self.state.adj_matrix, self.state.solutions)\n\n        self.fitness = self.compute_fitness()\n\n        # Get k-nearest neighbors of current solutions\n        sol = 2 * self.state.solutions - 1\n        mem_info, revisited, avg_sim, max_sim = self.memory.get_knn(sol, k=self.memory_params['k'])\n\n        self.state.mem_info = mem_info\n\n        # COMPUTE REWARDS (only during training)\n        R = {}\n        if not self.testing:\n            # Main reward: improvement in fitness\n            self.reward = (self.fitness - self.best_fitness_values)\n            R['Fitness improvement'] = self.reward.mean().item()\n            # make zero the negative --> only positive rewards when improving the best found\n            self.reward[self.reward < 0] = 0\n\n            # Normalize reward\n            # self.reward /= self.n\n\n            # Add a small positive reward to those who are in local maxima and revisited states\n            R['Re-Visited'] = revisited.float().mean().item()\n\n            revisited_idx = revisited != 0\n\n            # Punish for visiting the same solution again\n            if (self.memory_params['memory_type'] != 'none') and (not self.testing):\n                self.reward[revisited_idx] -= self.env_params['revisit_punishment'] * revisited[revisited_idx]\n\n            R['Reward'] = self.reward\n\n            R['Avg similarity'] = avg_sim\n            R['Max similarity'] = max_sim\n\n        # Check if episode is done\n        done = (self.non_improving_steps.sum() >= (self.patience * self.batch_size)) or (self.iteration >= self.max_iterations)\n\n        # Update fitness records\n        self.non_improving_steps += 1\n        best_idx = self.fitness > self.best_fitness_values\n        self.best_fitness_values[best_idx] = self.fitness[best_idx]\n        self.non_improving_steps[best_idx] = 0\n\n        return self.state, R, done\n\n    def compute_fitness(self):\n        \"\"\"\n        :return: (torch.Tensor) Fitness of the current solutions. Shape: (batch_size,)\n        \"\"\"\n        #is_independent_set = self.is_independent_set_vectorized(self.state.solutions, self.state.adj_matrix)\n        #assert torch.all(is_independent_set)\n\n        # Calculate fitness as the number of nodes in the set\n        fitness = torch.sum(self.state.solutions, dim=1)\n\n        return fitness\n\n    def create_action_mask(self, adj_matrix, solutions):\n        \"\"\"\n        :param adj_matrix: (torch.Tensor) Adjacency matrix of the graph. Shape: (batch_size, n, n)\n        :param solutions: (torch.Tensor) Solutions of the graph. Shape: (batch_size, n)\n        :return: (torch.Tensor) Action mask. Shape: (batch_size, n)\n        \"\"\"\n        if self.testing:\n            # Reshape and transpose solutions for matrix multiplication\n            reshaped_solutions = solutions.unsqueeze(-1).transpose(1, 2)  # Shape: (batch_size, 1, n)\n\n            # Perform matrix multiplication adj_matrix will be automatically broadcasted to match the batch size\n            adjacent_mask = torch.matmul(reshaped_solutions, adj_matrix[0]).squeeze(1)  # Shape: (batch_size, n)\n\n            # Nodes that can't be added (any adjacent node is in the set)\n            action_mask = (adjacent_mask > 0) & (solutions == 0)\n\n        else:\n            # Expand solutions to match the adjacency matrix shape\n            expanded_solutions = solutions.unsqueeze(2)  # Shape: (batch_size, n, 1)\n\n            # Use batch matrix multiplication to find if any adjacent node is in the set\n            adjacent_mask = torch.bmm(adj_matrix, expanded_solutions).squeeze(2)  # Shape: (batch_size, n)\n\n            # Nodes that can't be added (any adjacent node is in the set)\n            action_mask = (adjacent_mask > 0) & (solutions == 0)\n\n        return action_mask.to(self.device)\n\n    def generate_batch_of_solutions(self, adj_matrix, seed=None):\n        \"\"\"\n        :param adj_matrix: (torch.Tensor) Adjacency matrix of the graph. Shape: (batch_size, n, n)\n        :param seed: (int) Seed for initializing solutions\n        :return: (torch.Tensor) Solutions of the graph. Shape: (batch_size, n)\n        \"\"\"\n        if seed is not None:\n            torch.manual_seed(seed)\n            random.seed(seed)\n\n        solutions = torch.zeros(self.batch_size, self.n, dtype=torch.float32).to(self.device)\n\n        if self.initialization == 'greedy':\n\n            # Precompute the neighbors for each node in each graph\n            if self.testing:\n                neighbors = [torch.nonzero(adj_matrix[0], as_tuple=False)]\n            else:\n                neighbors = [torch.nonzero(adj_matrix[b], as_tuple=False) for b in range(self.batch_size)]\n\n            for b in range(self.batch_size):\n                d = 0 if self.testing else b\n\n                available_nodes = set(range(self.n))\n                node_neighbors = neighbors[d]\n\n                while available_nodes:\n                    node = random.sample(list(available_nodes), 1)[0]\n\n                    # Vectorized check for independent set condition\n                    if not torch.any((adj_matrix[d, node] == 1) & (solutions[b] == 1)):\n                        solutions[b, node] = 1\n                        # Remove the node and its neighbors\n                        neighbor_nodes = node_neighbors[node_neighbors[:, 0] == node][:, 1]\n                        available_nodes -= {node, *neighbor_nodes.tolist()}\n                    else:\n                        available_nodes.remove(node)\n\n        elif self.initialization == 'greedy2':\n            # Precompute the neighbors for each node in each graph\n            if self.testing:\n                neighbors = [torch.nonzero(adj_matrix[0], as_tuple=False)]\n            else:\n                neighbors = [torch.nonzero(adj_matrix[b], as_tuple=False) for b in range(self.batch_size)]\n\n            for b in range(self.batch_size):\n                d = 0 if self.testing else b\n\n                if b==0:\n                    G = nx.from_numpy_array(adj_matrix[d].cpu().numpy())\n                    while 1:\n                        min_val = 2 * self.n\n                        node = None\n                        for n in G.nodes():\n                            if G.degree(n) < min_val and G.degree(n) != 0:\n                                min_val = G.degree(n)\n                                node = n\n                        if node is None:  # stop if all the nodes are in the independent set\n                            break\n                        solutions[b, node] = 1\n                        G.remove_nodes_from([n for n in G.neighbors(node)])\n                else:\n                    available_nodes = set(range(self.n))\n                    node_neighbors = neighbors[d]\n                    while available_nodes:\n                        node = random.sample(list(available_nodes), 1)[0]\n\n                        # Vectorized check for independent set condition\n                        if not torch.any((adj_matrix[d, node] == 1) & (solutions[b] == 1)):\n                            solutions[b, node] = 1\n                            # Remove the node and its neighbors\n                            neighbor_nodes = node_neighbors[node_neighbors[:, 0] == node][:, 1]\n                            available_nodes -= {node, *neighbor_nodes.tolist()}\n                        else:\n                            available_nodes.remove(node)\n\n        return solutions\n\n    @staticmethod\n    def is_independent_set_vectorized(solutions, adj_matrix):\n        \"\"\"\n        :param solutions: (torch.Tensor) Solution of the graph. Shape: (batch_size, n)\n        :param adj_matrix: (torch.Tensor) Adjacency matrix of the graph. Shape: (batch_size, n, n)\n        :return: (torch.Tensor) Whether the solutions are an independent set. Shape: (batch_size,)\n        \"\"\"\n        # Expand solutions to match the adjacency matrix shape for batch matrix multiplication\n        expanded_solutions = solutions.unsqueeze(2)  # Shape: (batch_size, n, 1)\n\n        # Use batch matrix multiplication to find if any adjacent node is in the set\n        adjacent_solution = torch.bmm(adj_matrix, expanded_solutions).squeeze(2)  # Shape: (batch_size, n)\n\n        # Check for each solution if there are no adjacent nodes in the set\n        # A node is part of the solution if it's marked as 1 in 'solutions'\n        # It's an independent set if none of these nodes have adjacent nodes also in the set\n        is_independent_set = torch.all((adjacent_solution <= 1) | (solutions == 0), dim=1)\n\n        return is_independent_set\n\n# ==========================================\n# File: MIS/env/memory.py\n# Function/Context: Memory, OperationMemory, select_memory\n# ==========================================\nimport numpy as np\nimport torch\n\n\nclass Memory:\n    def __init__(self, state_dim, memory_aggr, memory_size=10000, n_memories=1, device='cpu'):\n        \"\"\"\n        Memory that saves State-Action pairs and returns the average value of the k-nearest neighbours of a state\n        Key: Solution state of the problem\n        Value: One-hot encoding of the performed action\n        \"\"\"\n        self.state_dim = state_dim\n        self.memory_size = memory_size\n        self.memory_aggr = memory_aggr\n        self.n_memories = n_memories # number of memories to use. If 1, use a single shared memory for all the states\n        self.device = device\n\n        # Initialize memories and index\n        self.state_memories = [torch.zeros((0, self.state_dim)) for _ in range(self.n_memories)]\n        self.action_memories = [torch.zeros((0, self.state_dim, 2)) for _ in range(self.n_memories)]\n\n        self.used_memory = 0\n\n    def save_in_memory(self, state, action):\n        batch_size = state.shape[0]\n        batch_range = torch.arange(batch_size)\n\n        # Get the current values in solutions\n        cur_values = state[batch_range, action]\n\n        # Double the actions so that we can distinguish between 0->1 and 1->0\n        double_action = action.clone()\n        double_action[cur_values == 1] += self.state_dim\n\n        # Perform one-hot encoding\n        one_hot_actions = torch.nn.functional.one_hot(double_action, num_classes=2*self.state_dim).float()\n        one_hot_actions = one_hot_actions.view(batch_size, 2, self.state_dim)\n        one_hot_actions = one_hot_actions.transpose(1, 2).contiguous().view(batch_size, self.state_dim, 2)\n\n        #one_hot_actions = one_hot_actions.cpu()\n        #state = state.cpu()\n\n        for idx in range(self.n_memories):\n            # If memory is full, remove the oldest state\n            if len(self.state_memories[idx]) >= self.memory_size:\n                if self.n_memories == 1:\n                    self.state_memories[idx] = torch.roll(self.state_memories[idx], -batch_size, dims=0)\n                    self.state_memories[idx][-batch_size:] = state\n                    self.action_memories[idx] = torch.roll(self.action_memories[idx], -batch_size, dims=0)\n                    self.action_memories[idx][-batch_size:] = one_hot_actions\n                else:\n                    self.state_memories[idx] = torch.roll(self.state_memories[idx], -1, dims=0)\n                    self.state_memories[idx][-1] = state[idx]\n                    self.action_memories[idx] = torch.roll(self.action_memories[idx], -1, dims=0)\n                    self.action_memories[idx][-1] = one_hot_actions[idx].unsqueeze(0)\n            else:\n                if self.n_memories == 1:\n                    self.state_memories[idx] = torch.vstack([self.state_memories[idx], state])\n                    self.action_memories[idx] = torch.vstack([self.action_memories[idx], one_hot_actions])\n                else:\n                    self.state_memories[idx] = torch.vstack([self.state_memories[idx], state[idx]])\n                    self.action_memories[idx] = torch.vstack([self.action_memories[idx], one_hot_actions[idx].unsqueeze(0)])\n\n        if self.n_memories == 1:\n            self.used_memory += batch_size\n        else:\n            self.used_memory += 1\n\n    def get_knn(self, state, k):\n        # Get k nearest states\n        batch_size = state.shape[0]\n        state = state.float()\n        k = self.used_memory if k > self.used_memory else k\n\n        nearest_actions = torch.zeros((batch_size, self.state_dim, 2))\n\n        avg_similarity = np.zeros(batch_size)\n        max_similarity = np.zeros(batch_size)\n        revisited = torch.zeros(batch_size)\n        for idx in range(self.n_memories):\n            if self.n_memories != 1:  # Multiple memories\n                inner_products = torch.mm(state[idx:idx+1], self.state_memories[idx].t())  # Result is pomo_size * used_memory\n                similarity, indices = torch.topk(inner_products, k, largest=True, sorted=True)\n\n                #similarity, indices = self.index[idx].search(state[idx:idx+1], k)\n                # similarity.shape = (1, k) and indices.shape = (1, k)\n\n                revisited[idx] = (similarity == self.state_dim).sum()\n                avg_similarity[idx] = torch.mean(similarity)\n                max_similarity[idx] = similarity[:, 0]\n\n                nearest_acts = self.action_memories[idx][indices.flatten(), :].reshape(indices.shape + (self.state_dim, 2))\n                # nearest_acts.shape = (1, k, state_dim, 2)\n                # Aggregate among k neighbors: Weighted based on similarity.\n                if self.memory_aggr == 'sum':  # No weighting\n                    nearest_actions[idx] = torch.sum(nearest_acts, dim=1)\n                elif self.memory_aggr == 'linear':  # Linear weighted sum\n                    # similarity from [-N, N] --> [0, N] --> [0, 1]\n                    sim = (similarity + self.state_dim) / (2 * self.state_dim)\n                    nearest_actions[idx] = torch.sum(nearest_acts * sim[:, :, None, None], dim=1)\n                elif self.memory_aggr == 'exp':  # Exponential weighted sum\n                    sim = (similarity + self.state_dim) / (2 * self.state_dim)\n                    nearest_actions[idx] = torch.sum(nearest_acts * (torch.exp(torch.log(torch.tensor(2)) * (sim[:, :, None, None])) - 1), dim=1)\n                    # nearest_actions.shape = (batch_size, state_dim, 2)\n            else:  # Single memory\n                inner_products = torch.mm(state, self.state_memories[idx].t())  # Result is pomo_size * used_memory\n                similarity, indices = torch.topk(inner_products, k, largest=True, sorted=True)\n\n                #similarity, indices = self.index[idx].search(state, k)\n                # similarity.shape = (batch_size, k) and indices.shape = (batch_size, k)\n\n                revisited = (similarity == self.state_dim).sum(axis=1)\n                avg_similarity = torch.mean(similarity, dim=1)\n                max_similarity = similarity[:, 0]\n\n                nearest_acts = self.action_memories[idx][indices, :].reshape(indices.shape + (self.state_dim, 2))\n                # nearest_acts.shape = (batch_size, k, state_dim, 2)\n                # Aggregate among k neighbors: Weighted based on similarity.\n                if self.memory_aggr == 'sum':  # No weighting\n                    nearest_actions = torch.sum(nearest_acts, dim=1)\n                elif self.memory_aggr == 'linear':  # Linear weighted sum\n                    # similarity from [-N, N] --> [0, N] --> [0, 1]\n                    sim = (similarity + self.state_dim) / (2 * self.state_dim)\n                    nearest_actions = torch.sum(nearest_acts * sim[:, :, None, None], dim=1)\n                elif self.memory_aggr == 'exp':  # Exponential weighted sum\n                    sim = (similarity + self.state_dim) / (2 * self.state_dim)\n                    nearest_actions = torch.sum(nearest_acts * (torch.exp(torch.log(torch.tensor(2)) * sim[:, :, None, None]) - 1), dim=1)\n                    # nearest_actions.shape = (batch_size, state_dim, 2)\n\n        return nearest_actions, revisited, avg_similarity, max_similarity\n\n\nclass OperationMemory:\n    def __init__(self, mem_type, state_dim, batch_size, memory_size=10000, device='cpu'):\n        self.mem_type = mem_type\n        self.state_dim = state_dim\n        self.batch_size = batch_size\n        self.batch_range = torch.arange(batch_size)\n        self.memory_size = memory_size\n        self.device = device\n\n        self.last_changed = -1 * torch.ones(batch_size, 2*state_dim)\n        self.state_memories = [torch.zeros((0, self.state_dim)) for _ in range(batch_size)]\n\n        self.used_memory = 0\n\n    def save_in_memory(self, state, action):\n        batch_size = state.shape[0]\n        batch_range = torch.arange(batch_size)\n\n        # Get the current values in solutions\n        cur_values = state[batch_range, action]\n\n        # Double the actions so that we can distinguish between 0->1 and 1->0\n        double_action = action.clone()\n        double_action[cur_values == 1] += self.state_dim\n\n        double_action = double_action.cpu()\n        state = state.cpu()\n\n        for idx in range(self.batch_size):\n            # If memory is full, remove the oldest state\n            if len(self.state_memories[idx]) >= self.memory_size:\n                self.state_memories[idx] = torch.roll(self.state_memories[idx], -1, dims=0)\n                self.state_memories[idx][-1] = state[idx]\n            else:\n                self.state_memories[idx] = torch.vstack([self.state_memories[idx], state[idx]])\n\n        # Update last changed\n        self.last_changed[self.last_changed != -1] += 1\n        self.last_changed[self.batch_range, double_action] = 1\n\n        # Update used memory\n        self.used_memory += 1\n\n    def get_knn(self, state, k):\n        # K must be used_memory or less\n        k = self.used_memory if k > self.used_memory else k\n\n        # Initialize tensors\n        revisited = torch.zeros(self.batch_size)\n        avg_similarity = np.zeros(self.batch_size)\n        max_similarity = np.zeros(self.batch_size)\n\n        # State to float\n        state = state.float()\n\n        # For each batch element, get the k nearest neighbors\n        for idx in range(self.batch_size):\n            # Get similarities and indices of the k nearest neighbors\n            inner_products = torch.mm(state[idx:idx + 1], self.state_memories[idx].t())  # Result is pomo_size * used_memory\n            similarity, indices = torch.topk(inner_products, k, largest=True, sorted=True)\n\n            # Check how many times the same state has been visited\n            revisited[idx] = (similarity == self.state_dim).sum()\n\n            # Compute average and max similarity\n            avg_similarity[idx] = torch.mean(similarity)\n            max_similarity[idx] = similarity[:, 0]\n\n        mem_info = None\n        if self.mem_type == 'op_based':  # Gather information about the last times each operation has been used\n            mem_info = self.last_changed.clone().view(self.batch_size, 2, self.state_dim)\n            mem_info = mem_info.transpose(1, 2).contiguous().view(self.batch_size, self.state_dim, 2)\n            mem_info = mem_info.to(self.device)\n\n        return mem_info, revisited, avg_similarity, max_similarity\n\n\ndef select_memory(memory_type, mem_aggr, state_dim, batch_size, testing, device):\n    if memory_type in ['op_based', 'none']:\n        memory = OperationMemory(mem_type=memory_type,\n                                 state_dim=state_dim,\n                                 batch_size=batch_size,\n                                 device=device)\n    else:\n        if memory_type == 'shared' and testing:\n            n_memories = 1\n        else:  # 'individual' or training\n            n_memories = batch_size\n\n        memory = Memory(state_dim=state_dim,\n                        memory_aggr=mem_aggr,\n                        n_memories=n_memories,\n                        device=device)\n    return memory\n\n# ==========================================\n# File: TSP/TSPTrainer.py\n# Function/Context: TSPTrainer\n# ==========================================\nimport torch\nfrom logging import getLogger\nimport pickle\nfrom env.TSPEnv import TSPEnv as Env\nfrom nets.TSPModel import TSPModel as Model\nfrom torch.optim import AdamW as Optimizer\nfrom torch.optim.lr_scheduler import MultiStepLR as Scheduler\nfrom utils import *\n\ntorch.set_float32_matmul_precision('high')\n\n\nclass TSPTrainer:\n    def __init__(self, env_params, model_params, optimizer_params, trainer_params):\n        # Save arguments\n        self.env_params = env_params\n        self.model_params = model_params\n        self.optimizer_params = optimizer_params\n        self.trainer_params = trainer_params\n\n        self.marco = self.env_params['memory_type'] != 'none'\n        if self.marco:\n            assert trainer_params['finetune']\n\n        # Result folder, logger\n        self.logger = getLogger(name='trainer')\n\n        # Device\n        device = torch.device('cuda') if (trainer_params['use_cuda'] and torch.cuda.is_available()) else torch.device('cpu')\n        self.device = device\n        torch.set_default_device(device)\n        torch.set_default_dtype(torch.float32)\n\n        # Main Components\n        self.model = Model(**self.model_params)\n        self.env = Env(**self.env_params)\n        self.optimizer = Optimizer(self.model.parameters(), **self.optimizer_params['optimizer'])\n        self.scheduler = Scheduler(self.optimizer, **self.optimizer_params['scheduler'])\n\n        # print number of params in model\n        self.logger.info('Number of Model Parameters: {}'.format(sum([p.numel() for p in self.model.parameters()])))\n\n        # Restore\n        self.start_epoch = 1\n        model_load = trainer_params['model_load']\n        if model_load['enable']:\n            path = model_load['path']\n            epoch = model_load['epoch']\n            checkpoint_fullname = f'{path}/checkpoint-{epoch}.pt'\n            checkpoint = torch.load(checkpoint_fullname, map_location=device)\n            self.model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n            self.logger.info('Saved Model Loaded !!')\n\n        if trainer_params['finetune']:\n            # Freeze all parameters in the model\n            for param in self.model.parameters():\n                param.requires_grad = False\n\n            # Unfreeze the parameters of self.model.decoder.mem_linear\n            for param in self.model.decoder.mem_FF.parameters():\n                param.requires_grad = True\n\n            # print the params that are being trained\n            self.logger.info('Number of Trainable Parameters: {}'.format(sum([p.numel() for p in self.model.parameters() if p.requires_grad])))\n\n        # Load test data\n        self.test_graphs = {}\n        self.opt_test_fitness = {}\n        for test_size, test_batch_size in zip(self.trainer_params['test_sizes'], self.trainer_params['test_batch_sizes']):\n            path = f\"../data/tsp/tsp{test_size}_test_concorde.pkl\"\n            graphs = pickle.load(open(path, 'rb'))\n            coord_m = np.array(graphs[:test_batch_size])\n            self.test_graphs[test_size] = torch.from_numpy(coord_m).to(device).float()\n            path = f\"../data/tsp/tsp{test_size}_test_concorde_costs.pkl\"\n            opt_score = pickle.load(open(path, 'rb'))\n            self.opt_test_fitness[test_size] = torch.tensor(opt_score[:test_batch_size])\n\n        # utility\n        self.time_estimator = TimeEstimator()\n        self.result_folder = get_result_folder()\n        self.result_log = LogData()\n\n    def _train_one_batch_marco(self, batch_size):\n        with torch.no_grad():\n            reset_state, _, _ = self.env.reset()\n            self.model.pre_forward(reset_state)\n\n            # Deterministic Rollout\n            bl_R, _ = self.rollout(batch_size, deterministic=True)\n            bl_reward = bl_R['reward'] / self.env.problem_size\n\n        # Sampling\n        avg_scores = []\n        avg_losses = []\n        avg_distance = []\n        for _ in range(4):\n            R, prob_list = self.rollout(batch_size, deterministic=False)\n\n            reward = R['reward'] / self.env.problem_size\n            #punishment = R['punishment'] / self.env.problem_size\n            # Score\n            max_pomo_reward, _ = reward.max(dim=1)  # get best results from pomo\n            score_mean = -max_pomo_reward.float().mean()  # negative sign to make positive value\n\n            avg_dist = 1 - R['avg_sim']\n            # avg_dist[avg_dist > 0.15] = 0.15\n            min_dist = 1 - R['max_sim']\n            # Objective = minimize cost, minimize similarity, improve previous cost\n            reward = reward - bl_reward\n            worse_idx = (reward < bl_reward)\n            if self.env_params['punish_type'] == 'avg':\n                advantage = reward + self.env_params['repeat_punishment'] * avg_dist\n            elif self.env_params['punish_type'] == 'max':\n                advantage = reward + self.env_params['repeat_punishment'] * min_dist\n            else:\n                raise NotImplementedError\n\n            #advantage[worse_idx] = 0\n\n            # Loss\n            log_prob = prob_list.log()\n            advantage = advantage[:, :, None].expand(-1, -1, log_prob.shape[2])\n            loss = -advantage * log_prob  # Minus Sign: To Increase REWARD\n\n            loss_mean = loss.mean()\n\n            # Step & Return\n            self.model.zero_grad()\n            loss_mean.backward(retain_graph=True)\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.trainer_params['max_grad_norm'])\n            self.optimizer.step()\n\n            avg_scores.append(score_mean.item())\n            avg_losses.append(loss_mean.item())\n            avg_distance.append(avg_dist.mean().item())\n        return np.mean(avg_scores), np.mean(avg_losses), np.mean(avg_distance)\n\n    def rollout(self, batch_size, deterministic):\n        prob_list = torch.zeros(size=(batch_size, self.env.pomo_size, 0))\n        state, reward, done = self.env.pre_step()\n        while not done:\n            # Get actions\n            selected, prob = self.model(state, deterministic)\n\n            # Step the environment\n            state, reward, done = self.env.step(selected)\n\n            if not deterministic:\n                prob_list = torch.cat((prob_list, prob[:, :, None]), dim=2)\n\n        return reward, prob_list\n\n# ==========================================\n# File: TSP/env/TSPEnv.py\n# Function/Context: TSPEnv\n# ==========================================\nfrom dataclasses import dataclass\nimport torch\nfrom env.TSPMemory import select_memory\nfrom env.utils import get_random_problems, tsp_coordinate_augmentations, get_distances_from_coords\n\n\n@dataclass\nclass Reset_State:\n    coords: torch.Tensor\n    # shape: (batch, node, 2)\n    dist: torch.Tensor\n    # shape: (batch, node, node)\n\n\n@dataclass\nclass Step_State:\n    BATCH_IDX: torch.Tensor\n    POMO_IDX: torch.Tensor\n    # shape: (batch, pomo)\n    current_node: torch.Tensor = None\n    # shape: (batch, pomo)\n    ninf_mask: torch.Tensor = None\n    # shape: (batch, pomo, node)\n    edge_solution: torch.Tensor = None\n    # shape: (batch, pomo, node, node)\n    edge_memory: torch.Tensor = None\n    # shape: (batch, pomo, node, node)\n\n\nclass TSPEnv:\n    def __init__(self, **env_params):\n        # Params\n        self.env_params = env_params\n        self.problem_size = env_params['problem_size']\n        self.pomo_size = env_params['pomo_size']\n\n        self.retrieve_freq = self.env_params['retrieve_freq']\n\n        # Static\n        self.batch_size = None\n        self.BATCH_IDX = None\n        self.POMO_IDX = None\n        self.coords = None  # shape: (batch, node, 2)\n        self.dist = None  # shape: (batch, node, node)\n\n        # Dynamic\n        self.step_state = None\n        self.selected_count = None\n        self.current_node = None  # shape: (batch, pomo)\n        self.selected_node_list = None  # shape: (batch, pomo, 0~problem)\n        self.punishments = []\n\n        # Memory\n        self.memory = None\n\n    def load_problems(self, batch_size, coords=None, aug_factor=1):\n        self.batch_size = batch_size\n\n        # Generate or load instances\n        if coords is None:\n            self.coords = get_random_problems(batch_size, self.problem_size)\n        else:\n            self.coords = coords  # Use loaded instances\n        # coords.shape: (batch, problem, 2)\n\n        # Compute distances\n        self.dist = get_distances_from_coords(self.coords)\n        # dist.shape: (batch, problem, problem)\n\n        # Augment data\n        if aug_factor > 1:\n            self.batch_size = self.batch_size * aug_factor\n            #self.coords = augment_xy_data_by_8_fold(self.coords)\n            self.coords = tsp_coordinate_augmentations(self.coords, aug_factor)\n            # shape: (aug_factor*batch, problem, 2)\n\n        self.BATCH_IDX = torch.arange(self.batch_size)[:, None].expand(self.batch_size, self.pomo_size)\n        self.POMO_IDX = torch.arange(self.pomo_size)[None, :].expand(self.batch_size, self.pomo_size)\n\n    def initialize_memory(self, testing, device):\n        # Initialize memory\n        self.memory = select_memory(mem_aggr=self.env_params['mem_aggr'],\n                                    state_dim=self.problem_size**2,\n                                    batch_size=self.batch_size,\n                                    device=device)\n        if (not testing) and self.env_params['memory_type'] == 'none':\n            assert self.env_params['repeat_punishment'] == 0, \"repeat_punishment is not 0 but memory_type is none\"\n\n    def reset(self):\n        # Create reset state\n        reset_state = Reset_State(coords=self.coords, dist=self.dist)\n        self.punishments = [torch.zeros(self.batch_size, self.pomo_size)]\n        reward = None\n        done = False\n        return reset_state, reward, done\n\n\n    def pre_step(self):\n        # Create dynamic variables\n        self.selected_count = 0\n        self.current_node = None\n        self.selected_node_list = torch.zeros((self.batch_size, self.pomo_size, 0), dtype=torch.long)\n        # shape: (batch, pomo, 0~problem)\n\n        # Create step state\n        self.step_state = Step_State(BATCH_IDX=self.BATCH_IDX, POMO_IDX=self.POMO_IDX)\n        self.step_state.ninf_mask = torch.zeros((self.batch_size, self.pomo_size, self.problem_size))\n        # shape: (batch, pomo, problem)\n        self.step_state.edge_solution = torch.zeros((self.batch_size, self.pomo_size, self.problem_size, self.problem_size), dtype=torch.long)\n        # shape: (batch, pomo, problem, problem)\n        if self.env_params['memory_type'] != 'none':\n            self.step_state.edge_memory = torch.zeros((self.batch_size, self.pomo_size, self.problem_size, self.problem_size), dtype=torch.float32)\n            # shape: (batch, pomo, problem, problem)\n\n        reward = None\n        done = False\n        return self.step_state, reward, done\n\n    def step(self, selected):\n        # selected.shape: (batch, pomo)\n\n        self.selected_count += 1\n        self.current_node = selected\n        # shape: (batch, pomo)\n\n        self.selected_node_list = torch.cat((self.selected_node_list, self.current_node[:, :, None]), dim=2)\n        # shape: (batch, pomo, 0~problem)\n\n        # Update edge-based solutions\n        if self.selected_count > 1:\n            prev_selected = self.selected_node_list[:, :, -2]\n            self.step_state.edge_solution[self.BATCH_IDX, self.POMO_IDX, prev_selected, self.current_node] = 1\n            self.step_state.edge_solution[self.BATCH_IDX, self.POMO_IDX, self.current_node, prev_selected] = 1\n\n        # Update step state\n        self.step_state.current_node = self.current_node  # shape: (batch, pomo)\n        self.step_state.ninf_mask[self.BATCH_IDX, self.POMO_IDX, self.current_node] = float('-inf')  # shape: (batch, pomo, node)\n\n        # Retrieve from memory\n        avg_sim, max_sim = None, None\n        if self.selected_count > 0 and (self.selected_count % self.retrieve_freq == 0):\n            edge_mem, avg_sim, max_sim = self.memory.get_knn(self.step_state.edge_solution, self.selected_count, self.env_params['k'], return_similarity=True)\n            if (self.env_params['memory_type'] != 'none') and (edge_mem is not None):\n                self.step_state.edge_memory = edge_mem\n\n        # Returning values\n        done = (self.selected_count == self.problem_size)\n        R = {'reward': None, 'punishment': None}\n        if done:\n            # add final edge\n            self.step_state.edge_solution[self.BATCH_IDX, self.POMO_IDX, self.selected_node_list[:, :, -1], self.selected_node_list[:, :, 0]] = 1\n            self.step_state.edge_solution[self.BATCH_IDX, self.POMO_IDX, self.selected_node_list[:, :, 0], self.selected_node_list[:, :, -1]] = 1\n            reward = -self._get_travel_distance()  # note the minus sign!\n            if self.env_params['memory_type'] != 'none':\n                # Save final solution in memory\n                self.memory.save_in_memory(self.step_state.edge_solution)\n\n                if avg_sim is not None:\n                    R['avg_sim'] = avg_sim\n                    R['max_sim'] = max_sim\n\n            R['reward'] = reward\n\n        return self.step_state, R, done\n\n    def _get_travel_distance(self):\n        travel_distances = self.step_state.edge_solution * self.dist.unsqueeze(1).expand(self.batch_size, self.pomo_size, self.problem_size, self.problem_size) / 2\n        # sum over last and second last dimensions\n        return travel_distances.sum(dim=[2, 3])\n\n# ==========================================\n# File: TSP/env/TSPMemory.py\n# Function/Context: Memory\n# ==========================================\nimport numpy as np\nimport torch\n\n\nclass Memory:\n    def __init__(self, state_dim, memory_aggr, memory_size=10000, n_memories=1, device='cpu'):\n        \"\"\"\n        Memory that saves States and returns the average value of the k-nearest neighbours of a state\n        \"\"\"\n        self.state_dim = state_dim\n        self.n = np.sqrt(state_dim).astype('int')\n        self.memory_size = memory_size\n        self.memory_aggr = memory_aggr\n        self.n_memories = n_memories # number of memories to use. If 1, use a single shared memory for all the states\n        self.device = device\n\n        # Initialize memories and index\n        self.state_memories = [torch.zeros((0, self.state_dim)) for _ in range(self.n_memories)]\n\n        self.used_memory = 0\n\n    def save_in_memory(self, state):\n        batch_size, pomo_size = state.shape[0], state.shape[1]\n\n        state = state.view(batch_size, pomo_size, self.state_dim).float()\n\n        for idx in range(self.n_memories):\n            # If memory is full, remove the oldest state\n            if len(self.state_memories[idx]) >= self.memory_size:\n                self.state_memories[idx] = torch.roll(self.state_memories[idx], -pomo_size, dims=0)\n                self.state_memories[idx][-pomo_size:] = state\n            else:\n                self.state_memories[idx] = torch.vstack([self.state_memories[idx], state[idx]])\n\n        self.used_memory += pomo_size\n\n    def get_knn(self, state, step, k, return_similarity=False):\n        if self.used_memory == 0:\n            return None, None, None\n\n        norm_factor = float(2*step)\n\n        # Get k nearest states using faiss\n        batch_size, pomo_size = state.shape[0], state.shape[1]\n        state = state.view(batch_size, pomo_size, self.state_dim).float()\n\n        k = self.used_memory if k > self.used_memory else k\n\n        nearest_states = torch.zeros((batch_size, pomo_size, self.state_dim))\n\n        avg_similarity = torch.zeros((batch_size, pomo_size))\n        max_similarity = torch.zeros((batch_size, pomo_size))\n        for idx in range(self.n_memories):\n            inner_products = torch.mm(state[idx], self.state_memories[idx].t())  # Result dim: (pomo_size, used_memory)\n            similarity, indices = torch.topk(inner_products, k, largest=True, sorted=True)\n            # similarity.shape = (pomo, k) and indices.shape = (pomo, k)\n\n            if return_similarity:\n                avg_similarity[idx] = similarity.mean(1) / norm_factor\n                max_similarity[idx] = similarity.max(1)[0] / norm_factor\n\n            nearest_s = self.state_memories[idx][indices]\n            # nearest_s.shape = (pomo, k, state_dim)\n            # Aggregate among k neighbors: Weighted based on similarity.\n            if self.memory_aggr == 'sum':  # No weighting\n                nearest_states[idx] = torch.sum(nearest_s, dim=1)\n            elif self.memory_aggr == 'linear':  # Linear weighted sum\n                # similarity from [0, N] --> [0, 1]\n                sim = similarity / norm_factor\n                nearest_states[idx] = torch.sum(nearest_s * sim[:, :, None], dim=1)\n                # nearest_s = (pomo, k, state_dim) and sim = (pomo, k), nearest_states = (batch, pomo, state_dim)\n                # Divide by k\n                nearest_states[idx] = nearest_states[idx] / k\n        nearest_states = nearest_states.reshape(batch_size, pomo_size, self.n, self.n)\n        return nearest_states, avg_similarity, max_similarity\n\n\ndef select_memory(mem_aggr, state_dim, batch_size, device):\n    n_memories = batch_size\n\n    memory = Memory(state_dim=state_dim,\n                    memory_aggr=mem_aggr,\n                    n_memories=n_memories,\n                    device=device)\n    return memory\n\n# ==========================================\n# File: TSP/nets/TSPModel.py\n# Function/Context: TSPModel\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom env.TSPEnv import Reset_State, Step_State\nfrom nets.layers import GTLayer, reshape_by_heads, MLP_swiglu, Activation\n\n\nclass TSPModel(nn.Module):\n\n    def __init__(self, **model_params):\n        super().__init__()\n        self.model_params = model_params\n        self.embedding_dim = self.model_params['embedding_dim']\n        self.encoder = TSP_Encoder(**model_params)\n        self.decoder = TSP_Decoder(**model_params)\n        self.encoded_nodes = None\n        # shape: (batch, problem, EMBEDDING_DIM)\n\n    def pre_forward(self, reset_state: Reset_State):\n        self.encoded_nodes = self.encoder(reset_state)\n        # shape: (batch, problem, EMBEDDING_DIM)\n        self.decoder.set_kv(self.encoded_nodes)\n\n    def forward(self, state: Step_State, deterministic=False, use_memory=True):\n        batch_size = state.BATCH_IDX.size(0)\n        pomo_size = state.BATCH_IDX.size(1)\n        problem_size = state.ninf_mask.size(-1)\n\n        if state.current_node is None:  # First node\n            selected = torch.arange(pomo_size)[None, :].expand(batch_size, pomo_size)\n            if pomo_size > problem_size: # Add more initial nodes between 0 and problem_size-1\n                selected = selected % problem_size\n\n            prob = torch.ones(size=(batch_size, pomo_size))\n            encoded_first_node = self.encoded_nodes.gather(dim=1, index=selected[:, :, None].expand(batch_size, pomo_size, self.embedding_dim))\n            # shape: (batch, pomo, embedding)\n\n            self.decoder.set_q1(encoded_first_node)\n\n        else:\n            encoded_last_node = self.encoded_nodes.gather(dim=1, index=state.current_node[:, :, None].expand(batch_size, pomo_size, self.embedding_dim))\n            # shape: (batch, pomo, embedding)\n            if self.model_params['use_memory']:\n                # Gather the mem info given by the current_node\n                edge_mem_last_node = state.edge_memory.gather(dim=2, index=state.current_node[:, :, None, None].expand(batch_size, pomo_size, 1, problem_size))\n\n                logits = self.decoder(encoded_last_node, edge_mem_last_node.squeeze(2), ninf_mask=state.ninf_mask, use_memory=use_memory)\n            else:\n                logits = self.decoder(encoded_last_node, None, ninf_mask=state.ninf_mask, use_memory=False)\n\n            # shape: (batch, pomo, problem)\n\n            if not deterministic:  # Sample\n                probs = F.softmax(logits, dim=2)\n                # shape: (batch, pomo, problem)\n\n                selected = probs.reshape(batch_size * pomo_size, -1).multinomial(1).squeeze(dim=1).reshape(batch_size, pomo_size)\n                # shape: (batch, pomo)\n\n                prob = probs[state.BATCH_IDX, state.POMO_IDX, selected].reshape(batch_size, pomo_size)\n                # shape: (batch, pomo)\n\n            else:\n                selected = logits.argmax(dim=2)\n                # shape: (batch, pomo)\n\n                prob = None\n\n        return selected, prob\n\n\nclass TSP_Encoder(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n        self.model_params = model_params\n        embedding_dim = self.model_params['embedding_dim']\n        encoder_layer_num = self.model_params['encoder_layer_num']\n\n        self.coord_embedding = nn.Linear(2, embedding_dim, bias=False)\n        self.dist_embedding = nn.Linear(1, embedding_dim, bias=False)\n\n        self.layers = nn.ModuleList([GTLayer(**model_params) for _ in range(encoder_layer_num)])\n\n    def forward(self, data: Reset_State):\n        h = self.coord_embedding(data.coords)\n        #h = torch.ones(data.coords.shape[0], data.coords.shape[1], self.model_params['embedding_dim'])\n        # shape: (batch, problem, embedding)\n\n        e = self.dist_embedding(data.dist.unsqueeze(-1))\n        # shape: (batch, problem, problem, embedding)\n\n        for layer in self.layers:\n            h = layer(h, e)\n\n        return h\n\n\nclass TSP_Decoder(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n        self.model_params = model_params\n        self.embedding_dim = self.model_params['embedding_dim']\n        self.sqrt_embedding_dim = self.model_params['embedding_dim'] ** (1 / 2)\n        self.head_num = self.model_params['head_num']\n        qkv_dim = self.embedding_dim // self.head_num\n\n        self.W_q_first = nn.Linear(self.embedding_dim, self.head_num * qkv_dim, bias=False)\n        self.W_q_last = nn.Linear(self.embedding_dim, self.head_num * qkv_dim, bias=False)\n        self.W_kv = nn.Linear(self.embedding_dim, 2 * self.head_num * qkv_dim, bias=False)\n\n        if self.model_params['use_memory']:\n            self.mem_FF = MLP_swiglu(2, 1, self.embedding_dim, **model_params)\n\n        self.multi_head_combine = nn.Linear(self.head_num * qkv_dim, self.embedding_dim)\n\n        self.k = None  # saved key, for multi-head attention\n        self.v = None  # saved value, for multi-head_attention\n        self.single_head_key = None  # saved, for single-head attention\n        self.q_first = None  # saved q1, for multi-head attention\n\n    def set_kv(self, encoded_nodes):\n        # encoded_nodes.shape: (batch, problem, embedding)\n\n        k, v = self.W_kv(encoded_nodes).split(self.embedding_dim, dim=-1)\n        self.k = reshape_by_heads(k, head_num=self.head_num)\n        self.v = reshape_by_heads(v, head_num=self.head_num)\n        # shape: (batch, head_num, pomo, qkv_dim)\n\n        self.single_head_key = encoded_nodes.transpose(1, 2)\n        # shape: (batch, embedding, problem)\n\n    def set_q1(self, encoded_q1):\n        # encoded_q.shape: (batch, n, embedding)  # n can be 1 or pomo\n        self.q_first = reshape_by_heads(self.W_q_first(encoded_q1), head_num=self.head_num)\n        # shape: (batch, head_num, n, qkv_dim)\n\n    def forward(self, encoded_last_node, mem_last_node, ninf_mask, use_memory=True):\n        # encoded_last_node.shape: (batch, pomo, embedding)\n        # ninf_mask.shape: (batch, pomo, problem)\n        batch_size, pomo_size = encoded_last_node.size(0), encoded_last_node.size(1)\n        problem_size = self.k.size(2)\n\n        #  Embedding of the last visited node\n        q_last = reshape_by_heads(self.W_q_last(encoded_last_node), head_num=self.head_num)\n        # shape: (batch, head_num, pomo, qkv_dim)\n\n        # Sum the first and last visited.\n        q = self.q_first + q_last\n        # shape: (batch, head_num, pomo, qkv_dim)\n\n        # Multi-Head Attention\n        # q   (batch, head_num, pomo, qkv_dim)\n        # k   (batch, head_num, problem, qkv_dim)\n        # v   (batch, head_num, problem, qkv_dim)\n        # mem (batch, pomo, problem)\n        out_concat = torch.nn.functional.scaled_dot_product_attention(q, self.k, self.v, attn_mask=ninf_mask[:, None, :, :].expand(batch_size, self.head_num, pomo_size, problem_size))\n        out_concat = out_concat.transpose(1, 2).reshape(batch_size, pomo_size, self.embedding_dim)\n        # shape: (batch, pomo, embedding)\n\n        mh_atten_out = self.multi_head_combine(out_concat)\n        # shape: (batch, pomo, embedding)\n\n        #  Single-Head Attention, for probability calculation\n        score = torch.matmul(mh_atten_out, self.single_head_key)\n        # shape: (batch, pomo, problem)\n\n        if use_memory:\n            score = torch.stack((score, mem_last_node), dim=-1)\n            score = self.mem_FF(score).squeeze(-1)\n\n        score_scaled = score / self.sqrt_embedding_dim\n        # shape: (batch, pomo, problem)\n\n        score_clipped = self.model_params['logit_clipping'] * torch.tanh(score_scaled)\n\n        logits = score_clipped + ninf_mask\n\n        return logits",
  "description": "Combined Analysis:\n- [MIS/env/Env.py]: This file implements the core environment for the Maximum Independent Set (MIS) problem within the MARCO framework. It contains the State dataclass and Env class that manage the optimization process. The key components include: 1) State representation with adjacency matrix, solutions, action masks, and memory information; 2) Fitness computation (objective function) as the sum of nodes in the independent set; 3) Action masking to enforce the independent set constraint (no adjacent nodes); 4) Memory integration through save_in_memory and get_knn methods; 5) Reward calculation based on fitness improvement and memory-based penalties for revisiting states; 6) Solution initialization methods (greedy algorithms). This directly implements the improvement-based approach for MIS described in the paper, where solutions are iteratively refined through node flips while using memory to avoid redundant exploration.\n- [MIS/env/memory.py]: This file implements the core memory module of the MARCO framework, which is essential for preventing redundant exploration and promoting diverse solutions. The code provides two memory implementations: 1) Memory class that stores state-action pairs and retrieves k-nearest neighbors using different aggregation methods (sum, linear, exponential), and 2) OperationMemory class that tracks operation usage times. The select_memory function dynamically chooses the appropriate memory type based on configuration. This directly implements the memory-augmentation mechanism described in the paper's algorithm steps, where past solutions are stored and retrieved to guide future decisions in combinatorial optimization problems.\n- [TSP/TSPTrainer.py]: This file implements the core MARCO training logic for TSP. Key aspects: 1) Implements memory-augmented training through the '_train_one_batch_marco' method which uses baseline solutions and similarity penalties to avoid redundant exploration. 2) The rollout method implements the constructive solution-building process where the neural model sequentially selects nodes to build TSP tours. 3) The training objective combines tour length minimization with diversity promotion through similarity penalties (avg_sim/max_sim metrics). 4) Supports both standard RL training and MARCO's memory-augmented training with configurable punishment types. 5) Implements the POMO (Policy Optimization with Multiple Optima) approach with parallel rollouts.\n- [TSP/env/TSPEnv.py]: This file implements the core TSP environment for the MARCO framework. It directly encodes the TSP optimization model: 1) The objective function is computed in _get_travel_distance() as the sum of edge distances in the tour (with division by 2 due to symmetric edge representation). 2) The permutation constraint is enforced through the step() method's node selection mechanism using ninf_mask to prevent revisiting nodes. 3) The memory-augmented algorithm is implemented through initialize_memory(), step()'s memory retrieval (get_knn), and solution storage (save_in_memory). The edge_solution tensor (batch  pomo  node  node) represents the tour as an adjacency matrix, and the environment constructs solutions incrementally via node selections, matching the paper's constructive method for TSP.\n- [TSP/env/TSPMemory.py]: This file implements the core memory module of the MARCO framework for TSP. The Memory class stores past solution states and retrieves k-nearest neighbors using similarity metrics (inner products). It supports multiple memory instances (n_memories) for parallel threads and two aggregation methods ('sum' and 'linear') to combine retrieved states. This directly corresponds to the paper's memory-augmented approach that prevents redundant exploration and promotes diverse solutions by leveraging historical states during constructive solution building.\n- [TSP/nets/TSPModel.py]: This file implements the core neural constructive method for TSP within the MARCO framework. The TSPModel class uses an encoder-decoder architecture with attention mechanisms to sequentially construct tours. Key aspects matching the paper: 1) Constructive solution building through sequential node selection (forward method). 2) Memory integration via edge_memory in decoder when use_memory=True (implements memory-augmented guidance). 3) Reinforcement learning compatibility through probabilistic sampling vs deterministic selection. 4) Graph Transformer encoder processes node coordinates and pairwise distances. The decoder combines multi-head attention with memory features to compute selection logits, implementing the neural component of MARCO's TSP solver.",
  "dependencies": [
    "env.TSPEnv.Reset_State",
    "networkx",
    "env.utils.get_distances_from_coords",
    "torch.nn.functional.one_hot",
    "pickle",
    "env.memory.select_memory",
    "logging",
    "nets.TSPModel",
    "torch.optim.lr_scheduler.MultiStepLR",
    "env.TSPEnv",
    "env.utils.tsp_coordinate_augmentations",
    "torch.nn.functional",
    "nets.layers.GTLayer",
    "nets.layers.MLP_swiglu",
    "env.TSPMemory.select_memory",
    "env.TSPEnv.Step_State",
    "env.generators.RandomErdosRenyiGraphGenerator",
    "random",
    "dataclasses",
    "numpy",
    "nets.layers.Activation",
    "nets.layers.reshape_by_heads",
    "utils (custom module containing AverageMeter, TimeEstimator, LogData, get_result_folder, util_print_log_array)",
    "env.utils.get_random_problems",
    "torch.nn",
    "torch",
    "torch.optim.AdamW"
  ]
}