{
  "paper_id": "Combinatorial_Optimization_with_Policy_Adaptation_using_Late",
  "title": "Combinatorial Optimization with Policy Adaptation using Latent Space Search",
  "abstract": "Combinatorial Optimization underpins many real-world applications and yet, designing performant algorithms to solve these complex, typically NP-hard, problems remains a significant research challenge. Reinforcement Learning (RL) provides a versatile framework for designing heuristics across a broad spectrum of problem domains. However, despite notable progress, RL has not yet supplanted industrial solvers as the go-to solution. Current approaches emphasize pre-training heuristics that construct solutions but often rely on search procedures with limited variance, such as stochastically sampling numerous solutions from a single policy or employing computationally expensive fine-tuning of the policy on individual problem instances. Building on the intuition that performant search at inference time should be anticipated during pre-training, we propose COMPASS, a novel RL approach that parameterizes a distribution of diverse and specialized policies conditioned on a continuous latent space. We evaluate COMPASS across three canonical problems - Travelling Salesman, Capacitated Vehicle Routing, and Job-Shop Scheduling - and demonstrate that our search strategy (i) outperforms state-of-the-art approaches in 9 out of 11 standard benchmarking tasks and (ii) generalizes better, surpassing all other approaches on a set of 18 procedurally transformed instance distributions.",
  "problem_description_natural": "The paper addresses the challenge of solving combinatorial optimization (CO) problems—specifically the Travelling Salesman Problem (TSP), Capacitated Vehicle Routing Problem (CVRP), and Job-Shop Scheduling Problem (JSSP)—which involve finding an optimal ordering, labeling, or subset of discrete elements to minimize or maximize an objective function under constraints. These problems are typically NP-hard, making exact solutions infeasible for large instances, so the focus is on learning effective heuristics using reinforcement learning that can adapt efficiently at inference time without retraining.",
  "problem_type": "combinatorial optimization",
  "datasets": [
    "TSP100",
    "TSP125",
    "TSP150",
    "TSP200",
    "CVRP100",
    "CVRP125",
    "CVRP150",
    "CVRP200",
    "JSSP 10x10",
    "JSSP 15x15",
    "JSSP 20x15",
    "Procedurally mutated TSP instances (Bossek et al., 2019)",
    "Procedurally mutated CVRP instances (Bossek et al., 2019)"
  ],
  "performance_metrics": [
    "Objective Value (Obj.)",
    "Optimality Gap (%)",
    "Runtime (Time)"
  ],
  "lp_model": {
    "objective": "$\\min \\sum_{i=1}^{n} \\sum_{j=1}^{n} c_{ij} x_{ij}$",
    "constraints": [
      "$\\sum_{j=1}^{n} x_{ij} = 1$ for all $i = 1, \\ldots, n$",
      "$\\sum_{i=1}^{n} x_{ij} = 1$ for all $j = 1, \\ldots, n$",
      "$x_{ii} = 0$ for all $i = 1, \\ldots, n$",
      "$\\sum_{i \\in S} \\sum_{j \\notin S} x_{ij} \\geq 1$ for all subsets $S \\subset \\{1, \\ldots, n\\}$ with $S \\neq \\emptyset$ and $S \\neq \\{1, \\ldots, n\\}$",
      "$x_{ij} \\in \\{0,1\\}$ for all $i,j = 1, \\ldots, n$"
    ],
    "variables": [
      "$x_{ij}$: binary decision variable indicating whether edge from city $i$ to city $j$ is included in the tour",
      "$c_{ij}$: Euclidean distance between cities $i$ and $j$, with cities uniformly sampled in a unit square as per the paper's instances"
    ]
  },
  "raw_latex_model": "$$\\begin{aligned} \\min & \\sum_{i=1}^{n} \\sum_{j=1}^{n} c_{ij} x_{ij} \\\\ \\text{s.t.} & \\sum_{j=1}^{n} x_{ij} = 1, \\quad \\forall i = 1, \\ldots, n \\\\ & \\sum_{i=1}^{n} x_{ij} = 1, \\quad \\forall j = 1, \\ldots, n \\\\ & x_{ii} = 0, \\quad \\forall i = 1, \\ldots, n \\\\ & \\sum_{i \\in S} \\sum_{j \\notin S} x_{ij} \\geq 1, \\quad \\forall S \\subset \\{1, \\ldots, n\\}, S \\neq \\emptyset, S \\neq \\{1, \\ldots, n\\} \\\\ & x_{ij} \\in \\{0,1\\}, \\quad \\forall i,j = 1, \\ldots, n \\end{aligned}$$",
  "algorithm_description": "COMPASS is a reinforcement learning method that learns a latent space of diverse and specialized policies conditioned on continuous vectors. During training, it samples multiple latent vectors, evaluates the conditioned policies, and updates only the best-performing policy to encourage specialization. At inference time, it uses an evolution strategy (CMA-ES) to search the latent space for high-performing policies tailored to each problem instance, without retraining the neural network parameters."
}