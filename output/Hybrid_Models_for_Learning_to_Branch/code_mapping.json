{
  "file_path": "03_train_hybrid.py, 05_evaluate_hybrid.py, models/concat-pre/model.py, models/concat/model.py, models/film-pre/model.py, models/film/model.py, models/hybridsvm-film/model.py, models/hybridsvm/model.py, models/mlp/model.py",
  "function_name": "PolicyBranching, Policy, Policy.forward, Policy, Policy, Policy, Policy.forward, Policy.forward",
  "code_snippet": "\n\n# ==========================================\n# File: 03_train_hybrid.py\n# Function/Context: \n# ==========================================\nimport os\nimport importlib\nimport argparse\nimport sys\nimport pathlib\nimport pickle\nimport numpy as np\nfrom time import strftime\nfrom shutil import copyfile\nimport gzip\nimport torch\n\nimport utilities\nfrom utilities import log, _loss_fn, _distillation_loss, _compute_root_loss\n\nfrom utilities_hybrid import HybridDataset as Dataset, load_batch\n\ndef pretrain(model, dataloader):\n    \"\"\"\n    Pre-normalizes a model (i.e., PreNormLayer layers) over the given samples.\n\n    Parameters\n    ----------\n    model : model.BaseModel\n        A base model, which may contain some model.PreNormLayer layers.\n    dataloader : torch.utils.data.DataLoader\n        Dataset to use for pre-training the model.\n    Return\n    ------\n    number of PreNormLayer layers processed.\n    \"\"\"\n    model.pre_train_init()\n    i = 0\n    while True:\n        for batch in dataloader:\n            root_g, node_g, node_attr = [map(lambda x:x if x is None else x.to(device) , y) for y in batch]\n            root_c, root_ei, root_ev, root_v, root_n_cs, root_n_vs, *_ = root_g\n            g_c, g_ei, g_ev, g_v, g_n_cs, g_n_vs, candss = node_g\n            cand_features, n_cands, best_cands, cand_scores, weights = node_attr\n\n            batched_states = (root_c, root_ei, root_ev, root_v, root_n_cs, root_n_vs, candss, cand_features, None)\n\n            if not model.pre_train(batched_states):\n                break\n\n        res = model.pre_train_next()\n        if res is None:\n            break\n        else:\n            layer = res\n\n        i += 1\n\n    return i\n\ndef process(model, teacher, dataloader, top_k, optimizer=None):\n    \"\"\"\n    Executes a forward and backward pass of model over the dataset.\n\n    Parameters\n    ----------\n    model : model.BaseModel\n        A base model, which may contain some model.PreNormLayer layers.\n    teacher : model.BaseModel\n        A pretrained model when args.no_e2e is True, and an expert model when it is True.\n    dataloader : torch.utils.data.DataLoader\n        Dataset to use for training the model.\n    top_k : list\n        list of `k` (int) to estimate for accuracy using these many candidates\n    optimizer :  torch.optim\n        optimizer to use for SGD. No gradient computation takes place if its None.\n\n    Return\n    ------\n    mean_loss : np.float\n        mean loss of model on data in dataloader\n    mean_kacc : np.array\n        computed accuracy for `top_k` candidates\n    \"\"\"\n    mean_loss = 0\n    mean_kacc = np.zeros(len(top_k))\n\n    n_samples_processed = 0\n    accum_iter = 0\n    for batch in dataloader:\n        root_g, node_g, node_attr = [map(lambda x:x if x is None else x.to(device) , y) for y in batch]\n        root_c, root_ei, root_ev, root_v, root_n_cs, root_n_vs, root_cands, root_n_cands = root_g\n        node_c, node_ei, node_ev, node_v, node_n_cs, node_n_vs, candss = node_g\n        cand_features, n_cands, best_cands, cand_scores, weights  = node_attr\n        cands_root_v = None\n\n        # use teacher\n        with torch.no_grad():\n            if teacher is not None:\n                if args.no_e2e:\n                    root_v, _ = teacher((root_c, root_ei, root_ev, root_v, root_n_cs, root_n_vs))\n                    cands_root_v = root_v[candss]\n\n                # KD - get soft targets\n                if args.distilled:\n                    _, soft_targets = teacher((node_c, node_ei, node_ev, node_v, node_n_cs, node_n_vs))\n                    soft_targets = torch.unsqueeze(torch.gather(input=torch.squeeze(soft_targets, 0), dim=0, index=candss), 0)\n                    soft_targets = model.pad_output(soft_targets, n_cands)  # apply padding now\n\n        batched_states = (root_c, root_ei, root_ev, root_v, root_n_cs, root_n_vs, candss, cand_features, cands_root_v)\n        batch_size = n_cands.shape[0]\n        weights /= batch_size # sum loss\n\n        if optimizer:\n            optimizer.zero_grad()\n            var_feats, logits, film_parameters = model(batched_states)  # eval mode\n            logits = model.pad_output(logits, n_cands)  # apply padding now\n\n            # node loss\n            if args.distilled:\n                loss = _distillation_loss(logits, soft_targets, best_cands, weights, T, alpha)\n            else:\n                loss = _loss_fn(logits, best_cands, weights)\n\n            # AT loss\n            if args.at != \"\":\n                loss  += args.beta_at * _compute_root_loss(args.at, model, var_feats, root_n_vs, root_cands, root_n_cands, batch_size, root_cands_separation)\n\n            # regularization\n            if (\n                args.l2 > 0\n                and film_parameters is not None\n            ):\n                beta_norm = (1-film_parameters[:, :, 0]).norm()\n                gamma_norm = film_parameters[:, :, 1].norm()\n                loss += args.l2 * (beta_norm + gamma_norm)\n\n            loss.backward()\n            accum_iter += 1\n            if accum_iter % accum_steps == 0:\n                optimizer.step()\n                accum_iter = 0\n        else:\n            with torch.no_grad():\n                var_feats, logits, film_parameters = model(batched_states)  # eval mode\n                logits = model.pad_output(logits, n_cands)  # apply padding now\n\n                # node loss\n                if args.distilled:\n                    loss = _distillation_loss(logits, soft_targets, best_cands, weights, T, alpha)\n                else:\n                    loss = _loss_fn(logits, best_cands, weights)\n\n                # AT loss\n                if args.at != \"\":\n                    loss  += args.beta_at * _compute_root_loss(args.at, model, var_feats, root_n_vs, root_cands, root_n_cands, batch_size, root_cands_separation)\n\n                # regularization\n                if (\n                    args.l2 > 0\n                    and film_parameters is not None\n                ):\n                    beta_norm = (1-film_parameters[:, :, 0]).norm()\n                    gamma_norm = film_parameters[:, :, 1].norm()\n                    loss += args.l2 * (beta_norm + gamma_norm)\n\n        true_scores = model.pad_output(torch.reshape(cand_scores, (1, -1)), n_cands)\n        true_bestscore = torch.max(true_scores, dim=-1, keepdims=True).values\n        true_scores = true_scores.cpu().numpy()\n        true_bestscore = true_bestscore.cpu().numpy()\n\n        kacc = []\n        for k in top_k:\n            pred_top_k = torch.topk(logits, k=k).indices.cpu().numpy()\n            pred_top_k_true_scores = np.take_along_axis(true_scores, pred_top_k, axis=1)\n            kacc.append(np.mean(np.any(pred_top_k_true_scores == true_bestscore, axis=1)))\n        kacc = np.asarray(kacc)\n\n        mean_loss += loss.detach_().item() * batch_size\n        mean_kacc += kacc * batch_size\n        n_samples_processed += batch_size\n\n    mean_loss /= n_samples_processed\n    mean_kacc /= n_samples_processed\n\n    return mean_loss, mean_kacc\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        'problem',\n        help='MILP instance type to process.',\n        choices=['setcover', 'cauctions', 'facilities', 'indset'],\n    )\n    parser.add_argument(\n        '-m', '--model',\n        help='model to be trained.',\n        type=str,\n        default='film',\n    )\n    parser.add_argument(\n        '-s', '--seed',\n        help='Random generator seed.',\n        type=utilities.valid_seed,\n        default=0,\n    )\n    parser.add_argument(\n        '-g', '--gpu',\n        help='CUDA GPU id (-1 for CPU).',\n        type=int,\n        default=0,\n    )\n    parser.add_argument(\n        '--data_path',\n        help='name of the folder',\n        type=str,\n        default=\"data/samples/\",\n    )\n    parser.add_argument(\n        '--no_e2e',\n        help='if training is with a pretrained GCNN.',\n        action=\"store_true\"\n    )\n    parser.add_argument(\n        '--distilled',\n        help='if distillation should be used',\n        action=\"store_true\"\n    )\n    parser.add_argument(\n        '--at',\n        help='type of auxiliary task',\n        type=str,\n        default='',\n        choices=['ED', 'MHE', '']\n    )\n    parser.add_argument(\n        '--beta_at',\n        help='weight for at loss function',\n        type=float,\n        default=0,\n    )\n    parser.add_argument(\n        '--l2',\n        help='regularization film weights',\n        type=float,\n        default=0.0\n    )\n    args = parser.parse_args()\n\n    if (\n        args.model in ['concat', 'film']\n        and args.no_e2e\n    ):\n        args.model = f\"{args.model}-pre\"\n\n    ### HYPER PARAMETERS ###\n    max_epochs = 1000\n    epoch_size = 312\n    batch_size = 32\n    accum_steps = 1 # step() is called after  batch_size * accum_steps samples\n    pretrain_batch_size = 128\n    valid_batch_size = 128\n    lr = 0.001\n    patience = 15\n    early_stopping = 30\n    top_k = [1, 3, 5, 10]\n    num_workers = 5\n    teacher_model = \"baseline_torch\" # used only if args.distilled or args.no_e2e is True\n    T = 2 # used only if args.distilled is True\n    alpha = 0.9 # used only if args.distilled is True\n\n    root_cands_separation=False\n    if args.problem == \"facilities\":\n        # facilities have larger problem size (LPs have 10000 variables)\n        # these settings are chosen so that training is feasible in considerable time (about 6-12 hours)\n        lr = 0.005\n        epoch_size=312*3\n        batch_size = 16\n        accum_steps = 2\n        patience=10\n        early_stopping=20\n        pretrain_batch_size = 64\n        valid_batch_size = 32\n        root_cands_separation=True\n        num_workers=7\n\n    problem_folders = {\n        'setcover': '500r_1000c_0.05d',\n        'cauctions': '100_500',\n        'facilities': '100_100_5',\n        'indset': '750_4',\n    }\n\n    # DIRECTORY NAMING\n    modeldir = f\"{args.model}\"\n    if args.distilled:\n        modeldir = f\"{args.model}_distilled\"\n\n    if args.at != \"\":\n        modeldir = f\"{modeldir}_{args.at}_{args.beta_at}\"\n\n    if args.l2 > 0:\n        modeldir = f\"{modeldir}_l2_{args.l2}\"\n\n    running_dir = f\"trained_models/{args.problem}/{modeldir}/{args.seed}\"\n\n    os.makedirs(running_dir)\n\n    ### LOG ###\n    logfile = os.path.join(running_dir, 'log.txt')\n\n    log(f\"max_epochs: {max_epochs}\", logfile)\n    log(f\"epoch_size: {epoch_size}\", logfile)\n    log(f\"batch_size: {batch_size}\", logfile)\n    log(f\"pretrain_batch_size: {pretrain_batch_size}\", logfile)\n    log(f\"valid_batch_size : {valid_batch_size }\", logfile)\n    log(f\"lr: {lr}\", logfile)\n    log(f\"patience : {patience }\", logfile)\n    log(f\"early_stopping : {early_stopping }\", logfile)\n    log(f\"top_k: {top_k}\", logfile)\n    log(f\"problem: {args.problem}\", logfile)\n    log(f\"gpu: {args.gpu}\", logfile)\n    log(f\"seed {args.seed}\", logfile)\n    log(f\"e2e: {not args.no_e2e}\", logfile)\n    log(f\"KD: {args.distilled}\", logfile)\n    log(f\"AT: {args.at} beta={args.beta_at}\", logfile)\n    log(f\"l2: {args.l2}\", logfile)\n\n    ### NUMPY / TORCH SETUP ###\n    if args.gpu == -1:\n        os.environ['CUDA_VISIBLE_DEVICES'] = ''\n        device = torch.device(\"cpu\")\n    else:\n        os.environ['CUDA_VISIBLE_DEVICES'] = f'{args.gpu}'\n        device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    rng = np.random.RandomState(args.seed)\n    torch.manual_seed(rng.randint(np.iinfo(int).max))\n\n    ### SET-UP DATASET ###\n    problem_folder = problem_folders[args.problem]\n    train_files = list(pathlib.Path(f\"{args.data_path}/{args.problem}/{problem_folder}/train\").glob('sample_*.pkl'))\n    valid_files = list(pathlib.Path(f\"{args.data_path}/{args.problem}/{problem_folder}/valid\").glob('sample_*.pkl'))\n\n    log(f\"{len(train_files)} training samples\", logfile)\n    log(f\"{len(valid_files)} validation samples\", logfile)\n\n    train_files = [str(x) for x in train_files]\n    valid_files = [str(x) for x in valid_files]\n\n    valid_data = Dataset(valid_files, args.data_path)\n    valid_data = torch.utils.data.DataLoader(valid_data, batch_size=valid_batch_size,\n                            shuffle = False, num_workers = num_workers, collate_fn = load_batch)\n\n    pretrain_files = [f for i, f in enumerate(train_files) if i % 10 == 0]\n    pretrain_data = Dataset(pretrain_files, args.data_path)\n    pretrain_data = torch.utils.data.DataLoader(pretrain_data, batch_size=pretrain_batch_size,\n                            shuffle = False, num_workers = num_workers, collate_fn = load_batch)\n\n    ### MODEL LOADING ###\n    sys.path.insert(0, os.path.abspath(f'models/{args.model}'))\n    import model\n    importlib.reload(model)\n    distilled_model = model.Policy()\n    del sys.path[0]\n    distilled_model.to(device)\n\n    ### TEACHER MODEL LOADING ###\n    teacher = None\n    if (\n        args.distilled\n        or args.no_e2e\n    ):\n        sys.path.insert(0, os.path.abspath(f'models/{teacher_model}'))\n        import model\n        importlib.reload(model)\n        teacher = model.GCNPolicy()\n        del sys.path[0]\n        teacher.restore_state(f\"trained_models/{args.problem}/{teacher_model}/{args.seed}/best_params.pkl\")\n        teacher.to(device)\n        teacher.eval()\n\n    model = distilled_model\n\n    ### TRAINING LOOP ###\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=patience, verbose=True)\n    best_loss = np.inf\n    for epoch in range(max_epochs + 1):\n        log(f\"EPOCH {epoch}...\", logfile)\n\n        if (\n            epoch == 0\n            and not args.no_e2e\n        ):\n            n = pretrain(model=model, dataloader=pretrain_data)\n            log(f\"PRETRAINED {n} LAYERS\", logfile)\n        else:\n            # bugfix: tensorflow's shuffle() seems broken...\n            epoch_train_files = rng.choice(train_files, epoch_size * batch_size * accum_steps, replace=True)\n            train_data = Dataset(epoch_train_files, args.data_path)\n            train_data = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n                                    shuffle = False, num_workers = num_workers, collate_fn = load_batch)\n            train_loss, train_kacc = process(model, teacher, train_data, top_k, optimizer)\n            log(f\"TRAIN LOSS: {train_loss:0.3f} \" + \"\".join([f\" acc@{k}: {acc:0.3f}\" for k, acc in zip(top_k, train_kacc)]), logfile)\n\n        # TEST\n        valid_loss, valid_kacc = process(model, teacher, valid_data, top_k, None)\n        log(f\"VALID LOSS: {valid_loss:0.3f} \" + \"\".join([f\" acc@{k}: {acc:0.3f}\" for k, acc in zip(top_k, valid_kacc)]), logfile)\n\n        if valid_loss < best_loss:\n            plateau_count = 0\n            best_loss = valid_loss\n            model.save_state(os.path.join(running_dir, 'best_params.pkl'))\n            log(f\"  best model so far\", logfile)\n        else:\n            plateau_count += 1\n            if plateau_count % early_stopping == 0:\n                log(f\"  {plateau_count} epochs without improvement, early stopping\", logfile)\n                break\n            if plateau_count % patience == 0:\n                lr *= 0.2\n                log(f\"  {plateau_count} epochs without improvement, decreasing l\", logfile)\n\n# ==========================================\n# File: 05_evaluate_hybrid.py\n# Function/Context: PolicyBranching\n# ==========================================\nimport os\nimport sys\nimport importlib\nimport argparse\nimport csv\nimport math\nimport numpy as np\nimport time\nimport pickle\nimport pathlib\nimport pyscipopt as scip\n\nimport tensorflow as tf\nimport torch\nimport utilities\nfrom utilities import _get_model_type\n\n\nclass PolicyBranching(scip.Branchrule):\n\n    def __init__(self, policy, device):\n        super().__init__()\n\n        self.policy_type = policy['type']\n        self.policy_name = policy['name']\n        self.device = device\n\n        model = policy['model']\n        model.restore_state(policy['parameters'])\n        model.to(device)\n        model.eval()\n        self.policy_get_root_params = model.get_params\n        self.policy_predict = model.predict\n\n        self.teacher = None\n        if policy['teacher_type'] is not None:\n            self.teacher = policy['teacher_model']\n            self.teacher.restore_state(policy['teacher_parameters'])\n            self.teacher.to(device)\n            self.teacher.eval()\n\n    def branchinitsol(self):\n        self.ndomchgs = 0\n        self.ncutoffs = 0\n        self.state_buffer = {}\n        self.khalil_root_buffer = {}\n\n    def branchexeclp(self, allowaddcons):\n\n        candidate_vars, *_ = self.model.getPseudoBranchCands()\n        candidate_mask = [var.getCol().getIndex() for var in candidate_vars]\n        state = utilities.extract_state(self.model, self.state_buffer)\n        c,e,v =  state\n        if self.model.getNNodes() == 1:\n            state = (\n                torch.as_tensor(c['values'], dtype=torch.float32),\n                torch.as_tensor(e['indices'], dtype=torch.long),\n                torch.as_tensor(e['values'], dtype=torch.float32),\n                torch.as_tensor(v['values'], dtype=torch.float32),\n                torch.as_tensor(c['values'].shape[0], dtype=torch.int32),\n                torch.as_tensor(v['values'].shape[0], dtype=torch.int32),\n            )\n            state = map(lambda x:x.to(self.device), state)\n            with torch.no_grad():\n                if self.teacher is not None:\n                    root_feats, _ = self.teacher(state)\n                    state = root_feats\n                self.root_params = self.policy_get_root_params(state)\n\n        v = v['values'][candidate_mask]\n        state_khalil = utilities.extract_khalil_variable_features(self.model, candidate_vars, self.khalil_root_buffer)\n\n        var_feats = np.concatenate([v, state_khalil, np.ones((v.shape[0],1))], axis=1)\n        var_feats = utilities._preprocess(var_feats, mode=\"min-max-2\")\n        var_feats = torch.as_tensor(var_feats, dtype=torch.float32).to(device)\n\n        with torch.no_grad():\n            var_logits = self.policy_predict(var_feats, self.root_params[candidate_mask]).cpu().numpy()\n\n        best_var = candidate_vars[var_logits.argmax()]\n        self.model.branchVar(best_var)\n        result = scip.SCIP_RESULT.BRANCHED\n\n        # fair node counting\n        if result == scip.SCIP_RESULT.REDUCEDDOM:\n            self.ndomchgs += 1\n        elif result == scip.SCIP_RESULT.CUTOFF:\n            self.ncutoffs += 1\n\n        return {'result': result}\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        'problem',\n        help='MILP instance type to process.',\n        choices=['setcover', 'cauctions', 'facilities', 'indset'],\n    )\n    parser.add_argument(\n        '-g', '--gpu',\n        help='CUDA GPU id (-1 for CPU).',\n        type=int,\n        default=-1,\n    )\n    parser.add_argument(\n        '-s', '--seed',\n        help='seed for parallelizing the evaluation. Uses all seeds if not provided.',\n        type=int,\n        default=-1\n    )\n    parser.add_argument(\n        '-l', '--level',\n        help='size of instances to evaluate. Default is all.',\n        type=str,\n        default='all',\n        choices=['all', 'small', 'medium', 'big']\n    )\n    parser.add_argument(\n        '--trained_models',\n        help='Directory of trained models.',\n        type=str,\n        default='trained_models/'\n    )\n    parser.add_argument(\n        '-m', '--model_string',\n        help='searches for this string in respective trained_models folder',\n        type=str,\n        default='',\n    )\n    parser.add_argument(\n        '--model_name',\n        help='searches for this model_name in respective trained_models folder',\n        type=str,\n        default='',\n    )\n    args = parser.parse_args()\n\n    teacher_model = \"baseline_torch\" # used if pretrained model is used\n    instances = []\n    seeds = [0, 1, 2]\n    time_limit = 2700\n\n    result_dir = f\"eval_results/{args.problem}\"\n    os.makedirs(result_dir, exist_ok=True)\n    device = \"CPU\" if args.gpu == -1 else \"GPU\"\n    result_file = f\"{result_dir}/hybrid_{device}_{time.strftime('%Y%m%d-%H%M%S')}.csv\"\n\n    ### MODELS TO EVALUATE ###\n    basedir = f\"{args.trained_models}/{args.problem}\"\n    if args.model_string != \"\":\n        models_to_evaluate = [y for y in pathlib.Path(basedir).iterdir() if args.model_string in y.name]\n        assert len(models_to_evaluate) > 0, f\"no model matched the model_string: {args.model_string} in {basedir}\"\n    elif args.model_name != \"\":\n        model_path = pathlib.Path(f\"{basedir}/{args.model_name}\")\n        assert model_path.exists(), f\"path: {model_path} doesn't exist in {basedir}\"\n        models_to_evaluate = [model_path]\n    else:\n        models_to_evaluate = [y for y in pathlib.Path(f\"{basedir}\").iterdir()]\n        assert len(models_to_evaluate) > 0, f\"no model found in {basedir}\"\n\n    if args.problem == 'setcover':\n        instances += [{'type': 'small', 'path': f\"data/instances/setcover/transfer_500r_1000c_0.05d/instance_{i+1}.lp\"} for i in range(20)]\n        instances += [{'type': 'medium', 'path': f\"data/instances/setcover/transfer_1000r_1000c_0.05d/instance_{i+1}.lp\"} for i in range(20)]\n        instances += [{'type': 'big', 'path': f\"data/instances/setcover/transfer_2000r_1000c_0.05d/instance_{i+1}.lp\"} for i in range(20)]\n\n    elif args.problem == 'cauctions':\n        instances += [{'type': 'small', 'path': f\"data/instances/cauctions/transfer_100_500/instance_{i+1}.lp\"} for i in range(20)]\n        instances += [{'type': 'medium', 'path': f\"data/instances/cauctions/transfer_200_1000/instance_{i+1}.lp\"} for i in range(20)]\n        instances += [{'type': 'big', 'path': f\"data/instances/cauctions/transfer_300_1500/instance_{i+1}.lp\"} for i in range(20)]\n\n    elif args.problem == 'facilities':\n        instances += [{'type': 'small', 'path': f\"data/instances/facilities/transfer_100_100_5/instance_{i+1}.lp\"} for i in range(20)]\n        instances += [{'type': 'medium', 'path': f\"data/instances/facilities/transfer_200_100_5/instance_{i+1}.lp\"} for i in range(20)]\n        instances += [{'type': 'big', 'path': f\"data/instances/facilities/transfer_400_100_5/instance_{i+1}.lp\"} for i in range(20)]\n\n    elif args.problem == 'indset':\n        instances += [{'type': 'small', 'path': f\"data/instances/indset/transfer_750_4/instance_{i+1}.lp\"} for i in range(20)]\n        instances += [{'type': 'medium', 'path': f\"data/instances/indset/transfer_1000_4/instance_{i+1}.lp\"} for i in range(20)]\n        instances += [{'type': 'big', 'path': f\"data/instances/indset/transfer_1500_4/instance_{i+1}.lp\"} for i in range(20)]\n\n    else:\n        raise NotImplementedError\n\n    ### SEEDS TO EVALUATE ###\n    if args.seed != -1:\n        seeds = [args.seed]\n\n    ### PROBLEM SIZES TO EVALUATE ###\n    if args.level != \"all\":\n        instances = [x for x in instances if x['type'] == args.level]\n\n    branching_policies = []\n\n    ### MODEL PARAMETERS\n    for model_path in models_to_evaluate:\n        try:\n            model_type = _get_model_type(model_path.name)\n        except ValueError as e:\n            print(e, \"skipping ...\")\n            continue\n\n        for seed in seeds:\n            policy = {\n                'type': model_type,\n                'name': model_path.name,\n                'seed': seed,\n                'parameters': str(model_path / f\"{seed}/best_params.pkl\"),\n                'teacher_type': None,\n                'teacher_parameters': None\n            }\n            if \"-pre\" in model_type:\n                policy['teacher_type'] = teacher_model\n                policy['teacher_parameters'] = f\"{basedir}/{teacher_model}/{seed}/best_params.pkl\"\n            branching_policies.append(policy)\n\n    print(f\"problem: {args.problem}\")\n    print(f\"gpu: {args.gpu}\")\n    print(f\"time limit: {time_limit} s\")\n\n    ### NUMPY / TORCH SETUP ###\n    if args.gpu == -1:\n        os.environ['CUDA_VISIBLE_DEVICES'] = ''\n        device = torch.device(\"cpu\")\n    else:\n        os.environ['CUDA_VISIBLE_DEVICES'] = f'{args.gpu}'\n        device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # load and assign tensorflow models to policies (share models and update parameters)\n    loaded_models = {}\n    for policy in branching_policies:\n        if policy['type'] not in loaded_models:\n            sys.path.insert(0, os.path.abspath(f\"models/{policy['type']}\"))\n            import model\n            importlib.reload(model)\n            loaded_models[policy['type']] = model.Policy()\n            del sys.path[0]\n            loaded_models[policy['type']].to(device)\n            loaded_models[policy['type']].eval()\n\n        if (\n            policy['teacher_type'] is not None\n            and policy['teacher_type'] not in loaded_models\n        ):\n            sys.path.insert(0, os.path.abspath(f\"models/{policy['teacher_type']}\"))\n            import model\n            importlib.reload(model)\n            loaded_models[policy['teacher_type']] = model.GCNPolicy()\n            del sys.path[0]\n            loaded_models[policy['teacher_type']].to(device)\n            loaded_models[policy['teacher_type']].eval()\n\n        policy['model'] = loaded_models[policy['type']]\n        policy['teacher_model'] = loaded_models[policy['teacher_type']] if policy['teacher_type'] is not None else None\n\n    print(\"running SCIP...\")\n\n    fieldnames = [\n        'problem',\n        'device',\n        'policy',\n        'seed',\n        'type',\n        'instance',\n        'nnodes',\n        'nlps',\n        'stime',\n        'gap',\n        'status',\n        'ndomchgs',\n        'ncutoffs',\n        'walltime',\n        'proctime',\n    ]\n\n    with open(result_file, 'w', newline='') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n\n        for instance in instances:\n            print(f\"{instance['type']}: {instance['path']}...\")\n\n            for policy in branching_policies:\n                torch.manual_seed(policy['seed'])\n\n                m = scip.Model()\n                m.setIntParam('display/verblevel', 0)\n                m.readProblem(f\"{instance['path']}\")\n                utilities.init_scip_params(m, seed=policy['seed'])\n                m.setIntParam('timing/clocktype', 1)  # 1: CPU user seconds, 2: wall clock time\n                m.setRealParam('limits/time', time_limit)\n\n                brancher = PolicyBranching(policy, device)\n                m.includeBranchrule(\n                    branchrule=brancher,\n                    name=f\"{policy['type']}:{policy['name']}\",\n                    desc=f\"Custom MLPOpt branching policy.\",\n                    priority=666666, maxdepth=-1, maxbounddist=1)\n\n                walltime = time.perf_counter()\n                proctime = time.process_time()\n\n                m.optimize()\n\n                walltime = time.perf_counter() - walltime\n                proctime = time.process_time() - proctime\n\n                stime = m.getSolvingTime()\n                nnodes = m.getNNodes()\n                nlps = m.getNLPs()\n                gap = m.getGap()\n                status = m.getStatus()\n                ndomchgs = brancher.ndomchgs\n                ncutoffs = brancher.ncutoffs\n\n                writer.writerow({\n                    'policy': f\"{policy['type']}:{policy['name']}\",\n                    'seed': policy['seed'],\n                    'type': instance['type'],\n                    'instance': instance['path'],\n                    'nnodes': nnodes,\n                    'nlps': nlps,\n                    'stime': stime,\n                    'gap': gap,\n                    'status': status,\n                    'ndomchgs': ndomchgs,\n                    'ncutoffs': ncutoffs,\n                    'walltime': walltime,\n                    'proctime': proctime,\n                    'problem':args.problem,\n                    'device': device\n                })\n\n                csvfile.flush()\n                m.freeProb()\n\n                print(f\"  {policy['type']}:{policy['name']} {policy['seed']} - {nnodes} ({nnodes+2*(ndomchgs+ncutoffs)}) nodes {nlps} lps {stime:.2f} ({walltime:.2f} wall {proctime:.2f} proc) s. {status}\")\n\n# ==========================================\n# File: models/concat-pre/model.py\n# Function/Context: Policy\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nclass BaseModel(torch.nn.Module):\n    def initialize_parameters(self):\n        for l in self.modules():\n            if isinstance(l, torch.nn.Linear):\n                torch.nn.init.orthogonal_(l.weight.data, gain=1)\n                if l.bias is not None:\n                    torch.nn.init.constant_(l.bias.data, 0)\n\n    def _initialize_parameters(self):\n        for l in self.modules():\n            if isinstance(l, torch.nn.Linear):\n                torch.nn.init.xavier_normal_(l.weight.data)\n                if l.bias is not None:\n                    torch.nn.init.constant_(l.bias.data, 0)\n\n    def pretrain_init(self):\n        for module in self.modules():\n            if isinstance(module, PreNormLayer):\n                module.start_updates()\n\n    def pretrain(self, state):\n        with torch.no_grad():\n            try:\n                self.forward(state)\n                return False\n            except PreNormException:\n                return True\n\n    def pretrain_next(self):\n        for module in self.modules():\n            if isinstance(module, PreNormLayer) \\\n                    and module.waiting_updates and module.received_updates:\n                module.stop_updates()\n                return module\n        return None\n\n    def save_state(self, filepath):\n        torch.save(self.state_dict(), filepath)\n\n    def restore_state(self, filepath):\n        self.load_state_dict(torch.load(filepath, map_location=torch.device('cpu')))\n\nclass Policy(BaseModel):\n    def __init__(self):\n        super(Policy, self).__init__()\n\n        self.n_input_feats = 92\n        self.root_emb_size = 64\n        self.ff_size = 256\n\n        self.activation = torch.nn.LeakyReLU()\n\n        self.normalize_emb = nn.Sequential(\n            nn.LayerNorm(self.root_emb_size),\n            nn.Linear(self.root_emb_size, self.root_emb_size),\n            self.activation\n        )\n        # OUTPUT\n        self.output_module = nn.Sequential(\n            nn.Linear(self.n_input_feats + self.root_emb_size, self.ff_size, bias=True),\n            self.activation,\n            nn.Linear(self.ff_size, self.ff_size, bias=True),\n            self.activation,\n            nn.Linear(self.ff_size, self.ff_size, bias=True),\n            self.activation,\n            nn.Linear(self.ff_size, 1, bias=False)\n        )\n\n        self.initialize_parameters()\n\n    @staticmethod\n    def pad_output(output, n_vars_per_sample, pad_value=-1e8):\n        n_vars_max = torch.max(n_vars_per_sample)\n\n        output = torch.split(\n            tensor=output,\n            split_size_or_sections=n_vars_per_sample.tolist(),\n            dim=1,\n        )\n\n        output = torch.cat([\n            F.pad(x,\n                pad=[0, n_vars_max - x.shape[1], 0, 0],\n                mode='constant',\n                value=pad_value)\n            for x in output\n        ], dim=0)\n\n        return output\n\n    def forward(self, inputs):\n        \"\"\"\n        Implements forward pass of the model\n\n        Parameters\n        ----------\n        root_c : torch.tensor\n            constraint features at the root node\n        root_ei : torch.tensor\n            indices to represent constraint-variable edges of the root node\n        root_ev : torch.tensor\n            edge features of the root node\n        root_v : torch.tensor\n            variable features at the root node\n        root_n_cs : torch.tensor\n            number of constraints per sample\n        root_n_vs : torch.tensor\n            number of variables per sample\n        candss : torch.tensor\n            candidate variable (strong branching candidates) indices at the root node\n        cand_feats : torch.tensor\n            candidate variable (strong branching candidates) features at a local node\n        cand_root_feats : torch.tensor\n            candidate root variable features at the root node\n\n        Return\n        ------\n        root_var_feats : torch.tensor\n            variable features computed from root gcnn (only if applicable)\n        logits : torch.tensor\n            output logits at the current node\n        parameters : torch.tensor\n            film-parameters to compute these logits\n        \"\"\"\n        cand_feats, cand_root_feats = inputs[-2:]\n\n        root_feats = self.normalize_emb(cand_root_feats)\n        input = torch.cat([cand_feats, root_feats], axis=1)\n        output = self.output_module(input)\n\n        output = torch.reshape(output, [1, -1])\n        return None, output, None\n\n    def get_params(self, root_feats):\n        \"\"\"\n        Returns parameters/variable representations inferred at the root node.\n\n        Parameters\n        ----------\n        root_feats : torch.tensor\n            variable embeddings as computed by the root node GNN\n\n        Returns\n        -------\n        (torch.tensor): variable representations / parameters as inferred from root gcnn and to be used else where in the tree.\n        \"\"\"\n        return self.normalize_emb(root_feats)\n\n    def predict(self, cand_feats, root_feats):\n        \"\"\"\n        Predicts score for each candindate represented by cand_feats\n\n        Parameters\n        ----------\n        cand_feats : torch.tensor\n            (2D) representing input features of variables at any node in the tree\n        film_parameters : torch.tensor\n            (2D) parameters that are used to module MLP outputs. Same size as cand_feats.\n\n        Returns\n        -------\n        (torch.tensor) : (1D) a score for each candidate\n        \"\"\"\n        input = torch.cat([cand_feats, root_feats], axis=1)\n        output = self.output_module(input)\n        output = torch.reshape(output, [1, -1])\n        return output\n\n# ==========================================\n# File: models/concat/model.py\n# Function/Context: Policy.forward\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass PreNormException(Exception):\n    pass\n\nclass PreNormLayer(nn.Module):\n    \"\"\"\n    Our pre-normalization layer, whose purpose is to normalize an input layer\n    to zero mean and unit variance to speed-up and stabilize GCN training. The\n    layer's parameters are aimed to be computed during the pre-training phase.\n    \"\"\"\n    def __init__(self, n_units, shift=True, scale=True):\n        super(PreNormLayer, self).__init__()\n        assert shift or scale\n\n        if shift:\n            self.register_buffer(f\"shift\", torch.zeros((n_units,), dtype=torch.float32))\n        else:\n            self.shift = None\n\n        if scale:\n            self.register_buffer(f\"scale\", torch.ones((n_units,), dtype=torch.float32))\n        else:\n            self.scale = None\n\n        self.n_units = n_units\n        self.waiting_updates = False\n        self.received_updates = False\n\n    def forward(self, input):\n        if self.waiting_updates:\n            self.update_stats(input)\n            self.received_updates = True\n            raise PreNormException\n\n        if self.shift is not None:\n            input = input + self.shift\n\n        if self.scale is not None:\n            input = input * self.scale\n\n        return input\n\n    def start_updates(self):\n        \"\"\"\n        Initializes the pre-training phase.\n        \"\"\"\n        self.avg = 0\n        self.var = 0\n        self.m2 = 0\n        self.count = 0\n        self.waiting_updates = True\n        self.received_updates = False\n\n    def update_stats(self, input):\n        \"\"\"\n        Online mean and variance estimation. See: Chan et al. (1979) Updating\n        Formulae and a Pairwise Algorithm for Computing Sample Variances.\n        https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online_algorithm\n        \"\"\"\n        assert self.n_units == 1 or input.shape[-1] == self.n_units, f\"Expected input dimension of size {self.n_units}, got {input.shape[-1]}.\"\n\n        input = input.reshape([-1, self.n_units])\n        sample_avg = torch.mean(input, dim=0)\n        sample_var = torch.mean((input - sample_avg) ** 2, dim=0)\n        sample_count = input.numel() / self.n_units\n\n        delta = sample_avg - self.avg\n\n        self.m2 = self.var * self.count + sample_var * sample_count + delta ** 2 * self.count * sample_count / (\n                self.count + sample_count)\n\n        self.count += sample_count\n        self.avg += delta * sample_count / self.count\n        self.var = self.m2 / self.count if self.count > 0 else 1\n\n    def stop_updates(self):\n        \"\"\"\n        Ends pre-training for that layer, and fixes the layers's parameters.\n        \"\"\"\n\n        assert self.count > 0\n        if self.shift is not None:\n            self.shift = - self.avg\n\n        if self.scale is not None:\n            self.var = torch.where(torch.eq(self.var, 0.0), torch.ones_like(self.var), self.var) # NaN check trick\n            self.scale = 1 / torch.sqrt(self.var)\n\n        del self.avg, self.var, self.m2, self.count\n        self.waiting_updates = False\n\nclass BipartiteGraphConvolution(nn.Module):\n    \"\"\"\n    Partial bipartite graph convolution (either left-to-right or right-to-left).\n    \"\"\"\n\n    def __init__(self, emb_size, activation, initializer, right_to_left=False):\n        super(BipartiteGraphConvolution, self).__init__()\n\n        self.emb_size = emb_size\n        self.activation = activation\n        self.initializer = initializer\n        self.right_to_left = right_to_left\n        self.edge_nfeats = 1\n\n        # feature layers\n        self.feature_module_left = nn.Sequential(\n            nn.Linear(self.emb_size, self.emb_size, bias=True),\n            self.activation\n        )\n\n        self.feature_module_edge = nn.Sequential(\n            nn.Linear(self.edge_nfeats, self.emb_size, bias=False),\n            self.activation\n        )\n\n        self.feature_module_right = nn.Sequential(\n            nn.Linear(self.emb_size, self.emb_size, bias=False),\n            self.activation\n        )\n\n        self.feature_module_final = nn.Sequential(\n            PreNormLayer(1, shift=False), # normalize after summation trick\n            self.activation,\n            nn.Linear(self.emb_size, self.emb_size, bias=True)\n        )\n\n        self.post_conv_module = nn.Sequential(\n            PreNormLayer(1, shift=False), # normalize after summation trick\n        )\n\n        # output_layers\n        self.output_module = nn.Sequential(\n            nn.Linear(self.emb_size + self.emb_size, self.emb_size, bias=True),\n            self.activation,\n            nn.Linear(self.emb_size, self.emb_size, bias=True)\n        )\n\n    def forward(self, inputs):\n        \"\"\"\n        Perfoms a partial graph convolution on the given bipartite graph.\n\n        Inputs\n        ------\n        left_features: 2D float tensor\n            Features of the left-hand-side nodes in the bipartite graph\n        edge_indices: 2D int tensor\n            Edge indices in left-right order\n        edge_features: 2D float tensor\n            Features of the edges\n        right_features: 2D float tensor\n            Features of the right-hand-side nodes in the bipartite graph\n        scatter_out_size: 1D int tensor\n            Output size (left_features.shape[0] or right_features.shape[0], unknown at compile time)\n        \"\"\"\n        left_features, edge_indices, edge_features, right_features, scatter_out_size = inputs\n        device = left_features.device\n\n        if self.right_to_left:\n            scatter_dim = 0\n            prev_features = left_features\n        else:\n            scatter_dim = 1\n            prev_features = right_features\n\n        # compute joint features\n        joint_features = self.feature_module_final(\n            self.feature_module_left(left_features)[edge_indices[0]] +\n            self.feature_module_edge(edge_features) +\n            self.feature_module_right(right_features)[edge_indices[1]]\n        )\n\n        # perform convolution\n        conv_output = torch.zeros([scatter_out_size, self.emb_size], device=device).index_add_(0, edge_indices[scatter_dim], joint_features)\n        conv_output = self.post_conv_module(conv_output)\n\n        # apply final module\n        output = self.output_module(torch.cat([\n            conv_output,\n            prev_features\n        ], axis=1))\n\n        return output\n\nclass GCNPolicy(nn.Module):\n    \"\"\"\n    Our bipartite Graph Convolutional neural Network (GCN) model.\n    \"\"\"\n\n    def __init__(self):\n        super(GCNPolicy, self).__init__()\n\n        self.emb_size = 64\n        self.cons_nfeats = 5\n        self.edge_nfeats = 1\n        self.var_nfeats = 19\n\n        self.activation = nn.ReLU()\n        self.initializer = lambda x: torch.nn.init.orthogonal_(x, gain=1)\n\n        # CONSTRAINT EMBEDDING\n        self.cons_embedding = nn.Sequential(\n            PreNormLayer(n_units=self.cons_nfeats),\n            nn.Linear(self.cons_nfeats, self.emb_size, bias=True),\n            self.activation,\n            nn.Linear(self.emb_size, self.emb_size, bias=True),\n            self.activation\n        )\n\n        # EDGE EMBEDDING\n        self.edge_embedding = nn.Sequential(\n            PreNormLayer(self.edge_nfeats),\n        )\n\n        # VARIABLE_EMBEDDING\n        self.var_embedding = nn.Sequential(\n            PreNormLayer(n_units=self.var_nfeats),\n            nn.Linear(self.var_nfeats, self.emb_size, bias=True),\n            self.activation,\n            nn.Linear(self.emb_size, self.emb_size, bias=True),\n            self.activation\n        )\n\n        # GRAPH CONVOLUTIONS\n        self.conv_v_to_c = BipartiteGraphConvolution(self.emb_size, self.activation, self.initializer, right_to_left=True)\n        self.conv_c_to_v = BipartiteGraphConvolution(self.emb_size, self.activation, self.initializer)\n\n        # self.initialize_parameters()\n\n    def forward(self, inputs):\n        \"\"\"\n        Accepts stacked mini-batches, i.e. several bipartite graphs aggregated\n        as one. In that case the number of variables per samples has to be\n        provided, and the output consists in a padded dense tensor.\n\n        Parameters\n        ----------\n        inputs: list of tensors\n            Model input as a bipartite graph. May be batched into a stacked graph.\n\n        Inputs\n        ------\n        constraint_features: 2D float tensor\n            Constraint node features (n_constraints x n_constraint_features)\n        edge_indices: 2D int tensor\n            Edge constraint and variable indices (2, n_edges)\n        edge_features: 2D float tensor\n            Edge features (n_edges, n_edge_features)\n        variable_features: 2D float tensor\n            Variable node features (n_variables, n_variable_features)\n        n_cons_per_sample: 1D int tensor\n            Number of constraints for each of the samples stacked in the batch.\n        n_vars_per_sample: 1D int tensor\n            Number of variables for each of the samples stacked in the batch.\n        \"\"\"\n        constraint_features, edge_indices, edge_features, variable_features, n_cons_per_sample, n_vars_per_sample = inputs\n        n_cons_total = torch.sum(n_cons_per_sample)\n        n_vars_total = torch.sum(n_vars_per_sample)\n\n        # EMBEDDINGS\n        constraint_features = self.cons_embedding(constraint_features)\n        edge_features = self.edge_embedding(edge_features)\n        variable_features = self.var_embedding(variable_features)\n\n        # GRAPH CONVOLUTIONS\n        constraint_features = self.conv_v_to_c((\n            constraint_features, edge_indices, edge_features, variable_features, n_cons_total))\n        constraint_features = self.activation(constraint_features)\n\n        variable_features = self.conv_c_to_v((\n            constraint_features, edge_indices, edge_features, variable_features, n_vars_total))\n        variable_features = self.activation(variable_features)\n\n        return variable_features\n\nclass BaseModel(nn.Module):\n    def initialize_parameters(self):\n        for l in self.modules():\n            if isinstance(l, torch.nn.Linear):\n                self.initializer(l.weight.data)\n                if l.bias is not None:\n                    torch.nn.init.constant_(l.bias.data, 0)\n\n    def pre_train_init(self):\n        for module in self.modules():\n            if isinstance(module, PreNormLayer) or module._get_name() == \"PreNormLayer\":\n                module.start_updates()\n\n    def pre_train(self, state):\n        with torch.no_grad():\n            try:\n                self.forward(state)\n                return False\n            except PreNormException:\n                return True\n\n    def pre_train_next(self):\n        for module in self.modules():\n            if (isinstance(module, PreNormLayer) or module._get_name() == \"PreNormLayer\") \\\n                    and module.waiting_updates and module.received_updates:\n                module.stop_updates()\n                return module\n        return None\n\n    def save_state(self, filepath):\n        torch.save(self.state_dict(), filepath)\n\n    def restore_state(self, filepath):\n        self.load_state_dict(torch.load(filepath, map_location=torch.device('cpu')))\n\nclass Policy(BaseModel):\n    def __init__(self):\n        super(Policy, self).__init__()\n\n        self.root_gcn = GCNPolicy()\n        self.n_input_feats = 92\n        self.root_emb_size = self.root_gcn.emb_size\n        self.ff_size = 256\n\n        self.activation = torch.nn.LeakyReLU()\n        self.initializer = lambda x: torch.nn.init.orthogonal_(x, gain=1)\n\n        self.normalize_emb = nn.Sequential(\n            nn.LayerNorm(self.root_emb_size)\n        )\n        # OUTPUT\n        self.output_module = nn.Sequential(\n            nn.Linear(self.n_input_feats + self.root_emb_size, self.ff_size, bias=True),\n            self.activation,\n            nn.Linear(self.ff_size, self.ff_size, bias=True),\n            self.activation,\n            nn.Linear(self.ff_size, self.ff_size, bias=True),\n            self.activation,\n            nn.Linear(self.ff_size, 1, bias=False)\n        )\n\n        self.initialize_parameters()\n\n    @staticmethod\n    def pad_output(output, n_vars_per_sample, pad_value=-1e8):\n        n_vars_max = torch.max(n_vars_per_sample)\n\n        output = torch.split(\n            tensor=output,\n            split_size_or_sections=n_vars_per_sample.tolist(),\n            dim=1,\n        )\n\n        output = torch.cat([\n            F.pad(x,\n                pad=[0, n_vars_max - x.shape[1], 0, 0],\n                mode='constant',\n                value=pad_value)\n            for x in output\n        ], dim=0)\n\n        return output\n\n    def forward(self, inputs):\n        \"\"\"\n        Implements forward pass of the model\n\n        Parameters\n        ----------\n        root_c : torch.tensor\n            constraint features at the root node\n        root_ei : torch.tensor\n            indices to represent constraint-variable edges of the root node\n        root_ev : torch.tensor\n            edge features of the root node\n        root_v : torch.tensor\n            variable features at the root node\n        root_n_cs : torch.tensor\n            number of constraints per sample\n        root_n_vs : torch.tensor\n            number of variables per sample\n        candss : torch.tensor\n            candidate variable (strong branching candidates) indices at the root node\n        cand_feats : torch.tensor\n            candidate variable (strong branching candidates) features at a local node\n        cand_root_feats : torch.tensor\n            candidate root variable features at the root node\n\n        Return\n        ------\n        root_var_feats : torch.tensor\n            variable features computed from root gcnn (only if applicable)\n        logits : torch.tensor\n            output logits at the current node\n        parameters : torch.tensor\n            film-parameters to compute these logits (only if applicable)\n        \"\"\"\n        root_c, root_ei, root_ev, root_v, root_n_cs, root_n_vs, candss, cand_feats, _ = inputs\n\n        variable_features = self.root_gcn((root_c, root_ei, root_ev, root_v, root_n_cs, root_n_vs))\n        root_feats = variable_features[candss]\n\n        root_feats = self.normalize_emb(root_feats)\n        input = torch.cat([cand_feats, root_feats], axis=1)\n        output = self.output_module(input)\n\n        output = torch.reshape(output, [1, -1])\n        return F.normalize(variable_features, p=2, dim=1), output, None\n\n    def get_params(self, state):\n        \"\"\"\n        Returns parameters/variable representations inferred at the root node.\n\n        Parameters\n        ----------\n        inputs : torch.tensor\n            inputs to be used by the root node GNN\n\n        Returns\n        -------\n        (torch.tensor): variable representations / parameters as inferred from root gcnn and to be used else where in the tree.\n        \"\"\"\n        variable_features = self.root_gcn(state)\n        return self.normalize_emb(variable_features)\n\n    def predict(self, cand_feats, root_params):\n        \"\"\"\n        Predicts score for each candindate represented b\n        \"\"\"\n\n# ==========================================\n# File: models/film-pre/model.py\n# Function/Context: Policy\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nclass BaseModel(torch.nn.Module):\n    def initialize_parameters(self):\n        for l in self.modules():\n            if isinstance(l, torch.nn.Linear):\n                torch.nn.init.orthogonal_(l.weight.data, gain=1)\n                if l.bias is not None:\n                    torch.nn.init.constant_(l.bias.data, 0)\n\n    def _initialize_parameters(self):\n        for l in self.modules():\n            if isinstance(l, torch.nn.Linear):\n                torch.nn.init.xavier_normal_(l.weight.data)\n                if l.bias is not None:\n                    torch.nn.init.constant_(l.bias.data, 0)\n\n    def pretrain_init(self):\n        for module in self.modules():\n            if isinstance(module, PreNormLayer):\n                module.start_updates()\n\n    def pretrain(self, state):\n        with torch.no_grad():\n            try:\n                self.forward(state)\n                return False\n            except PreNormException:\n                return True\n\n    def pretrain_next(self):\n        for module in self.modules():\n            if isinstance(module, PreNormLayer) \\\n                    and module.waiting_updates and module.received_updates:\n                module.stop_updates()\n                return module\n        return None\n\n    def save_state(self, filepath):\n        torch.save(self.state_dict(), filepath)\n\n    def restore_state(self, filepath):\n        self.load_state_dict(torch.load(filepath, map_location=torch.device('cpu')))\n\nclass MLP(nn.Module):\n    def __init__(self, in_size, out_size, activation):\n        super(MLP, self).__init__()\n\n        self.out = nn.Linear(in_size, out_size, bias=True)\n        self.activation = activation\n\n    def forward(self, input, betagamma):\n        x = self.activation(self.out(input))\n        x = x * betagamma[:, 0] + betagamma[:, 1]\n        return x\n\nclass Policy(BaseModel):\n    def __init__(self):\n        super(Policy, self).__init__()\n\n        self.n_input_feats = 92\n        self.root_emb_size = 64\n        self.ff_size = 256\n        self.n_layers = 3\n\n        self.activation = torch.nn.LeakyReLU()\n\n        # FILM GENEARTOR\n        self.film_generator = nn.Sequential(\n            nn.LayerNorm(self.root_emb_size),\n            nn.Linear(self.root_emb_size, self.root_emb_size),\n            self.activation,\n            nn.Linear(self.root_emb_size, self.n_layers * self.ff_size * 2)\n        )\n\n        # OUTPUT\n        self.network = nn.ModuleList([\n            MLP(self.n_input_feats, self.ff_size, self.activation),\n            MLP(self.ff_size, self.ff_size, self.activation),\n            MLP(self.ff_size, self.ff_size, self.activation)\n        ])\n        self.out = nn.Linear(self.ff_size, 1, bias=False)\n\n        self.initialize_parameters()\n\n    @staticmethod\n    def pad_output(output, n_vars_per_sample, pad_value=-1e8):\n        n_vars_max = torch.max(n_vars_per_sample)\n\n        output = torch.split(\n            tensor=output,\n            split_size_or_sections=n_vars_per_sample.tolist(),\n            dim=1,\n        )\n\n        output = torch.cat([\n            F.pad(x,\n                pad=[0, n_vars_max - x.shape[1], 0, 0],\n                mode='constant',\n                value=pad_value)\n            for x in output\n        ], dim=0)\n\n        return output\n\n    def forward(self, inputs):\n        \"\"\"\n        Implements forward pass of the model\n\n        Parameters\n        ----------\n        root_c : torch.tensor\n            constraint features at the root node\n        root_ei : torch.tensor\n            indices to represent constraint-variable edges of the root node\n        root_ev : torch.tensor\n            edge features of the root node\n        root_v : torch.tensor\n            variable features at the root node\n        root_n_cs : torch.tensor\n            number of constraints per sample\n        root_n_vs : torch.tensor\n            number of variables per sample\n        candss : torch.tensor\n            candidate variable (strong branching candidates) indices at the root node\n        cand_feats : torch.tensor\n            candidate variable (strong branching candidates) features at a local node\n        cand_root_feats : torch.tensor\n            candidate root variable features at the root node\n\n        Return\n        ------\n        root_var_feats : torch.tensor\n            variable features computed from root gcnn (only if applicable)\n        logits : torch.tensor\n            output logits at the current node\n        parameters : torch.tensor\n            film-parameters to compute these logits (only if applicable)\n        \"\"\"\n        cand_feats, cand_root_feats = inputs[-2:]\n\n        film_parameters = self.film_generator(cand_root_feats)\n        film_parameters = film_parameters.view(-1, self.n_layers, 2, self.ff_size)\n\n        x = cand_feats\n        for n, subnet in enumerate(self.network):\n            x = subnet(x, film_parameters[:, n])\n\n        output = self.out(x)\n        output = torch.reshape(output, [1, -1])\n        return None, output, film_parameters\n\n    def get_params(self, root_feats):\n        \"\"\"\n        Returns parameters/variable representations inferred at the root node.\n\n        Parameters\n        ----------\n        root_feats : torch.tensor\n            variable embeddings as computed by the root node GNN\n\n        Returns\n        -------\n        (torch.tensor): variable representations / parameters as inferred from root gcnn and to be used else where in the tree.\n        \"\"\"\n        film_parameters = self.film_generator(root_feats)\n        return film_parameters.view(-1, self.n_layers, 2, self.ff_size)\n\n    def predict(self, cand_feats, film_parameters):\n        \"\"\"\n        Predicts score for each candindate represented by cand_feats\n\n        Parameters\n        ----------\n        cand_feats : torch.tensor\n            (2D) representing input features of variables at any node in the tree\n        film_parameters : torch.tensor\n            (2D) parameters that are used to module MLP outputs. Same size as cand_feats.\n\n        Returns\n        -------\n        (torch.tensor) : (1D) a score for each candidate\n        \"\"\"\n        x = cand_feats\n        for n, subnet in enumerate(self.network):\n            x = subnet(x, film_parameters[:, n])\n        output = self.out(x)\n        output = torch.reshape(output, [1, -1])\n        return output\n\n# ==========================================\n# File: models/film/model.py\n# Function/Context: Policy\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass PreNormException(Exception):\n    pass\n\nclass PreNormLayer(nn.Module):\n    \"\"\"\n    Our pre-normalization layer, whose purpose is to normalize an input layer\n    to zero mean and unit variance to speed-up and stabilize GCN training. The\n    layer's parameters are aimed to be computed during the pre-training phase.\n    \"\"\"\n    def __init__(self, n_units, shift=True, scale=True):\n        super(PreNormLayer, self).__init__()\n        assert shift or scale\n\n        if shift:\n            self.register_buffer(f\"shift\", torch.zeros((n_units,), dtype=torch.float32))\n        else:\n            self.shift = None\n\n        if scale:\n            self.register_buffer(f\"scale\", torch.ones((n_units,), dtype=torch.float32))\n        else:\n            self.scale = None\n\n        self.n_units = n_units\n        self.waiting_updates = False\n        self.received_updates = False\n\n    def forward(self, input):\n        if self.waiting_updates:\n            self.update_stats(input)\n            self.received_updates = True\n            raise PreNormException\n\n        if self.shift is not None:\n            input = input + self.shift\n\n        if self.scale is not None:\n            input = input * self.scale\n\n        return input\n\n    def start_updates(self):\n        \"\"\"\n        Initializes the pre-training phase.\n        \"\"\"\n        self.avg = 0\n        self.var = 0\n        self.m2 = 0\n        self.count = 0\n        self.waiting_updates = True\n        self.received_updates = False\n\n    def update_stats(self, input):\n        \"\"\"\n        Online mean and variance estimation. See: Chan et al. (1979) Updating\n        Formulae and a Pairwise Algorithm for Computing Sample Variances.\n        https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online_algorithm\n        \"\"\"\n        assert self.n_units == 1 or input.shape[-1] == self.n_units, f\"Expected input dimension of size {self.n_units}, got {input.shape[-1]}.\"\n\n        input = input.reshape([-1, self.n_units])\n        sample_avg = torch.mean(input, dim=0)\n        sample_var = torch.mean((input - sample_avg) ** 2, dim=0)\n        sample_count = input.numel() / self.n_units\n\n        delta = sample_avg - self.avg\n\n        self.m2 = self.var * self.count + sample_var * sample_count + delta ** 2 * self.count * sample_count / (\n                self.count + sample_count)\n\n        self.count += sample_count\n        self.avg += delta * sample_count / self.count\n        self.var = self.m2 / self.count if self.count > 0 else 1\n\n    def stop_updates(self):\n        \"\"\"\n        Ends pre-training for that layer, and fixes the layers's parameters.\n        \"\"\"\n\n        assert self.count > 0\n        if self.shift is not None:\n            self.shift = - self.avg\n\n        if self.scale is not None:\n            self.var = torch.where(torch.eq(self.var, 0.0), torch.ones_like(self.var), self.var) # NaN check trick\n            self.scale = 1 / torch.sqrt(self.var)\n\n        del self.avg, self.var, self.m2, self.count\n        self.waiting_updates = False\n\nclass BipartiteGraphConvolution(nn.Module):\n    \"\"\"\n    Partial bipartite graph convolution (either left-to-right or right-to-left).\n    \"\"\"\n\n    def __init__(self, emb_size, activation, initializer, right_to_left=False):\n        super(BipartiteGraphConvolution, self).__init__()\n\n        self.emb_size = emb_size\n        self.activation = activation\n        self.initializer = initializer\n        self.right_to_left = right_to_left\n        self.edge_nfeats = 1\n\n        # feature layers\n        self.feature_module_left = nn.Sequential(\n            nn.Linear(self.emb_size, self.emb_size, bias=True),\n            self.activation\n        )\n\n        self.feature_module_edge = nn.Sequential(\n            nn.Linear(self.edge_nfeats, self.emb_size, bias=False),\n            self.activation\n        )\n\n        self.feature_module_right = nn.Sequential(\n            nn.Linear(self.emb_size, self.emb_size, bias=False),\n            self.activation\n        )\n\n        self.feature_module_final = nn.Sequential(\n            PreNormLayer(1, shift=False), # normalize after summation trick\n            self.activation,\n            nn.Linear(self.emb_size, self.emb_size, bias=True)\n        )\n\n        self.post_conv_module = nn.Sequential(\n            PreNormLayer(1, shift=False), # normalize after summation trick\n        )\n\n        # output_layers\n        self.output_module = nn.Sequential(\n            nn.Linear(self.emb_size + self.emb_size, self.emb_size, bias=True),\n            self.activation,\n            nn.Linear(self.emb_size, self.emb_size, bias=True)\n        )\n\n    def forward(self, inputs):\n        \"\"\"\n        Perfoms a partial graph convolution on the given bipartite graph.\n\n        Inputs\n        ------\n        left_features: 2D float tensor\n            Features of the left-hand-side nodes in the bipartite graph\n        edge_indices: 2D int tensor\n            Edge indices in left-right order\n        edge_features: 2D float tensor\n            Features of the edges\n        right_features: 2D float tensor\n            Features of the right-hand-side nodes in the bipartite graph\n        scatter_out_size: 1D int tensor\n            Output size (left_features.shape[0] or right_features.shape[0], unknown at compile time)\n        \"\"\"\n        left_features, edge_indices, edge_features, right_features, scatter_out_size = inputs\n        device = left_features.device\n\n        if self.right_to_left:\n            scatter_dim = 0\n            prev_features = left_features\n        else:\n            scatter_dim = 1\n            prev_features = right_features\n\n        # compute joint features\n        joint_features = self.feature_module_final(\n            self.feature_module_left(left_features)[edge_indices[0]] +\n            self.feature_module_edge(edge_features) +\n            self.feature_module_right(right_features)[edge_indices[1]]\n        )\n\n        # perform convolution\n        conv_output = torch.zeros([scatter_out_size, self.emb_size], device=device).index_add_(0, edge_indices[scatter_dim], joint_features)\n        conv_output = self.post_conv_module(conv_output)\n\n        # apply final module\n        output = self.output_module(torch.cat([\n            conv_output,\n            prev_features\n        ], axis=1))\n\n        return output\n\nclass GCNPolicy(nn.Module):\n    \"\"\"\n    Our bipartite Graph Convolutional neural Network (GCN) model.\n    \"\"\"\n\n    def __init__(self):\n        super(GCNPolicy, self).__init__()\n\n        self.emb_size = 64\n        self.cons_nfeats = 5\n        self.edge_nfeats = 1\n        self.var_nfeats = 19\n\n        self.activation = nn.ReLU()\n        self.initializer = lambda x: torch.nn.init.orthogonal_(x, gain=1)\n\n        # CONSTRAINT EMBEDDING\n        self.cons_embedding = nn.Sequential(\n            PreNormLayer(n_units=self.cons_nfeats),\n            nn.Linear(self.cons_nfeats, self.emb_size, bias=True),\n            self.activation,\n            nn.Linear(self.emb_size, self.emb_size, bias=True),\n            self.activation\n        )\n\n        # EDGE EMBEDDING\n        self.edge_embedding = nn.Sequential(\n            PreNormLayer(self.edge_nfeats),\n        )\n\n        # VARIABLE_EMBEDDING\n        self.var_embedding = nn.Sequential(\n            PreNormLayer(n_units=self.var_nfeats),\n            nn.Linear(self.var_nfeats, self.emb_size, bias=True),\n            self.activation,\n            nn.Linear(self.emb_size, self.emb_size, bias=True),\n            self.activation\n        )\n\n        # GRAPH CONVOLUTIONS\n        self.conv_v_to_c = BipartiteGraphConvolution(self.emb_size, self.activation, self.initializer, right_to_left=True)\n        self.conv_c_to_v = BipartiteGraphConvolution(self.emb_size, self.activation, self.initializer)\n\n        # OUTPUT\n        self.output_module = nn.Sequential(\n            nn.Linear(self.emb_size, self.emb_size, bias=True),\n            self.activation,\n            nn.Linear(self.emb_size, 1, bias=False)\n        )\n\n        # self.initialize_parameters()\n\n    def forward(self, inputs, logits=True):\n        \"\"\"\n        Accepts stacked mini-batches, i.e. several bipartite graphs aggregated\n        as one. In that case the number of variables per samples has to be\n        provided, and the output consists in a padded dense tensor.\n\n        Parameters\n        ----------\n        inputs: list of tensors\n            Model input as a bipartite graph. May be batched into a stacked graph.\n\n        Inputs\n        ------\n        constraint_features: 2D float tensor\n            Constraint node features (n_constraints x n_constraint_features)\n        edge_indices: 2D int tensor\n            Edge constraint and variable indices (2, n_edges)\n        edge_features: 2D float tensor\n            Edge features (n_edges, n_edge_features)\n        variable_features: 2D float tensor\n            Variable node features (n_variables, n_variable_features)\n        n_cons_per_sample: 1D int tensor\n            Number of constraints for each of the samples stacked in the batch.\n        n_vars_per_sample: 1D int tensor\n            Number of variables for each of the samples stacked in the batch.\n        \"\"\"\n        constraint_features, edge_indices, edge_features, variable_features, n_cons_per_sample, n_vars_per_sample = inputs\n        n_cons_total = torch.sum(n_cons_per_sample)\n        n_vars_total = torch.sum(n_vars_per_sample)\n\n        # EMBEDDINGS\n        constraint_features = self.cons_embedding(constraint_features)\n        edge_features = self.edge_embedding(edge_features)\n        variable_features = self.var_embedding(variable_features)\n\n        # GRAPH CONVOLUTIONS\n        constraint_features = self.conv_v_to_c((\n            constraint_features, edge_indices, edge_features, variable_features, n_cons_total))\n        constraint_features = self.activation(constraint_features)\n\n        variable_features = self.conv_c_to_v((\n            constraint_features, edge_indices, edge_features, variable_features, n_vars_total))\n        variable_features = self.activation(variable_features)\n\n        # OUTPUT\n        output = None\n        if logits:\n            output = self.output_module(variable_features)\n            output = torch.reshape(output, [1, -1])\n\n        return variable_features, output\n\nclass MLP(nn.Module):\n    def __init__(self, in_size, out_size, activation):\n        super(MLP, self).__init__()\n\n        self.out = nn.Linear(in_size, out_size, bias=True)\n        self.activation = activation\n\n    def forward(self, input, betagamma):\n        x = self.activation(self.out(input))\n        x = x * betagamma[:, 0] + betagamma[:, 1]\n        return x\n\nclass BaseModel(nn.Module):\n    def initialize_parameters(self):\n        for l in self.modules():\n            if isinstance(l, torch.nn.Linear):\n                self.initializer(l.weight.data)\n                if l.bias is not None:\n                    torch.nn.init.constant_(l.bias.data, 0)\n\n    def pre_train_init(self):\n        for module in self.modules():\n            if isinstance(module, PreNormLayer) or module._get_name() == \"PreNormLayer\":\n                module.start_updates()\n\n    def pre_train(self, state):\n        with torch.no_grad():\n            try:\n                self.forward(state)\n                return False\n            except PreNormException:\n                return True\n\n    def pre_train_next(self):\n        for module in self.modules():\n            if (isinstance(module, PreNormLayer) or module._get_name() == \"PreNormLayer\") \\\n                    and module.waiting_updates and module.received_updates:\n                module.stop_updates()\n                return module\n        return None\n\n    def save_state(self, filepath):\n        torch.save(self.state_dict(), filepath)\n\n    def restore_state(self, filepath):\n        self.load_state_dict(torch.load(filepath, map_location=torch.device('cpu')))\n\nclass Policy(BaseModel):\n    def __init__(self):\n        super(Policy, self).__init__()\n\n        self.root_gcn = GCNPolicy()\n        self.n_input_feats = 92\n        self.root_emb_size = self.root_gcn.emb_size\n        self.ff_size = 256\n        self.n_layers = 3\n\n        self.activation = torch.nn.LeakyReLU()\n        self.initializer = lambda x: torch.nn.init.orthogonal_(x, gain=1)\n\n        # FILM GENEARTOR\n        self.film_generator = nn.Sequential(\n            nn.LayerNorm(self.root_emb_size),\n            nn.Linear(self.root_emb_size, self.n_layers * self.ff_size * 2)\n        )\n\n        # OUTPUT\n        self.network = nn.ModuleList([\n            MLP(self.n_input_feats, self.ff_size, self.activation),\n            MLP(self.ff_size, self.ff_size, self.activation),\n            MLP(self.ff_size, self.ff_size, self.activation)\n        ])\n        self.out = nn.Linear(self.ff_size, 1, bias=False)\n\n        self.initialize_parameters()\n\n    @staticmethod\n    def pad_output(output, n_vars_per_sample, pad_value=-1e8):\n        n_vars_max = torch.max(n_vars_per_sample)\n\n        output = torch.split(\n            tensor=output,\n            split_size_or_sections=n_vars_per_sample.tolist(),\n            dim=1,\n        )\n\n        output = torch.cat([\n            F.pad(x,\n                pad=[0, n_vars_max - x.shape[1], 0, 0],\n                mode='constant',\n                value=pad_value)\n            for x in output\n        ], dim=0)\n\n        return output\n\n    @staticmethod\n    def pad_features(features, n_vars_per_sample, pad_value=0):\n        n_vars_max = torch.max(n_vars_per_sample)\n        output = torch.split(\n            tensor=features,\n            split_size_or_sections=n_vars_per_sample.tolist(),\n            dim=0,\n        )\n\n        output = torch.cat([\n            F.pad(x,\n                pad=[0, 0, 0, n_vars_max - x.shape[0]],\n                mode='constant',\n                value=pad_value).unsqueeze(dim=0)\n            for x in output\n        ], dim=0)\n\n        return output\n\n    def forward(self, inputs):\n        \"\"\"\n        Implements forward pass of the model\n\n        Parameters\n        ----------\n        root_c : torch.tensor\n            constraint features at the root node\n        root_ei : torch.tensor\n            indices to represent constraint-variable edges of the root node\n        root_ev : torch.tensor\n            edge features of the root node\n        root_v : torch.tensor\n            variable features at the root node\n        root_n_cs : torch.tensor\n            number of constraints per sample\n        root_n_vs : torch.tensor\n            number of variables per sample\n        candss : torch.tensor\n            candidate variable (strong branching candidates) indices at the root node\n        cand_feats : torch.tensor\n            candidate variable (strong branching candidates) features at a local node\n        cand_root_feats : torch.tensor\n            candidate root variable features at the root node\n\n        Return\n        ------\n        root_var_feats : torch.tensor\n            variable features computed from root gcnn (only if applicable)\n        logi\n            \n            ---\n\n# ==========================================\n# File: models/hybridsvm-film/model.py\n# Function/Context: Policy\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass PreNormException(Exception):\n    pass\n\n\nclass PreNormLayer(nn.Module):\n    \"\"\"\n    Our pre-normalization layer, whose purpose is to normalize an input layer\n    to zero mean and unit variance to speed-up and stabilize GCN training. The\n    layer's parameters are aimed to be computed during the pre-training phase.\n    \"\"\"\n    def __init__(self, n_units, shift=True, scale=True):\n        super(PreNormLayer, self).__init__()\n        assert shift or scale\n\n        if shift:\n            self.register_buffer(f\"shift\", torch.zeros((n_units,), dtype=torch.float32))\n        else:\n            self.shift = None\n\n        if scale:\n            self.register_buffer(f\"scale\", torch.ones((n_units,), dtype=torch.float32))\n        else:\n            self.scale = None\n\n        self.n_units = n_units\n        self.waiting_updates = False\n        self.received_updates = False\n\n    def forward(self, input):\n        if self.waiting_updates:\n            self.update_stats(input)\n            self.received_updates = True\n            raise PreNormException\n\n        if self.shift is not None:\n            input = input + self.shift\n\n        if self.scale is not None:\n            input = input * self.scale\n\n        return input\n\n    def start_updates(self):\n        \"\"\"\n        Initializes the pre-training phase.\n        \"\"\"\n        self.avg = 0\n        self.var = 0\n        self.m2 = 0\n        self.count = 0\n        self.waiting_updates = True\n        self.received_updates = False\n\n    def update_stats(self, input):\n        \"\"\"\n        Online mean and variance estimation. See: Chan et al. (1979) Updating\n        Formulae and a Pairwise Algorithm for Computing Sample Variances.\n        https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online_algorithm\n        \"\"\"\n        assert self.n_units == 1 or input.shape[-1] == self.n_units, f\"Expected input dimension of size {self.n_units}, got {input.shape[-1]}.\"\n\n        input = input.reshape([-1, self.n_units])\n        sample_avg = torch.mean(input, dim=0)\n        sample_var = torch.mean((input - sample_avg) ** 2, dim=0)\n        sample_count = input.numel() / self.n_units\n\n        delta = sample_avg - self.avg\n\n        self.m2 = self.var * self.count + sample_var * sample_count + delta ** 2 * self.count * sample_count / (\n                self.count + sample_count)\n\n        self.count += sample_count\n        self.avg += delta * sample_count / self.count\n        self.var = self.m2 / self.count if self.count > 0 else 1\n\n    def stop_updates(self):\n        \"\"\"\n        Ends pre-training for that layer, and fixes the layers's parameters.\n        \"\"\"\n\n        assert self.count > 0\n        if self.shift is not None:\n            self.shift = - self.avg\n\n        if self.scale is not None:\n            self.var = torch.where(torch.eq(self.var, 0.0), torch.ones_like(self.var), self.var) # NaN check trick\n            self.scale = 1 / torch.sqrt(self.var)\n\n        del self.avg, self.var, self.m2, self.count\n        self.waiting_updates = False\n\n\nclass BipartiteGraphConvolution(nn.Module):\n    \"\"\"\n    Partial bipartite graph convolution (either left-to-right or right-to-left).\n    \"\"\"\n\n    def __init__(self, emb_size, activation, initializer, right_to_left=False):\n        super(BipartiteGraphConvolution, self).__init__()\n\n        self.emb_size = emb_size\n        self.activation = activation\n        self.initializer = initializer\n        self.right_to_left = right_to_left\n        self.edge_nfeats = 1\n\n        # feature layers\n        self.feature_module_left = nn.Sequential(\n            nn.Linear(self.emb_size, self.emb_size, bias=True),\n            self.activation\n        )\n\n        self.feature_module_edge = nn.Sequential(\n            nn.Linear(self.edge_nfeats, self.emb_size, bias=False),\n            self.activation\n        )\n\n        self.feature_module_right = nn.Sequential(\n            nn.Linear(self.emb_size, self.emb_size, bias=False),\n            self.activation\n        )\n\n        self.feature_module_final = nn.Sequential(\n            PreNormLayer(1, shift=False), # normalize after summation trick\n            self.activation,\n            nn.Linear(self.emb_size, self.emb_size, bias=True)\n        )\n\n        self.post_conv_module = nn.Sequential(\n            PreNormLayer(1, shift=False), # normalize after summation trick\n        )\n\n        # output_layers\n        self.output_module = nn.Sequential(\n            nn.Linear(self.emb_size + self.emb_size, self.emb_size, bias=True),\n            self.activation,\n            nn.Linear(self.emb_size, self.emb_size, bias=True)\n        )\n\n    def forward(self, inputs):\n        \"\"\"\n        Perfoms a partial graph convolution on the given bipartite graph.\n\n        Inputs\n        ------\n        left_features: 2D float tensor\n            Features of the left-hand-side nodes in the bipartite graph\n        edge_indices: 2D int tensor\n            Edge indices in left-right order\n        edge_features: 2D float tensor\n            Features of the edges\n        right_features: 2D float tensor\n            Features of the right-hand-side nodes in the bipartite graph\n        scatter_out_size: 1D int tensor\n            Output size (left_features.shape[0] or right_features.shape[0], unknown at compile time)\n        \"\"\"\n        left_features, edge_indices, edge_features, right_features, scatter_out_size = inputs\n        device = left_features.device\n\n        if self.right_to_left:\n            scatter_dim = 0\n            prev_features = left_features\n        else:\n            scatter_dim = 1\n            prev_features = right_features\n\n        # compute joint features\n        joint_features = self.feature_module_final(\n            self.feature_module_left(left_features)[edge_indices[0]] +\n            self.feature_module_edge(edge_features) +\n            self.feature_module_right(right_features)[edge_indices[1]]\n        )\n\n        # perform convolution\n        conv_output = torch.zeros([scatter_out_size, self.emb_size], device=device).index_add_(0, edge_indices[scatter_dim], joint_features)\n        conv_output = self.post_conv_module(conv_output)\n\n        # apply final module\n        output = self.output_module(torch.cat([\n            conv_output,\n            prev_features\n        ], axis=1))\n\n        return output\n\n\nclass GCNPolicy(nn.Module):\n    \"\"\"\n    Our bipartite Graph Convolutional neural Network (GCN) model.\n    \"\"\"\n\n    def __init__(self):\n        super(GCNPolicy, self).__init__()\n\n        self.emb_size = 64\n        self.cons_nfeats = 5\n        self.edge_nfeats = 1\n        self.var_nfeats = 19\n\n        self.activation = nn.ReLU()\n        self.initializer = lambda x: torch.nn.init.orthogonal_(x, gain=1)\n\n        # CONSTRAINT EMBEDDING\n        self.cons_embedding = nn.Sequential(\n            PreNormLayer(n_units=self.cons_nfeats),\n            nn.Linear(self.cons_nfeats, self.emb_size, bias=True),\n            self.activation,\n            nn.Linear(self.emb_size, self.emb_size, bias=True),\n            self.activation\n        )\n\n        # EDGE EMBEDDING\n        self.edge_embedding = nn.Sequential(\n            PreNormLayer(self.edge_nfeats),\n        )\n\n        # VARIABLE_EMBEDDING\n        self.var_embedding = nn.Sequential(\n            PreNormLayer(n_units=self.var_nfeats),\n            nn.Linear(self.var_nfeats, self.emb_size, bias=True),\n            self.activation,\n            nn.Linear(self.emb_size, self.emb_size, bias=True),\n            self.activation\n        )\n\n        # GRAPH CONVOLUTIONS\n        self.conv_v_to_c = BipartiteGraphConvolution(self.emb_size, self.activation, self.initializer, right_to_left=True)\n        self.conv_c_to_v = BipartiteGraphConvolution(self.emb_size, self.activation, self.initializer)\n\n        # self.initialize_parameters()\n\n    def forward(self, inputs):\n        \"\"\"\n        Accepts stacked mini-batches, i.e. several bipartite graphs aggregated\n        as one. In that case the number of variables per samples has to be\n        provided, and the output consists in a padded dense tensor.\n\n        Parameters\n        ----------\n        inputs: list of tensors\n            Model input as a bipartite graph. May be batched into a stacked graph.\n\n        Inputs\n        ------\n        constraint_features: 2D float tensor\n            Constraint node features (n_constraints x n_constraint_features)\n        edge_indices: 2D int tensor\n            Edge constraint and variable indices (2, n_edges)\n        edge_features: 2D float tensor\n            Edge features (n_edges, n_edge_features)\n        variable_features: 2D float tensor\n            Variable node features (n_variables, n_variable_features)\n        n_cons_per_sample: 1D int tensor\n            Number of constraints for each of the samples stacked in the batch.\n        n_vars_per_sample: 1D int tensor\n            Number of variables for each of the samples stacked in the batch.\n        \"\"\"\n        constraint_features, edge_indices, edge_features, variable_features, n_cons_per_sample, n_vars_per_sample = inputs\n        n_cons_total = torch.sum(n_cons_per_sample)\n        n_vars_total = torch.sum(n_vars_per_sample)\n\n        # EMBEDDINGS\n        constraint_features = self.cons_embedding(constraint_features)\n        edge_features = self.edge_embedding(edge_features)\n        variable_features = self.var_embedding(variable_features)\n\n        # GRAPH CONVOLUTIONS\n        constraint_features = self.conv_v_to_c((\n            constraint_features, edge_indices, edge_features, variable_features, n_cons_total))\n        constraint_features = self.activation(constraint_features)\n\n        variable_features = self.conv_c_to_v((\n            constraint_features, edge_indices, edge_features, variable_features, n_vars_total))\n        variable_features = self.activation(variable_features)\n\n        return variable_features\n\n\nclass BaseModel(nn.Module):\n    def initialize_parameters(self):\n        for l in self.modules():\n            if isinstance(l, torch.nn.Linear):\n                self.initializer(l.weight.data)\n                if l.bias is not None:\n                    torch.nn.init.constant_(l.bias.data, 0)\n\n    def pre_train_init(self):\n        for module in self.modules():\n            if isinstance(module, PreNormLayer) or module._get_name() == \"PreNormLayer\":\n                module.start_updates()\n\n    def pre_train(self, state):\n        with torch.no_grad():\n            try:\n                self.forward(state)\n                return False\n            except PreNormException:\n                return True\n\n    def pre_train_next(self):\n        for module in self.modules():\n            if (isinstance(module, PreNormLayer) or module._get_name() == \"PreNormLayer\") \\\n                    and module.waiting_updates and module.received_updates:\n                module.stop_updates()\n                return module\n        return None\n\n    def save_state(self, filepath):\n        torch.save(self.state_dict(), filepath)\n\n    def restore_state(self, filepath):\n        self.load_state_dict(torch.load(filepath, map_location=torch.device('cpu')))\n\n\nclass Policy(BaseModel):\n    def __init__(self):\n        super(Policy, self).__init__()\n\n        self.root_gcn = GCNPolicy()\n        self.n_input_feats = 92\n        self.root_emb_size = self.root_gcn.emb_size\n        self.n_layers = 1\n\n        self.activation = torch.nn.LeakyReLU()\n        self.initializer = lambda x: torch.nn.init.orthogonal_(x, gain=1)\n\n        # HYPERNET GENEARTOR\n        self.weight_generator = nn.Sequential(\n            nn.LayerNorm(self.root_emb_size),\n            nn.Linear(self.root_emb_size, self.n_layers * self.n_input_feats * 3)\n        )\n\n        self.initialize_parameters()\n\n    @staticmethod\n    def pad_output(output, n_vars_per_sample, pad_value=-1e8):\n        n_vars_max = torch.max(n_vars_per_sample)\n\n        output = torch.split(\n            tensor=output,\n            split_size_or_sections=n_vars_per_sample.tolist(),\n            dim=1,\n        )\n\n        output = torch.cat([\n            F.pad(x,\n                pad=[0, n_vars_max - x.shape[1], 0, 0],\n                mode='constant',\n                value=pad_value)\n            for x in output\n        ], dim=0)\n\n        return output\n\n    def forward(self, inputs):\n        \"\"\"\n        Implements forward pass of the model\n\n        Parameters\n        ----------\n        root_c : torch.tensor\n            constraint features at the root node\n        root_ei : torch.tensor\n            indices to represent constraint-variable edges of the root node\n        root_ev : torch.tensor\n            edge features of the root node\n        root_v : torch.tensor\n            variable features at the root node\n        root_n_cs : torch.tensor\n            number of constraints per sample\n        root_n_vs : torch.tensor\n            number of variables per sample\n        candss : torch.tensor\n            candidate variable (strong branching candidates) indices at the root node\n        cand_feats : torch.tensor\n            candidate variable (strong branching candidates) features at a local node\n        cand_root_feats : torch.tensor\n            candidate root variable features at the root node\n\n        Return\n        ------\n        root_var_feats : torch.tensor\n            variable features computed from root gcnn (only if applicable)\n        logits : torch.tensor\n            output logits at the current node\n        parameters : torch.tensor\n            film-parameters to compute these logits (only if applicable)\n        \"\"\"\n        root_c, root_ei, root_ev, root_v, root_n_cs, root_n_vs, candss, cand_feats, _ = inputs\n\n        variable_features = self.root_gcn((root_c, root_ei, root_ev, root_v, root_n_cs, root_n_vs))\n        cand_root_feats = variable_features[candss]\n\n        film_parameters = self.weight_generator(cand_root_feats)\n        film_parameters = film_parameters.view(-1, 3, self.n_layers * self.n_input_feats)\n\n        cand_feats = cand_feats.repeat(1, self.n_layers)\n        film_feats = self.activation(film_parameters[:,0] * cand_feats + film_parameters[:,1])\n        output = torch.sum(film_parameters[:,2] * film_feats, axis=-1)\n        output = torch.reshape(output, [1, -1])\n        return F.normalize(variable_features, p=2, dim=1), output, film_parameters\n\n    def get_params(self, inputs):\n        \"\"\"\n        Returns parameters/variable representations inferred at the root node.\n\n        Parameters\n        ----------\n        inputs : torch.tensor\n            inputs to be used by the root node GNN\n\n        Returns\n        -------\n        (torch.tensor): variable representations / parameters as inferred from root gcnn and to be used else where in the tree.\n        \"\"\"\n        variable_features = self.root_gcn(inputs)\n        film_parameters = self.weight_generator(variable_features)\n        return film_parameters.view(-1, 3, self.n_layers * self.n_input_feats)\n\n    def predict(self, cand_feats, film_parameters):\n        \"\"\"\n        Predicts scores for candidate variables using the generated FiLM parameters.\n        \"\"\"\n        cand_feats = cand_feats.repeat(1, self.n_layers)\n        film_feats = self.activation(film_parameters[:,0] * cand_feats + film_parameters[:,1])\n        output = torch.sum(film_parameters[:,2] * film_feats, axis=-1)\n        output = torch.reshape(output, [1, -1])\n        return output\n\n# ==========================================\n# File: models/hybridsvm/model.py\n# Function/Context: Policy.forward\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass PreNormException(Exception):\n    pass\n\n\nclass PreNormLayer(nn.Module):\n    \"\"\"\n    Our pre-normalization layer, whose purpose is to normalize an input layer\n    to zero mean and unit variance to speed-up and stabilize GCN training. The\n    layer's parameters are aimed to be computed during the pre-training phase.\n    \"\"\"\n    def __init__(self, n_units, shift=True, scale=True):\n        super(PreNormLayer, self).__init__()\n        assert shift or scale\n\n        if shift:\n            self.register_buffer(f\"shift\", torch.zeros((n_units,), dtype=torch.float32))\n        else:\n            self.shift = None\n\n        if scale:\n            self.register_buffer(f\"scale\", torch.ones((n_units,), dtype=torch.float32))\n        else:\n            self.scale = None\n\n        self.n_units = n_units\n        self.waiting_updates = False\n        self.received_updates = False\n\n    def forward(self, input):\n        if self.waiting_updates:\n            self.update_stats(input)\n            self.received_updates = True\n            raise PreNormException\n\n        if self.shift is not None:\n            input = input + self.shift\n\n        if self.scale is not None:\n            input = input * self.scale\n\n        return input\n\n    def start_updates(self):\n        \"\"\"\n        Initializes the pre-training phase.\n        \"\"\"\n        self.avg = 0\n        self.var = 0\n        self.m2 = 0\n        self.count = 0\n        self.waiting_updates = True\n        self.received_updates = False\n\n    def update_stats(self, input):\n        \"\"\"\n        Online mean and variance estimation. See: Chan et al. (1979) Updating\n        Formulae and a Pairwise Algorithm for Computing Sample Variances.\n        https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online_algorithm\n        \"\"\"\n        assert self.n_units == 1 or input.shape[-1] == self.n_units, f\"Expected input dimension of size {self.n_units}, got {input.shape[-1]}.\"\n\n        input = input.reshape([-1, self.n_units])\n        sample_avg = torch.mean(input, dim=0)\n        sample_var = torch.mean((input - sample_avg) ** 2, dim=0)\n        sample_count = input.numel() / self.n_units\n\n        delta = sample_avg - self.avg\n\n        self.m2 = self.var * self.count + sample_var * sample_count + delta ** 2 * self.count * sample_count / (\n                self.count + sample_count)\n\n        self.count += sample_count\n        self.avg += delta * sample_count / self.count\n        self.var = self.m2 / self.count if self.count > 0 else 1\n\n    def stop_updates(self):\n        \"\"\"\n        Ends pre-training for that layer, and fixes the layers's parameters.\n        \"\"\"\n\n        assert self.count > 0\n        if self.shift is not None:\n            self.shift = - self.avg\n\n        if self.scale is not None:\n            self.var = torch.where(torch.eq(self.var, 0.0), torch.ones_like(self.var), self.var) # NaN check trick\n            self.scale = 1 / torch.sqrt(self.var)\n\n        del self.avg, self.var, self.m2, self.count\n        self.waiting_updates = False\n\n\nclass BipartiteGraphConvolution(nn.Module):\n    \"\"\"\n    Partial bipartite graph convolution (either left-to-right or right-to-left).\n    \"\"\"\n\n    def __init__(self, emb_size, activation, initializer, right_to_left=False):\n        super(BipartiteGraphConvolution, self).__init__()\n\n        self.emb_size = emb_size\n        self.activation = activation\n        self.initializer = initializer\n        self.right_to_left = right_to_left\n        self.edge_nfeats = 1\n\n        # feature layers\n        self.feature_module_left = nn.Sequential(\n            nn.Linear(self.emb_size, self.emb_size, bias=True),\n            self.activation\n        )\n\n        self.feature_module_edge = nn.Sequential(\n            nn.Linear(self.edge_nfeats, self.emb_size, bias=False),\n            self.activation\n        )\n\n        self.feature_module_right = nn.Sequential(\n            nn.Linear(self.emb_size, self.emb_size, bias=False),\n            self.activation\n        )\n\n        self.feature_module_final = nn.Sequential(\n            PreNormLayer(1, shift=False), # normalize after summation trick\n            self.activation,\n            nn.Linear(self.emb_size, self.emb_size, bias=True)\n        )\n\n        self.post_conv_module = nn.Sequential(\n            PreNormLayer(1, shift=False), # normalize after summation trick\n        )\n\n        # output_layers\n        self.output_module = nn.Sequential(\n            nn.Linear(self.emb_size + self.emb_size, self.emb_size, bias=True),\n            self.activation,\n            nn.Linear(self.emb_size, self.emb_size, bias=True)\n        )\n\n    def forward(self, inputs):\n        \"\"\"\n        Perfoms a partial graph convolution on the given bipartite graph.\n\n        Inputs\n        ------\n        left_features: 2D float tensor\n            Features of the left-hand-side nodes in the bipartite graph\n        edge_indices: 2D int tensor\n            Edge indices in left-right order\n        edge_features: 2D float tensor\n            Features of the edges\n        right_features: 2D float tensor\n            Features of the right-hand-side nodes in the bipartite graph\n        scatter_out_size: 1D int tensor\n            Output size (left_features.shape[0] or right_features.shape[0], unknown at compile time)\n        \"\"\"\n        left_features, edge_indices, edge_features, right_features, scatter_out_size = inputs\n        device = left_features.device\n\n        if self.right_to_left:\n            scatter_dim = 0\n            prev_features = left_features\n        else:\n            scatter_dim = 1\n            prev_features = right_features\n\n        # compute joint features\n        joint_features = self.feature_module_final(\n            self.feature_module_left(left_features)[edge_indices[0]] +\n            self.feature_module_edge(edge_features) +\n            self.feature_module_right(right_features)[edge_indices[1]]\n        )\n\n        # perform convolution\n        conv_output = torch.zeros([scatter_out_size, self.emb_size], device=device).index_add_(0, edge_indices[scatter_dim], joint_features)\n        conv_output = self.post_conv_module(conv_output)\n\n        # apply final module\n        output = self.output_module(torch.cat([\n            conv_output,\n            prev_features\n        ], axis=1))\n\n        return output\n\n\nclass GCNPolicy(nn.Module):\n    \"\"\"\n    Our bipartite Graph Convolutional neural Network (GCN) model.\n    \"\"\"\n\n    def __init__(self):\n        super(GCNPolicy, self).__init__()\n\n        self.emb_size = 64\n        self.cons_nfeats = 5\n        self.edge_nfeats = 1\n        self.var_nfeats = 19\n\n        self.activation = nn.ReLU()\n        self.initializer = lambda x: torch.nn.init.orthogonal_(x, gain=1)\n\n        # CONSTRAINT EMBEDDING\n        self.cons_embedding = nn.Sequential(\n            PreNormLayer(n_units=self.cons_nfeats),\n            nn.Linear(self.cons_nfeats, self.emb_size, bias=True),\n            self.activation,\n            nn.Linear(self.emb_size, self.emb_size, bias=True),\n            self.activation\n        )\n\n        # EDGE EMBEDDING\n        self.edge_embedding = nn.Sequential(\n            PreNormLayer(self.edge_nfeats),\n        )\n\n        # VARIABLE_EMBEDDING\n        self.var_embedding = nn.Sequential(\n            PreNormLayer(n_units=self.var_nfeats),\n            nn.Linear(self.var_nfeats, self.emb_size, bias=True),\n            self.activation,\n            nn.Linear(self.emb_size, self.emb_size, bias=True),\n            self.activation\n        )\n\n        # GRAPH CONVOLUTIONS\n        self.conv_v_to_c = BipartiteGraphConvolution(self.emb_size, self.activation, self.initializer, right_to_left=True)\n        self.conv_c_to_v = BipartiteGraphConvolution(self.emb_size, self.activation, self.initializer)\n\n        # self.initialize_parameters()\n\n    def forward(self, inputs):\n        \"\"\"\n        Accepts stacked mini-batches, i.e. several bipartite graphs aggregated\n        as one. In that case the number of variables per samples has to be\n        provided, and the output consists in a padded dense tensor.\n\n        Parameters\n        ----------\n        inputs: list of tensors\n            Model input as a bipartite graph. May be batched into a stacked graph.\n\n        Inputs\n        ------\n        constraint_features: 2D float tensor\n            Constraint node features (n_constraints x n_constraint_features)\n        edge_indices: 2D int tensor\n            Edge constraint and variable indices (2, n_edges)\n        edge_features: 2D float tensor\n            Edge features (n_edges, n_edge_features)\n        variable_features: 2D float tensor\n            Variable node features (n_variables, n_variable_features)\n        n_cons_per_sample: 1D int tensor\n            Number of constraints for each of the samples stacked in the batch.\n        n_vars_per_sample: 1D int tensor\n            Number of variables for each of the samples stacked in the batch.\n        \"\"\"\n        constraint_features, edge_indices, edge_features, variable_features, n_cons_per_sample, n_vars_per_sample = inputs\n        n_cons_total = torch.sum(n_cons_per_sample)\n        n_vars_total = torch.sum(n_vars_per_sample)\n\n        # EMBEDDINGS\n        constraint_features = self.cons_embedding(constraint_features)\n        edge_features = self.edge_embedding(edge_features)\n        variable_features = self.var_embedding(variable_features)\n\n        # GRAPH CONVOLUTIONS\n        constraint_features = self.conv_v_to_c((\n            constraint_features, edge_indices, edge_features, variable_features, n_cons_total))\n        constraint_features = self.activation(constraint_features)\n\n        variable_features = self.conv_c_to_v((\n            constraint_features, edge_indices, edge_features, variable_features, n_vars_total))\n        variable_features = self.activation(variable_features)\n\n        return variable_features\n\n\nclass BaseModel(nn.Module):\n    def initialize_parameters(self):\n        for l in self.modules():\n            if isinstance(l, torch.nn.Linear):\n                self.initializer(l.weight.data)\n                if l.bias is not None:\n                    torch.nn.init.constant_(l.bias.data, 0)\n\n    def pre_train_init(self):\n        for module in self.modules():\n            if isinstance(module, PreNormLayer) or module._get_name() == \"PreNormLayer\":\n                module.start_updates()\n\n    def pre_train(self, state):\n        with torch.no_grad():\n            try:\n                self.forward(state)\n                return False\n            except PreNormException:\n                return True\n\n    def pre_train_next(self):\n        for module in self.modules():\n            if (isinstance(module, PreNormLayer) or module._get_name() == \"PreNormLayer\") \\\n                    and module.waiting_updates and module.received_updates:\n                module.stop_updates()\n                return module\n        return None\n\n    def save_state(self, filepath):\n        torch.save(self.state_dict(), filepath)\n\n    def restore_state(self, filepath):\n        self.load_state_dict(torch.load(filepath, map_location=torch.device('cpu')))\n\n\nclass Policy(BaseModel):\n    def __init__(self):\n        super(Policy, self).__init__()\n\n        self.root_gcn = GCNPolicy()\n        self.n_input_feats = 92\n        self.root_emb_size = self.root_gcn.emb_size\n\n        self.activation = torch.nn.LeakyReLU()\n        self.initializer = lambda x: torch.nn.init.orthogonal_(x, gain=1)\n\n        # HYPERNET GENEARTOR\n        self.weight_generator = nn.Sequential(\n            nn.LayerNorm(self.root_emb_size),\n            nn.Linear(self.root_emb_size, self.n_input_feats)\n        )\n\n        self.initialize_parameters()\n\n    @staticmethod\n    def pad_output(output, n_vars_per_sample, pad_value=-1e8):\n        n_vars_max = torch.max(n_vars_per_sample)\n\n        output = torch.split(\n            tensor=output,\n            split_size_or_sections=n_vars_per_sample.tolist(),\n            dim=1,\n        )\n\n        output = torch.cat([\n            F.pad(x,\n                pad=[0, n_vars_max - x.shape[1], 0, 0],\n                mode='constant',\n                value=pad_value)\n            for x in output\n        ], dim=0)\n\n        return output\n\n    def forward(self, inputs):\n        \"\"\"\n        Implements forward pass of the model\n\n        Parameters\n        ----------\n        root_c : torch.tensor\n            constraint features at the root node\n        root_ei : torch.tensor\n            indices to represent constraint-variable edges of the root node\n        root_ev : torch.tensor\n            edge features of the root node\n        root_v : torch.tensor\n            variable features at the root node\n        root_n_cs : torch.tensor\n            number of constraints per sample\n        root_n_vs : torch.tensor\n            number of variables per sample\n        candss : torch.tensor\n            candidate variable (strong branching candidates) indices at the root node\n        cand_feats : torch.tensor\n            candidate variable (strong branching candidates) features at a local node\n        cand_root_feats : torch.tensor\n            candidate root variable features at the root node\n\n        Return\n        ------\n        root_var_feats : torch.tensor\n            variable features computed from root gcnn (only if applicable)\n        logits : torch.tensor\n            output logits at the current node\n        parameters : torch.tensor\n            film-parameters to compute these logits (only if applicable)\n        \"\"\"\n        root_c, root_ei, root_ev, root_v, root_n_cs, root_n_vs, candss, cand_feats, _ = inputs\n\n        variable_features = self.root_gcn((root_c, root_ei, root_ev, root_v, root_n_cs, root_n_vs))\n        cand_root_feats = variable_features[candss]\n\n        dot_weights = self.weight_generator(cand_root_feats)\n        dot_weights = dot_weights.view(-1, self.n_input_feats)\n\n        output = torch.sum(cand_feats * dot_weights, axis=-1)\n        output = torch.reshape(output, [1, -1])\n        return F.normalize(variable_features, p=2, dim=1), output, None\n\n    def get_params(self, inputs):\n        \"\"\"\n        Returns parameters/variable representations inferred at the root node.\n\n        Parameters\n        ----------\n        inputs : torch.tensor\n            inputs to be used by the root node GNN\n\n        Returns\n        -------\n        (torch.tensor): variable representations / parameters as inferred from root gcnn and to be used else where in the tree.\n        \"\"\"\n        variable_features = self.root_gcn(inputs)\n        dot_weights = self.weight_generator(variable_features)\n        return dot_weights.view(-1, self.n_input_feats)\n\n    def predict(self, cand_feats, dot_weights):\n        \"\"\"\n        Predicts score for each candindate represented by cand_feats\n\n        Parameters\n        ----------\n        cand_feats : torch.tensor\n            (2D) representing input features of variables at any node in the tree\n        dot_weights : torch.tensor\n            (2D) parameters that are used \n        \"\"\"\n\n# ==========================================\n# File: models/mlp/model.py\n# Function/Context: Policy.forward\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nclass Model(torch.nn.Module):\n    def initialize_parameters(self):\n        for l in self.modules():\n            if isinstance(l, torch.nn.Linear):\n                torch.nn.init.orthogonal_(l.weight.data, gain=1)\n                if l.bias is not None:\n                    torch.nn.init.constant_(l.bias.data, 0)\n\n    def _initialize_parameters(self):\n        for l in self.modules():\n            if isinstance(l, torch.nn.Linear):\n                torch.nn.init.xavier_normal_(l.weight.data)\n                if l.bias is not None:\n                    torch.nn.init.constant_(l.bias.data, 0)\n\n    def pretrain_init(self):\n        for module in self.modules():\n            if isinstance(module, PreNormLayer):\n                module.start_updates()\n\n    def pretrain(self, state):\n        with torch.no_grad():\n            try:\n                self.forward(state)\n                return False\n            except PreNormException:\n                return True\n\n    def pretrain_next(self):\n        for module in self.modules():\n            if isinstance(module, PreNormLayer) \\\n                    and module.waiting_updates and module.received_updates:\n                module.stop_updates()\n                return module\n        return None\n\n    def save_state(self, filepath):\n        torch.save(self.state_dict(), filepath)\n\n    def restore_state(self, filepath):\n        self.load_state_dict(torch.load(filepath, map_location=torch.device('cpu')))\n\nclass Policy(Model):\n    def __init__(self):\n        super(Model, self).__init__()\n\n        self.n_input_feats = 92\n        self.ff_size = 256\n\n        self.activation = torch.nn.ReLU()\n\n        # OUTPUT\n        self.output_module = nn.Sequential(\n            nn.Linear(self.n_input_feats, self.ff_size, bias=True),\n            self.activation,\n            nn.Linear(self.ff_size, self.ff_size, bias=True),\n            self.activation,\n            nn.Linear(self.ff_size, self.ff_size, bias=True),\n            self.activation,\n            nn.Linear(self.ff_size, 1, bias=False)\n        )\n\n        self.initialize_parameters()\n\n    @staticmethod\n    def pad_output(output, n_vars_per_sample, pad_value=-1e8):\n        n_vars_max = torch.max(n_vars_per_sample)\n\n        output = torch.split(\n            tensor=output,\n            split_size_or_sections=n_vars_per_sample.tolist(),\n            dim=1,\n        )\n\n        output = torch.cat([\n            F.pad(x,\n                pad=[0, n_vars_max - x.shape[1], 0, 0],\n                mode='constant',\n                value=pad_value)\n            for x in output\n        ], dim=0)\n\n        return output\n\n    def forward(self, inputs):\n        features = inputs\n        output = self.output_module(features)\n        output = torch.reshape(output, [1, -1])\n        return output",
  "description": "Combined Analysis:\n- [03_train_hybrid.py]: This file implements the training pipeline for hybrid models (GNN+MLP) for learning to branch in MILPs. It contains the core training logic including: 1) Data loading and batching for hybrid models, 2) Pre-training of normalization layers, 3) Forward/backward passes with hybrid model architectures (concat/film), 4) Loss computation combining node-level branching loss, auxiliary tasks (AT), and regularization, 5) Knowledge distillation from teacher models, 6) Training loop with early stopping and learning rate scheduling. The code directly implements the hybrid training methodology described in the paper where GNN extracts root node features and MLP performs fast inference at other nodes.\n- [05_evaluate_hybrid.py]: This file implements the core evaluation pipeline for hybrid branching models. The PolicyBranching class is a SCIP branchrule that uses trained machine learning models (hybrid GNN-MLP) to make branching decisions. Key aspects: 1) At root node, extracts MILP graph features and computes GNN embeddings if using pretrained models. 2) At each node, combines variable LP features with Khalil features and uses MLP for fast inference. 3) Implements the hybrid approach where GNN provides structural embeddings at root, and MLP uses these embeddings for efficient branching at all nodes. The main script orchestrates evaluation across multiple instances, models, and seeds, measuring solver performance metrics.\n- [models/concat-pre/model.py]: This file implements the 'concat-pre' hybrid model from the paper. The Policy class represents a neural network that combines root node embeddings (presumably from a GNN) with local candidate features through concatenation. The model takes candidate features and root embeddings as input, normalizes the root embeddings, concatenates them, and passes through a multi-layer perceptron (MLP) to produce branching scores. This aligns with the paper's hybrid approach where GNN captures structural information at the root, and MLP provides fast inference at other nodes. The forward method processes inputs and returns logits for candidate variables, implementing the core branching decision logic.\n- [models/concat/model.py]: This file implements the hybrid GNN+MLP model from the paper. The Policy class combines a root GCN (GCNPolicy) that processes the MILP's bipartite graph structure at the root node, with an MLP (output_module) that makes fast branching decisions at other nodes. The forward method shows the hybrid architecture: root GCN extracts structural embeddings, which are concatenated with local candidate features and passed through the MLP to produce branching scores. This matches the paper's core algorithm of using GNNs for structural understanding and MLPs for efficient inference.\n- [models/film-pre/model.py]: This file implements the core hybrid MLP component of the paper's branching policy. The Policy class uses a FiLM (Feature-wise Linear Modulation) generator to produce conditional parameters from root node embeddings, which then modulate a 3-layer MLP processing candidate variable features. This architecture enables efficient CPU-only inference by separating expensive GNN computations (done once at root) from fast MLP evaluations (done at each branching node), directly implementing the paper's hybrid approach to balance decision quality and computational cost.\n- [models/film/model.py]: This file implements the core hybrid GNN-MLP model from the paper. The Policy class combines a GCN (GCNPolicy) for processing the MILP's bipartite graph structure at the root node with an MLP network (with FiLM conditioning) for fast inference at other nodes. The GCN captures structural information via bipartite graph convolutions between constraints and variables, while the MLP processes candidate variable features conditioned on the root node's graph embedding. This matches the paper's hybrid architecture designed for efficient CPU-based branching decisions.\n- [models/hybridsvm-film/model.py]: This file implements the hybrid GNN+FiLM model described in the paper. The Policy class uses a GNN (GCNPolicy) to process the root node's bipartite graph representation of the MILP, extracting structural features. It then uses a hypernetwork (weight_generator) to produce FiLM parameters that condition a lightweight MLP (implemented in forward and predict) for fast branching decisions at other nodes. This matches the paper's core algorithm: using a GNN for structural understanding at the root and a fast, parameterized MLP for efficient inference throughout the branch-and-bound tree.\n- [models/hybridsvm/model.py]: This file implements the core hybrid GNN-MLP model from the paper 'Hybrid Models for Learning to Branch'. The Policy class combines a Graph Neural Network (GCNPolicy) that processes the MILP's bipartite graph structure at the root node with a lightweight weight generator (hypernetwork) that produces parameters for fast inference at other nodes. The forward method computes variable embeddings via the GNN, then uses these embeddings to generate weights for a dot product with candidate features, implementing the hybrid approach where structural information is captured once at the root and reused efficiently throughout the tree.\n- [models/mlp/model.py]: This file implements the MLP component of the hybrid model described in the paper. The Policy class defines a Multi-Layer Perceptron (MLP) with 4 linear layers and ReLU activations that processes variable features (92-dimensional input) to produce branching scores. This aligns with the paper's approach of using MLPs for fast inference at non-root nodes in the branch-and-bound tree, complementing the GNN used at the root node. The forward method takes variable features as input and outputs scores for branching decisions, which is the core MLP inference logic for variable selection.",
  "dependencies": [
    "utilities",
    "PreNormException",
    "pickle",
    "sys",
    "MLP",
    "time",
    "pyscipopt",
    "csv",
    "torch.nn.functional",
    "BipartiteGraphConvolution",
    "importlib",
    "pathlib",
    "PreNormLayer",
    "utilities_hybrid",
    "os",
    "argparse",
    "numpy",
    "teacher_model (from models/{teacher_model})",
    "model (from models/{args.model})",
    "torch.nn",
    "GCNPolicy",
    "BaseModel",
    "torch"
  ]
}