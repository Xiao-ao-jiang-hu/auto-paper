{
  "file_path": "src/qap/main.py, src/qap/model.py",
  "function_name": "train, Siamese_GNN",
  "code_snippet": "\n\n# ==========================================\n# File: src/qap/main.py\n# Function/Context: train\n# ==========================================\n#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\nimport numpy as np\nimport os\n# import dependencies\nfrom data_generator import Generator\nfrom model import Siamese_GNN\nfrom Logger import Logger\nimport time\nimport matplotlib\nmatplotlib.use('Agg')\nfrom matplotlib import pyplot as plt\n\n#Pytorch requirements\nimport unicodedata\nimport string\nimport re\nimport random\nimport argparse\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import init\nfrom torch.autograd import Variable\nfrom torch import optim\nimport torch.nn.functional as F\n\nparser = argparse.ArgumentParser()\n\n###############################################################################\n#                             General Settings                                #\n###############################################################################\n\nparser.add_argument('--num_examples_train', nargs='?', const=1, type=int,\n                    default=int(20000))\nparser.add_argument('--num_examples_test', nargs='?', const=1, type=int,\n                    default=int(1000))\nparser.add_argument('--edge_density', nargs='?', const=1, type=float,\n                    default=0.2)\nparser.add_argument('--random_noise', action='store_true')\nparser.add_argument('--noise', nargs='?', const=1, type=float, default=0.03)\nparser.add_argument('--noise_model', nargs='?', const=1, type=int, default=2)\nparser.add_argument('--generative_model', nargs='?', const=1, type=str,\n                    default='ErdosRenyi')\nparser.add_argument('--iterations', nargs='?', const=1, type=int,\n                    default=int(60000))\nparser.add_argument('--batch_size', nargs='?', const=1, type=int, default=1)\nparser.add_argument('--mode', nargs='?', const=1, type=str, default='train')\nparser.add_argument('--path_dataset', nargs='?', const=1, type=str, default='')\nparser.add_argument('--path_logger', nargs='?', const=1, type=str, default='')\nparser.add_argument('--print_freq', nargs='?', const=1, type=int, default=100)\nparser.add_argument('--test_freq', nargs='?', const=1, type=int, default=500)\nparser.add_argument('--save_freq', nargs='?', const=1, type=int, default=2000)\nparser.add_argument('--clip_grad_norm', nargs='?', const=1, type=float,\n                    default=40.0)\n\n###############################################################################\n#                                 GNN Settings                                #\n###############################################################################\n\nparser.add_argument('--num_features', nargs='?', const=1, type=int,\n                    default=20)\nparser.add_argument('--num_layers', nargs='?', const=1, type=int,\n                    default=20)\nparser.add_argument('--J', nargs='?', const=1, type=int, default=4)\n\nargs = parser.parse_args()\n\nif torch.cuda.is_available():\n    dtype = torch.cuda.FloatTensor\n    dtype_l = torch.cuda.LongTensor\n    torch.cuda.manual_seed(0)\nelse:\n    dtype = torch.FloatTensor\n    dtype_l = torch.LongTensor\n    torch.manual_seed(0)\n\nbatch_size = args.batch_size\ncriterion = nn.CrossEntropyLoss()\ntemplate1 = '{:<10} {:<10} {:<10} {:<15} {:<10} {:<10} {:<10} '\ntemplate2 = '{:<10} {:<10.5f} {:<10.5f} {:<15} {:<10} {:<10} {:<10.3f} \\n'\n\ndef compute_loss(pred, labels):\n    pred = pred.view(-1, pred.size()[-1])\n    labels = labels.view(-1)\n    return criterion(pred, labels)\n\ndef train(siamese_gnn, logger, gen):\n    labels = (Variable(torch.arange(0, gen.N).unsqueeze(0).expand(batch_size,\n              gen.N)).type(dtype_l))\n    optimizer = torch.optim.Adamax(siamese_gnn.parameters(), lr=1e-3)\n    for it in range(args.iterations):\n        start = time.time()\n        input = gen.sample_batch(batch_size, cuda=torch.cuda.is_available())\n        pred = siamese_gnn(*input)\n        loss = compute_loss(pred, labels)\n        siamese_gnn.zero_grad()\n        loss.backward()\n        nn.utils.clip_grad_norm(siamese_gnn.parameters(), args.clip_grad_norm)\n        optimizer.step()\n        logger.add_train_loss(loss)\n        logger.add_train_accuracy(pred, labels)\n        elapsed = time.time() - start\n        if it % logger.args['print_freq'] == 0:\n            logger.plot_train_accuracy()\n            logger.plot_train_loss()\n            loss = loss.data.cpu().numpy()#[0]\n            info = ['iteration', 'loss', 'accuracy', 'edge_density',\n                    'noise', 'model', 'elapsed']\n            out = [it, loss.item(), logger.accuracy_train[-1].item(), args.edge_density,\n                   args.noise, args.generative_model, elapsed]\n            print(template1.format(*info))\n            print(template2.format(*out))\n            # test(siamese_gnn, logger, gen)\n        if it % logger.args['save_freq'] == 0:\n            logger.save_model(siamese_gnn)\n            logger.save_results()\n    print('Optimization finished.')\n\nif __name__ == '__main__':\n    logger = Logger(args.path_logger)\n    logger.write_settings(args)\n    siamese_gnn = Siamese_GNN(args.num_features, args.num_layers, args.J + 2)\n    if torch.cuda.is_available():\n        siamese_gnn.cuda()\n    gen = Generator(args.path_dataset)\n    # generator setup\n    gen.num_examples_train = args.num_examples_train\n    gen.num_examples_test = args.num_examples_test\n    gen.J = args.J\n    gen.edge_density = args.edge_density\n    gen.random_noise = args.random_noise\n    gen.noise = args.noise\n    gen.noise_model = args.noise_model\n    gen.generative_model = args.generative_model\n    # load dataset\n    # print(gen.random_noise)\n    gen.load_dataset()\n    if args.mode == 'train':\n        train(siamese_gnn, logger, gen)\n    # elif args.mode == 'test':\n    #     test(siamese_gnn, logger, gen)\n\n# ==========================================\n# File: src/qap/model.py\n# Function/Context: Siamese_GNN\n# ==========================================\n#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\nimport matplotlib\nmatplotlib.use('Agg')\n\n# Pytorch requirements\nimport unicodedata\nimport string\nimport re\nimport random\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import init\nfrom torch.autograd import Variable\nfrom torch import optim\nimport torch.nn.functional as F\n\nif torch.cuda.is_available():\n    dtype = torch.cuda.FloatTensor\n    dtype_l = torch.cuda.LongTensor\nelse:\n    dtype = torch.FloatTensor\n    dtype_l = torch.cuda.LongTensor\n\ndef sinkhorn_knopp(A, iterations=1):\n    A_size = A.size()\n    for it in range(iterations):\n        A = A.view(A_size[0]*A_size[1], A_size[2])\n        A = F.softmax(A)\n        A = A.view(*A_size).permute(0, 2, 1)\n        A = A.view(A_size[0]*A_size[1], A_size[2])\n        A = F.softmax(A)\n        A = A.view(*A_size).permute(0, 2, 1)\n    return A\n\ndef gmul(input):\n    W, x = input\n    # x is a tensor of size (bs, N, num_features)\n    # W is a tensor of size (bs, N, N, J)\n    x_size = x.size()\n    W_size = W.size()\n    N = W_size[-2]\n    J = W_size[-1]\n    W = W.split(1, 3)\n    W = torch.cat(W, 1).squeeze(3) # W is now a tensor of size (bs, J*N, N)\n    output = torch.bmm(W, x) # output has size (bs, J*N, num_features)\n    output = output.split(N, 1)\n    output = torch.cat(output, 2) # output has size (bs, N, J*num_features)\n    return output\n\nclass Gconv_last(nn.Module):\n    def __init__(self, feature_maps, J):\n        super(Gconv_last, self).__init__()\n        self.num_inputs = J*feature_maps[0]\n        self.num_outputs = feature_maps[2]\n        self.fc = nn.Linear(self.num_inputs, self.num_outputs)\n\n    def forward(self, input):\n        W = input[0]\n        x = gmul(input) # out has size (bs, N, num_inputs)\n        x_size = x.size()\n        x = x.contiguous()\n        x = x.view(x_size[0]*x_size[1], -1)\n        x = self.fc(x) # has size (bs*N, num_outputs)\n        x = x.view(*x_size[:-1], self.num_outputs)\n        return W, x\n\nclass Gconv(nn.Module):\n    def __init__(self, feature_maps, J):\n        super(Gconv, self).__init__()\n        self.num_inputs = J*feature_maps[0]\n        self.num_outputs = feature_maps[2]\n        self.fc1 = nn.Linear(self.num_inputs, self.num_outputs // 2)\n        self.fc2 = nn.Linear(self.num_inputs, self.num_outputs // 2)\n        self.bn = nn.BatchNorm1d(self.num_outputs)\n\n    def forward(self, input):\n        W = input[0]\n        x = gmul(input) # out has size (bs, N, num_inputs)\n        x_size = x.size()\n        x = x.contiguous()\n        x = x.view(-1, self.num_inputs)\n        x1 = F.relu(self.fc1(x)) # has size (bs*N, num_outputs)\n        x2 = self.fc2(x)\n        x = torch.cat((x1, x2), 1)\n        x = self.bn(x)\n        x = x.view(*x_size[:-1], self.num_outputs)\n        return W, x\n\nclass GNN(nn.Module):\n    def __init__(self, num_features, num_layers, J):\n        super(GNN, self).__init__()\n        self.num_features = num_features\n        self.num_layers = num_layers\n        self.featuremap_in = [1, 1, num_features]\n        self.featuremap_mi = [num_features, num_features, num_features]\n        self.featuremap_end = [num_features, num_features, num_features]\n        self.layer0 = Gconv(self.featuremap_in, J)\n        for i in range(num_layers):\n            module = Gconv(self.featuremap_mi, J)\n            self.add_module('layer{}'.format(i + 1), module)\n        self.layerlast = Gconv_last(self.featuremap_end, J)\n\n    def forward(self, input):\n        cur = self.layer0(input)\n        for i in range(self.num_layers):\n            cur = self._modules['layer{}'.format(i+1)](cur)\n        out = self.layerlast(cur)\n        return out[1]\n\nclass Siamese_GNN(nn.Module):\n    def __init__(self, num_features, num_layers, J):\n        super(Siamese_GNN, self).__init__()\n        self.gnn = GNN(num_features, num_layers, J)\n\n    def forward(self, g1, g2):\n        emb1 = self.gnn(g1)\n        emb2 = self.gnn(g2)\n        # embx are tensors of size (bs, N, num_features)\n        out = torch.bmm(emb1, emb2.permute(0, 2, 1))\n        return out # out has size (bs, N, N)\n\nif __name__ == '__main__':\n    # test modules\n    bs =  4\n    num_features = 10\n    num_layers = 5\n    N = 8\n    x = torch.ones((bs, N, num_features))\n    W1 = torch.eye(N).unsqueeze(0).unsqueeze(-1).expand(bs, N, N, 1)\n    W2 = torch.ones(N).unsqueeze(0).unsqueeze(-1).expand(bs, N, N, 1)\n    J = 2\n    W = torch.cat((W1, W2), 3)\n    input = [Variable(W), Variable(x)]\n    ######################### test gmul ##############################\n    # feature_maps = [num_features, num_features, num_features]\n    # out = gmul(input)\n    # print(out[0, :, num_features:])\n    ######################### test gconv ##############################\n    # feature_maps = [num_features, num_features, num_features]\n    # gconv = Gconv(feature_maps, J)\n    # _, out = gconv(input)\n    # print(out.size())\n    ######################### test gnn ##############################\n    # x = torch.ones((bs, N, 1))\n    # input = [Variable(W), Variable(x)]\n    # gnn = GNN(num_features, num_layers, J)\n    # out = gnn(input)\n    # print(out.size())\n    ######################### test siamese gnn ##############################\n    x = torch.ones((bs, N, 1))\n    input1 = [Variable(W), Variable(x)]\n    input2 = [Variable(W.clone()), Variable(x.clone())]\n    siamese_gnn = Siamese_GNN(num_features, num_layers, J)\n    out = siamese_gnn(input1, input2)\n    print(out.size())",
  "description": "Combined Analysis:\n- [src/qap/main.py]: This file implements the training pipeline for the GNN-based QAP solver described in the paper. It sets up the data generator (which creates noisy graph pairs with known permutations), initializes the Siamese GNN model, and runs the training loop. The core logic aligns with the paper's data-driven approach: generating QAP instances (A, B) where B is a permuted and noisy version of A, then training a GNN to recover the permutation. The training uses cross-entropy loss between predicted and true node correspondences, with gradient clipping and Adamax optimization. The file doesn't directly implement the mathematical optimization model (trace maximization) but implements the learning algorithm that approximates solutions to that model.\n- [src/qap/model.py]: This file implements the core Graph Neural Network (GNN) architecture for solving the Quadratic Assignment Problem (QAP) as described in the paper. The Siamese_GNN class takes two graph inputs (each with adjacency tensor W and node features x) and produces a compatibility matrix of size (batch_size, N, N) via a bilinear product of node embeddings. This matrix approximates the assignment matrix X in the QAP objective trace(AXBX^T). The architecture uses multiple Gconv layers with J adjacency powers, batch normalization, and ReLU activations. The sinkhorn_knopp function provides a differentiable approximation to permutation matrices via Sinkhorn normalization, enabling gradient-based training. The implementation directly corresponds to the paper's data-driven approach where GNNs learn to predict solutions from training instances.",
  "dependencies": [
    "model.Siamese_GNN",
    "numpy",
    "data_generator.Generator",
    "torch.autograd.Variable",
    "torch.nn.functional",
    "torch.nn",
    "torch.optim",
    "matplotlib.pyplot",
    "argparse",
    "torch",
    "matplotlib",
    "Logger.Logger",
    "time"
  ]
}