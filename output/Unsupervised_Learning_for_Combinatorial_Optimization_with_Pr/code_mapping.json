{
  "file_path": "approximate_computing/train_atheta/loss.py, approximate_computing/train_atheta/model_con.py, edge_covering_node_matching/train_atheta/loss_aff.py, edge_covering_node_matching/train_atheta/loss_con.py, edge_covering_node_matching/train_atheta/model_aff.py, edge_covering_node_matching/train_atheta/model_con.py, edge_covering_node_matching/train_proxy/model_aff.py, edge_covering_node_matching/train_proxy/model_con.py, resource_binding/train_atheta/loss.py, resource_binding/train_atheta/model.py, resource_binding/train_dsp_con/model.py, resource_binding/train_lut_con/model.py",
  "function_name": "ErdosLoss.forward, PNA_con, PNA_con2, LastLayer, ErdosLoss, ErdosLoss, Penalty, SAGE_edge_aff, SAGE_edge_aff2, Aff_Mnist, Aff_Mnist2, SAGE_edge_con, SAGE_edge_con2, Con_Mnist, Con_Mnist2, Aff_Mnist, SAGE_edge_con.forward, Con_Mnist.forward, ErdosLoss.forward, LINEAR_model2, LINEAR_dsp, LINEAR_lut, LINEAR_dsp, LINEAR_lut",
  "code_snippet": "\n\n# ==========================================\n# File: approximate_computing/train_atheta/loss.py\n# Function/Context: ErdosLoss.forward\n# ==========================================\nimport torch\nimport sys\nsys.path.append(\"..\")\n\n\nclass ErdosLoss(torch.nn.Module):\n    def __init__(self, gpu_num, model):\n        super(ErdosLoss,self).__init__()\n        self.gpu_num = gpu_num\n        self.proxy = model\n        self.device = torch.device(\"cuda:\"+str(gpu_num))\n\n    def forward(self, x, fixed_feature, alpha, edge_index, batch):\n        \"\"\"the erdos loss function\n        \"\"\"\n        input_feature = torch.cat([fixed_feature, x],1)\n        relative_error = self.proxy(input_feature, edge_index, batch) # it's 100 times the relative error\n        x = x.reshape(-1, 15)\n        x = torch.sum(x, dim = 1)\n        loss = relative_error - alpha * x\n        erdos_loss = torch.mean(loss)\n\n        return erdos_loss\n\n# ==========================================\n# File: approximate_computing/train_atheta/model_con.py\n# Function/Context: PNA_con, PNA_con2, LastLayer\n# ==========================================\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn import Embedding, Linear, ModuleList, ReLU, Sequential\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom torch_geometric.datasets import ZINC\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.nn import BatchNorm, PNAConv, global_add_pool, global_mean_pool\nfrom torch_geometric.utils import degree\nfrom torch_geometric.nn.conv import MessagePassing\n\nclass LastLayer(MessagePassing):\n    def __init__(self):\n        super(LastLayer, self).__init__(aggr='add')  \n    def forward(self, x, edge_index):\n        log_score = self.propagate(edge_index, x=x)\n        log_score = log_score + torch.log(x + 1e-6)\n        return torch.exp(log_score)\n    def message(self, x_j):\n        return torch.log(x_j+1e-6)\n\nclass PNA_con(torch.nn.Module):\n    def __init__(self, gpu_num, save_path):\n        super().__init__()\n        self.gpu_num = gpu_num\n        self.save_path = save_path\n        deg_file = torch.load(self.save_path+'/deg.pt').to(torch.device(\"cuda:\"+str(self.gpu_num)))\n        self.pre_lin = Linear(1, 80)\n        aggregators = ['mean', 'min', 'max', 'std']\n        scalers = ['identity', 'amplification', 'attenuation']\n     \n        self.convs = ModuleList()\n        self.batch_norms = ModuleList()\n        for _ in range(4):\n            conv = PNAConv(in_channels=80, out_channels=80,\n                           aggregators=aggregators, scalers=scalers, deg=deg_file,\n                           towers=5, pre_layers=1, post_layers=1,\n                           divide_input=False)\n            self.convs.append(conv)\n            self.batch_norms.append(BatchNorm(80))\n        self.linear_propagation = LastLayer()\n        self.mlp_1 = Sequential(Linear(40, 20))\n        self.mlp_2 = Sequential(Linear(20, 1))\n        self.mlp_lin = Sequential(Linear(40, 1))\n\n    def forward(self, x, edge_index, batch):\n        operation = torch.clone(x[:,0]).reshape(-1,1)\n        assignment = torch.clone(x[:,1]).reshape(-1,1)\n        operation = self.pre_lin(operation)\n        for conv, batch_norm in zip(self.convs, self.batch_norms):\n            operation = F.relu(batch_norm(conv(operation, edge_index)))\n        operation_front = operation[:,:40]\n        operation_behind = operation[:,40:]\n        x_combine = operation_front * assignment + operation_behind\n        x_combine = self.linear_propagation(x_combine, edge_index)\n        x_combine = global_mean_pool(x_combine, batch)\n        x_lin = self.mlp_lin(x_combine)\n        x_concave = self.mlp_1(x_combine)\n        x_concave = 8 - x_concave.relu()\n        x_concave = self.mlp_2(x_concave)\n        x_out = x_concave + x_lin\n        return x_out\n\n\nclass PNA_con2(torch.nn.Module):\n    def __init__(self, gpu_num, save_path):\n        super().__init__()\n        self.gpu_num = gpu_num\n        self.save_path = save_path\n        deg_file = torch.load(self.save_path+'/deg.pt').to(torch.device(\"cuda:\"+str(self.gpu_num)))\n        self.pre_lin = Linear(1, 75)\n        aggregators = ['mean', 'min', 'max', 'std']\n        scalers = ['identity', 'amplification', 'attenuation']\n     \n        self.convs = ModuleList()\n        self.batch_norms = ModuleList()\n        for _ in range(4):\n            conv = PNAConv(in_channels=75, out_channels=75,\n                           aggregators=aggregators, scalers=scalers, deg=deg_file,\n                           towers=5, pre_layers=1, post_layers=1,\n                           divide_input=False)\n            self.convs.append(conv)\n            self.batch_norms.append(BatchNorm(75))\n        self.mlp = Sequential(Linear(76, 50), ReLU(), Linear(50, 25), ReLU(),\n                              Linear(25, 1))\n\n    def forward(self, x, alpha, edge_index, batch):\n        fixed_feature = torch.clone(x[:,0]).reshape(-1, 1)\n        x = self.pre_lin(x)\n        for conv, batch_norm in zip(self.convs, self.batch_norms):\n            x = F.relu(batch_norm(conv(x, edge_index)))\n        to_attach = alpha[batch].float().reshape(-1,1)\n        x = torch.cat([x, to_attach], -1)\n        x = self.mlp(x)\n        x = x.sigmoid()\n        return x, fixed_feature\n\n# ==========================================\n# File: edge_covering_node_matching/train_atheta/loss_aff.py\n# Function/Context: ErdosLoss\n# ==========================================\nimport torch\nimport sys\nsys.path.append(\"..\")\nfrom model_aff import Aff_Mnist, ResBlock\nfrom torch_geometric.nn.conv import MessagePassing\n\n\nclass Penalty(MessagePassing):\n    def __init__(self, gpu_num):\n        super(Penalty, self).__init__(aggr='add')  \n        self.device = torch.device(\"cuda:\"+str(gpu_num) if torch.cuda.is_available() else \"cpu\")\n    def forward(self, x, edge_index, edge_attr):\n        log_score = self.propagate(edge_index, x=x, edge_attr = edge_attr)\n        return torch.exp(log_score)*16*200*3\n    def message(self, edge_attr):\n        one = torch.ones(edge_attr.shape[0]).to(self.device).reshape(-1,1)\n        return torch.log(one-edge_attr+1e-6)\n\n\n\nclass ErdosLoss(torch.nn.Module):\n    def __init__(self, gpu_num, proxy):\n        super(ErdosLoss,self).__init__()\n        self.penalty = Penalty(gpu_num)\n        self.gpu_num = gpu_num\n        self.device = torch.device(\"cuda:\"+str(gpu_num) if torch.cuda.is_available() else \"cpu\")\n        self.proxy = proxy\n    def forward(self, x, edge_index, edge_feature, batch):\n        \"\"\"the erdos loss function\n        \"\"\"\n\n        x = x.unsqueeze(1) # x: B C H W\n        _, loss_1 = self.proxy(x,edge_index,edge_feature, batch)\n        loss_2 = self.penalty(x,edge_index, edge_feature)\n        erdos_loss1 = torch.mean(loss_1)\n        erdos_loss2 = torch.mean(loss_2)\n\n        return erdos_loss1 + erdos_loss2\n\n# ==========================================\n# File: edge_covering_node_matching/train_atheta/loss_con.py\n# Function/Context: ErdosLoss, Penalty\n# ==========================================\nimport torch\nimport sys\nsys.path.append(\"..\")\nfrom model_aff import Aff_Mnist, ResBlock\nfrom torch_geometric.nn.conv import MessagePassing\n\n\nclass Penalty(MessagePassing):\n    def __init__(self, gpu_num):\n        super(Penalty, self).__init__(aggr='add')  \n        self.device = torch.device(\"cuda:\"+str(gpu_num) if torch.cuda.is_available() else \"cpu\")\n    def forward(self, x, edge_index, edge_attr):\n        log_score = self.propagate(edge_index, x=x, edge_attr = edge_attr)\n        return torch.exp(log_score)*16*200*3\n    def message(self, edge_attr):\n        one = torch.ones(edge_attr.shape[0]).to(self.device).reshape(-1,1)\n        return torch.log(one-edge_attr+1e-6)\n\n\n\nclass ErdosLoss(torch.nn.Module):\n    def __init__(self, gpu_num, proxy):\n        super(ErdosLoss,self).__init__()\n        self.penalty = Penalty(gpu_num)\n        self.gpu_num = gpu_num\n        self.device = torch.device(\"cuda:\"+str(gpu_num) if torch.cuda.is_available() else \"cpu\")\n        self.proxy = proxy\n    def forward(self, x, edge_index, edge_feature, batch):\n        \"\"\"the erdos loss function\n        \"\"\"\n\n        x = x.unsqueeze(1) # x: B C H W\n        _, loss_1 = self.proxy(x,edge_index,edge_feature, batch)\n        loss_2 = self.penalty(x,edge_index, edge_feature)\n        erdos_loss1 = torch.mean(loss_1)\n        erdos_loss2 = torch.mean(loss_2)\n\n        return erdos_loss1 + erdos_loss2\n\n# ==========================================\n# File: edge_covering_node_matching/train_atheta/model_aff.py\n# Function/Context: SAGE_edge_aff, SAGE_edge_aff2, Aff_Mnist, Aff_Mnist2\n# ==========================================\nimport torch\nfrom torch.nn import Linear,LeakyReLU,BatchNorm1d,Sigmoid\nimport torch.nn.functional as F\nfrom torch_geometric.nn import SAGEConv\n\nfrom typing import Union, Tuple, Any\nfrom torch_geometric.typing import OptPairTensor, Adj, Size, OptTensor\nfrom torch import Tensor\nfrom torch.nn import Linear\nimport torch.nn.functional as F\nfrom torch_sparse import SparseTensor, matmul\nfrom torch_geometric.nn.conv import MessagePassing\nimport torch.nn as nn\n\nclass SAGEConv_edge(MessagePassing):\n    def __init__(self, in_channels: Union[int, Tuple[int, int]],\n                 out_channels: int, normalize: bool = False,\n                 root_weight: bool = True,\n                 bias: bool = True, **kwargs):  # yapf: disable\n        kwargs.setdefault('aggr', 'mean')\n        super(SAGEConv_edge, self).__init__(**kwargs)\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.normalize = normalize\n        self.root_weight = root_weight\n\n        if isinstance(in_channels, int):\n            in_channels = (in_channels, in_channels)\n        self.lin_l = Linear(in_channels[0], out_channels, bias=bias)\n        if self.root_weight:\n            self.lin_r = Linear(in_channels[1], out_channels, bias=False)\n        self.reset_parameters()\n        \n        self.lin_e = Linear(1, in_channels[0])\n    def reset_parameters(self):\n        self.lin_l.reset_parameters()\n        if self.root_weight:\n            self.lin_r.reset_parameters()\n    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj, edge_attr: OptTensor = None, size: Size = None):\n        if isinstance(x, Tensor):\n            x: OptPairTensor = (x, x)\n        \n        # propagate_type: (x: OptPairTensor)\n        if isinstance(edge_index, Tensor):\n            assert edge_attr is not None\n            #assert x[0].size(-1) == edge_attr.size(-1)\n        \n        out = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=size)\n        out = self.lin_l(out)\n        x_r = x[1]\n        if self.root_weight and x_r is not None:\n            out += self.lin_r(x_r)\n        if self.normalize:\n            out = F.normalize(out, p=2., dim=-1)\n        return out\n    def message(self, x_j: Tensor, edge_attr: Tensor):\n        #return x_j\n        edge_feature = self.lin_e(edge_attr)\n        return F.relu(x_j + edge_feature)\n    def message_and_aggregate(self, adj_t: SparseTensor, x: OptPairTensor):\n        adj_t = adj_t.set_value(None, layout=None)\n        return matmul(adj_t, x[0], reduce=self.aggr)\n    def __repr__(self):\n        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels, self.out_channels)\n\n\n\nclass SAGE_edge_aff(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels,out_channels):\n        super(SAGE_edge_aff, self).__init__()\n        torch.manual_seed(12345)\n\n        self.prelin = Linear(in_channels,hidden_channels)\n        self.conv1 = SAGEConv(hidden_channels, hidden_channels)\n        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n        self.conv3 = SAGEConv(hidden_channels, out_channels)\n\n        self.lin_hh1 = Linear(hidden_channels, hidden_channels)\n        self.lin_hh2 = Linear(hidden_channels, hidden_channels)\n        self.lin_oo = Linear(out_channels, out_channels)\n        self.lin_oh = Linear(out_channels, hidden_channels)\n        self.lin_h1 = Linear(hidden_channels, 1)\n\n        self.leakyrelu = LeakyReLU()\n        self.norm_h = BatchNorm1d(hidden_channels)\n        self.norm_o = BatchNorm1d(out_channels)\n\n        self.lin96 = Linear(192, 1)\n        self.lin32 = Linear(64, 1)\n\n    def forward(self, x, edge_index, edge_attr, batch):\n\n        x = self.prelin(x)\n        x = x.relu()\n\n        x = self.conv1(x, edge_index)\n        x = x.relu()\n        x = self.lin_hh1(x)\n        x = self.leakyrelu(x)\n\n        x = self.conv2(x, edge_index)\n        x = x.relu()\n        x = self.lin_hh2(x)\n        x = self.leakyrelu(x)\n   \n        x = self.conv3(x, edge_index)\n        x = x.relu()\n        x = self.lin_oo(x)\n        x = self.leakyrelu(x)  # n * 128\n       \n        edge_index_t = edge_index.t()\n        edge_left = edge_index_t[:,0]\n        edge_right = edge_index_t[:,1]\n        lifted_x_left = x[edge_left] # E * 128\n        lifted_x_right = x[edge_right] # E*128\n        lifted_x_left_96 = lifted_x_left[:,:96]  # E * 96\n        lifted_x_right_96 = lifted_x_right[:,:96]\n        lifted_x_left_32 = lifted_x_left[:,96:]   \n        lifted_x_right_32 = lifted_x_right[:,96:]\n\n        node_feature_96 = torch.cat([lifted_x_left_96,lifted_x_right_96],1) # E * (96 * 2)\n        node_feature_32 = torch.cat([lifted_x_left_32,lifted_x_right_32],1) # E * (32 * 2)\n        \n        node_feature_96 = self.lin96(node_feature_96)\n        node_feature_96 = node_feature_96.relu()\n        node_feature_32 = self.lin32(node_feature_32)\n        node_feature_32 = node_feature_32.relu()\n        \n        edge_out = node_feature_96 * edge_attr + node_feature_32 # E * 1\n        edge_out = edge_out.reshape(-1,48) # num_graph * 48\n        edge_out = torch.mean(edge_out, 1).reshape(-1,1)\n        return edge_out\n\n\nclass Aff_Mnist(torch.nn.Module):\n    def __init__(self, Res_Block, in_channels, hidden_channels,out_channels):\n        super(Aff_Mnist, self).__init__()\n        torch.manual_seed(12345)\n        self.ResNet = ResNet(Res_Block)\n        self.SAGE_edge_aff = SAGE_edge_aff(in_channels, hidden_channels, out_channels)\n\n    def forward(self, x, edge_index, edge_attr, batch):\n        res_out, sage_input = self.ResNet(x)\n        predict = self.SAGE_edge_aff(sage_input, edge_index, edge_attr, batch)\n        return res_out, predict\n\n\nclass Aff_Mnist2(torch.nn.Module):\n    def __init__(self, Res_Block, in_channels, hidden_channels,out_channels):\n        super(Aff_Mnist2, self).__init__()\n        torch.manual_seed(12345)\n        self.ResNet = ResNet(Res_Block)\n        self.SAGE_edge_aff2 = SAGE_edge_aff2(in_channels, hidden_channels, out_channels)\n\n    def forward(self, x, edge_index, edge_attr):\n        res_out, sage_input = self.ResNet(x)\n        predict = self.SAGE_edge_aff2(sage_input, edge_index, edge_attr)\n        return res_out, predict\n\n# ==========================================\n# File: edge_covering_node_matching/train_atheta/model_con.py\n# Function/Context: SAGE_edge_con, SAGE_edge_con2, Con_Mnist, Con_Mnist2\n# ==========================================\nimport torch\nfrom torch.nn import Linear,LeakyReLU,BatchNorm1d\nimport torch.nn.functional as F\nfrom torch_geometric.nn import SAGEConv\n\n\nfrom typing import Union, Tuple, Any\nfrom torch_geometric.typing import OptPairTensor, Adj, Size, OptTensor\nfrom torch import Tensor\nfrom torch.nn import Linear\nimport torch.nn.functional as F\nfrom torch_sparse import SparseTensor, matmul\nfrom torch_geometric.nn.conv import MessagePassing\nimport torch.nn as nn\n\nclass SAGEConv_edge(MessagePassing):\n    def __init__(self, in_channels: Union[int, Tuple[int, int]],\n                 out_channels: int, normalize: bool = False,\n                 root_weight: bool = True,\n                 bias: bool = True, **kwargs):  # yapf: disable\n        kwargs.setdefault('aggr', 'mean')\n        super(SAGEConv_edge, self).__init__(**kwargs)\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.normalize = normalize\n        self.root_weight = root_weight\n\n        if isinstance(in_channels, int):\n            in_channels = (in_channels, in_channels)\n        self.lin_l = Linear(in_channels[0], out_channels, bias=bias)\n        if self.root_weight:\n            self.lin_r = Linear(in_channels[1], out_channels, bias=False)\n        self.reset_parameters()\n        \n        self.lin_e = Linear(1, in_channels[0])\n    def reset_parameters(self):\n        self.lin_l.reset_parameters()\n        if self.root_weight:\n            self.lin_r.reset_parameters()\n    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj, edge_attr: OptTensor = None, size: Size = None):\n        if isinstance(x, Tensor):\n            x: OptPairTensor = (x, x)\n        \n        # propagate_type: (x: OptPairTensor)\n        if isinstance(edge_index, Tensor):\n            assert edge_attr is not None\n            #assert x[0].size(-1) == edge_attr.size(-1)\n        \n        out = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=size)\n        out = self.lin_l(out)\n        x_r = x[1]\n        if self.root_weight and x_r is not None:\n            out += self.lin_r(x_r)\n        if self.normalize:\n            out = F.normalize(out, p=2., dim=-1)\n        return out\n    def message(self, x_j: Tensor, edge_attr: Tensor):\n        #return x_j\n        edge_feature = self.lin_e(edge_attr)\n        return F.relu(x_j + edge_feature)\n    def message_and_aggregate(self, adj_t: SparseTensor, x: OptPairTensor):\n        adj_t = adj_t.set_value(None, layout=None)\n        return matmul(adj_t, x[0], reduce=self.aggr)\n    def __repr__(self):\n        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels, self.out_channels)\n\n\n\nclass SAGE_edge_con(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels,out_channels):\n        super(SAGE_edge_con, self).__init__()\n        torch.manual_seed(12345)\n\n        self.prelin = Linear(in_channels,hidden_channels)\n        self.conv1 = SAGEConv(hidden_channels, hidden_channels)\n        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n        self.conv3 = SAGEConv(hidden_channels, out_channels)\n\n        self.lin_hh1 = Linear(hidden_channels, hidden_channels)\n        self.lin_hh2 = Linear(hidden_channels, hidden_channels)\n        self.lin_oo = Linear(out_channels, out_channels)\n        self.lin_oh = Linear(out_channels, hidden_channels)\n        self.lin_h1 = Linear(hidden_channels, 1)\n\n        self.leakyrelu = LeakyReLU()\n        self.norm_h = BatchNorm1d(hidden_channels)\n        self.norm_o = BatchNorm1d(out_channels)\n\n        self.lin96 = Linear(192, 1)\n        self.lin32 = Linear(64, 1)\n        self.lin96_2 = Linear(192, 1)\n        self.lin32_2 = Linear(64, 1)\n\n        self.lin_1_64 = Linear(1, 32)\n        self.lin_64_1 = Linear(32, 1)\n\n    def forward(self, x, edge_index, edge_attr, batch):\n\n        x = self.prelin(x)\n        x = x.relu()\n\n        x = self.conv1(x, edge_index)\n        x = x.relu()\n        x = self.lin_hh1(x)\n        x = self.leakyrelu(x)\n\n        x = self.conv2(x, edge_index)\n        x = x.relu()\n        x = self.lin_hh2(x)\n        x = self.leakyrelu(x)\n   \n        x = self.conv3(x, edge_index)\n        x = x.relu()\n        x = self.lin_oo(x)\n        x = self.leakyrelu(x)  # n * 128\n       \n        edge_index_t = edge_index.t()\n        edge_left = edge_index_t[:,0]\n        edge_right = edge_index_t[:,1]\n        lifted_x_left = x[edge_left] # E * 128\n        lifted_x_right = x[edge_right] # E*128\n        lifted_x_left_96 = lifted_x_left[:,:96]  # E * 96\n        lifted_x_right_96 = lifted_x_right[:,:96]\n        lifted_x_left_32 = lifted_x_left[:,96:]   \n        lifted_x_right_32 = lifted_x_right[:,96:]\n\n        node_feature_96 = torch.cat([lifted_x_left_96,lifted_x_right_96],1) # E * (96 * 2)\n        node_feature_32 = torch.cat([lifted_x_left_32,lifted_x_right_32],1) # E * (32 * 2)\n        \n        node_feature_96_2 = self.lin96_2(node_feature_96)\n        node_feature_96_2 = node_feature_96_2\n        node_feature_32_2 = self.lin32_2(node_feature_32)\n        node_feature_32_2 = node_feature_32_2\n        \n        # the affine model\n        edge_out_con = node_feature_96_2 * edge_attr + node_feature_32_2\n        edge_out_con = edge_out_con.reshape(-1,48)\n        edge_out_con = torch.mean(edge_out_con, 1).reshape(-1,1)\n        this_affine = edge_out_con.clone()\n        # the con model\n        edge_out_con = self.lin_1_64(edge_out_con)\n        edge_out_con = 200 - edge_out_con.relu()\n        edge_out_con = self.lin_64_1(edge_out_con)\n        # add them together\n        #edge_out_con = edge_out_con + this_affine\n        \n        return edge_out_con\n\n# ==========================================\n# File: edge_covering_node_matching/train_proxy/model_aff.py\n# Function/Context: Aff_Mnist\n# ==========================================\nimport torch\nfrom torch.nn import Linear,LeakyReLU,BatchNorm1d\nimport torch.nn.functional as F\nfrom torch_geometric.nn import SAGEConv\n\nfrom typing import Union, Tuple, Any, Callable\nfrom torch_geometric.typing import OptPairTensor, Adj, Size, OptTensor\nfrom torch import Tensor\nfrom torch.nn import Linear\nimport torch.nn.functional as F\nfrom torch_sparse import SparseTensor, matmul\nfrom torch_geometric.nn.conv import MessagePassing\nimport torch.nn as nn\n\nclass SAGEConv_edge(MessagePassing):\n    def __init__(self, in_channels: Union[int, Tuple[int, int]],\n                 out_channels: int, normalize: bool = False,\n                 root_weight: bool = True,\n                 bias: bool = True, **kwargs):  # yapf: disable\n        kwargs.setdefault('aggr', 'mean')\n        super(SAGEConv_edge, self).__init__(**kwargs)\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.normalize = normalize\n        self.root_weight = root_weight\n\n        if isinstance(in_channels, int):\n            in_channels = (in_channels, in_channels)\n        self.lin_l = Linear(in_channels[0], out_channels, bias=bias)\n        if self.root_weight:\n            self.lin_r = Linear(in_channels[1], out_channels, bias=False)\n        self.reset_parameters()\n        \n        self.lin_e = Linear(1, in_channels[0])\n    def reset_parameters(self):\n        self.lin_l.reset_parameters()\n        if self.root_weight:\n            self.lin_r.reset_parameters()\n    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj, edge_attr: OptTensor = None, size: Size = None):\n        if isinstance(x, Tensor):\n            x: OptPairTensor = (x, x)\n        \n        # propagate_type: (x: OptPairTensor)\n        if isinstance(edge_index, Tensor):\n            assert edge_attr is not None\n            #assert x[0].size(-1) == edge_attr.size(-1)\n        \n        out = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=size)\n        out = self.lin_l(out)\n        x_r = x[1]\n        if self.root_weight and x_r is not None:\n            out += self.lin_r(x_r)\n        if self.normalize:\n            out = F.normalize(out, p=2., dim=-1)\n        return out\n    def message(self, x_j: Tensor, edge_attr: Tensor):\n        #return x_j\n        edge_feature = self.lin_e(edge_attr)\n        return F.relu(x_j + edge_feature)\n    def message_and_aggregate(self, adj_t: SparseTensor, x: OptPairTensor):\n        adj_t = adj_t.set_value(None, layout=None)\n        return matmul(adj_t, x[0], reduce=self.aggr)\n    def __repr__(self):\n        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels, self.out_channels)\n\n\n\nclass SAGE_edge_aff(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels,out_channels):\n        super(SAGE_edge_aff, self).__init__()\n        torch.manual_seed(12345)\n\n        self.prelin = Linear(in_channels,hidden_channels)\n        self.conv1 = SAGEConv(hidden_channels, hidden_channels)\n        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n        self.conv3 = SAGEConv(hidden_channels, out_channels)\n\n        self.lin_hh1 = Linear(hidden_channels, hidden_channels)\n        self.lin_hh2 = Linear(hidden_channels, hidden_channels)\n        self.lin_oo = Linear(out_channels, out_channels)\n        self.lin_oh = Linear(out_channels, hidden_channels)\n        self.lin_h1 = Linear(hidden_channels, 1)\n\n        self.leakyrelu = LeakyReLU()\n        self.norm_h = BatchNorm1d(hidden_channels)\n        self.norm_o = BatchNorm1d(out_channels)\n\n        self.lin96 = Linear(192, 1)\n        self.lin32 = Linear(64, 1)\n\n    def forward(self, x, edge_index, edge_attr, batch):\n\n        x = self.prelin(x)\n        x = x.relu()\n\n        x = self.conv1(x, edge_index)\n        x = x.relu()\n        x = self.lin_hh1(x)\n        x = self.leakyrelu(x)\n\n        x = self.conv2(x, edge_index)\n        x = x.relu()\n        x = self.lin_hh2(x)\n        x = self.leakyrelu(x)\n   \n        x = self.conv3(x, edge_index)\n        x = x.relu()\n        x = self.lin_oo(x)\n        x = self.leakyrelu(x)  # n * 128\n       \n        edge_index_t = edge_index.t()\n        edge_left = edge_index_t[:,0]\n        edge_right = edge_index_t[:,1]\n        lifted_x_left = x[edge_left] # E * 128\n        lifted_x_right = x[edge_right] # E*128\n        lifted_x_left_96 = lifted_x_left[:,:96]  # E * 96\n        lifted_x_right_96 = lifted_x_right[:,:96]\n        lifted_x_left_32 = lifted_x_left[:,96:]   \n        lifted_x_right_32 = lifted_x_right[:,96:]\n\n        node_feature_96 = torch.cat([lifted_x_left_96,lifted_x_right_96],1) # E * (96 * 2)\n        node_feature_32 = torch.cat([lifted_x_left_32,lifted_x_right_32],1) # E * (32 * 2)\n        \n        node_feature_96 = self.lin96(node_feature_96)\n        node_feature_96 = node_feature_96.relu()\n        node_feature_32 = self.lin32(node_feature_32)\n        node_feature_32 = node_feature_32.relu()\n        \n        edge_out = node_feature_96 * edge_attr + node_feature_32 # E * 1\n        edge_out = edge_out.reshape(-1,48) # num_graph * 48\n        edge_out = torch.mean(edge_out, 1).reshape(-1,1)\n        return edge_out\n\n\nclass ResBlock(nn.Module):\n    def __init__(self, inchannel, outchannel, stride=1):\n        super(ResBlock, self).__init__()\n        self.left = nn.Sequential(\n            nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),\n            nn.BatchNorm2d(outchannel),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(outchannel)\n        )\n        self.shortcut = nn.Sequential()\n        if stride != 1 or inchannel != outchannel:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(outchannel)\n            )\n            \n    def forward(self, x):\n        out = self.left(x)\n        out = out + self.shortcut(x)\n        out = F.relu(out)\n        \n        return out\n\nclass ResNet(nn.Module):\n    def __init__(self, ResBlock):\n        super(ResNet, self).__init__()\n        self.inchannel = 64\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU()\n        )\n        self.layer1 = self.make_layer(ResBlock, 64, 2, stride=1)\n        self.layer2 = self.make_layer(ResBlock, 64, 2, stride=2)\n        self.layer3 = self.make_layer(ResBlock, 64, 2, stride=2)        \n        self.layer4 = self.make_layer(ResBlock, 64, 2, stride=2)        \n        self.fc = nn.Linear(64, 16)\n        self.fc2 = nn.Linear(16, 1)\n        \n    def make_layer(self, block, channels, num_blocks, stride):\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.inchannel, channels, stride))\n            self.inchannel = channels\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        sage_input = self.fc(out)\n        res_out = self.fc2(sage_input)\n        return res_out, sage_input\n\n\nclass Aff_Mnist(torch.nn.Module):\n    def __init__(self, Res_Block, in_channels, hidden_channels,out_channels):\n        super(Aff_Mnist, self).__init__()\n        torch.manual_seed(12345)\n        self.ResNet = ResNet(Res_Block)\n        self.SAGE_edge_aff = SAGE_edge_aff(in_channels, hidden_channels, out_channels)\n\n    def forward(self, x, edge_index, edge_attr, batch):\n        res_out, sage_input = self.ResNet(x)\n        predict = self.SAGE_edge_aff(sage_input, edge_index, edge_attr, batch)\n        return res_out, predict\n\n# ==========================================\n# File: edge_covering_node_matching/train_proxy/model_con.py\n# Function/Context: SAGE_edge_con.forward, Con_Mnist.forward\n# ==========================================\nimport torch\nfrom torch.nn import Linear,LeakyReLU,BatchNorm1d,Sigmoid\nimport torch.nn.functional as F\nfrom torch_geometric.nn import SAGEConv\nfrom torch_geometric.nn import global_mean_pool\nfrom typing import Union, Tuple, Any\nfrom torch_geometric.typing import OptPairTensor, Adj, Size, OptTensor\nfrom torch import Tensor\nfrom torch.nn import Linear\nimport torch.nn.functional as F\nfrom torch_sparse import SparseTensor, matmul\nfrom torch_geometric.nn.conv import MessagePassing\nimport torch.nn as nn\n\nclass SAGEConv_edge(MessagePassing):\n    def __init__(self, in_channels: Union[int, Tuple[int, int]],\n                 out_channels: int, normalize: bool = False,\n                 root_weight: bool = True,\n                 bias: bool = True, **kwargs):  # yapf: disable\n        kwargs.setdefault('aggr', 'mean')\n        super(SAGEConv_edge, self).__init__(**kwargs)\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.normalize = normalize\n        self.root_weight = root_weight\n\n        if isinstance(in_channels, int):\n            in_channels = (in_channels, in_channels)\n        self.lin_l = Linear(in_channels[0], out_channels, bias=bias)\n        if self.root_weight:\n            self.lin_r = Linear(in_channels[1], out_channels, bias=False)\n        self.reset_parameters()\n        \n        self.lin_e = Linear(1, in_channels[0])\n    def reset_parameters(self):\n        self.lin_l.reset_parameters()\n        if self.root_weight:\n            self.lin_r.reset_parameters()\n    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj, edge_attr: OptTensor = None, size: Size = None):\n        if isinstance(x, Tensor):\n            x: OptPairTensor = (x, x)\n        \n        # propagate_type: (x: OptPairTensor)\n        if isinstance(edge_index, Tensor):\n            assert edge_attr is not None\n            #assert x[0].size(-1) == edge_attr.size(-1)\n        \n        out = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=size)\n        out = self.lin_l(out)\n        x_r = x[1]\n        if self.root_weight and x_r is not None:\n            out += self.lin_r(x_r)\n        if self.normalize:\n            out = F.normalize(out, p=2., dim=-1)\n        return out\n    def message(self, x_j: Tensor, edge_attr: Tensor):\n        #return x_j\n        edge_feature = self.lin_e(edge_attr)\n        return F.relu(x_j + edge_feature)\n    def message_and_aggregate(self, adj_t: SparseTensor, x: OptPairTensor):\n        adj_t = adj_t.set_value(None, layout=None)\n        return matmul(adj_t, x[0], reduce=self.aggr)\n    def __repr__(self):\n        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels, self.out_channels)\n\n\n\nclass SAGE_edge_con(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels,out_channels):\n        super(SAGE_edge_con, self).__init__()\n        torch.manual_seed(12345)\n\n        self.prelin = Linear(in_channels,hidden_channels)\n        self.conv1 = SAGEConv(hidden_channels, hidden_channels)\n        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n        self.conv3 = SAGEConv(hidden_channels, out_channels)\n\n        self.lin_hh1 = Linear(hidden_channels, hidden_channels)\n        self.lin_hh2 = Linear(hidden_channels, hidden_channels)\n        self.lin_oo = Linear(out_channels, out_channels)\n        self.lin_oh = Linear(out_channels, hidden_channels)\n        self.lin_h1 = Linear(hidden_channels, 1)\n\n        self.leakyrelu = LeakyReLU()\n        self.norm_h = BatchNorm1d(hidden_channels)\n        self.norm_o = BatchNorm1d(out_channels)\n\n        self.lin96 = Linear(192, 1)\n        self.lin32 = Linear(64, 1)\n        self.lin96_2 = Linear(192, 1)\n        self.lin32_2 = Linear(64, 1)\n\n        self.lin_1_64 = Linear(1, 32)\n        self.lin_64_1 = Linear(32, 1)\n\n    def forward(self, x, edge_index, edge_attr, batch):\n\n        x = self.prelin(x)\n        x = x.relu()\n\n        x = self.conv1(x, edge_index)\n        x = x.relu()\n        x = self.lin_hh1(x)\n        x = self.leakyrelu(x)\n\n        x = self.conv2(x, edge_index)\n        x = x.relu()\n        x = self.lin_hh2(x)\n        x = self.leakyrelu(x)\n   \n        x = self.conv3(x, edge_index)\n        x = x.relu()\n        x = self.lin_oo(x)\n        x = self.leakyrelu(x)  # n * 128\n       \n        edge_index_t = edge_index.t()\n        edge_left = edge_index_t[:,0]\n        edge_right = edge_index_t[:,1]\n        lifted_x_left = x[edge_left] # E * 128\n        lifted_x_right = x[edge_right] # E*128\n        lifted_x_left_96 = lifted_x_left[:,:96]  # E * 96\n        lifted_x_right_96 = lifted_x_right[:,:96]\n        lifted_x_left_32 = lifted_x_left[:,96:]   \n        lifted_x_right_32 = lifted_x_right[:,96:]\n\n        node_feature_96 = torch.cat([lifted_x_left_96,lifted_x_right_96],1) # E * (96 * 2)\n        node_feature_32 = torch.cat([lifted_x_left_32,lifted_x_right_32],1) # E * (32 * 2)\n        \n        node_feature_96_2 = self.lin96_2(node_feature_96)\n        node_feature_96_2 = node_feature_96_2\n        node_feature_32_2 = self.lin32_2(node_feature_32)\n        node_feature_32_2 = node_feature_32_2\n        \n        # the affine model\n        edge_out_con = node_feature_96_2 * edge_attr + node_feature_32_2\n        edge_out_con = edge_out_con.reshape(-1,48)\n        edge_out_con = torch.mean(edge_out_con, 1).reshape(-1,1)\n        this_affine = edge_out_con.clone()\n        # the con model\n        edge_out_con = self.lin_1_64(edge_out_con)\n        edge_out_con = 200 - edge_out_con.relu()\n        edge_out_con = self.lin_64_1(edge_out_con)\n        # add them together\n        #edge_out_con = edge_out_con + this_affine\n        \n        return edge_out_con\n\n\nclass ResBlock(nn.Module):\n    def __init__(self, inchannel, outchannel, stride=1):\n        super(ResBlock, self).__init__()\n        self.left = nn.Sequential(\n            nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),\n            nn.BatchNorm2d(outchannel),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(outchannel)\n        )\n        self.shortcut = nn.Sequential()\n        if stride != 1 or inchannel != outchannel:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(outchannel)\n            )\n            \n    def forward(self, x):\n        out = self.left(x)\n        out = out + self.shortcut(x)\n        out = F.relu(out)\n        \n        return out\n\nclass ResNet(nn.Module):\n    def __init__(self, ResBlock):\n        super(ResNet, self).__init__()\n        self.inchannel = 64\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU()\n        )\n        self.layer1 = self.make_layer(ResBlock, 64, 2, stride=1)\n        self.layer2 = self.make_layer(ResBlock, 64, 2, stride=2)\n        self.layer3 = self.make_layer(ResBlock, 64, 2, stride=2)        \n        self.layer4 = self.make_layer(ResBlock, 64, 2, stride=2)        \n        self.fc = nn.Linear(64, 16)\n        self.fc2 = nn.Linear(16, 1)\n        \n    def make_layer(self, block, channels, num_blocks, stride):\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.inchannel, channels, stride))\n            self.inchannel = channels\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        sage_input = self.fc(out)\n        res_out = self.fc2(sage_input)\n        return res_out, sage_input\n\nclass weightConstraint(object):\n    def __init__(self):\n        pass\n    \n    def __call__(self,module):\n        if hasattr(module,'weight'):\n            w=module.weight.data\n            w=w.clamp(0)\n            module.weight.data=w\n            \nclass Con_Mnist(torch.nn.Module):\n    def __init__(self, Res_Block, in_channels, hidden_channels,out_channels):\n        super(Con_Mnist, self).__init__()\n        torch.manual_seed(12345)\n        self.ResNet = ResNet(Res_Block)\n        self.SAGE_edge_con = SAGE_edge_con(in_channels, hidden_channels, out_channels)\n\n    def forward(self, x, edge_index, edge_attr, batch):\n        res_out, sage_input = self.ResNet(x)\n        predict = self.SAGE_edge_con(sage_input, edge_index, edge_attr, batch)\n        return res_out, predict\n\n# ==========================================\n# File: resource_binding/train_atheta/loss.py\n# Function/Context: ErdosLoss.forward\n# ==========================================\nimport torch\nfrom torch.nn import Linear\nimport torch.nn.functional as F\n\nfrom torch_geometric.nn import global_mean_pool\n\nimport sys\nsys.path.append(\"..\")\nfrom model import LINEAR_lut, LINEAR_dsp\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel_dsp = LINEAR_dsp(in_channels = 10, hidden_channels = 128, out_channels = 512)\nmodel_dsp.load_state_dict(torch.load(' '),torch.device(\"cpu\"))\nmodel_dsp.to(device)\nmodel_dsp.eval()\n\nmodel_lut = LINEAR_lut(in_channels = 10, hidden_channels = 128, out_channels = 512)\nmodel_lut.load_state_dict(torch.load(' '),torch.device(\"cpu\"))\nmodel_lut.to(device)\nmodel_lut.eval()\n\nclass ErdosLoss(torch.nn.Module):\n    def __init__(self):\n        super(ErdosLoss,self).__init__()\n\n    def forward(self, x, alpha, fixed_feature, edge_index, batch):\n        \"\"\"the erdos loss function\n        \"\"\"\n        x = torch.cat([fixed_feature, x],1)\n        predicted_dsp = model_dsp(x, edge_index, batch)\n        predicted_lut = model_lut(x, edge_index, batch)\n       \n        loss = predicted_lut + alpha * predicted_dsp\n        \n        erdos_loss = torch.mean(loss)\n\n        return erdos_loss\n\n# ==========================================\n# File: resource_binding/train_atheta/model.py\n# Function/Context: LINEAR_model2, LINEAR_dsp, LINEAR_lut\n# ==========================================\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn import Embedding, Linear, ModuleList, ReLU, Sequential, LeakyReLU, BatchNorm1d\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom torch_geometric.datasets import ZINC\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.nn import BatchNorm, PNAConv, global_add_pool, global_mean_pool, SAGEConv\nfrom torch_geometric.utils import degree\n\nfrom torch_geometric.nn.conv import MessagePassing\n\nclass LastLayer(MessagePassing):\n    def __init__(self):\n        super(LastLayer, self).__init__(aggr='add')  \n    def forward(self, x, edge_index):\n        log_score = self.propagate(edge_index, x=x)\n        log_score = log_score + torch.log(x + 1e-6)\n        return torch.exp(log_score)\n    def message(self, x_j):\n        return torch.log(x_j+1e-6)\n\n\nclass LINEAR_model2(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels,out_channels):\n        super(LINEAR_model2, self).__init__()\n        torch.manual_seed(12345)\n\n        self.prelin = Linear(in_channels,hidden_channels)\n        self.conv1 = SAGEConv(hidden_channels, hidden_channels)\n        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n        self.conv3 = SAGEConv(hidden_channels, out_channels)\n\n        self.lin_hh1 = Linear(hidden_channels, hidden_channels)\n        self.lin_hh2 = Linear(hidden_channels, hidden_channels)\n        self.lin_oh = Linear(out_channels + 1, hidden_channels)\n        self.lin_h1 = Linear(hidden_channels, 1)\n\n        self.leakyrelu1 = LeakyReLU()\n        self.leakyrelu2 = LeakyReLU()\n        self.norm_o = BatchNorm1d(out_channels)\n        self.norm_h = BatchNorm1d(hidden_channels)\n\n    def forward(self, x, alpha, edge_index, batch):\n        fixed_feature = torch.clone(x[:,:10])\n        mask = torch.clone(x[:,9]).reshape(-1,1)\n        x = self.prelin(x)\n        x = self.conv1(x, edge_index)\n        x = x.relu()\n        x = self.lin_hh1(x)\n        x = F.dropout(x, p=0.1, training=self.training)\n        x = self.leakyrelu1(x)\n\n        x = self.conv2(x, edge_index)\n        x = x.relu()\n        x = self.lin_hh2(x)\n        x = F.dropout(x, p=0.1, training=self.training)\n        x = self.leakyrelu1(x)\n   \n        x = self.conv3(x, edge_index)\n        x = x.relu()\n\n        to_attach = alpha[batch].float().reshape(-1,1)\n        x = torch.cat([x, to_attach], -1)\n        x = self.lin_oh(x)\n        x = F.dropout(x, p=0.1, training=self.training)\n        x = self.leakyrelu1(x)\n        x = self.lin_h1(x)\n        x = 10 * x.sigmoid()\n        x = x * mask\n        return x, fixed_feature\n\nclass LINEAR_dsp(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels,out_channels):\n        super(LINEAR_dsp, self).__init__()\n        torch.manual_seed(12345)\n\n        self.prelin = Linear(in_channels,hidden_channels)\n        self.conv1 = SAGEConv(hidden_channels, hidden_channels)\n        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n        self.conv3 = SAGEConv(hidden_channels, out_channels)\n\n        self.lin_hh1 = Linear(hidden_channels, hidden_channels)\n        self.lin_hh2 = Linear(hidden_channels, hidden_channels)\n\n        self.lin_oo = Linear(out_channels, out_channels)\n        self.lin_oo2 = Linear(out_channels, out_channels)\n        self.lin_oo_ = Linear(out_channels, out_channels)\n        self.lin_oo__ = Linear(out_channels, out_channels)\n        self.lin_oh = Linear(out_channels, hidden_channels)\n        self.lin_h1 = Linear(hidden_channels, 1)\n        self.leakyrelu = LeakyReLU()\n        self.norm_o = BatchNorm1d(out_channels)\n        self.norm_h = BatchNorm1d(hidden_channels)\n     \n        self.linear_prop = LastLayer()\n        self.lin_64_32 = Linear(256, 128)\n        self.lin32_1 = Linear(128, 1)\n        self.lin_linear = Linear(256, 1)\n        #self.lin_xvar_64 = Linear(1, 64)\n\n    def forward(self, x, edge_index, batch):\n        z = torch.clone(x[:,:10])\n        x_var = torch.clone(x[:,10]).reshape(-1,1)\n        \n        z = self.prelin(z)\n        \n        z = self.conv1(z, edge_index)\n        \n        z = z.relu()\n        z = self.lin_hh1(z)\n        z = self.leakyrelu(z)\n        \n        z = self.conv2(z, edge_index)\n        z = z.relu()\n        z = self.lin_hh2(z)\n        z = self.leakyrelu(z)\n        \n        z = self.conv3(z, edge_index)\n        z = z.relu()\n        z = self.lin_oo_(z)\n        z = z.relu()\n        z = self.lin_oo__(z)\n        z2 = self.lin_oo2(z)\n        z = self.lin_oo(z)\n        \n        z = z.relu()\n        z_front = z[:,:256]\n        z_behind = z[:,256:]\n        \n        z2 = z2.relu()\n        z2_front = z2[:,:256]\n        z2_behind = z2[:,256:]\n        \n        x_combine = z_front * x_var + z_behind\n        x_combine = self.linear_prop(x_combine,edge_index)\n        x_combine = global_mean_pool(x_combine, batch) \n        x_combine = self.lin_64_32(x_combine)\n        x_combine = 300 - x_combine.relu()\n        x_combine = self.lin32_1(x_combine)\n\n        x_linear = z2_front * x_var + z2_behind\n        x_linear = self.linear_prop(x_linear, edge_index)\n        x_linear = global_mean_pool(x_linear, batch)\n        x_linear = self.lin_linear(x_linear)\n        x_out = x_combine + x_linear\n        return x_out\n\nclass LINEAR_lut(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels,out_channels):\n        super(LINEAR_lut, self).__init__()\n        torch.manual_seed(12345)\n\n        self.prelin = Linear(in_channels,hidden_channels)\n        self.conv1 = SAGEConv(hidden_channels, hidden_channels)\n        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n        self.conv3 = SAGEConv(hidden_channels, out_channels)\n\n        self.lin_hh1 = Linear(hidden_channels, hidden_channels)\n        self.lin_hh2 = Linear(hidden_channels, hidden_channels)\n        self.lin_oo = Linear(out_channels, out_channels)\n        self.lin_oo2 = Linear(out_channels, out_channels)\n        self.lin_oh = Linear(out_channels, hidden_channels)\n        self.lin_h1 = Linear(hidden_channels, 1)\n\n \n        self.leakyrelu = LeakyReLU()\n        self.norm_o = BatchNorm1d(out_channels)\n        self.norm_h = BatchNorm1d(hidden_channels)\n     \n        self.linear_prop = LastLayer()\n        self.lin_64_1 = Linear(256, 128)\n        self.lin32_1 = Linear(128, 1)\n        self.lin_xvar_64 = Linear(1, 64)\n\n    def forward(self, x, edge_index, batch):\n        z = torch.clone(x[:,:10])\n        x_var = torch.clone(x[:,10]).reshape(-1,1)\n        \n        z = self.prelin(z)\n        \n        z = self.conv1(z, edge_index)\n        \n        z = z.relu()\n        z = self.lin_hh1(z)\n        \n        z = self.conv2(z, edge_index)\n        z = z.relu()\n        z = self.lin_hh2(z)\n   \n        z = self.conv3(z, edge_index)\n        z2 = self.lin_oo2(z)\n        z = self.lin_oo(z)\n        \n        z = z.relu()\n        z_front = z[:,:256]\n        z_behind = z[:,256:]\n\n        z2 = z2.relu()\n        z2_front = z2[:,:256]\n        z2_behind = z2[:,256:]\n        \n        x_combine = z_front * x_var + z_behind\n\n        x_combine = self.linear_prop(x_combine,edge_index)\n\n        x_combine = global_mean_pool(x_combine, batch) \n   \n        x_combine = self.lin_64_1(x_combine)\n        x_combine = 7000 - x_combine.relu()\n        x_combine = self.lin32_1(x_combine)\n\n        x_linear = z2_front * x_var + z2_behind\n        x_linear = self.linear_prop(x_linear, edge_index)\n        x_linear = global_mean_pool(x_linear, batch)\n        x_linear = self.lin_linear(x_linear)\n        x_out = x_combine + x_linear\n        \n        return x_out\n\n# ==========================================\n# File: resource_binding/train_dsp_con/model.py\n# Function/Context: LINEAR_dsp\n# ==========================================\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn import Embedding, Linear, ModuleList, ReLU, Sequential, LeakyReLU, BatchNorm1d\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom torch_geometric.datasets import ZINC\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.nn import BatchNorm, PNAConv, global_add_pool, global_mean_pool, SAGEConv\nfrom torch_geometric.utils import degree\n\nfrom torch_geometric.nn.conv import MessagePassing\n\nclass LastLayer(MessagePassing):\n    def __init__(self):\n        super(LastLayer, self).__init__(aggr='add')  \n    def forward(self, x, edge_index):\n        #edge_index\n        #import pdb; pdb.set_trace()\n        log_score = self.propagate(edge_index, x=x)\n        #print(torch.exp(log_score))\n        log_score = log_score + torch.log(x + 1e-6)\n        return torch.exp(log_score)\n    def message(self, x_j):\n        return torch.log(x_j+1e-6)\n\nclass weightConstraint(object):\n    def __init__(self):\n        pass\n    \n    def __call__(self,module):\n        if hasattr(module,'weight'):\n            #print(\"Entered\")\n            w=module.weight.data\n            w=w.clamp(0)\n            module.weight.data=w\n            #print(\"done\")\n\nclass LINEAR_dsp(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels,out_channels):\n        super(LINEAR_dsp, self).__init__()\n        torch.manual_seed(12345)\n\n        self.prelin = Linear(in_channels,hidden_channels)\n        self.conv1 = SAGEConv(hidden_channels, hidden_channels)\n        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n        self.conv3 = SAGEConv(hidden_channels, out_channels)\n\n        self.lin_hh1 = Linear(hidden_channels, hidden_channels)\n        self.lin_hh2 = Linear(hidden_channels, hidden_channels)\n\n        self.lin_oo = Linear(out_channels, out_channels)\n        self.lin_oo2 = Linear(out_channels, out_channels)\n        self.lin_oo_ = Linear(out_channels, out_channels)\n        self.lin_oo__ = Linear(out_channels, out_channels)\n        self.lin_oh = Linear(out_channels, hidden_channels)\n        self.lin_h1 = Linear(hidden_channels, 1)\n        self.leakyrelu = LeakyReLU()\n        self.norm_o = BatchNorm1d(out_channels)\n        self.norm_h = BatchNorm1d(hidden_channels)\n     \n        self.linear_prop = LastLayer()\n        self.lin_64_32 = Linear(256, 128)\n        self.lin32_1 = Linear(128, 1)\n        self.lin_linear = Linear(256, 1)\n        #self.lin_xvar_64 = Linear(1, 64)\n\n    def forward(self, x, edge_index, batch):\n        z = torch.clone(x[:,:10])\n        x_var = torch.clone(x[:,10]).reshape(-1,1)\n        \n        z = self.prelin(z)\n        \n        #z = z.relu()\n        z = self.conv1(z, edge_index)\n        \n        z = z.relu()\n        z = self.lin_hh1(z)\n        #z = F.dropout(z, p=0.1, training=self.training)\n        z = self.leakyrelu(z)\n        \n        z = self.conv2(z, edge_index)\n        z = z.relu()\n        z = self.lin_hh2(z)\n        #z = F.dropout(z, p=0.1, training=self.training)\n        z = self.leakyrelu(z)\n        \n        z = self.conv3(z, edge_index)\n        z = z.relu()\n        z = self.lin_oo_(z)\n        z = z.relu()\n        z = self.lin_oo__(z)\n        z2 = self.lin_oo2(z)\n        z = self.lin_oo(z)\n        \n        #z = F.dropout(z, p=0.1, training=self.training)\n        z = z.relu()\n        z_front = z[:,:256]\n        z_behind = z[:,256:]\n        \n        z2 = z2.relu()\n        z2_front = z2[:,:256]\n        z2_behind = z2[:,256:]\n        #x_var = torch.square(self.lin_xvar_64(x_var))\n        x_combine = z_front * x_var + z_behind\n        x_combine = self.linear_prop(x_combine,edge_index)\n        x_combine = global_mean_pool(x_combine, batch) \n        x_combine = self.lin_64_32(x_combine)\n        x_combine = 300 - x_combine.relu()\n        x_combine = self.lin32_1(x_combine)\n\n        x_linear = z2_front * x_var + z2_behind\n        x_linear = self.linear_prop(x_linear, edge_index)\n        x_linear = global_mean_pool(x_linear, batch)\n        x_linear = self.lin_linear(x_linear)\n        x_out = x_combine + x_linear\n        return x_out\n\n# ==========================================\n# File: resource_binding/train_lut_con/model.py\n# Function/Context: LINEAR_lut\n# ==========================================\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn import Embedding, Linear, ModuleList, ReLU, Sequential, LeakyReLU, BatchNorm1d\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom torch_geometric.datasets import ZINC\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.nn import BatchNorm, PNAConv, global_add_pool, global_mean_pool, SAGEConv\nfrom torch_geometric.utils import degree\n\nfrom torch_geometric.nn.conv import MessagePassing\n\nclass LastLayer(MessagePassing):\n    def __init__(self):\n        super(LastLayer, self).__init__(aggr='add')  \n    def forward(self, x, edge_index):\n        log_score = self.propagate(edge_index, x=x)\n        #print(torch.exp(log_score))\n        log_score = log_score + torch.log(x + 1e-6)\n        return torch.exp(log_score)\n    def message(self, x_j):\n        return torch.log(x_j+1e-6)\n\n\nclass weightConstraint(object):\n    def __init__(self):\n        pass\n    \n    def __call__(self,module):\n        if hasattr(module,'weight'):\n            #print(\"Entered\")\n            w=module.weight.data\n            w=w.clamp(0)\n            module.weight.data=w\n            #print(\"done\")\n            \nclass LINEAR_lut(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels,out_channels):\n        super(LINEAR_lut, self).__init__()\n        torch.manual_seed(12345)\n\n        self.prelin = Linear(in_channels,hidden_channels)\n        self.conv1 = SAGEConv(hidden_channels, hidden_channels)\n        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n        self.conv3 = SAGEConv(hidden_channels, out_channels)\n\n        self.lin_hh1 = Linear(hidden_channels, hidden_channels)\n        self.lin_hh2 = Linear(hidden_channels, hidden_channels)\n        self.lin_oo = Linear(out_channels, out_channels)\n        self.lin_oo2 = Linear(out_channels, out_channels)\n        self.lin_oh = Linear(out_channels, hidden_channels)\n        self.lin_h1 = Linear(hidden_channels, 1)\n\n \n        self.leakyrelu = LeakyReLU()\n        self.norm_o = BatchNorm1d(out_channels)\n        self.norm_h = BatchNorm1d(hidden_channels)\n     \n        self.linear_prop = LastLayer()\n        self.lin_64_1 = Linear(256, 128)\n        self.lin32_1 = Linear(128, 1)\n        self.lin_xvar_64 = Linear(1, 64)\n\n    def forward(self, x, edge_index, batch):\n        z = torch.clone(x[:,:10])\n        x_var = torch.clone(x[:,10]).reshape(-1,1)\n        \n        z = self.prelin(z)\n        \n        #z = z.relu()\n        z = self.conv1(z, edge_index)\n        \n        z = z.relu()\n        z = self.lin_hh1(z)\n        #z = F.dropout(z, p=0.1, training=self.training)\n        #z = self.leakyrelu(z)\n        \n        z = self.conv2(z, edge_index)\n        z = z.relu()\n        z = self.lin_hh2(z)\n        #z = F.dropout(z, p=0.1, training=self.training)\n        #z = self.leakyrelu(z)\n   \n        z = self.conv3(z, edge_index)\n        #z = z.relu()\n        z2 = self.lin_oo2(z)\n        z = self.lin_oo(z)\n        \n        #z = F.dropout(z, p=0.1, training=self.training)\n        z = z.relu()\n        z_front = z[:,:256]\n        z_behind = z[:,256:]\n\n        z2 = z2.relu()\n        z2_front = z2[:,:256]\n        z2_behind = z2[:,256:]\n        \n        #x_var = torch.square(self.lin_xvar_64(x_var))\n        x_combine = z_front * x_var + z_behind\n\n        x_combine = self.linear_prop(x_combine,edge_index)\n\n        x_combine = global_mean_pool(x_combine, batch) \n   \n        x_combine = self.lin_64_1(x_combine)\n        x_combine = 7000 - x_combine.relu()\n        x_combine = self.lin32_1(x_combine)\n\n        x_linear = z2_front * x_var + z2_behind\n        x_linear = self.linear_prop(x_linear, edge_index)\n        x_linear = global_mean_pool(x_linear, batch)\n        x_linear = self.lin_linear(x_linear)\n        x_out = x_combine + x_linear\n        \n        return x_out",
  "description": "Combined Analysis:\n- [approximate_computing/train_atheta/loss.py]: This file implements the core relaxed loss function from the paper's optimization model. The ErdosLoss class computes the relaxed objective f_r(X;C) + g_r(X;C) in entry-wise concave form. Specifically:\n1. The proxy neural network (self.proxy) approximates the expensive objective f, outputting relative_error (scaled by 100).\n2. The term -alpha * x (after summing reshaped x) acts as a concave relaxation of constraints or regularization, where alpha corresponds to the Lagrange multiplier .\n3. The loss is averaged over the batch, matching the unsupervised training procedure where the network A_ outputs continuous solutions X.\n4. This directly implements Step 2 (constructing entry-wise concave objective/constraints) and the gradient-based optimization of the relaxed problem.\n- [approximate_computing/train_atheta/model_con.py]: This file implements the neural network component (A_) of the paper's framework. Specifically, it defines two graph neural network models (PNA_con and PNA_con2) that output continuous soft solutions (X) for combinatorial optimization problems. The models use PNAConv layers to process graph-structured data, and their architectures incorporate design elements aligned with the paper's methodology: 1) PNA_con uses a specialized LastLayer for message passing that computes a product-like aggregation, and includes a concave branch (via ReLU and subtraction) combined with a linear branch, reflecting the entry-wise concave relaxation design. 2) PNA_con2 incorporates an external parameter alpha (likely related to the configuration C) and uses a sigmoid activation to output values in [0,1], representing soft binary assignments. These models are designed to be trained unsupervised to minimize a relaxed objective, as per the paper's algorithm. However, the file does not contain the training loop, loss function (relaxed objective), or rounding procedurethose would be in other files. Thus, this file implements the core neural network architecture that maps configurations C to soft solutions X, which is a key step in the overall framework.\n- [edge_covering_node_matching/train_atheta/loss_aff.py]: This file implements the core relaxed loss function from the paper's optimization model. The ErdosLoss class computes the combined relaxed loss f_r(X;C) +  g_r(X;C). Specifically: 1) loss_1 (from self.proxy) corresponds to the relaxed objective f_r. 2) loss_2 (from Penalty) corresponds to the relaxed constraint penalty  g_r, where the constraint is formulated in an entry-wise concave form via log(1 - edge_attr). The forward method returns the sum of these two terms, exactly matching the paper's unsupervised training objective for the neural network A_. The Penalty class uses a message-passing structure to aggregate constraint violations across graph edges.\n- [edge_covering_node_matching/train_atheta/loss_con.py]: This file implements a key component of the core optimization logic from the paper: the loss function used during training to minimize the relaxed objective and constraints. The ErdosLoss class computes a loss that sums two terms: (1) loss_1 from a proxy neural network (likely representing the relaxed objective f_r), and (2) loss_2 from a Penalty class (likely representing the relaxed constraint g_r in entry-wise concave form). The Penalty class uses message passing to compute a term based on edge attributes (e.g., torch.log(one - edge_attr + 1e-6)), which enforces constraints like g(X) < 1 in a concave manner. This aligns with the paper's framework of minimizing f_r +  g_r during unsupervised training, where  is implicitly handled via scaling (e.g., *16*200*3 in Penalty.forward). The code is specific to graph-based problems like edge covering and node matching, using graph neural network components.\n- [edge_covering_node_matching/train_atheta/model_aff.py]: This file implements the neural network component _ from the paper's framework. Specifically, it contains several GNN architectures (SAGE_edge_aff, SAGE_edge_aff2) and combined models (Aff_Mnist, Aff_Mnist2) that output soft solutions X  [0,1]^n. These models are designed for the edge covering and node matching problem (Problem I in the paper). The code implements the neural network that parameterizes the continuous solution, which is then optimized via gradient descent on the relaxed objective. However, the file does NOT contain the complete optimization logic - it lacks the entry-wise concave relaxation of the objective/constraints, the training loop with gradient optimization, and the rounding procedure to obtain discrete solutions.\n- [edge_covering_node_matching/train_atheta/model_con.py]: This file implements the neural network A_ (proxy model) from the paper's framework. Specifically, it contains several GNN architectures (SAGE_edge_con, SAGE_edge_con2) that output continuous edge predictions X  [0,1]^n, which correspond to the relaxed optimization variables. The models take graph data (node features, edge indices, edge attributes) and output edge-level predictions. The Con_Mnist and Con_Mnist2 classes combine ResNet for image processing with GNNs for graph-based predictions, aligning with the paper's application to feature-based edge covering and node matching problems. The code implements the neural network component A_ that produces soft solutions X, which are then used in the relaxed loss minimization f_r(X;C) + g_r(X;C) during training.\n- [edge_covering_node_matching/train_proxy/model_aff.py]: This file implements the neural network proxy model (A_) for the edge covering and node matching problem described in the paper. The Aff_Mnist class combines a ResNet for processing MNIST image inputs with a graph neural network (SAGE_edge_aff) that operates on graph-structured data. The model outputs continuous predictions (soft solutions X) for edge selection variables, which aligns with the paper's framework of learning a continuous relaxation of the binary optimization variables. The architecture specifically handles the PCO aspect where the objective/constraints require learning from data (MNIST images and graph structures).\n- [edge_covering_node_matching/train_proxy/model_con.py]: This file implements the neural network proxy model for the edge covering and node matching problem, which is a specific application of the paper's framework. The Con_Mnist class combines a ResNet for processing graph configuration features (MNIST digits) with a graph neural network (SAGE_edge_con) that outputs constraint values for edges. The SAGE_edge_con model uses edge attributes and node features to compute constraint values in an entry-wise concave form (via the affine transformation and subsequent concave transformation). This aligns with the paper's requirement for entry-wise concave constraint functions to enable principled relaxation and rounding. The model outputs continuous values that represent relaxed constraint evaluations, which are then used in the unsupervised training loss.\n- [resource_binding/train_atheta/loss.py]: This file implements the core relaxed loss function from the paper's framework. Specifically:\n1. It defines the ErdosLoss class which computes the relaxed objective f_r(;C) + g_r(;C) for the resource binding problem (Application II in the paper).\n2. The loss combines two proxy models: predicted_lut (proxy for LUT usage objective f) and predicted_dsp (proxy for DSP constraint g), with  acting as the penalty parameter  for the constraint.\n3. The forward method takes the continuous optimization variable x (), concatenates it with fixed features, passes through pre-trained proxy networks, and computes the linear combination loss = f_r + g_r.\n4. This exactly matches the paper's Algorithm 1 step of minimizing the relaxed loss L_r() = E_C[f_r(_(C);C) + g_r(_(C);C)] via gradient descent.\n5. The proxy models (LINEAR_lut and LINEAR_dsp) are pre-trained neural networks that approximate the true objective/constraint functions, consistent with the PCO setting where direct evaluation is expensive.\n6. The code uses PyTorch for automatic differentiation, enabling unsupervised training of the network A_ that produces soft solutions .\n- [resource_binding/train_atheta/model.py]: This file implements the neural network models (A_) for the unsupervised learning framework described in the paper. Specifically:\n\n1. LINEAR_model2 implements the neural network A_ that outputs soft solutions X  [0,1]^n for the optimization variables. The forward method returns both the soft solution (x) and fixed features, with the output constrained via sigmoid and masking.\n\n2. LINEAR_dsp and LINEAR_lut implement proxy models for the objective/constraint functions f and g in the resource allocation problem (Problem II). These models predict DSP and LUT usage respectively, using graph neural networks with SAGEConv layers and a custom LastLayer message passing module.\n\n3. The models use entry-wise operations (z_front * x_var + z_behind) that can be designed to be concave with respect to the optimization variables, aligning with the paper's requirement for entry-wise concave structure.\n\n4. The implementation uses PyTorch Geometric for graph neural network components, which is appropriate for the graph-structured problems addressed in the paper.\n\nThis file contains the core neural network architectures that would be trained using the unsupervised loss f_r(X;C) + g_r(X;C) as described in the paper's framework.\n- [resource_binding/train_dsp_con/model.py]: This file implements the neural network architecture (LINEAR_dsp) that serves as the proxy model A_ in the paper's framework. The model takes configuration C (graph-structured input with node features and edge indices) and outputs a continuous solution vector (soft assignment) for the relaxed optimization problem. Specifically, it processes graph data through SAGEConv layers and linear transformations, then uses a custom LastLayer (a message passing layer) to compute entry-wise concave transformations (via log-sum-exp operations). The forward pass combines learned representations with optimization variables (x_var) to produce a scalar output that corresponds to the relaxed objective value f_r(;C) or constraint g_r(;C). This aligns with Step 2 of the paper's framework: constructing entry-wise concave relaxations of the objective/constraints and parameterizing the solution via a neural network.\n- [resource_binding/train_lut_con/model.py]: This file implements the neural network architecture (LINEAR_lut) that serves as the proxy model A_ in the paper's framework. The model takes configuration C (represented as graph features x and edge_index) and outputs a continuous solution X. Specifically:\n1. The model processes graph-structured data using SAGEConv layers, which aligns with the paper's application to graph-based CO problems.\n2. The forward pass produces a continuous output x_out through a combination of graph convolutions and linear transformations.\n3. The LastLayer class implements a custom message-passing operation that applies an entry-wise concave-like transformation (log-sum-exp pattern), which is consistent with the paper's requirement for entry-wise concave relaxations.\n4. The weightConstraint class enforces non-negative weights, which may relate to maintaining convexity properties.\n5. The model outputs a continuous value that can be interpreted as the relaxed solution X, which would later be rounded to obtain the discrete solution X.\n\nThis implementation directly corresponds to Step 2 of the paper's algorithm: training a neural network A_ to output soft solutions X that minimize the relaxed loss. The architecture is tailored for the resource allocation problem (circuit design) mentioned in the paper.",
  "dependencies": [
    "weightConstraint (custom class)",
    "sys",
    "torch.optim.lr_scheduler",
    "torch_geometric.typing",
    "torch_sparse",
    "torch_geometric.nn (BatchNorm, PNAConv, global_add_pool, global_mean_pool, SAGEConv)",
    "torch_geometric.nn",
    "torch_geometric.utils (degree)",
    "model.LINEAR_dsp",
    "torch.nn.functional",
    "typing",
    "LastLayer (custom class)",
    "model.LINEAR_lut",
    "model_aff.ResBlock",
    "torch_geometric.datasets",
    "torch_geometric.nn.conv (MessagePassing)",
    "torch_geometric.data",
    "torch_geometric.nn.conv.MessagePassing",
    "model (external proxy neural network)",
    "torch_geometric.nn.global_mean_pool",
    "model_aff.Aff_Mnist",
    "torch_geometric.utils",
    "torch.nn (Embedding, Linear, ModuleList, ReLU, Sequential, LeakyReLU, BatchNorm1d)",
    "torch_geometric.nn.conv",
    "weightConstraint",
    "LastLayer",
    "torch.nn",
    "torch_geometric",
    "model_aff (local module for proxy neural network)",
    "torch"
  ]
}