{
  "file_path": "CVRP/source/TRAIN_N_EVAL/Train_Grouped_Actors.py, CVRP/source/mo_cvrp.py, KP/source/MODEL__Actor/grouped_actors.py, KP/source/TRAIN_N_EVAL/Train_Grouped_Actors.py, KP/source/mo_knapsack_problem.py, TSP-3O/source/mo_travelling_saleman_problem.py, TSP/source/MODEL__Actor/grouped_actors.py, TSP/source/TRAIN_N_EVAL/Train_Grouped_Actors.py, TSP/source/mo_travelling_saleman_problem.py, TSPT2-3O/source/mo_travelling_saleman_problem.py, TSPT2/source/mo_travelling_saleman_problem.py",
  "function_name": "TRAIN, GROUP_ENVIRONMENT._get_travel_distance, ACTOR, TRAIN, GROUP_STATE, GROUP_ENVIRONMENT, STATE, ENVIRONMENT, GROUP_ENVIRONMENT._get_group_travel_distance, ACTOR, TRAIN, GROUP_ENVIRONMENT._get_group_travel_distance, GROUP_ENVIRONMENT._get_group_travel_distance, ENVIRONMENT, GROUP_ENVIRONMENT, STATE, GROUP_STATE",
  "code_snippet": "\n\n# ==========================================\n# File: CVRP/source/TRAIN_N_EVAL/Train_Grouped_Actors.py\n# Function/Context: TRAIN\n# ==========================================\nimport numpy as np\nimport time\nfrom IPython.core.debugger import set_trace\nfrom HYPER_PARAMS import *\nfrom TORCH_OBJECTS import *\nfrom source.utilities import Average_Meter\nfrom source.mo_cvrp import CVRP_DATA_LOADER__RANDOM, GROUP_ENVIRONMENT\n\ndef TRAIN(grouped_actor, epoch, timer_start, logger):\n    grouped_actor.train()\n    dist_AM = Average_Meter()\n    actor_loss_AM = Average_Meter()\n    train_loader = CVRP_DATA_LOADER__RANDOM(num_sample=TRAIN_DATASET_SIZE,\n                                            num_nodes=PROBLEM_SIZE,\n                                            batch_size=BATCH_SIZE)\n    logger_start = time.time()\n    episode = 0\n    for depot_xy, node_xy, node_demand in train_loader:\n        batch_s = depot_xy.size(0)\n        episode = episode + batch_s\n        env = GROUP_ENVIRONMENT(depot_xy, node_xy, node_demand)\n        group_s = PROBLEM_SIZE\n        group_state, reward, done = env.reset(group_size=group_s)\n        grouped_actor.reset(group_state)\n        first_action = LongTensor(np.zeros((batch_s, group_s)))\n        group_state, reward, done = env.step(first_action)\n        second_action = LongTensor(np.arange(group_s)+1)[None, :].expand(batch_s, group_s)\n        group_state, reward, done = env.step(second_action)\n        group_prob_list = Tensor(np.zeros((batch_s, group_s, 0)))\n        while not done:\n            action_probs = grouped_actor.get_action_probabilities(group_state)\n            action = action_probs.reshape(batch_s*group_s, -1).multinomial(1)\\\n                .squeeze(dim=1).reshape(batch_s, group_s)\n            action[group_state.finished] = 0\n            group_state, reward, done = env.step(action)\n            batch_idx_mat = torch.arange(batch_s)[:, None].expand(batch_s, group_s)\n            group_idx_mat = torch.arange(group_s)[None, :].expand(batch_s, group_s)\n            chosen_action_prob = action_probs[batch_idx_mat, group_idx_mat, action].reshape(batch_s, group_s)\n            chosen_action_prob[group_state.finished] = 1\n            group_prob_list = torch.cat((group_prob_list, chosen_action_prob[:, :, None]), dim=2)\n        group_reward = reward\n        group_log_prob = group_prob_list.log().sum(dim=2)\n        group_advantage = group_reward - group_reward.mean(dim=1, keepdim=True)\n        group_loss = -group_advantage * group_log_prob\n        loss = group_loss.mean()\n        grouped_actor.optimizer.zero_grad()\n        loss.backward()\n        grouped_actor.optimizer.step()\n        max_reward, _ = group_reward.max(dim=1)\n        dist_AM.push(-max_reward)\n        actor_loss_AM.push(group_loss.detach())\n        if (time.time()-logger_start > LOG_PERIOD_SEC) or (episode == TRAIN_DATASET_SIZE):\n            timestr = time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-timer_start))\n            log_str = 'Ep:{:03d}-{:07d}({:5.1f}%)  T:{:s}  ALoss:{:+5f}  Avg.dist:{:5f}' \\\n                .format(epoch, episode, episode/TRAIN_DATASET_SIZE*100,\n                        timestr, actor_loss_AM.result(), dist_AM.result())\n            logger.info(log_str)\n            logger_start = time.time()\n    grouped_actor.lr_stepper.step()\n\n# ==========================================\n# File: CVRP/source/mo_cvrp.py\n# Function/Context: GROUP_ENVIRONMENT._get_travel_distance\n# ==========================================\n    def _get_travel_distance(self):\n        all_node_xy = self.data[:, None, :, 0:2].expand(self.batch_s, self.group_s, -1, 2)\n        # shape = (batch, group, problem+1, 2)\n        gathering_index = self.group_state.selected_node_list[:, :, :, None].expand(-1, -1, -1, 2)\n        # shape = (batch, group, selected_count, 2)\n\n        # obj1: travel_distances\n        ordered_seq = all_node_xy.gather(dim=2, index=gathering_index)\n        # shape = (batch, group, selected_count, 2)\n\n        rolled_seq = ordered_seq.roll(dims=2, shifts=-1)\n        segment_lengths = ((ordered_seq-rolled_seq)**2).sum(3).sqrt()\n        # size = (batch, group, selected_count)\n\n        travel_distances = segment_lengths.sum(2)\n        # size = (batch, group)\n\n        # obj2: makespans\n        # not_idx = (gathering_index[:, :, :, 0] > 0)\n        not_idx = (gathering_index[:, :, :, 0] > 0).roll(dims=2, shifts=-1)\n        cum_lengths = torch.cumsum(segment_lengths, dim=2)\n\n        cum_lengths[not_idx] = 0\n        sorted_cum_lengths, _ = cum_lengths.sort(dim=2)\n\n        rolled_sorted_cum_lengths = sorted_cum_lengths.roll(dims=2, shifts=1)\n        diff_mat = sorted_cum_lengths - rolled_sorted_cum_lengths\n        diff_mat[diff_mat < 0] = 0\n\n        makespans, _ = torch.max(diff_mat, dim=2)\n\n        objs = torch.stack([travel_distances, makespans], dim=2)\n        return objs\n\n# ==========================================\n# File: KP/source/MODEL__Actor/grouped_actors.py\n# Function/Context: ACTOR\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\n\n# For debugging\nfrom IPython.core.debugger import set_trace\n\n# Hyper Parameters\nfrom HYPER_PARAMS import *\nfrom TORCH_OBJECTS import *\n\n########################################\n# ACTOR\n########################################\n\nclass ACTOR(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n\n        self.encoder = Encoder()\n        self.node_prob_calculator = Next_Node_Probability_Calculator_for_group()\n\n        self.batch_s = None\n        self.encoded_nodes_and_dummy = None\n        self.encoded_nodes = None\n        self.encoded_graph = None\n\n    def reset(self, group_state):\n        self.batch_s = group_state.item_data.size(0)\n        self.encoded_nodes_and_dummy = Tensor(np.zeros((self.batch_s, PROBLEM_SIZE+1, EMBEDDING_DIM)))\n        self.encoded_nodes_and_dummy[:, :PROBLEM_SIZE, :] = self.encoder(group_state.item_data)\n        self.encoded_nodes = self.encoded_nodes_and_dummy[:, :PROBLEM_SIZE, :]\n        # shape = (batch, problem, EMBEDDING_DIM)\n        self.encoded_graph = self.encoded_nodes.mean(dim=1, keepdim=True)\n        # shape = (batch, 1, EMBEDDING_DIM)\n\n        self.node_prob_calculator.reset(self.encoded_nodes)\n\n    def get_action_probabilities(self, group_state):\n\n        probs = self.node_prob_calculator(graph=self.encoded_graph, capacity=group_state.capacity,\n                                          ninf_mask=group_state.fit_ninf_mask)\n        # shape = (batch, group, problem)\n\n        return probs\n\n\n########################################\n# ACTOR_SUB_NN : ENCODER\n########################################\n\nclass Encoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.embedding = nn.Linear(3, EMBEDDING_DIM)\n        self.layers = nn.ModuleList([Encoder_Layer() for _ in range(ENCODER_LAYER_NUM)])\n\n    def forward(self, item_data):\n        # item_data.shape = (batch, problem, 2)\n\n        embedded_input = self.embedding(item_data)\n        # shape = (batch, problem, EMBEDDING_DIM)\n\n        out = embedded_input\n        for layer in self.layers:\n            out = layer(out)\n\n        return out\n\n\nclass Encoder_Layer(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.Wq = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wk = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wv = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.multi_head_combine = nn.Linear(HEAD_NUM * KEY_DIM, EMBEDDING_DIM)\n\n        self.addAndNormalization1 = Add_And_Normalization_Module()\n        self.feedForward = Feed_Forward_Module()\n        self.addAndNormalization2 = Add_And_Normalization_Module()\n\n    def forward(self, input1):\n        # input.shape = (batch, problem, EMBEDDING_DIM)\n\n        q = reshape_by_heads(self.Wq(input1), head_num=HEAD_NUM)\n        k = reshape_by_heads(self.Wk(input1), head_num=HEAD_NUM)\n        v = reshape_by_heads(self.Wv(input1), head_num=HEAD_NUM)\n        # q shape = (batch, HEAD_NUM, problem, KEY_DIM)\n\n        out_concat = multi_head_attention(q, k, v)\n        # shape = (batch, problem, HEAD_NUM*KEY_DIM)\n\n        multi_head_out = self.multi_head_combine(out_concat)\n        # shape = (batch, problem, EMBEDDING_DIM)\n\n        out1 = self.addAndNormalization1(input1, multi_head_out)\n        out2 = self.feedForward(out1)\n        out3 = self.addAndNormalization2(out1, out2)\n\n        return out3\n\n\n########################################\n# ACTOR_SUB_NN : Next_Node_Probability_Calculator\n########################################\n\nclass Next_Node_Probability_Calculator_for_group(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.Wq = nn.Linear(1+EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wk = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wv = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n\n        self.multi_head_combine = nn.Linear(HEAD_NUM * KEY_DIM, EMBEDDING_DIM)\n\n        self.Wk_logit = nn.Linear(EMBEDDING_DIM, EMBEDDING_DIM, bias=False)\n\n        self.k = None  # saved key, for multi-head attention\n        self.v = None  # saved value, for multi-head_attention\n        self.single_head_key = None  # saved, for single-head attention\n\n    def reset(self, encoded_nodes):\n        # encoded_nodes.shape = (batch, problem, EMBEDDING_DIM)\n\n        self.k = reshape_by_heads(self.Wk(encoded_nodes), head_num=HEAD_NUM)\n        self.v = reshape_by_heads(self.Wv(encoded_nodes), head_num=HEAD_NUM)\n        # shape = (batch, HEAD_NUM, problem, KEY_DIM)\n\n        # self.single_head_key = encoded_nodes.transpose(1, 2)\n        self.single_head_key = self.Wk_logit(encoded_nodes).transpose(1, 2)\n        # shape = (batch, EMBEDDING_DIM, problem)\n\n    def forward(self, graph, capacity, ninf_mask=None):\n        # graph.shape = (batch, 1, EMBEDDING_DIM)\n        # capacity.shape = (batch, group)\n        #  ninf_mask.shape = (batch, group, problem)\n\n        batch_s = capacity.size(0)\n        group_s = capacity.size(1)\n\n        #  Multi-Head Attention\n        #######################################################\n        input1 = graph.expand(batch_s, group_s, EMBEDDING_DIM)\n        input2 = capacity[:, :, None]\n        input_cat = torch.cat((input1, input2), dim=2)\n        # shape = (batch, group, 1+EMBEDDING_DIM)\n\n        q = reshape_by_heads(self.Wq(input_cat), head_num=HEAD_NUM)\n        # shape = (batch, HEAD_NUM, group, KEY_DIM)\n\n        out_concat = multi_head_attention(q, self.k, self.v, ninf_mask=ninf_mask)\n        # shape = (batch, group, HEAD_NUM*KEY_DIM)\n\n        mh_atten_out = self.multi_head_combine(out_concat)\n        # shape = (batch, group, EMBEDDING_DIM)\n\n        #  Single-Head Attention, for probability calculation\n        #######################################################      \n        score = torch.matmul(mh_atten_out, self.single_head_key)\n        # shape = (batch, group, problem)\n\n        score_scaled = score / np.sqrt(EMBEDDING_DIM)\n        # shape = (batch, group, problem)\n\n        score_clipped = LOGIT_CLIPPING * torch.tanh(score_scaled)\n\n        if ninf_mask is None:\n            score_masked = score_clipped\n        else:\n            score_masked = score_clipped + ninf_mask\n\n        probs = F.softmax(score_masked, dim=2)\n        # shape = (batch, group, problem)\n\n        return probs\n\n\n########################################\n# NN SUB CLASS / FUNCTIONS\n########################################\n\ndef pick_nodes_for_each_group(encoded_nodes, node_index_to_pick):\n    # encoded_nodes.shape = (batch, problem, EMBEDDING_DIM)\n    # node_index_to_pick.shape = (batch, group_s)\n    batch_s = node_index_to_pick.size(0)\n    group_s = node_index_to_pick.size(1)\n\n    gathering_index = node_index_to_pick[:, :, None].expand(batch_s, group_s, EMBEDDING_DIM)\n    # shape = (batch, group, EMBEDDING_DIM)\n\n    picked_nodes = encoded_nodes.gather(dim=1, index=gathering_index)\n    # shape = (batch, group, EMBEDDING_DIM)\n\n    return picked_nodes\n\n\ndef reshape_by_heads(qkv, head_num):\n    # q.shape = (batch, C, head_num*key_dim)\n\n    batch_s = qkv.size(0)\n    C = qkv.size(1)\n\n    q_reshaped = qkv.reshape(batch_s, C, head_num, -1)\n    # shape = (batch, C, head_num, key_dim)\n\n    q_transposed = q_reshaped.transpose(1, 2)\n    # shape = (batch, head_num, C, key_dim)\n\n    return q_transposed\n\n\ndef multi_head_attention(q, k, v, ninf_mask=None):\n    # q shape = (batch, head_num, n, key_dim)   : n can be either 1 or group\n    # k,v shape = (batch, head_num, problem, key_dim)\n    # ninf_mask.shape = (batch, group, problem)\n\n    batch_s = q.size(0)\n    head_num = q.size(1)\n    n = q.size(2)\n    key_dim = q.size(3)\n    problem_s = k.size(2)\n\n    score = torch.matmul(q, k.transpose(2, 3))\n    # shape = (batch, head_num, n, TSP_SIZE)\n\n    score_scaled = score / np.sqrt(key_dim)\n    if ninf_mask is not None:\n        score_scaled = score_scaled + ninf_mask[:, None, :, :].expand(batch_s, head_num, n, problem_s)\n\n    weights = nn.Softmax(dim=3)(score_scaled)\n    # shape = (batch, head_num, n, problem)\n\n    out = torch.matmul(weights, v)\n    # shape = (batch, head_num, n, key_dim)\n\n    out_transposed = out.transpose(1, 2)\n    # shape = (batch, n, head_num, key_dim)\n\n    out_concat = out_transposed.reshape(batch_s, n, head_num * key_dim)\n    # shape = (batch, n, head_num*key_dim)\n\n    return out_concat\n\n\nclass Add_And_Normalization_Module(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.norm_by_EMB = nn.BatchNorm1d(EMBEDDING_DIM, affine=True)\n        # 'Funny' Batch_Norm, as it will normalized by EMB dim\n\n    def forward(self, input1, input2):\n        # input.shape = (batch, problem, EMBEDDING_DIM)\n\n        batch_s = input1.size(0)\n        problem_s = input1.size(1)\n\n        added = input1 + input2\n        normalized = self.norm_by_EMB(added.reshape(batch_s * problem_s, EMBEDDING_DIM))\n\n        return normalized.reshape(batch_s, problem_s, EMBEDDING_DIM)\n\n\nclass Feed_Forward_Module(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.W1 = nn.Linear(EMBEDDING_DIM, FF_HIDDEN_DIM)\n        self.W2 = nn.Linear(FF_HIDDEN_DIM, EMBEDDING_DIM)\n\n    def forward(self, input1):\n        # input.shape = (batch, problem, EMBEDDING_DIM)\n\n        return self.W2(F.relu(self.W1(input1)))\n\n# ==========================================\n# File: KP/source/TRAIN_N_EVAL/Train_Grouped_Actors.py\n# Function/Context: TRAIN\n# ==========================================\nimport numpy as np\n\n# For Logging\nimport time\n\n# For debugging\nfrom IPython.core.debugger import set_trace\n\n# Hyper Parameters\nfrom HYPER_PARAMS import *\nfrom TORCH_OBJECTS import *\n\nfrom source.utilities import Average_Meter\nfrom source.mo_knapsack_problem import KNAPSACK_DATA_LOADER__RANDOM, GROUP_ENVIRONMENT\n\n\n########################################\n# TRAIN\n########################################\n\ndef TRAIN(grouped_actor, epoch, timer_start, logger):\n\n    grouped_actor.train()\n\n    score_AM = Average_Meter()\n    actor_loss_AM = Average_Meter()\n    train_loader = KNAPSACK_DATA_LOADER__RANDOM(num_sample=TRAIN_DATASET_SIZE,\n                                                num_items=PROBLEM_SIZE,\n                                                batch_size=BATCH_SIZE)\n\n    logger_start = time.time()\n    episode = 0\n    for item_data in train_loader:\n        # item_data.shape = (batch, problem, 2)\n\n        batch_s = item_data.size(0)\n        episode = episode + batch_s\n\n        # Actor Group Move\n        ###############################################\n        env = GROUP_ENVIRONMENT(item_data)\n        group_s = PROBLEM_SIZE\n        group_state, reward, done = env.reset(group_size=group_s)\n        grouped_actor.reset(group_state)\n\n        # First Move is given\n        first_action = LongTensor(np.arange(group_s))[None, :].expand(batch_s, group_s)\n        group_state, reward, done = env.step(first_action)\n\n        group_prob_list = Tensor(np.zeros((batch_s, group_s, 0)))\n        while not done:\n            action_probs = grouped_actor.get_action_probabilities(group_state)\n            # shape = (batch, group, problem)\n            action = action_probs.reshape(batch_s*group_s, PROBLEM_SIZE).multinomial(1).squeeze(dim=1).reshape(batch_s, group_s)\n            # shape = (batch, group)\n\n            action_w_finisehd = action.clone()\n            action_w_finisehd[group_state.finished] = PROBLEM_SIZE  # dummy item\n            group_state, reward, done = env.step(action_w_finisehd)\n\n            batch_idx_mat = torch.arange(batch_s)[:, None].expand(batch_s, group_s)\n            group_idx_mat = torch.arange(group_s)[None, :].expand(batch_s, group_s)\n            chosen_action_prob = action_probs[batch_idx_mat, group_idx_mat, action].reshape(batch_s, group_s)\n            # shape = (batch, group)\n            chosen_action_prob[group_state.finished] = 1  # done episode will gain no more probability\n            group_prob_list = torch.cat((group_prob_list, chosen_action_prob[:, :, None]), dim=2)\n            # shape = (batch, group, x)\n\n        # LEARNING - Actor\n        ###############################################\n        group_reward = reward\n        # shape = (batch, group)\n        group_log_prob = group_prob_list.log().sum(dim=2)\n        # shape = (batch, group)\n\n        group_advantage = group_reward - group_reward.mean(dim=1, keepdim=True)\n\n        group_loss = -group_advantage * group_log_prob\n        # shape = (batch, group)\n        loss = group_loss.mean()\n\n        grouped_actor.optimizer.zero_grad()\n        loss.backward()\n        grouped_actor.optimizer.step()\n\n        # RECORDING\n        ###############################################\n        max_reward, _ = group_reward.max(dim=1)\n        score_AM.push(max_reward)\n        actor_loss_AM.push(group_loss.detach())\n\n        # LOGGING\n        ###############################################\n        if (time.time()-logger_start > LOG_PERIOD_SEC) or (episode == TRAIN_DATASET_SIZE):\n            timestr = time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-timer_start))\n            log_str = 'Ep:{:03d}-{:07d}({:5.1f}%)  T:{:s}  ALoss:{:+5f}  Avg.dist:{:5f}' \\\n                .format(epoch, episode, episode/TRAIN_DATASET_SIZE*100,\n                        timestr, actor_loss_AM.result(),\n                        score_AM.result())\n            logger.info(log_str)\n            logger_start = time.time()\n\n    # LR STEP, after each epoch\n    grouped_actor.lr_stepper.step()\n\n# ==========================================\n# File: KP/source/mo_knapsack_problem.py\n# Function/Context: GROUP_STATE, GROUP_ENVIRONMENT, STATE, ENVIRONMENT\n# ==========================================\n####################################\n# EXTERNAL LIBRARY\n####################################\nimport torch\nimport numpy as np\n\n# For debugging\nfrom IPython.core.debugger import set_trace\n\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\n\n####################################\n# PROJECT VARIABLES\n####################################\nfrom HYPER_PARAMS import *\nfrom TORCH_OBJECTS import *\n\n\n####################################\n# DATA\n####################################\ndef KNAPSACK_DATA_LOADER__RANDOM(num_sample, num_items, batch_size):\n    dataset = KnapSack_Dataset__Random(num_sample=num_sample, num_items=num_items)\n    data_loader = DataLoader(dataset=dataset,\n                             batch_size=batch_size,\n                             shuffle=False,\n                             num_workers=0,\n                             collate_fn=knapsack_collate_fn)\n    return data_loader\n\n\nclass KnapSack_Dataset__Random(Dataset):\n    def __init__(self, num_sample, num_items):\n        self.num_sample = num_sample\n        self.num_items = num_items\n\n    def __getitem__(self, index):\n        data = np.random.rand(self.num_items, 3)\n        return data\n\n    def __len__(self):\n        return self.num_sample\n\n\ndef knapsack_collate_fn(batch):\n    return Tensor(batch)\n\n\n####################################\n# STATE\n####################################\nclass STATE:\n\n    def __init__(self, item_data, capacity):\n        self.batch_s = item_data.size(0)\n        self.items_and_a_dummy = Tensor(np.zeros((self.batch_s, PROBLEM_SIZE+1, 2)))\n        self.items_and_a_dummy[:, :PROBLEM_SIZE, :] = item_data\n        self.item_data = self.items_and_a_dummy[:, :PROBLEM_SIZE, :]\n        # shape = (batch, problem, 2)\n\n        # History\n        ####################################\n        self.current_node = None\n        self.selected_count = 0\n        self.selected_node_list = LongTensor(np.zeros((self.batch_s, 0)))\n        # shape = (batch, selected_count)\n\n        # Status\n        ####################################\n        self.accumulated_value = Tensor(np.zeros((self.batch_s,)))\n        # shape = (batch,)\n        self.capacity = Tensor(np.ones((self.batch_s,))) * capacity\n        # shape = (batch,)\n        self.ninf_mask_w_dummy = Tensor(np.zeros((self.batch_s, PROBLEM_SIZE+1)))\n        self.ninf_mask = self.ninf_mask_w_dummy[:, :PROBLEM_SIZE]\n        # shape = (batch, problem)\n        self.fit_ninf_mask = None\n        self.finished = BoolTensor(np.zeros((self.batch_s,)))\n        # shape = (batch,)\n\n    def move_to(self, selected_item_idx):\n        # selected_item_idx.shape = (batch,)\n\n        # History\n        ####################################\n        self.current_node = selected_item_idx\n        self.selected_count += 1\n        self.selected_node_list = torch.cat((self.selected_node_list, selected_item_idx[:, None]), dim=1)\n\n        # Status\n        ####################################\n        gathering_index = selected_item_idx[:, None, None].expand(self.batch_s, 1, 2)\n        selected_item = self.items_and_a_dummy.gather(dim=1, index=gathering_index).squeeze(dim=1)\n        # shape = (batch, 2)\n\n        self.accumulated_value += selected_item[:, 1]\n        self.capacity -= selected_item[:, 0]\n\n        self.ninf_mask_w_dummy[torch.arange(self.batch_s), selected_item_idx] = -np.inf\n        unfit_bool = (self.capacity[:, None] - self.item_data[:, :, 0]) < 0\n        # shape = (batch, problem)\n        self.fit_ninf_mask = self.ninf_mask.clone()\n        self.fit_ninf_mask[unfit_bool] = -np.inf\n\n        self.finished = (self.fit_ninf_mask == -np.inf).all(dim=1)\n        # shape = (batch,)\n        self.fit_ninf_mask[self.finished[:, None].expand(self.batch_s, PROBLEM_SIZE)] = 0  # do not mask finished epi.\n\n\nclass GROUP_STATE:\n\n    def __init__(self, group_size, item_data, capacity):\n        self.batch_s = item_data.size(0)\n        self.group_s = group_size\n        self.items_and_a_dummy = Tensor(np.zeros((self.batch_s, PROBLEM_SIZE+1, 3)))\n        self.items_and_a_dummy[:, :PROBLEM_SIZE, :] = item_data\n        self.item_data = self.items_and_a_dummy[:, :PROBLEM_SIZE, :]\n        # shape = (batch, problem, 2)\n\n        # History\n        ####################################\n        self.current_node = None\n        # shape = (batch_s, group)\n        self.selected_count = 0\n        self.selected_node_list = LongTensor(np.zeros((self.batch_s, self.group_s, 0)))\n        # shape = (batch_s, group, selected_count)\n\n        # Status\n        ####################################\n        self.accumulated_value_obj1 = Tensor(np.zeros((self.batch_s, self.group_s)))\n        self.accumulated_value_obj2 = Tensor(np.zeros((self.batch_s, self.group_s)))\n        # shape = (batch, group)\n        self.capacity = Tensor(np.ones((self.batch_s, self.group_s))) * capacity\n        # shape = (batch, group)\n        self.ninf_mask_w_dummy = Tensor(np.zeros((self.batch_s, self.group_s, PROBLEM_SIZE+1)))\n        self.ninf_mask = self.ninf_mask_w_dummy[:, :, :PROBLEM_SIZE]\n        # shape = (batch, group, problem)\n        self.fit_ninf_mask = None\n        self.finished = BoolTensor(np.zeros((self.batch_s, self.group_s)))\n        # shape = (batch, group)\n\n\n    def move_to(self, selected_idx_mat):\n        # selected_idx_mat.shape = (batch, group)\n\n        # History\n        ####################################\n        self.current_node = selected_idx_mat\n        self.selected_count += 1\n        self.selected_node_list = torch.cat((self.selected_node_list, selected_idx_mat[:, :, None]), dim=2)\n\n        # Status\n        ####################################\n        items_mat = self.items_and_a_dummy[:, None, :, :].expand(self.batch_s, self.group_s, PROBLEM_SIZE+1, 3)\n        gathering_index = selected_idx_mat[:, :, None, None].expand(self.batch_s, self.group_s, 1, 3)\n        selected_item = items_mat.gather(dim=2, index=gathering_index).squeeze(dim=2)\n        # shape = (batch, group, 2)\n\n        self.accumulated_value_obj1 += selected_item[:, :, 1]\n        self.accumulated_value_obj2 += selected_item[:, :, 2]\n        self.capacity -= selected_item[:, :, 0]\n\n        batch_idx_mat = torch.arange(self.batch_s)[:, None].expand(self.batch_s, self.group_s)\n        group_idx_mat = torch.arange(self.group_s)[None, :].expand(self.batch_s, self.group_s)\n        self.ninf_mask_w_dummy[batch_idx_mat, group_idx_mat, selected_idx_mat] = -np.inf\n\n        unfit_bool = (self.capacity[:, :, None] - self.item_data[:, None, :, 0]) < 0\n        # shape = (batch, group, problem)\n        self.fit_ninf_mask = self.ninf_mask.clone()\n        self.fit_ninf_mask[unfit_bool] = -np.inf\n\n        self.finished = (self.fit_ninf_mask == -np.inf).all(dim=2)\n        # shape = (batch, group)\n        self.fit_ninf_mask[self.finished[:, :, None].expand(self.batch_s, self.group_s, PROBLEM_SIZE)] = 0\n        # do not mask finished episode\n\n\n\n\n\n####################################\n# ENVIRONMENT\n####################################\nclass ENVIRONMENT:\n\n    def __init__(self, item_data):\n        # item_data.shape = (batch, problem, 2)\n\n        self.item_data = item_data\n        self.batch_s = item_data.size(0)\n        self.state = None\n\n    def reset(self):\n        if PROBLEM_SIZE == 50:\n            capacity = 12.5\n        elif PROBLEM_SIZE == 100:\n            capacity = 25\n        elif PROBLEM_SIZE == 200:\n            capacity = 25\n        else:\n            raise NotImplementedError\n\n        self.state = STATE(item_data=self.item_data, capacity=capacity)\n\n        reward = None\n        done = False\n        return self.state, reward, done\n\n    def step(self, selected_item_idx):\n        # selected_node_idx.shape = (batch,)\n\n        # move state\n        self.state.move_to(selected_item_idx)\n\n        # returning values\n        done = self.state.finished.all()\n        if done:\n            reward = self.state.accumulated_value\n        else:\n            reward = None\n\n        return self.state, reward, done\n\n\nclass GROUP_ENVIRONMENT:\n\n    def __init__(self, item_data):\n        # seq.shape = (batch, problem, 2)\n\n        self.item_data = item_data\n        self.batch_s = item_data.size(0)\n        self.group_state = None\n\n    def reset(self, group_size):\n        if PROBLEM_SIZE == 50:\n            capacity = 12.5\n        elif PROBLEM_SIZE == 100:\n            capacity = 25\n        elif PROBLEM_SIZE == 200:\n            capacity = 25\n        else:\n            raise NotImplementedError\n\n        self.group_state = GROUP_STATE(group_size=group_size,\n                                       item_data=self.item_data, capacity=capacity)\n\n        reward = None\n        done = False\n        return self.group_state, reward, done\n\n    def step(self, selected_idx_mat):\n        # selected_idx_mat.shape = (batch, group)\n\n        # move state\n        self.group_state.move_to(selected_idx_mat)\n\n        # returning values\n        reward = None\n        done = self.group_state.finished.all()  # state.finished.shape = (batch, group)\n        if done:\n            # reward = self.group_state.accumulated_value\n            reward = torch.stack([self.group_state.accumulated_value_obj1,self.group_state.accumulated_value_obj2],axis = 2)\n        else:\n            reward = None\n\n        return self.group_state, reward, done\n\n# ==========================================\n# File: TSP-3O/source/mo_travelling_saleman_problem.py\n# Function/Context: GROUP_ENVIRONMENT._get_group_travel_distance\n# ==========================================\nimport torch\nimport numpy as np\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom HYPER_PARAMS import *\nfrom TORCH_OBJECTS import *\n\nclass GROUP_ENVIRONMENT:\n\n    def __init__(self, data):\n       \n\n        self.data = data\n        self.batch_s = data.size(0)\n        self.group_s = None\n        self.group_state = None\n\n    def reset(self, group_size):\n        self.group_s = group_size\n        self.group_state = GROUP_STATE(group_size=group_size, data=self.data)\n        reward = None\n        done = False\n        return self.group_state, reward, done\n\n    def step(self, selected_idx_mat):\n        # selected_idx_mat.shape = (batch, group)\n\n        # move state\n        self.group_state.move_to(selected_idx_mat)\n\n        # returning values\n        done = (self.group_state.selected_count == TSP_SIZE)\n        if done:\n            reward = -self._get_group_travel_distance()  # note the minus sign!\n        else:\n            reward = None\n        return self.group_state, reward, done\n\n    def _get_group_travel_distance(self):\n\n        gathering_index = self.group_state.selected_node_list.unsqueeze(3).expand(self.batch_s, -1, TSP_SIZE, 6)\n\n\n        seq_expanded = self.data[:, None, :, :].expand(self.batch_s, self.group_s, TSP_SIZE, 6)\n\n        ordered_seq = seq_expanded.gather(dim=2, index=gathering_index)\n\n\n        rolled_seq = ordered_seq.roll(dims=2, shifts=-1)\n\n        segment_lengths_obj1 = ((ordered_seq[:, :, :, :2]-rolled_seq[:, :, :, :2])**2).sum(3).sqrt()\n        segment_lengths_obj2 = ((ordered_seq[:, :, :, 2:4]-rolled_seq[:, :, :, 2:4])**2).sum(3).sqrt()\n        segment_lengths_obj3 = ((ordered_seq[:, :, :, 4:] - rolled_seq[:, :, :, 4:]) ** 2).sum(3).sqrt()\n\n        group_travel_distances_obj1 = segment_lengths_obj1.sum(2)\n        group_travel_distances_obj2 = segment_lengths_obj2.sum(2)\n        group_travel_distances_obj3 = segment_lengths_obj3.sum(2)\n\n        group_travel_distances_combined = torch.stack([group_travel_distances_obj1,group_travel_distances_obj2,group_travel_distances_obj3],axis = 2)\n\n\n        return group_travel_distances_combined\n\n# ==========================================\n# File: TSP/source/MODEL__Actor/grouped_actors.py\n# Function/Context: ACTOR\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\n\n# For debugging\nfrom IPython.core.debugger import set_trace\n\n# Hyper Parameters\nfrom HYPER_PARAMS import *\nfrom TORCH_OBJECTS import *\n\n########################################\n# ACTOR\n########################################\n\nclass ACTOR(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.box_select_probabilities = None\n        # shape = (batch, group, TSP_SIZE)\n\n        self.encoder = Encoder()\n        self.node_prob_calculator = Next_Node_Probability_Calculator_for_group()\n\n        self.batch_s = None\n        self.encoded_nodes = None\n\n    def reset(self, group_state):\n        self.batch_s = group_state.data.size(0)\n        self.encoded_nodes = self.encoder(group_state.data)\n        # shape = (batch_s, TSP_SIZE, EMBEDDING_DIM)\n\n        self.node_prob_calculator.reset(self.encoded_nodes, group_ninf_mask=group_state.ninf_mask)\n\n    def soft_reset(self, group_state):\n        self.node_prob_calculator.reset(self.encoded_nodes, group_ninf_mask=group_state.ninf_mask)\n\n    def update(self, group_state):\n        encoded_LAST_NODES = pick_nodes_for_each_group(self.encoded_nodes, group_state.current_node)\n        # shape = (batch_s, group, EMBEDDING_DIM)\n\n        probs = self.node_prob_calculator(encoded_LAST_NODES)\n        # shape = (batch_s, group, TSP_SIZE)\n        self.box_select_probabilities = probs\n\n    def get_action_probabilities(self):\n        return self.box_select_probabilities\n\n\n########################################\n# ACTOR_SUB_NN : ENCODER\n########################################\n\nclass Encoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.embedding = nn.Linear(4, EMBEDDING_DIM)\n        self.layers = nn.ModuleList([Encoder_Layer() for _ in range(ENCODER_LAYER_NUM)])\n\n    def forward(self, data):\n        # data.shape = (batch_s, TSP_SIZE, 2)\n\n        embedded_input = self.embedding(data)\n        # shape = (batch_s, TSP_SIZE, EMBEDDING_DIM)\n\n        out = embedded_input\n        for layer in self.layers:\n            out = layer(out)\n\n        return out\n\n\nclass Encoder_Layer(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.Wq = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wk = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wv = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.multi_head_combine = nn.Linear(HEAD_NUM * KEY_DIM, EMBEDDING_DIM)\n\n        self.addAndNormalization1 = Add_And_Normalization_Module()\n        self.feedForward = Feed_Forward_Module()\n        self.addAndNormalization2 = Add_And_Normalization_Module()\n\n    def forward(self, input1):\n        # input.shape = (batch_s, TSP_SIZE, EMBEDDING_DIM)\n\n        q = reshape_by_heads(self.Wq(input1), head_num=HEAD_NUM)\n        k = reshape_by_heads(self.Wk(input1), head_num=HEAD_NUM)\n        v = reshape_by_heads(self.Wv(input1), head_num=HEAD_NUM)\n        # q shape = (batch_s, HEAD_NUM, TSP_SIZE, KEY_DIM)\n\n        out_concat = multi_head_attention(q, k, v)\n        # shape = (batch_s, TSP_SIZE, HEAD_NUM*KEY_DIM)\n\n        multi_head_out = self.multi_head_combine(out_concat)\n        # shape = (batch_s, TSP_SIZE, EMBEDDING_DIM)\n\n        out1 = self.addAndNormalization1(input1, multi_head_out)\n        out2 = self.feedForward(out1)\n        out3 = self.addAndNormalization2(out1, out2)\n\n        return out3\n\n\n########################################\n# ACTOR_SUB_NN : Next_Node_Probability_Calculator\n########################################\n\nclass Next_Node_Probability_Calculator_for_group(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.Wq_graph = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wq_first = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wq_last = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wk = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wv = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n\n        self.multi_head_combine = nn.Linear(HEAD_NUM * KEY_DIM, EMBEDDING_DIM)\n\n        self.Wk_logit = nn.Linear(EMBEDDING_DIM, EMBEDDING_DIM, bias=False)\n\n        self.q_graph = None  # saved q1, for multi-head attention\n        self.q_first = None  # saved q2, for multi-head attention\n        self.k = None  # saved key, for multi-head attention\n        self.v = None  # saved value, for multi-head_attention\n        self.single_head_key = None  # saved, for single-head attention\n        self.group_ninf_mask = None  # reference to ninf_mask owned by state\n\n    def reset(self, encoded_nodes, group_ninf_mask):\n        # encoded_nodes.shape = (batch_s, TSP_SIZE, EMBEDDING_DIM)\n\n        encoded_graph = encoded_nodes.mean(dim=1, keepdim=True)\n        # shape = (batch_s, 1, EMBEDDING_DIM)\n        self.q_graph = reshape_by_heads(self.Wq_graph(encoded_graph), head_num=HEAD_NUM)\n        # shape = (batch_s, HEAD_NUM, 1, KEY_DIM)\n        self.q_first = None\n        # shape = (batch_s, HEAD_NUM, group, KEY_DIM)\n        self.k = reshape_by_heads(self.Wk(encoded_nodes), head_num=HEAD_NUM)\n        self.v = reshape_by_heads(self.Wv(encoded_nodes), head_num=HEAD_NUM)\n        # shape = (batch_s, HEAD_NUM, TSP_SIZE, KEY_DIM)\n        # self.single_head_key = encoded_nodes.transpose(1, 2)\n        self.single_head_key = self.Wk_logit(encoded_nodes).transpose(1, 2)\n        # shape = (batch_s, EMBEDDING_DIM, TSP_SIZE)\n        self.group_ninf_mask = group_ninf_mask\n        # shape = (batch_s, group, TSP_SIZE)\n\n    def forward(self, encoded_LAST_NODE):\n        # encoded_LAST_NODE.shape = (batch_s, group, EMBEDDING_DIM)\n\n        if self.q_first is None:\n            self.q_first = reshape_by_heads(self.Wq_first(encoded_LAST_NODE), head_num=HEAD_NUM)\n        # shape = (batch_s, HEAD_NUM, group, KEY_DIM)\n\n        #  Multi-Head Attention\n        #######################################################\n        q_last = reshape_by_heads(self.Wq_last(encoded_LAST_NODE), head_num=HEAD_NUM)\n        # shape = (batch_s, HEAD_NUM, group, KEY_DIM)\n\n        q = self.q_graph + self.q_first + q_last\n        # shape = (batch_s, HEAD_NUM, group, KEY_DIM)\n\n        out_concat = multi_head_attention(q, self.k, self.v, group_ninf_mask=self.group_ninf_mask)\n        # shape = (batch_s, group, HEAD_NUM*KEY_DIM)\n\n        mh_atten_out = self.multi_head_combine(out_concat)\n        # shape = (batch_s, group, EMBEDDING_DIM)\n\n        #  Single-Head Attention, for probability calculation\n        #######################################################      \n        score = torch.matmul(mh_atten_out, self.single_head_key)\n        # shape = (batch_s, group, TSP_SIZE)\n\n        score_scaled = score / np.sqrt(EMBEDDING_DIM)\n        # shape = (batch_s, group, TSP_SIZE)\n\n        score_clipped = LOGIT_CLIPPING * torch.tanh(score_scaled)\n\n        score_masked = score_clipped + self.group_ninf_mask.clone()\n\n        probs = F.softmax(score_masked, dim=2)\n        # shape = (batch_s, group, TSP_SIZE)\n\n        return probs\n\n\n########################################\n# NN SUB CLASS / FUNCTIONS\n########################################\n\ndef pick_nodes_for_each_group(encoded_nodes, node_index_to_pick):\n    # encoded_nodes.shape = (batch_s, TSP_SIZE, EMBEDDING_DIM)\n    # node_index_to_pick.shape = (batch_s, group_s)\n    batch_s = node_index_to_pick.size(0)\n    group_s = node_index_to_pick.size(1)\n\n    gathering_index = node_index_to_pick[:, :, None].expand(batch_s, group_s, EMBEDDING_DIM)\n    # shape = (batch_s, group, EMBEDDING_DIM)\n\n    picked_nodes = encoded_nodes.gather(dim=1, index=gathering_index)\n    # shape = (batch_s, group, EMBEDDING_DIM)\n\n    return picked_nodes\n\n\ndef reshape_by_heads(qkv, head_num):\n    # q.shape = (batch, C, head_num*key_dim)\n\n    batch_s = qkv.size(0)\n    C = qkv.size(1)\n\n    q_reshaped = qkv.reshape(batch_s, C, head_num, -1)\n    # shape = (batch, C, head_num, key_dim)\n\n    q_transposed = q_reshaped.transpose(1, 2)\n    # shape = (batch, head_num, C, key_dim)\n\n    return q_transposed\n\n\ndef multi_head_attention(q, k, v, ninf_mask=None, group_ninf_mask=None):\n    # q shape = (batch_s, head_num, n, key_dim)   : n can be either 1 or TSP_SIZE\n    # k,v shape = (batch_s, head_num, TSP_SIZE, key_dim)\n    # ninf_mask.shape = (batch_s, TSP_SIZE)\n    # group_ninf_mask.shape = (batch_s, group, TSP_SIZE)\n\n    batch_s = q.size(0)\n    head_num = q.size(1)\n    n = q.size(2)\n    key_dim = q.size(3)\n\n    score = torch.matmul(q, k.transpose(2, 3))\n    # shape = (batch_s, head_num, n, TSP_SIZE)\n\n    score_scaled = score / np.sqrt(key_dim)\n    if ninf_mask is not None:\n        score_scaled = score_scaled + ninf_mask[:, None, None, :].expand(batch_s, head_num, n, TSP_SIZE)\n    if group_ninf_mask is not None:\n        score_scaled = score_scaled + group_ninf_mask[:, None, :, :].expand(batch_s, head_num, n, TSP_SIZE)\n\n    weights = nn.Softmax(dim=3)(score_scaled)\n    # shape = (batch_s, head_num, n, TSP_SIZE)\n\n    out = torch.matmul(weights, v)\n    # shape = (batch_s, head_num, n, key_dim)\n\n    out_transposed = out.transpose(1, 2)\n    # shape = (batch_s, n, head_num, key_dim)\n\n    out_concat = out_transposed.reshape(batch_s, n, head_num * key_dim)\n    # shape = (batch_s, n, head_num*key_dim)\n\n    return out_concat\n\n\nclass Add_And_Normalization_Module(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.norm_by_EMB = nn.BatchNorm1d(EMBEDDING_DIM, affine=True)\n        # 'Funny' Batch_Norm, as it will normalized by EMB dim\n\n    def forward(self, input1, input2):\n        # input.shape = (batch_s, TSP_SIZE, EMBEDDING_DIM)\n\n        batch_s = input1.size(0)\n        added = input1 + input2\n        normalized = self.norm_by_EMB(added.reshape(batch_s * TSP_SIZE, EMBEDDING_DIM))\n\n        return normalized.reshape(batch_s, TSP_SIZE, EMBEDDING_DIM)\n\n\nclass Feed_Forward_Module(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.W1 = nn.Linear(EMBEDDING_DIM, FF_HIDDEN_DIM)\n        self.W2 = nn.Linear(FF_HIDDEN_DIM, EMBEDDING_DIM)\n\n    def forward(self, input1):\n        # input.shape = (batch_s, TSP_SIZE, EMBEDDING_DIM)\n\n        return self.W2(F.relu(self.W1(input1)))\n\n# ==========================================\n# File: TSP/source/TRAIN_N_EVAL/Train_Grouped_Actors.py\n# Function/Context: TRAIN\n# ==========================================\nimport numpy as np\nimport time\n\n# For debugging\nfrom IPython.core.debugger import set_trace\n\n# Hyper Parameters\nfrom HYPER_PARAMS import *\nfrom TORCH_OBJECTS import *\n\nfrom source.utilities import Average_Meter\nfrom source.mo_travelling_saleman_problem import TSP_DATA_LOADER__RANDOM, GROUP_ENVIRONMENT\n\nfrom tensorboard_logger import Logger as TbLogger\nfrom tqdm import tqdm\n\n\n########################################\n# TRAIN\n########################################\n\n# def TRAIN(actor_group, epoch, timer_start, logger):\ndef TRAIN(actor_group, epoch):\n\n    actor_group.train()\n\n    distance_AM = Average_Meter()\n    actor_loss_AM = Average_Meter()\n\n    train_loader = TSP_DATA_LOADER__RANDOM(num_sample=TRAIN_DATASET_SIZE, num_nodes=TSP_SIZE, batch_size=TRAIN_BATCH_SIZE)\n\n    tb_logger = TbLogger('logs/TSP_n{}_{}'.format(TSP_SIZE, time.strftime(\"%Y%m%dT%H%M%S\")))\n\n    # logger_start = time.time()\n    step = (epoch - 1) * (TRAIN_DATASET_SIZE // TRAIN_BATCH_SIZE + 1)\n    episode = 0\n    for data in tqdm(train_loader):\n        # data.shape = (batch_s, TSP_SIZE, 2)\n\n        batch_s = data.size(0)\n        episode = episode + batch_s\n\n        # Actor Group Move\n        ###############################################\n        env = GROUP_ENVIRONMENT(data)\n        group_s = TSP_SIZE\n        group_state, reward, done = env.reset(group_size=group_s)\n        actor_group.reset(group_state)\n\n        # First Move is given\n        first_action = LongTensor(np.arange(group_s))[None, :].expand(batch_s, group_s)\n        group_state, reward, done = env.step(first_action)\n\n        group_prob_list = Tensor(np.zeros((batch_s, group_s, 0)))\n        while not done:\n            actor_group.update(group_state)\n            action_probs = actor_group.get_action_probabilities()\n            # shape = (batch, group, TSP_SIZE)\n            action = action_probs.reshape(batch_s*group_s, -1).multinomial(1).squeeze(dim=1).reshape(batch_s, group_s)\n            # shape = (batch, group)\n            group_state, reward, done = env.step(action)\n\n            batch_idx_mat = torch.arange(batch_s)[:, None].expand(batch_s, group_s)\n            group_idx_mat = torch.arange(group_s)[None, :].expand(batch_s, group_s)\n            chosen_action_prob = action_probs[batch_idx_mat, group_idx_mat, action].reshape(batch_s, group_s)\n            # shape = (batch, group)\n            group_prob_list = torch.cat((group_prob_list, chosen_action_prob[:, :, None]), dim=2)\n\n        # LEARNING - Actor\n        ###############################################\n        group_reward = reward\n        group_log_prob = group_prob_list.log().sum(dim=2)\n        # shape = (batch, group)\n\n        group_advantage = group_reward - group_reward.mean(dim=1, keepdim=True)\n\n        group_loss = -group_advantage * group_log_prob\n        # shape = (batch, group)\n        loss = group_loss.mean()\n\n        actor_group.optimizer.zero_grad()\n        loss.backward()\n        actor_group.optimizer.step()\n\n        # RECORDING\n        ###############################################\n        max_reward, _ = group_reward.max(dim=1)\n        distance_AM.push(-max_reward)  # reward was given as negative dist\n        actor_loss_AM.push(group_loss.detach().reshape(-1))\n\n        # LOGGING\n        ###############################################\n        obj_avg = distance_AM.result()\n        loss_avg = actor_loss_AM.result()\n        step = step + 1\n        tb_logger.log_value('obj', obj_avg, step)\n        tb_logger.log_value('loss', loss_avg, step)\n\n        # if (time.time()-logger_start > LOG_PERIOD_SEC) or (episode == TRAIN_DATASET_SIZE):\n        #     timestr = time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-timer_start))\n        #     log_str = 'Ep:{:03d}-{:07d}({:5.1f}%)  T:{:s}  ALoss:{:+5f}  CLoss:{:5f}  Avg.dist:{:5f}' \\\n        #         .format(epoch, episode, episode/TRAIN_DATASET_SIZE*100,\n        #                 timestr, actor_loss_AM.result(), 0,\n        #                 distance_AM.result())\n        #     logger.info(log_str)\n        #     logger_start = time.time()\n    print('Ep:{}({}%)  T:{}  ALoss:{}  Avg.dist:{}'.format(epoch, epoch / TOTAL_EPOCH * 100,\n                                                              time.strftime(\"%H%M%S\"), loss_avg, obj_avg))\n\n    # LR STEP, after each epoch\n    # actor_group.lr_stepper.step()\n\n# ==========================================\n# File: TSP/source/mo_travelling_saleman_problem.py\n# Function/Context: GROUP_ENVIRONMENT._get_group_travel_distance\n# ==========================================\nimport torch\nimport numpy as np\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom HYPER_PARAMS import *\nfrom TORCH_OBJECTS import *\n\nclass GROUP_ENVIRONMENT:\n\n    def __init__(self, data):\n       \n\n        self.data = data\n        self.batch_s = data.size(0)\n        self.group_s = None\n        self.group_state = None\n\n    def reset(self, group_size):\n        self.group_s = group_size\n        self.group_state = GROUP_STATE(group_size=group_size, data=self.data)\n        reward = None\n        done = False\n        return self.group_state, reward, done\n\n    def step(self, selected_idx_mat):\n        # selected_idx_mat.shape = (batch, group)\n\n        # move state\n        self.group_state.move_to(selected_idx_mat)\n\n        # returning values\n        done = (self.group_state.selected_count == TSP_SIZE)\n        if done:\n            reward = -self._get_group_travel_distance()  # note the minus sign!\n        else:\n            reward = None\n        return self.group_state, reward, done\n\n    def _get_group_travel_distance(self):\n      \n        gathering_index = self.group_state.selected_node_list.unsqueeze(3).expand(self.batch_s, -1, TSP_SIZE, 4)\n       \n        \n        seq_expanded = self.data[:, None, :, :].expand(self.batch_s, self.group_s, TSP_SIZE, 4)\n\n        ordered_seq = seq_expanded.gather(dim=2, index=gathering_index)\n        \n        \n        rolled_seq = ordered_seq.roll(dims=2, shifts=-1)\n        \n        segment_lengths_obj1 = ((ordered_seq[:, :, :, :2]-rolled_seq[:, :, :, :2])**2).sum(3).sqrt()\n        segment_lengths_obj2 = ((ordered_seq[:, :, :, 2:]-rolled_seq[:, :, :, 2:])**2).sum(3).sqrt()\n\n        group_travel_distances_obj1 = segment_lengths_obj1.sum(2)\n        group_travel_distances_obj2 = segment_lengths_obj2.sum(2)\n    \n        group_travel_distances_combined = torch.stack([group_travel_distances_obj1,group_travel_distances_obj2],axis = 2)\n       \n        \n        return group_travel_distances_combined\n\n# ==========================================\n# File: TSPT2-3O/source/mo_travelling_saleman_problem.py\n# Function/Context: GROUP_ENVIRONMENT._get_group_travel_distance\n# ==========================================\nimport torch\nimport numpy as np\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom HYPER_PARAMS import *\nfrom TORCH_OBJECTS import *\n\nclass GROUP_ENVIRONMENT:\n\n    def __init__(self, data):\n       \n\n        self.data = data\n        self.batch_s = data.size(0)\n        self.group_s = None\n        self.group_state = None\n\n    def reset(self, group_size):\n        self.group_s = group_size\n        self.group_state = GROUP_STATE(group_size=group_size, data=self.data)\n        reward = None\n        done = False\n        return self.group_state, reward, done\n\n    def step(self, selected_idx_mat):\n        # selected_idx_mat.shape = (batch, group)\n\n        # move state\n        self.group_state.move_to(selected_idx_mat)\n\n        # returning values\n        done = (self.group_state.selected_count == TSP_SIZE)\n        if done:\n            reward = -self._get_group_travel_distance()  # note the minus sign!\n        else:\n            reward = None\n        return self.group_state, reward, done\n\n    def _get_group_travel_distance(self):\n\n        gathering_index = self.group_state.selected_node_list.unsqueeze(3).expand(self.batch_s, -1, TSP_SIZE, 6)\n\n\n        seq_expanded = self.data[:, None, :, :].expand(self.batch_s, self.group_s, TSP_SIZE, 6)\n\n        ordered_seq = seq_expanded.gather(dim=2, index=gathering_index)\n\n\n        rolled_seq = ordered_seq.roll(dims=2, shifts=-1)\n\n        segment_lengths_obj1 = ((ordered_seq[:, :, :, :2]-rolled_seq[:, :, :, :2])**2).sum(3).sqrt()\n        segment_lengths_obj2 = ((ordered_seq[:, :, :, 2:4]-rolled_seq[:, :, :, 2:4])**2).sum(3).sqrt()\n        segment_lengths_obj3 = ((ordered_seq[:, :, :, 4:] - rolled_seq[:, :, :, 4:]) ** 2).sum(3).sqrt()\n\n        group_travel_distances_obj1 = segment_lengths_obj1.sum(2)\n        group_travel_distances_obj2 = segment_lengths_obj2.sum(2)\n        group_travel_distances_obj3 = segment_lengths_obj3.sum(2)\n\n        group_travel_distances_combined = torch.stack([group_travel_distances_obj1,group_travel_distances_obj2,group_travel_distances_obj3],axis = 2)\n\n\n        return group_travel_distances_combined\n\n# ==========================================\n# File: TSPT2/source/mo_travelling_saleman_problem.py\n# Function/Context: ENVIRONMENT, GROUP_ENVIRONMENT, STATE, GROUP_STATE\n# ==========================================\nimport torch\nimport numpy as np\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom HYPER_PARAMS import *\nfrom TORCH_OBJECTS import *\n\ndef TSP_DATA_LOADER__RANDOM(num_sample, num_nodes, batch_size, seed=None):\n    if seed:\n        np.random.seed(seed)\n    dataset = TSP_Dataset__Random(num_sample=num_sample, num_nodes=num_nodes)\n    data_loader = DataLoader(dataset=dataset,\n                             batch_size=batch_size,\n                             shuffle=False,\n                             num_workers=0,\n                             collate_fn=TSP_collate_fn)\n    return data_loader\n\n\nclass TSP_Dataset__Random(Dataset):\n    def __init__(self, num_sample, num_nodes):\n        self.num_sample = num_sample\n        self.num_nodes = num_nodes\n\n    def __getitem__(self, index):\n        node_xy_data = np.random.rand(self.num_nodes, 3)\n        node_xy_data = np.concatenate((node_xy_data, np.zeros([self.num_nodes, 1])), axis=-1)\n        return node_xy_data\n\n    def __len__(self):\n        return self.num_sample\n\ndef TSP_collate_fn(batch):\n    node_xy = Tensor(batch)\n    return node_xy\n\nclass STATE:\n\n    def __init__(self, seq):\n        self.seq = seq\n        self.batch_s = seq.size(0)\n\n        self.current_node = None\n\n        # History\n        ####################################\n        self.selected_count = 0\n        self.available_mask = BoolTensor(np.ones((self.batch_s, TSP_SIZE)))\n        self.ninf_mask = Tensor(np.zeros((self.batch_s, TSP_SIZE)))\n        # shape = (batch_s, TSP_SIZE)\n        self.selected_node_list = LongTensor(np.zeros((self.batch_s, 0)))\n        # shape = (batch_s, selected_count)\n\n    def move_to(self, selected_node_idx):\n        # selected_node_idx.shape = (batch,)\n\n        self.current_node = selected_node_idx\n\n        # History\n        ####################################\n        self.selected_count += 1\n        self.available_mask[torch.arange(self.batch_s), selected_node_idx] = False\n        self.ninf_mask[torch.arange(self.batch_s), selected_node_idx] = -np.inf\n        self.selected_node_list = torch.cat((self.selected_node_list, selected_node_idx[:, None]), dim=1)\n\n\nclass GROUP_STATE:\n\n    def __init__(self, group_size, data):\n        # data.shape = (batch, group, 2)\n        self.batch_s = data.size(0)\n        self.group_s = group_size\n        self.data = data\n\n        # History\n        ####################################\n        self.selected_count = 0\n        self.current_node = None\n        # shape = (batch, group)\n        self.selected_node_list = LongTensor(np.zeros((self.batch_s, group_size, 0)))\n        # shape = (batch, group, selected_count)\n\n        # Status\n        ####################################\n        self.ninf_mask = Tensor(np.zeros((self.batch_s, group_size, TSP_SIZE)))\n        # shape = (batch, group, TSP_SIZE)\n\n\n    def move_to(self, selected_idx_mat):\n        # selected_idx_mat.shape = (batch, group)\n\n        # History\n        ####################################\n        self.selected_count += 1\n        self.current_node = selected_idx_mat\n        self.selected_node_list = torch.cat((self.selected_node_list, selected_idx_mat[:, :, None]), dim=2)\n\n        # Status\n        ####################################\n        batch_idx_mat = torch.arange(self.batch_s)[:, None].expand(self.batch_s, self.group_s)\n        group_idx_mat = torch.arange(self.group_s)[None, :].expand(self.batch_s, self.group_s)\n        self.ninf_mask[batch_idx_mat, group_idx_mat, selected_idx_mat] = -np.inf\n\n\nclass ENVIRONMENT:\n\n    def __init__(self, seq):\n    \n        self.seq = seq\n        self.batch_s = seq.size(0)\n        self.state = None\n\n    def reset(self):\n        self.state = STATE(self.seq)\n\n        reward = None\n        done = False\n        return self.state, reward, done\n\n    def step(self, selected_node_idx):\n        # selected_node_idx.shape = (batch,)\n\n        # move state\n        self.state.move_to(selected_node_idx)\n\n        # returning values\n        done = self.state.selected_count == TSP_SIZE\n        if done:\n            reward = -self._get_travel_distance()  # note the minus sign!\n        else:\n            reward = None\n        return self.state, reward, done\n\n    def _get_travel_distance(self):\n\n        gathering_index = self.state.selected_node_list.unsqueeze(2).expand(-1, TSP_SIZE, 4)\n        \n        ordered_seq = self.seq.gather(dim=1, index=gathering_index)\n\n        rolled_seq = ordered_seq.roll(dims=1, shifts=-1)\n        segment_lengths = ((ordered_seq-rolled_seq)**2).sum(2).sqrt()\n        # size = (batch, TSP_SIZE)\n\n        travel_distances = segment_lengths.sum(1)\n        # size = (batch,)\n        return travel_distances\n\nclass GROUP_ENVIRONMENT:\n\n    def __init__(self, data):\n       \n\n        self.data = data\n        self.batch_s = data.size(0)\n        self.group_s = None\n        self.group_state = None\n\n    def reset(self, group_size):\n        self.group_s = group_size\n        self.group_state = GROUP_STATE(group_size=group_size, data=self.data)\n        reward = None\n        done = False\n        return self.group_state, reward, done\n\n    def step(self, selected_idx_mat):\n        # selected_idx_mat.shape = (batch, group)\n\n        # move state\n        self.group_state.move_to(selected_idx_mat)\n\n        # returning values\n        done = (self.group_state.selected_count == TSP_SIZE)\n        if done:\n            reward = -self._get_group_travel_distance()  # note the minus sign!\n        else:\n            reward = None\n        return self.group_state, reward, done\n\n    def _get_group_travel_distance(self):\n      \n        gathering_index = self.group_state.selected_node_list.unsqueeze(3).expand(self.batch_s, -1, TSP_SIZE, 4)\n       \n        \n        seq_expanded = self.data[:, None, :, :].expand(self.batch_s, self.group_s, TSP_SIZE, 4)\n\n        ordered_seq = seq_expanded.gather(dim=2, index=gathering_index)\n        \n        \n        rolled_seq = ordered_seq.roll(dims=2, shifts=-1)\n        \n        segment_lengths_obj1 = ((ordered_seq[:, :, :, :2]-rolled_seq[:, :, :, :2])**2).sum(3).sqrt()\n        segment_lengths_obj2 = ((ordered_seq[:, :, :, 2:]-rolled_seq[:, :, :, 2:])**2).sum(3).sqrt()\n\n        group_travel_distances_obj1 = segment_lengths_obj1.sum(2)\n        group_travel_distances_obj2 = segment_lengths_obj2.sum(2)\n    \n        group_travel_distances_combined = torch.stack([group_travel_distances_obj1,group_travel_distances_obj2],axis = 2)\n       \n        \n        return group_travel_distances_combined",
  "description": "Combined Analysis:\n- [CVRP/source/TRAIN_N_EVAL/Train_Grouped_Actors.py]: This file implements the core training logic for the neural heuristic (POMO-based) component of EMNH for CVRP. It trains a grouped actor network using policy gradient reinforcement learning with advantage estimation. The code constructs vehicle routes sequentially through an environment interaction loop, computes rewards (negative distances), and optimizes using REINFORCE with baseline. This corresponds to the base neural heuristic framework mentioned in the paper's algorithm steps, specifically the policy optimization with multiple optima (POMO) approach adapted for CVRP constraints. The training follows the single-objective subproblem decomposition paradigm (though this file shows single-objective training, the multi-objective extension would involve scalarized rewards).\n- [CVRP/source/mo_cvrp.py]: This file implements the core multi-objective CVRP (MOCVRP) environment with two objectives: total traveling distance (f) and makespan (f). The GROUP_ENVIRONMENT._get_travel_distance() method computes both objectives exactly as described in the paper's optimization model. It calculates Euclidean distances between consecutive nodes in routes (travel_distances) and identifies the maximum route length (makespans). The code handles batch processing and group parallelism for multiple solutions, aligning with the POMO-based neural heuristic framework. Vehicle capacity constraints are enforced through STATE/GROUP_STATE classes via load tracking and masking mechanisms.\n- [KP/source/MODEL__Actor/grouped_actors.py]: This file implements the core neural network architecture (ACTOR) for the Multi-Objective Knapsack Problem (MOKP) as part of the EMNH framework. The ACTOR class uses an encoder (based on multi-head attention) to process item features (3-dimensional: likely value, weight, and scalarized objective) and a probability calculator that outputs selection probabilities for items given the remaining capacity. This directly corresponds to the neural construction heuristic (POMO) used in the paper's algorithm for solving scalarized single-objective subproblems. The model constructs solutions autoregressively by selecting items based on computed probabilities, respecting the knapsack capacity constraint via the ninf_mask. The implementation aligns with the paper's use of deep reinforcement learning for combinatorial optimization.\n- [KP/source/TRAIN_N_EVAL/Train_Grouped_Actors.py]: This file implements the core training logic for the multi-objective knapsack problem (MOKP) using the EMNH framework. The TRAIN function executes the key algorithm steps: 1) Decomposes the MOKP into multiple single-objective subproblems (group_size = PROBLEM_SIZE, each representing a different weight/preference vector). 2) Uses a neural construction heuristic (grouped_actor) based on reinforcement learning (policy gradient with baseline) to sequentially select items. 3) Implements the POMO (Policy Optimization with Multiple Optima) method with multiple parallel actors (group_s) for each instance. 4) Applies REINFORCE with baseline (group_advantage = reward - mean_reward) for policy optimization. 5) The environment (GROUP_ENVIRONMENT) handles the knapsack constraints (w_i x_i  W) and computes scalarized rewards for each preference vector. This directly corresponds to the paper's approach of solving decomposed single-objective subproblems using neural heuristics enhanced by meta-learning (though the meta-learning aspect is handled elsewhere in the codebase).\n- [KP/source/mo_knapsack_problem.py]: This file implements the core environment and state management for the Multi-Objective Knapsack Problem (MOKP) as described in the paper. Specifically:\n1. Implements the mathematical model: Each item has weight (dimension 0), value1 (dimension 1), and value2 (dimension 2). The constraint is capacity (sum of weights  capacity). The objectives are to maximize accumulated values (implemented as negative minimization in reward).\n2. Implements the GROUP_STATE and GROUP_ENVIRONMENT classes that handle multiple parallel solutions (group_s) corresponding to different weight vectors in the decomposition method, exactly matching the paper's scalarization approach.\n3. The state transition logic (move_to) updates accumulated values for both objectives, tracks capacity constraint via masks, and manages solution construction.\n4. The environment provides the reinforcement learning interface (reset/step) required by the POMO-based neural heuristic framework mentioned in the paper.\n5. The code directly corresponds to the MOKP formulation: f_m(x) = - v_{mi}x_i with constraint  w_i x_i  W, where x_i  {0,1}.\n- [TSP-3O/source/mo_travelling_saleman_problem.py]: This file implements the core multi-objective TSP (MOTSP) optimization model from the paper. Specifically, the GROUP_ENVIRONMENT._get_group_travel_distance() method computes the three objective functions (f1, f2, f3) exactly as described in the mathematical model: f_m(x) = _i c_{_i _{i+1}}^m where c_{ij}^m = ||x_i^m - x_j^m||_2. The code calculates Euclidean distances for three separate 2D coordinate sets (indices 0:2, 2:4, 4:6) across the Hamiltonian cycle, then returns the combined objectives as a tensor of shape (batch, group, 3). This directly implements the multi-objective optimization model for MOTSP with 3 objectives.\n- [TSP/source/MODEL__Actor/grouped_actors.py]: This file implements the core neural construction heuristic (actor) for the multi-objective traveling salesman problem (MOTSP) as described in the paper. The ACTOR class uses an encoder-decoder architecture with multi-head attention to sequentially construct TSP tours. The encoder processes node coordinates (4-dimensional input for 2 objectives with 2D coordinates each), and the decoder computes probabilities for selecting the next node. This implements the POMO (Policy Optimization with Multiple Optima) base model mentioned in the algorithm steps, which is used as the neural heuristic for solving scalarized single-objective subproblems. The grouped architecture allows simultaneous construction of multiple solutions for different preference vectors, aligning with the multi-objective decomposition approach.\n- [TSP/source/TRAIN_N_EVAL/Train_Grouped_Actors.py]: This file implements the core training loop for the POMO (Policy Optimization with Multiple Optima) neural heuristic, which serves as the base model in the EMNH framework. The TRAIN function performs: 1) Environment setup with GROUP_ENVIRONMENT for TSP instances, 2) Rollout generation using actor_group (neural policy) with multiple starting nodes (group_s = TSP_SIZE), 3) REINFORCE policy gradient optimization with baseline subtraction (group_advantage = group_reward - group_reward.mean()), and 4) Logging via tensorboard. This corresponds to the base neural construction heuristic mentioned in the paper's Algorithm Steps, specifically the POMO method that generates multiple solutions per instance. However, this implementation is for single-objective TSP training; the multi-objective extension (scalarization with weight vectors) and meta-learning components would be implemented elsewhere in the codebase.\n- [TSP/source/mo_travelling_saleman_problem.py]: This file implements the core multi-objective TSP (MOTSP) environment for the EMNH algorithm. The key function _get_group_travel_distance() directly implements the mathematical model for MOTSP with two objectives: it computes Euclidean distances for two separate coordinate sets (first two dimensions for objective 1, last two for objective 2) along constructed tours. The code handles batched and grouped solutions, calculating f(x) = x - x and f(x) = x - x for Hamiltonian cycles, matching the paper's MOTSP formulation. The environment uses reinforcement learning states (GROUP_STATE) to sequentially construct tours and returns negative combined distances as rewards for training.\n- [TSPT2-3O/source/mo_travelling_saleman_problem.py]: This file implements the core mathematical model for the Multi-Objective Traveling Salesman Problem (MOTSP) with three objectives. The key method _get_group_travel_distance() computes the exact objective functions f, f, f as described in the paper: f(x) =  c_{} where c = ||x - x||. The code calculates Euclidean distances for three separate coordinate sets (dimensions 0-1, 2-3, 4-5) along the constructed tour, then sums them to get three objective values. This directly implements the multi-objective optimization model for MOTSP Type 1 with M=3 objectives.\n- [TSPT2/source/mo_travelling_saleman_problem.py]: This file implements the core environment and state management for MOTSP Type 2, which directly corresponds to the mathematical model described in the paper. The key implementation includes: 1) Data generation with 4-dimensional node coordinates (x1,y1,x2,y2) representing two objectives, 2) STATE and GROUP_STATE classes that manage solution construction and constraints (Hamiltonian cycle), 3) ENVIRONMENT and GROUP_ENVIRONMENT classes that implement the step-by-step solution construction process with reward computation, 4) The _get_group_travel_distance() method exactly implements the multi-objective TSP mathematical model: f_m(x) = _{i=1}^{n} c_{_i _{i+1}}^m with c_{ij}^m = x_i^m - x_j^m_2 for m=1,2, where the Euclidean distances are computed separately for each objective dimension. This provides the foundation for the neural heuristic (POMO-based) construction process described in the paper.",
  "dependencies": [
    "source.mo_knapsack_problem.KNAPSACK_DATA_LOADER__RANDOM",
    "torch.utils.data.Dataset",
    "TSP_collate_fn",
    "tqdm.tqdm",
    "source.mo_travelling_saleman_problem.TSP_DATA_LOADER__RANDOM",
    "TORCH_OBJECTS.LongTensor",
    "TORCH_OBJECTS",
    "IPython.core.debugger.set_trace",
    "Average_Meter",
    "torch.utils.data.DataLoader",
    "time",
    "TSP_Dataset__Random",
    "CVRP_DATA_LOADER__RANDOM",
    "torch.nn.functional",
    "TORCH_OBJECTS.Tensor",
    "HYPER_PARAMS.TSP_SIZE",
    "CVRP_Dataset__Random",
    "ENVIRONMENT",
    "tensorboard_logger.Logger",
    "source.utilities.Average_Meter",
    "Add_And_Normalization_Module",
    "KnapSack_Dataset__Random",
    "GROUP_STATE",
    "CVRP_collate_fn",
    "IPython.core.debugger",
    "source.mo_knapsack_problem.GROUP_ENVIRONMENT",
    "multi_head_attention",
    "knapsack_collate_fn",
    "TORCH_OBJECTS.BoolTensor",
    "numpy",
    "STATE",
    "reshape_by_heads",
    "KNAPSACK_DATA_LOADER__RANDOM",
    "HYPER_PARAMS",
    "torch.nn",
    "pick_nodes_for_each_group",
    "GROUP_ENVIRONMENT",
    "Feed_Forward_Module",
    "source.mo_travelling_saleman_problem.GROUP_ENVIRONMENT",
    "torch"
  ]
}