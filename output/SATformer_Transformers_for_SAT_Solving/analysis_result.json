{
  "paper_id": "SATformer_Transformers_for_SAT_Solving",
  "title": "SATformer: Transformer-Based UNSAT Core Learning",
  "abstract": "This paper introduces SATformer, a novel Transformer-based approach for the Boolean Satisfiability (SAT) problem. Rather than solving the problem directly, SATformer approaches the problem from the opposite direction by focusing on unsatisfiability. Specifically, it models clause interactions to identify any unsatisfiable sub-problems. Using a graph neural network, we convert clauses into clause embeddings and employ a hierarchical Transformer-based model to understand clause correlation. SATformer is trained through a multi-task learning approach, using the single-bit satisfiability result and the minimal unsatisfiable core (MUC) for UNSAT problems as clause supervision. As an end-to-end learning-based satisfiability classifier, the performance of SATformer surpasses that of NeuroSAT significantly. Furthermore, we integrate the clause predictions made by SATformer into modern heuristic-based SAT solvers and validate our approach with a logic equivalence checking task. Experimental results show that our SATformer can decrease the runtime of existing solvers by an average of 21.33%.",
  "problem_description_natural": "The paper addresses the Boolean Satisfiability (SAT) problem, which asks whether there exists a truth assignment to Boolean variables that makes a given logical formula evaluate to true. The authors focus specifically on unsatisfiable (UNSAT) instances, aiming to identify minimal unsatisfiable cores (MUCs)â€”smallest subsets of clauses that are themselves unsatisfiable. By modeling clause-level interactions using a hierarchical Transformer architecture combined with a graph neural network, SATformer learns to predict both the overall satisfiability of a formula and which clauses are likely part of an MUC. This information is then used to guide and accelerate existing SAT solvers by prioritizing search on high-impact clauses.",
  "problem_type": "combinatorial optimization",
  "datasets": [
    "Generated SR(3-10) SAT instances",
    "Generated SR(20) SAT instances",
    "Generated SR(40) SAT instances",
    "Generated SR(60) SAT instances",
    "Industrial LEC instances"
  ],
  "performance_metrics": [
    "Classification Accuracy",
    "Runtime Reduction (%)",
    "Average Runtime (s)",
    "Number of Lemmas",
    "Model Parameters (# Param.)",
    "Floating-Point Operations (# FLOPs)"
  ],
  "lp_model": {
    "objective": "$\\min 0$",
    "constraints": [
      "$\\sum_{j \\in P_i} x_j + \\sum_{j \\in N_i} (1 - x_j) \\geq 1$ for all $i = 1,\\ldots,n$"
    ],
    "variables": [
      "$x_j \\in \\{0,1\\}$ for $j=1,\\ldots,m$: binary decision variable indicating the assignment to Boolean variable $x_j$"
    ]
  },
  "raw_latex_model": "$$\\begin{aligned} \\text{Given:} & \\quad \\text{Boolean variables } x_j \\in \\{0,1\\}, \\ j=1,\\ldots,m \\\\ & \\quad \\text{CNF formula } \\phi = C_1 \\land C_2 \\land \\ldots \\land C_n, \\text{ where each clause } C_i \\text{ is a disjunction of literals} \\\\ \\text{Find:} & \\quad \\text{An assignment to } x_j \\text{ such that } \\phi \\text{ is true} \\\\ \\text{Formulated as:} & \\quad \\min 0 \\\\ & \\quad \\text{s.t. } \\sum_{j \\in P_i} x_j + \\sum_{j \\in N_i} (1 - x_j) \\geq 1, \\quad \\forall i = 1,\\ldots,n \\\\ & \\quad \\text{where } P_i \\text{ is the set of variables appearing positively in clause } C_i, \\text{ and } N_i \\text{ is the set appearing negatively.} \\end{aligned}$$",
  "algorithm_description": "SATformer is a Transformer-based deep learning model that approaches the SAT problem by focusing on unsatisfiability. It uses a graph neural network (GNN) to convert clauses into embeddings from the Literal-Clause Graph (LCG), and a hierarchical Transformer-based model to capture clause correlations and interactions. The model is trained through multi-task learning with supervision from single-bit satisfiability results and minimal unsatisfiable cores (MUCs). It predicts satisfiability and clause contributions to UNSAT cores, which are then integrated as initialization heuristics (e.g., for VSIDS scores) into modern SAT solvers like CaDiCaL and Kissat to accelerate solving, particularly for UNSAT instances."
}