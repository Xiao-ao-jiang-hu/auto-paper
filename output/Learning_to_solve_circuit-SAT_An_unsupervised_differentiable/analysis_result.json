{
  "paper_id": "Learning_to_solve_circuit-SAT_An_unsupervised_differentiable",
  "title": "LEARNING TO SOLVE CIRCUIT-SAT: AN UNSUPERVISED DIFFERENTIABLE APPROACH",
  "abstract": "Recent efforts to combine Representation Learning with Formal Methods, commonly known as Neuro-Symbolic Methods, have given rise to a new trend of applying rich neural architectures to solve classical combinatorial optimization problems. In this paper, we propose a neural framework that can learn to solve the Circuit Satisfiability problem. Our framework is built upon two fundamental contributions: a rich embedding architecture that encodes the problem structure, and an end-to-end differentiable training procedure that mimics Reinforcement Learning and trains the model directly toward solving the SAT problem. The experimental results show the superior out-of-sample generalization performance of our framework compared to the recently developed NeuroSAT method.",
  "problem_description_natural": "The paper addresses the Circuit Satisfiability (Circuit-SAT) problem, which asks whether there exists an assignment of Boolean values to input variables of a given Boolean circuit—composed of logical gates such as AND, OR, and NOT—such that the output of the circuit evaluates to true. This is a canonical NP-complete problem in computer science. The authors propose a neural approach that learns to solve Circuit-SAT from data without explicit supervision of solutions, using a differentiable architecture that directly optimizes for finding satisfying assignments.",
  "problem_type": "combinatorial optimization",
  "datasets": [
    "Random k-SAT (3-10 variables)",
    "Random Graph k-Coloring Dataset-1",
    "Random Graph k-Coloring Dataset-2"
  ],
  "performance_metrics": [
    "Percentage of SAT problems solved",
    "Generalization to out-of-sample problem sizes",
    "Test time complexity (seconds per example)",
    "Batch solving time (minutes for 10,000 examples)"
  ],
  "lp_model": {
    "objective": "$\\max a_r$",
    "constraints": [
      "For each And node $v$ with parents $\\pi(v)$: $a_v \\leq a_u$ for all $u \\in \\pi(v)$",
      "For each And node $v$ with parents $\\pi(v)$: $a_v \\geq \\sum_{u \\in \\pi(v)} a_u - (|\\pi(v)| - 1)$",
      "For each Or node $v$ with parents $\\pi(v)$: $a_v \\geq a_u$ for all $u \\in \\pi(v)$",
      "For each Or node $v$ with parents $\\pi(v)$: $a_v \\leq \\sum_{u \\in \\pi(v)} a_u$",
      "For each Not node $v$ with parent $u$: $a_v = 1 - a_u$",
      "For the sink node $r$: $a_r = 1$"
    ],
    "variables": [
      "$a_v \\in \\{0,1\\}$ for all nodes $v \\in V$: binary variable representing the value at node $v$, where $V$ is the set of nodes in the DAG representing the Boolean circuit with gates And, Or, Not, and Variable nodes."
    ]
  },
  "raw_latex_model": "$$\\begin{aligned}\\text{Maximize} & \\quad a_r \\\\\\text{Subject to} & \\quad \\text{For each And node } v \\text{ with parents } \\pi(v): \\\\& \\quad a_v \\leq a_u \\quad \\forall u \\in \\pi(v) \\\\& \\quad a_v \\geq \\sum_{u \\in \\pi(v)} a_u - (|\\pi(v)| - 1) \\\\& \\quad \\text{For each Or node } v \\text{ with parents } \\pi(v): \\\\& \\quad a_v \\geq a_u \\quad \\forall u \\in \\pi(v) \\\\& \\quad a_v \\leq \\sum_{u \\in \\pi(v)} a_u \\\\& \\quad \\text{For each Not node } v \\text{ with parent } u: \\\\& \\quad a_v = 1 - a_u \\\\& \\quad \\text{For the sink node } r: \\quad a_r = 1 \\\\& \\quad a_v \\in \\{0,1\\} \\quad \\forall v \\in V\\end{aligned}$$",
  "algorithm_description": "The paper proposes a neural framework using Deep-Gated DAG Recursive Neural Networks (DG-DAGRNN) for embedding DAG-structured circuits, combined with an end-to-end differentiable training procedure that directly optimizes for finding satisfying assignments. This involves a solver network that generates soft assignments, an evaluator network with smooth min/max functions for gradient flow, and a loss function that pushes assignments to satisfy the circuit, incorporating an explore-exploit mechanism via temperature annealing."
}