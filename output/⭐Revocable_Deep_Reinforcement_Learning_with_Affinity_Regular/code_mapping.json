{
  "file_path": "environment_qap_ag.py, run_rgm_pytorch_willow.py",
  "function_name": "Environment",
  "code_snippet": "\n\n# ==========================================\n# File: environment_qap_ag.py\n# Function/Context: Environment\n# ==========================================\nimport random\nimport numpy as np\nimport gym\nimport copy\nimport datetime\nimport torch\nfrom utils.gm_solver import get_norm_affinity, get_aff_score_norm\n\nmaxRound = 300\n\n\n# am: affinity metric\n# ag: affinity graph\nclass Environment:\n    def __init__(self, am, beta=1):\n        self.N = int(np.sqrt(len(am)))\n        action_dims = self.N * self.N\n        ob_dims = self.N * self.N\n        self.beta = beta\n        self.action_dims = action_dims\n        self.action_space = gym.spaces.MultiDiscrete(action_dims)\n        self.observation_space = gym.spaces.Box(0, 10, (1, ob_dims // self.N, ob_dims // self.N))\n        self.current_sol = np.zeros((self.N, self.N), dtype=np.float32)\n        self.am = am\n        self.old_sol = copy.deepcopy(self.current_sol)\n        self.best_sol = copy.deepcopy(self.current_sol)\n        self.best_sol_position = 0\n        self.best_ans = self.calc_score(self.current_sol) - 100\n        self.ag = am\n        self.ag_weight = np.array(self.ag, dtype=np.float32)\n        self.ag_adjacent = np.zeros((self.N * self.N, self.N * self.N), dtype=np.float32)\n        self.max_rounds = 30\n        self.avail_actions = np.ones((self.N * self.N,)) * 1\n        for i in range(self.N * self.N):\n            for j in range(self.N * self.N):\n                if self.ag_weight[i][j] > 0 or i == j:\n                    self.ag_adjacent[i][j] = 1\n        self.rounds = 0\n\n    def calc_score(self, sol, normalize=False):\n        am = torch.FloatTensor(self.am).cuda()\n        if normalize:\n            sol = torch.FloatTensor(sol.T).cuda()\n            return self.beta * get_aff_score_norm(am, sol, self.N).cpu().detach().numpy()[0][0]\n        else:\n            sol = torch.FloatTensor(sol).cuda()\n        return self.beta * torch.matmul(torch.matmul(torch.reshape(sol, (1, -1)), am),\n                                        torch.reshape(sol, (-1, 1))).cpu().detach().numpy()[0][0]\n\n    def image_state(self):\n        avail_actions = np.ones((self.N * self.N,)) * 0\n        for i in range(self.N * self.N):\n            if self.check(i):\n                avail_actions[i] = 1\n        weights = np.reshape(self.ag_weight, (self.N * self.N, self.N * self.N))\n        state = np.concatenate(([self.current_sol.flatten()], [avail_actions]))\n        state = np.concatenate((state, weights))\n        state = np.array(state, dtype=np.float32)\n        return state\n\n    def reset(self):\n        self.current_sol = np.zeros((self.N, self.N))\n        self.old_sol = copy.deepcopy(self.current_sol)\n        self.best_sol = copy.deepcopy(self.current_sol)\n        self.best_sol_position = 0\n        self.best_ans = self.calc_score(self.current_sol) - 100\n        self.ag = self.am\n        self.rounds = 0\n        self.avail_actions = np.ones((self.N * self.N,)) * 1\n        for i in range(self.N * self.N):\n            if self.check(i):\n                self.avail_actions[i] = 1\n        # return [np.concatenate((self.get_state(), avail_actions, np.reshape(self.ag_adjacent, self.N * self.N * self.N * self.N),\n        #                         np.reshape(self.ag_weight, self.N * self.N * self.N * self.N)))]\n        return self.image_state()\n\n    def get_state(self):\n        return np.reshape(self.current_sol, (-1,))\n\n    def get_best_ans(self):\n        return self.best_sol_position, self.best_sol, self.best_ans\n\n    def check(self, action):\n        return self.avail_actions[action]\n\n    def check_sol(self, action):\n        x = action // self.N\n        y = action % self.N\n        if np.sum(self.current_sol[:, y]) >= 1:\n            return False\n        if np.sum(self.current_sol[x, :]) >= 1:\n            return False\n        # for i in range(self.N):\n        #     if self.current_sol[i][y] == 1:\n        #         return False\n        # for j in range(self.N):\n        #     if self.current_sol[x][j] == 1:\n        #         return False\n        return True\n\n    def step(self, action, normalize=False):\n        # action = action[0]\n        # action = np.argmax(action)\n        # self.current_sol[action // self.N][action % self.N] = 1\n        if self.check(action):\n            self.current_sol[action // self.N][action % self.N] = 1\n        else:\n            self.current_sol[action // self.N, :] = 0\n            self.current_sol[:, action % self.N] = 0\n            self.current_sol[action // self.N][action % self.N] = 1\n\n        old_ans = self.calc_score(self.old_sol, normalize)\n        current_ans = self.calc_score(self.current_sol, normalize)\n        self.rounds += 1\n        if current_ans > self.best_ans:\n            self.best_ans = current_ans\n            self.best_sol = copy.deepcopy(self.current_sol)\n            self.best_sol_position = self.rounds\n        self.old_sol = copy.deepcopy(self.current_sol)\n        self.avail_actions = np.ones((self.N * self.N,)) * 0\n        for i in range(self.N * self.N):\n            if self.check_sol(i):\n                self.avail_actions[i] = 1\n        # return [np.concatenate((self.get_state(), avail_actions, np.reshape(self.ag_adjacent, self.N * self.N * self.N * self.N),\n        #                         np.reshape(self.ag_weight, self.N * self.N * self.N * self.N)))], (current_ans - old_ans), np.sum(\n        #     self.get_state()) == self.N, [self.rounds]\n        return self.image_state(), (current_ans - old_ans), self.rounds == self.max_rounds, [self.best_sol_position]\n\n# ==========================================\n# File: run_rgm_pytorch_willow.py\n# Function/Context: \n# ==========================================\n#!/usr/bin/env python3\nfrom datetime import datetime\nimport random\nimport copy\nimport logging\n\nfrom prioritized_memory import Memory\nfrom multiprocessing import Pool, Queue, Process\nimport dqn_model_r\nfrom environment_qap_ag import Environment\nimport argparse\nimport time\nimport numpy as np\nimport collections\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib\n\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n\nfrom tensorboardX import SummaryWriter\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\nimport os\nfrom utils.rrwm import RRWM\nfrom utils.rrwm_mask import MaskRRWM\nfrom utils.hungarian import hungarian\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\nDYNAMIC = 0\n\nGAMMA = 0.9\nBATCH_SIZE = 20\nREPLAY_SIZE = 50000\nLEARNING_RATE = 1e-4\nSYNC_TARGET_FRAMES = 20\nREPLAY_START_SIZE = 1000\nMULTI_STEP = 1\n\nEPSILON_DECAY_LAST_FRAME = (10 ** 4) * 2.0\nEPSILON_BONUS = 1.1\nEPSILON_START = 1.0\nEPSILON_FINAL = 0.02\n\noutput_unary = []\n\nADDITION_RATE_DECAY = 0.1\nADDITION_RATE_FINAL = 0.01\nADDITION_RATE_BOOST = 5\n\nUNITS = 64\nHIDDEN_SIZE = 128\nT = 3\n\nMEAN_REWARD_BOUND = 1.05\n\nrrwm_solver = RRWM()\nrrwm_mask = MaskRRWM()\n\n\n# Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n\nclass ExperienceBuffer:\n    def __init__(self, capacity):\n        self.buffer = collections.deque(maxlen=capacity)\n\n    def __len__(self):\n        return len(self.buffer)\n\n    def append(self, experience):\n        self.buffer.append(experience)\n\n    def sample(self, batch_size):\n        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), np.array(dones,\n                                                                                                  dtype=np.uint8), np.array(\n            next_states)\n\n\nclass Agent:\n    def __init__(self, env, exp_buffer):\n        self.mistake = 1\n        self.env = env\n        self.exp_buffer = exp_buffer\n        self.length = 0\n        self._reset()\n\n    def _reset(self):\n        self.state = self.env.reset()\n        self.state_list = [self.state]\n        self.action_list = []\n        self.reward_list = []\n        self.random_list = []\n        self.acc_list = []\n        self.length = 0\n        self.total_reward = 0.0\n\n    def weights_init(self, m):\n        classname = m.__class__.__name__\n        if classname.find('Linear') != -1:\n            torch.nn.init.xavier_uniform(m.weight)\n\n    def set_env(self, env, reset=True):\n        self.env = env\n        if reset:\n            self.state = env.reset()\n        else:\n            self.state = env.image_state()\n        self.state_list = [self.state]\n\n    def append_sample(self, net, tgt_net, state, action, reward, next_state, done):\n        target = net(torch.autograd.Variable(torch.FloatTensor([state])).to(device), hard_mask=hard_mask,\n                     normalize=normalize).data\n        old_val = target[0][action].cpu().detach().numpy()\n        target_val = tgt_net(torch.autograd.Variable(torch.FloatTensor([next_state])).to(device), hard_mask=hard_mask,\n                             normalize=normalize).data\n        if done:\n            target[0][action] = torch.FloatTensor([reward])[0].cuda()\n        else:\n            target[0][action] = torch.FloatTensor([reward])[0].cuda() + GAMMA * torch.max(target_val)\n        error = abs(old_val - target[0][action].cpu().detach().numpy())\n        self.exp_buffer.add(error, (state, action, reward, next_state, done))\n\n    def play_step(self, net, tgt_net, addition, order, epsilon=0.0, use_addition=False, device=\"cpu\", k=-1):\n        self.length += 1\n        done_reward = None\n        rand = np.random.random()\n        if DYNAMIC:\n            state_a = np.array([self.state], copy=False)\n            state_v = torch.tensor(state_a).to(device)\n            q_vals_v = net(state_v, use_dynamic_embedding=True, hard_mask=hard_mask, normalize=normalize)\n            # q_vals_v = net(state_v, use_dynamic_embedding=True, hard_mask=(np.sum(self.env.best_sol) < self.env.N))\n        if rand < epsilon:\n            t = np.random.randint(0, self.env.action_dims)\n            action = t\n            self.random_list.append('!')\n        else:\n            if not DYNAMIC:\n                state_a = np.array([self.state], copy=False)\n                state_v = torch.tensor(state_a).to(device)\n                q_vals_v = net(state_v, use_dynamic_embedding=False, hard_mask=hard_mask, normalize=normalize)\n                # q_vals_v = net(state_v, use_dynamic_embedding=True, hard_mask=(np.sum(self.env.best_sol) < self.env.N), normalize=True)\n            self.random_list.append('*')\n            q_vals_v = q_vals_v.cpu().detach().numpy()\n            q_vals_v = q_vals_v[0]\n            # action = np.argmax(q_vals_v)\n            action_ = np.argmax(q_vals_v)\n            value = np.reshape(q_vals_v, (self.env.N, self.env.N))\n            value_n = (value - np.min(value)) / (np.max(value) - np.min(value))\n            if np.sum(self.env.current_sol) == 0:\n                output_unary.append(value_n)\n            while k > 0:\n                q_vals_v[np.argmax(q_vals_v)] = np.min(q_vals_v) - 1\n                k -= 1\n            if not self.env.check(np.argmax(q_vals_v)):\n                action = action_\n            else:\n                action = np.argmax(q_vals_v)\n        new_state, reward, is_done, pos = self.env.step(action, normalize=normalize)\n        self.action_list.append(action)\n\n        _, sol, _ = self.env.get_best_ans()\n        self.reward_list.append(reward)\n        self.total_reward += reward\n\n        if len(self.state_list) >= MULTI_STEP and isTrain:\n            self.append_sample(net, tgt_net, self.state_list[-MULTI_STEP], self.action_list[-MULTI_STEP],\n                               np.sum(self.reward_list[-MULTI_STEP:]), new_state, is_done)\n            if use_addition:\n                for _ in range(ADDITION_RATE_BOOST):\n                    self.append_sample(net, tgt_net, self.state_list[-MULTI_STEP], self.action_list[-MULTI_STEP],\n                                       np.sum(self.reward_list[-MULTI_STEP:]), new_state, is_done)\n        self.state = new_state\n        self.state_list.append(new_state)\n        best_sol = self.env.best_sol\n        random_list = []\n        if is_done or (hard_mask and np.sum(self.env.current_sol) == self.env.N) or (\n                ic and np.sum(self.env.best_sol) == inlier):\n            # done_reward = self.total_reward\n            done_reward = self.env.calc_score(self.env.best_sol, normalize=normalize)\n            random_list = copy.deepcopy(self.random_list)\n            # print(self.acc_list)\n            self._reset()\n        return done_reward, best_sol, random_list, pos\n\n\n# double dqn 参数更新\ndef calc_loss_double(net, tgt_net, device=\"cpu\"):\n    mini_batch, idxs, is_weights = agent.exp_buffer.sample(BATCH_SIZE)\n    mini_batch = np.array(mini_batch).transpose()\n    # states, actions, rewards, dones, next_states = batch\n    states = []\n    next_states = []\n    for i in range(len(mini_batch[0])):\n        states.append(np.vstack(mini_batch[0][i]))\n        next_states.append(np.vstack(mini_batch[3][i]))\n    states = np.array(states)\n    next_states = np.array(next_states)\n    actions = list(mini_batch[1])\n    rewards = list(mini_batch[2])\n    dones = np.array(mini_batch[4], dtype=int)\n\n    maxN = 0\n    for i in range(len(states)):\n        if int(np.sqrt(len(states[i]) - 2)) > maxN:\n            maxN = int(np.sqrt(len(states[i]) - 2))\n\n    new_states = []\n    for i in range(len(states)):\n        n = int(np.sqrt(len(states[i])))\n        old_sol = np.reshape(states[i][0], (n, n))\n        new_sol = np.pad(old_sol, ((0, maxN - n), (0, maxN - n)), 'constant', constant_values=0).reshape(1, maxN * maxN)\n        old_mask = np.reshape(states[i][1], (n, n))\n        new_mask = np.pad(old_mask, ((0, maxN - n), (0, maxN - n)), 'constant', constant_values=0).reshape(1,\n                                                                                                           maxN * maxN)\n        state = np.reshape(states[i][2:][:], (n, n, n, n))\n\n        new_K = np.pad(state, ((0, maxN - n), (0, maxN - n), (0, maxN - n), (0, maxN - n)), 'constant',\n                       constant_values=0)\n        new_K = np.reshape(new_K, (maxN * maxN, maxN * maxN))\n        new_state = np.concatenate((np.concatenate((new_sol, new_mask)), new_K))\n        new_states.append(new_state)\n        x = actions[i] // n\n        y = actions[i] % n\n        actions[i] = x * maxN + y\n\n    new_next_states = []\n    for i in range(len(next_states)):\n        n = int(np.sqrt(len(next_states[i])))\n        old_next_sol = np.reshape(next_states[i][0], (n, n))\n        new_next_sol = np.pad(old_next_sol, ((0, maxN - n), (0, maxN - n)), 'constant', constant_values=0).reshape(1,\n                                                                                                                   maxN * maxN)\n        old_next_mask = np.reshape(next_states[i][1], (n, n))\n        new_next_mask = np.pad(old_next_mask, ((0, maxN - n), (0, maxN - n)), 'constant', constant_values=0).reshape(1,\n                                                                                                                     maxN * maxN)\n\n        next_state = np.reshape(next_states[i][2:][:], (n, n, n, n))\n        new_next_K = np.pad(next_state, ((0, maxN - n), (0, maxN - n), (0, maxN - n), (0, maxN - n)), 'constant',\n                            constant_values=0)\n        new_next_K = np.reshape(new_next_K, (maxN * maxN, maxN * maxN))\n        new_next_state = np.concatenate((np.concatenate((new_next_sol, new_next_mask)), new_next_K))\n        new_next_states.append(new_next_state)\n\n    states_v = torch.FloatTensor(new_states).to(device)\n    next_states_v = torch.FloatTensor(new_next_states).to(device)\n    actions_v = torch.LongTensor(actions).to(device)\n    rewards_v = torch.FloatTensor(rewards).to(device)\n    done_mask = torch.ByteTensor(dones).to(device)\n\n    state_action_values = net(states_v, hard_mask=hard_mask, normalize=normalize).gather(1, actions_v.unsqueeze(\n        -1)).squeeze(-1)\n    next_state_selected = net(next_states_v, hard_mask=hard_mask, normalize=normalize).detach().argmax(1)\n    next_state_values = tgt_net(next_states_v, hard_mask=hard_mask, normalize=normalize).gather(1,\n                                                                                                next_state_selected.unsqueeze(\n                                                                                                    -1)).squeeze(-1)\n    next_state_values[done_mask] = 0.0\n    next_state_values = next_state_values.detach()\n\n    expected_state_action_values = next_state_values * GAMMA + rewards_v\n\n    errors = torch.abs(expected_state_action_values - state_action_values).detach().cpu().numpy()\n    for i in range(BATCH_SIZE):\n        idx = idxs[i]\n        agent.exp_buffer.update(idx, errors[i])\n\n    return (torch.FloatTensor(is_weights).to(device) * torch.nn.functional.mse_loss(\n        expected_state_action_values.to(device),\n        state_action_values.to(device))).mean()\n\n\nif __name__ == \"__main__\":\n    isTrain = True\n    cls = 'Car'\n    time0 = datetime.now()\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--cuda\", default=1, type=int, help=\"Enable cuda\")\n    parser.add_argument(\"--train\", default=isTrain, type=int, help=\"Training or just testing\")\n    parser.add_argument(\"--support\", default=0.0, type=float, help=\"Ratio of additional information\")\n    parser.add_argument(\"--cls\", default=cls, type=str)\n    parser.add_argument(\"--load_from_cls\", default=cls, type=str)\n    parser.add_argument(\"--gamma\", default=GAMMA, type=float)\n    parser.add_argument(\"--bs\", default=BATCH_SIZE, type=int)\n    parser.add_argument(\"--rs\", default=REPLAY_SIZE, type=int)\n    parser.add_argument(\"--lr\", default=LEARNING_RATE, type=float)\n    parser.add_argument(\"--sync\", default=SYNC_TARGET_FRAMES, type=int)\n    parser.add_argument(\"--ls\", default=REPLAY_START_SIZE, type=int)\n    parser.add_argument(\"--ms\", default=MULTI_STEP, type=int)\n    parser.add_argument(\"--ed\", default=EPSILON_DECAY_LAST_FRAME, type=int)\n    parser.add_argument(\"--eb\", default=EPSILON_BONUS, type=float)\n    parser.add_argument(\"--es\", default=EPSILON_START, type=float)\n    parser.add_argument(\"--ef\", default=EPSILON_FINAL, type=float)\n    parser.add_argument(\"--units\", default=UNITS, type=int)\n    parser.add_argument(\"--hs\", default=HIDDEN_SIZE, type=int)\n    parser.add_argument(\"--t\", default=T, type=int)\n    parser.add_argument(\"--ad\", default=ADDITION_RATE_DECAY, type=float)\n    parser.add_argument(\"--af\", default=ADDITION_RATE_FINAL, type=float)\n    parser.add_argument(\"--ab\", default=ADDITION_RATE_BOOST, type=int)\n    parser.add_argument(\"--b\", default=6, type=int)  # beam search size\n    parser.add_argument(\"--d\", default=0, type=int)  # whether to use the dynamic embedding\n    parser.add_argument(\"--outlier\", default=2, type=int)  # the number of outliers\n    parser.add_argument(\"--normalize\", default=1, type=int)  # whether to use the affinity regularization\n    parser.add_argument(\"--hard_mask\", default=1, type=int)  # whether to enable the revocable framework\n    parser.add_argument(\"--inlier_count\", default=0, type=int)  # whether to use the inlier count information\n    parser.add_argument(\"--inlier\", default=10, type=int)  # the inlier count information if used\n    args = parser.parse_args()\n    inlier = args.inlier\n    normalize = args.normalize\n    hard_mask = args.hard_mask\n    ic = args.inlier_count\n    isTrain = args.train\n    outlier = args.outlier\n    cls = args.cls\n    DYNAMIC = args.d\n    GAMMA = args.gamma\n    BATCH_SIZE = args.bs\n    REPLAY_SIZE = args.rs\n    LEARNING_RATE = args.lr\n    SYNC_TARGET_FRAMES = args.sync\n    REPLAY_START_SIZE = args.ls\n    MULTI_STEP = args.ms\n    EPSILON_DECAY_LAST_FRAME = args.ed\n    EPSILON_BONUS = args.eb\n    EPSILON_START = args.es\n    EPSILON_FINAL = args.ef\n    UNITS = args.units\n    HIDDEN_SIZE = args.hs\n    T = args.t\n    ADDITION_RATE_DECAY = args.ad\n    ADDITION_RATE_FINAL = args.af\n    ADDITION_RATE_BOOST = args.ab\n    b = args.b\n\n    args.save_dir = \"model/Willow-{}-{}-DQN-pytorch-{}-{}-{}-{}-{}-{}-{}-{}-{}-{}-{}-{}-{}-{}-{}-{}-{}-{}/\".format(\n        cls, DYNAMIC, GAMMA, BATCH_SIZE, REPLAY_SIZE, LEARNING_RATE, SYNC_TARGET_FRAMES, REPLAY_START_SIZE,\n        MULTI_STEP, EPSILON_DECAY_LAST_FRAME, EPSILON_BONUS, EPSILON_START, EPSILON_FINAL, UNITS,\n        HIDDEN_SIZE, T, args.support, ADDITION_RATE_DECAY, ADDITION_RATE_FINAL, ADDITION_RATE_BOOST)\n    args.log_dir = \"log/Willow-{}-{}/Willow-{}-DQN-pytorch-{}-{}-{}-{}-{}-{}-{}-{}-{}-{}-{}-{}-{}-{}-{}-{}-{}\".format(\n        cls, DYNAMIC, cls, GAMMA, BATCH_SIZE, REPLAY_SIZE, LEARNING_RATE, SYNC_TARGET_FRAMES, REPLAY_START_SIZE,\n        MULTI_STEP,",
  "description": "Combined Analysis:\n- [environment_qap_ag.py]: This file implements the core RL environment for the Quadratic Assignment Problem (QAP) formulation of graph matching. Key implementations include:\n1. Objective function: calc_score() computes vec(X)^T K vec(X) using torch operations (matching the QAP objective).\n2. Constraints: check_sol() enforces one-to-one matching constraints (X ∈ {0,1}^{n×n}, X1 ≤ 1, X^T1 ≤ 1).\n3. Revocable action mechanism: step() implements action revocation by clearing rows/columns when conflicts occur (lines 95-99).\n4. Sequential decision-making: The environment maintains state (current_sol) and provides step/reset interfaces for RL agents.\n5. Reward design: Rewards are computed as affinity score improvement (current_ans - old_ans).\n6. Association graph representation: Uses affinity matrix (am) to construct adjacency (ag_adjacent) for RL state representation.\nThe implementation directly corresponds to the paper's QAP formulation and RL framework, though affinity regularization is handled elsewhere (likely in the agent).\n- [run_rgm_pytorch_willow.py]: This file implements the main training and testing pipeline for the RGM algorithm. It contains the core reinforcement learning logic including: 1) The Agent class that interacts with the environment to sequentially select node correspondences (actions) using an ε-greedy policy, 2) The experience replay buffer for storing transitions, 3) The Double DQN loss calculation with prioritized experience replay, 4) The revocable action mechanism through the 'hard_mask' parameter that enables action reversal, 5) Affinity regularization through the 'normalize' parameter that modifies the reward structure. The code orchestrates the complete RL training loop (though truncated in the provided snippet) that implements the paper's key algorithm steps: sequential node matching with revocable actions and affinity-regularized rewards.",
  "dependencies": [
    "gym",
    "numpy",
    "copy",
    "prioritized_memory",
    "utils.gm_solver.get_norm_affinity",
    "utils.hungarian",
    "dqn_model_r",
    "collections",
    "utils.rrwm",
    "utils.gm_solver.get_aff_score_norm",
    "utils.rrwm_mask",
    "argparse",
    "torch",
    "tensorboardX",
    "matplotlib",
    "environment_qap_ag"
  ]
}