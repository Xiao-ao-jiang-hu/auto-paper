{
  "paper_id": "Backpropagation_through_Combinatorial_Algorithms_Identity_wi",
  "title": "Backpropagation Through Combinatorial Algorithms: Identity with Projection Works",
  "abstract": "Embedding discrete solvers as differentiable layers has given modern deep learning architectures combinatorial expressivity and discrete reasoning capabilities. The derivative of these solvers is zero or undefined, therefore a meaningful replacement is crucial for effective gradient-based learning. Prior works rely on smoothing the solver with input perturbations, relaxing the solver to continuous problems, or interpolating the loss landscape with techniques that typically require additional solver calls, introduce extra hyper-parameters, or compromise performance. We propose a principled approach to exploit the geometry of the discrete solution space to treat the solver as a negative identity on the backward pass and further provide a theoretical justification. Our experiments demonstrate that such a straightforward hyper-parameter-free approach is able to compete with previously more complex methods on numerous experiments such as backpropagation through discrete samplers, deep graph matching, and image retrieval. Furthermore, we substitute the previously proposed problem-specific and label-dependent margin with a generic regularization procedure that prevents cost collapse and increases robustness.",
  "problem_description_natural": "The paper addresses the challenge of end-to-end training of deep neural networks that include embedded discrete combinatorial solvers. These solvers output solutions to optimization problems of the form y(ω) = argmin_{y ∈ Y} ⟨ω, y⟩, where ω is a cost vector produced by a neural network and Y is a finite set of feasible discrete solutions (e.g., permutations, matchings, paths). Because the true gradient of such a solver is zero or undefined almost everywhere due to its piecewise-constant nature, standard backpropagation fails. The authors propose replacing the zero Jacobian with a 'negative identity' mapping during backpropagation—i.e., setting dℓ/dω = −dℓ/dy—and combine this with projections that respect invariances of the solver (e.g., invariance to constant shifts in cost for ranking problems). They also introduce noise-based regularization to prevent cost collapse and eliminate the need for ground-truth-dependent margins.",
  "problem_type": "combinatorial optimization",
  "datasets": [
    "MNIST",
    "BeerAdvocate",
    "Pascal VOC",
    "SPair-71k",
    "CUB-200-2011",
    "Globe TSP",
    "Warcraft Shortest Path"
  ],
  "performance_metrics": [
    "N-ELBO",
    "Test MSE",
    "Matching accuracy",
    "Recall R@1",
    "Test accuracy",
    "Path cost ratio"
  ],
  "lp_model": {
    "objective": "$\\min_{y \\in Y} \\langle \\omega, y \\rangle$",
    "constraints": [
      "$y \\in Y$",
      "where $Y \\subset \\mathbb{R}^n$ is a finite set of feasible solutions."
    ],
    "variables": [
      "$y$: a vector in $\\mathbb{R}^n$, the decision variable representing the solution.",
      "$\\omega$: a vector in $\\mathbb{R}^n$, the cost coefficients (input parameter)."
    ]
  },
  "raw_latex_model": "$$ y(\\omega) = \\arg\\min_{y \\in Y} \\langle \\omega, y \\rangle, $$ where $\\omega \\in W \\subseteq \\mathbb{R}^n$ is the cost vector and $Y \\subset \\mathbb{R}^n$ is a finite set of possible solutions.",
  "algorithm_description": "The paper proposes a hyperparameter-free method for backpropagation through combinatorial solvers that solve linear-cost optimization problems. The method, called Identity with Projection, treats the solver as a negative identity on the backward pass and uses projections (e.g., $P_{\\text{mean}}$, $P_{\\text{norm}}$, $P_{\\text{std}}$) to handle invariances in the cost vector, thereby providing informative gradients without additional solver calls."
}