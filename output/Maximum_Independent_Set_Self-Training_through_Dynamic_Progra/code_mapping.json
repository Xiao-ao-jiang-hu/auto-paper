{
  "file_path": "MIS/model.py, MIS/recursive_tree.py, MIS/train.py, MVC/model.py, MVC/recursion_tree.py, MVC/test.py",
  "function_name": "Comparator, build_tree, find_MIS_value, train_dataset, Comparator, test_dataset",
  "code_snippet": "\n\n# ==========================================\n# File: MIS/model.py\n# Function/Context: Comparator\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport warnings\nimport sys\n\n\nclass Comparator(nn.Module):\n\n    def __init__(self, D, device, num_gnn_layers=3, num_dense_layers=3):\n        super(Comparator, self).__init__()\n        assert num_dense_layers > 2  # For skip connection\n\n        self.d = D\n\n        self.gnn_f_layers = nn.ModuleList([\n            nn.Linear(3 * D, D) for _ in range(num_gnn_layers)\n        ])\n\n        self.gnn_s_layers = nn.ModuleList([\n            nn.Linear(3 * D, D) for _ in range(num_gnn_layers)\n        ])\n\n        self.gnn_a_layers = nn.ModuleList([\n            nn.Linear(3 * D, D) for _ in range(num_gnn_layers)\n        ])\n\n        self.gnn_layer_norms = nn.ModuleList([\n            nn.LayerNorm(3 * D) for _ in range(num_gnn_layers)\n        ])\n\n        self.dense_layers = nn.ModuleList()\n        for i in range(num_dense_layers):\n            if i == 0:\n                self.dense_layers.append(nn.Linear(3 * D, D))\n            elif i + 1 == num_dense_layers:\n                self.dense_layers.append(nn.Linear(2 * D, 1))\n\n            else:\n                self.dense_layers.append(nn.Linear(D, D))\n\n        self.dense_layer_norms = nn.ModuleList([\n            nn.LayerNorm(D) for _ in range(num_dense_layers - 1)\n        ])\n\n        self.to(device)\n\n    # Takes the adjecancy matrx A of the graph and computes the D-dimensional embedding of the graph\n\n    def Embed(self, A, device):\n        GELU = nn.GELU()\n\n        # Computes the complement (including self-loops, which we need anyway)\n        B = 1 - A\n\n        # Add self loops to original adjacency matrix!!!\n        A = A.fill_diagonal_(1)\n\n        n = A.shape[0]\n        X = torch.zeros(n, self.d * 3).to(device)\n\n        for i in range(len(self.gnn_a_layers)):\n            Y = self.gnn_f_layers[i](X)\n            Z = self.gnn_s_layers[i](X)\n            W = self.gnn_a_layers[i](X)\n\n            X = GELU(torch.cat((Z, A @ Y, B @ W), dim=-1))\n            X = self.gnn_layer_norms[i](X)\n\n        # Global pooling\n        X = torch.mean(X, dim=0)\n\n        return X\n\n    def forward(self, A, B, device):\n        warnings.filterwarnings(\"ignore\")\n        A = A.to(device)  # Add self loops, important!\n        B = B.to(device)  # Add self loops, important!\n        RELU = nn.ReLU()\n\n        X = self.Embed(A, device)  # compute the embedding of A\n        Y = self.Embed(B, device)  # compute the embedding of B\n\n        for i in range(len(self.dense_layers)):\n            if i + 1 == len(self.dense_layers):\n\n                X = torch.cat((X, X_0), dim=-1)\n                Y = torch.cat((Y, Y_0), dim=-1)\n\n            X = self.dense_layers[i](X)\n            Y = self.dense_layers[i](Y)\n            if i + 1 < len(self.dense_layers):\n\n                X = self.dense_layer_norms[i](RELU(X))\n                Y = self.dense_layer_norms[i](RELU(Y))\n            if i == 0:\n                X_0, Y_0 = X, Y\n        Z = torch.cat((X, Y), dim=0).reshape(1, 2)\n\n        m = torch.nn.Softmax()\n        return m(Z)  # Return the input to the FeedForward Net\n\n# ==========================================\n# File: MIS/recursive_tree.py\n# Function/Context: build_tree, find_MIS_value\n# ==========================================\nimport torch\nimport sys\nimport random\n\nfrom torch_geometric.data import Data\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.utils import to_networkx\nfrom tqdm import tqdm\n\nimport model\nimport warnings\nimport os\nimport math\nimport scipy.special\nimport networkx as nx\nimport pickle\nimport buffer\nimport numpy as np\nfrom heuristic_optimal_solvers import greedy_MIS\n\nrandom.seed(33)\n\n\nclass datasample:\n    def __init__(self, G1, G2, device):\n        self.G1 = G1\n        self.G2 = G2\n        self.target = self.create_target(device)\n\n    def create_target(self, device):\n        if self.G1.nodes[0]['MIS'] > self.G2.nodes[0]['MIS']:\n            return (torch.tensor([[1.0, 0.0]])).to(device)\n        elif self.G1.nodes[0]['MIS'] < self.G2.nodes[0]['MIS']:\n            return (torch.tensor([[0.0, 1.0]])).to(device)\n        else:\n            return (torch.tensor([[0.5, 0.5]])).to(device)\n        \n\n\ndef build_tree(G, cmp, device):\n    list_graphs = []\n    while 1:\n        non_indip_list = get_non_independent_set(G) \n        if not non_indip_list:  \n            break\n        node = random.choice(non_indip_list)  \n\n        # define G1 as G without the node 'node'\n        G1 = G.copy()\n        G1.remove_node(node)\n        G1 = nx.convert_node_labels_to_integers(G1)\n\n\n        # define G2 as G without the neighborhoods of 'node'\n        G2 = G.copy()\n        G2.remove_nodes_from([n for n in G.neighbors(node)])\n        G2 = nx.convert_node_labels_to_integers(G2)\n\n        with torch.no_grad():\n            G1_mat = torch.tensor(nx.adjacency_matrix(G1).todense(), dtype=torch.float).to(device)\n            G2_mat = torch.tensor(nx.adjacency_matrix(G2).todense(), dtype=torch.float).to(device)\n            index = torch.argmax(cmp.forward(G1_mat, G2_mat, device)).item()\n\n        if index == 0:\n            G = G1.copy()\n        else:\n            G = G2.copy()\n\n\n        list_graphs.append(G1)\n        list_graphs.append(G2)\n\n        \n    return list_graphs, G.number_of_nodes()\n\ndef find_MIS_value(cmp,adj,device):\n    while(1):\n\n        #get non-isaolated nodes\n        sum_rows = np.sum(adj,axis=1)\n        non_indep_nodes = np.nonzero(sum_rows)[0].tolist()\n        if not non_indep_nodes:  #stop if all the nodes are in the independent set\n            break\n\n        node = random.choice(non_indep_nodes) #pick a random node in G that is not in the independent set\n\n        #consider the graph without the node 'node'\n        adj_1 = np.delete(adj,node,axis=0)\n        adj_1 = np.delete(adj_1,node,axis=1)\n\n        #consider the graph without the neighbors of 'node'\n        list_neighbors = np.nonzero(adj[node])[0].tolist()\n        adj_2 = np.delete(adj,list_neighbors,axis=0)\n        adj_2 = np.delete(adj_2,list_neighbors,axis=1)\n\n        with torch.no_grad():\n            G1_mat = torch.tensor(adj_1.tolist(), dtype=torch.float).to(device)\n            G2_mat = torch.tensor(adj_2.tolist(), dtype=torch.float).to(device)\n            index = torch.argmax(cmp.forward(G1_mat, G2_mat, device)).item()\n\n        if index == 0:\n            adj = adj_1\n        else:\n            adj = adj_2\n\n    return adj.shape[0]\n\ndef get_non_independent_set(G):\n    list_indep = []\n    for n in G.nodes:\n        if nx.is_isolate(G, n) == False:\n            list_indep.append(n)\n    return list_indep\n\ndef run_several_times(list_graphs, times, cmp, device, mixed_rollout='False'):\n    for G in list_graphs:\n        \n        if mixed_rollout == 'True':\n            max_val = greedy_MIS(G)\n        elif mixed_rollout == 'False':\n            max_val = -1\n        else:\n            sys.exit('mixed_rollout has to be a string equal to either True or False')\n        for n in range(times):\n            val = find_MIS_value(cmp, nx.to_numpy_array(G), device)\n            if val > max_val:\n                max_val = val\n        nx.set_node_attributes(G, max_val, 'MIS')\n\n    return list_graphs\n\n\ndef create_list_datasamples(list_graphs, device, num_samples):\n    list_datasamples = []\n    for i in range(len(list_graphs)):\n        G1 = list_graphs[i]\n        for j in range(i + 1, len(list_graphs)):\n            if len(list_datasamples) == num_samples:\n                return list_datasamples\n            G2 = list_graphs[j]\n            list_datasamples.append(datasample(G1, G2, device))\n\n    return list_datasamples\n\ndef create_dataset_buffer(cmp, device, times, dim_datasamples, list_G,mixed_rollout='False',show_graphs_stats=True):\n    list_val = []\n    list_dataset = []\n\n    num = find_max_list_graph(dim_datasamples)\n\n    for G in list_G:\n        if show_graphs_stats:\n            print(f'Density: {G.number_of_edges() / (G.number_of_nodes() * (G.number_of_nodes() - 1)) * 2:<.3f}', G,sep=' | ')\n\n        list_graphs, val = build_tree(G, cmp, device)  \n        list_val.append(val)\n\n        if len(list_graphs) > num:\n            list_graphs = list_graphs[0:num]\n\n        list_graphs = run_several_times(list_graphs, times, cmp, device,mixed_rollout=mixed_rollout)\n\n        samples = create_list_datasamples(list_graphs, device, dim_datasamples)\n\n        if len(samples) > dim_datasamples:\n            list_dataset = list_dataset + samples[0:dim_datasamples]\n        else:\n            list_dataset = list_dataset + samples\n\n    return list_dataset, list_val\n\ndef find_max_list_graph(max_num):\n    i = 5\n    val = -1\n    while (val < max_num):\n        val = scipy.special.binom(i, 2)\n        i += 1\n    return i - 1\n\n# ==========================================\n# File: MIS/train.py\n# Function/Context: train_dataset\n# ==========================================\nimport torch\nimport matplotlib.pyplot as plt\nimport math\nimport random\nimport sys\nimport model\nimport networkx as nx\nimport time\nimport datetime\nimport os\nimport pickle\nimport numpy as np\nfrom recursive_tree import create_dataset_buffer\n\ndef create_output_target(cmp, list_datasamples, batch_size, ind, device):\n    G1 = list_datasamples[ind].G1\n    G2 = list_datasamples[ind].G2\n    G1_matrix = torch.tensor(nx.adjacency_matrix(G1).todense(), dtype=torch.float).to(device)\n    G2_matrix = torch.tensor(nx.adjacency_matrix(G2).todense(), dtype=torch.float).to(device)\n    output_vet = cmp.forward(G1_matrix, G2_matrix, device)\n    target_vet = list_datasamples[ind].target\n    ind += 1\n    for i in range(1, batch_size):\n        G1 = list_datasamples[ind].G1\n        G2 = list_datasamples[ind].G2\n        G1_matrix = torch.tensor(nx.adjacency_matrix(G1).todense(), dtype=torch.float).to(device)\n        G2_matrix = torch.tensor(nx.adjacency_matrix(G2).todense(), dtype=torch.float).to(device)\n        output = cmp.forward(G1_matrix, G2_matrix, device)\n        target = list_datasamples[ind].target\n        output_vet = torch.cat((output_vet, output), dim=0)\n        target_vet = torch.cat((target_vet, target), dim=0)\n        ind += 1\n    return output_vet, target_vet, ind\n\ndef calculate_accuracy(target_vet, output_vet):\n    accuracy = 0.0\n    cnt = 0\n    for target, output in zip(target_vet, output_vet):\n        if target[0] != 0.5:\n            output_class = torch.argmax(output)\n            target_class = torch.argmax(target)\n            if torch.sum(torch.abs(output_class - target_class)).item() == 0:\n                accuracy += 1.0\n            cnt += 1\n    return accuracy, cnt\n\ndef train_dataset(cmp, epochs_roll_out, optimizer, criterion, batch_size, buf, root_graphs_list, dim_datasamples,\n                 dim_dataset, root_graphs_per_iteration, times, device, D, gnn_depth, dense_depth, mixed_rollout='False', list_G_validation=None, dataset=''):\n    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    path = os.path.dirname(os.path.realpath(__file__))\n    model_path = os.path.join(path,'model_parameters') #path where I save the model parameters\n\n    cnt = 0\n    for i in range(0, len(root_graphs_list), root_graphs_per_iteration):\n\n        list_G = root_graphs_list[i:i + root_graphs_per_iteration]\n        list_datasamples, _ = create_dataset_buffer(cmp, device, times, dim_datasamples, list_G,mixed_rollout=mixed_rollout)\n\n        print('ITERATION NUMBER ' + str(cnt) )\n\n        buf.buffer_update(list_datasamples)\n\n        list_dataset = buf.get_samples(dim_dataset)\n\n        for epoch in range(epochs_roll_out):\n\n            num_it = math.floor(len(list_dataset) / batch_size)\n\n            ind = 0\n            loss_epoch = 0.0\n            accuracy = 0.0\n            den = 0\n            for j in range(num_it):\n                optimizer.zero_grad()\n                cmp.train()\n                output_vet, target_vet, ind = create_output_target(cmp, list_dataset, batch_size, ind, device)\n\n                loss = criterion(output_vet, target_vet)\n\n                loss_epoch += loss.item()\n                accuracy_,den_ = calculate_accuracy(target_vet, output_vet)\n                accuracy += accuracy_\n                den += den_ \n\n                loss.backward()\n                optimizer.step()\n                cmp.eval()\n\n            if (epoch+1)%10 == 0 and list_G_validation != None:\n                list_datasamples_validation, list_val_validation = create_dataset_buffer(cmp, device, times, np.floor(dim_datasamples/2), list_G_validation,mixed_rollout=mixed_rollout,show_graphs_stats=False)\n                ind = 0\n                output_vet_validation, target_vet_validation, ind = create_output_target(cmp, list_datasamples_validation, len(list_datasamples_validation), ind, device)\n                loss_validation = criterion(output_vet_validation,target_vet_validation)\n                loss_validation_ = loss_validation.item()\n                loss_validation_ = loss_validation_/len(output_vet_validation)\n                accuracy_validation,den_validation = calculate_accuracy(target_vet_validation,output_vet_validation)\n                accuracy_validation = 100*accuracy_validation / den_validation\n                indep_validation = np.array(list_val_validation).mean()\n\n            if den !=0 :\n                accuracy = 100 * accuracy / (den)\n                loss_epoch = loss_epoch/(num_it * batch_size)\n            \n            if (epoch+1)%10 == 0 and list_G_validation != None:\n                print(f'[EPOCH {epoch+1}]   train_loss: {loss_epoch:.4g} train_accuracy: {accuracy:.4g}    validation_loss: {loss_validation_:.4g} validation_accuracy: {accuracy_validation:.4g} validation_MIS: {indep_validation:.3g}')\n            else:\n                print(f'[EPOCH {epoch+1}]   train_loss: {loss_epoch:.4g} train_accuracy: {accuracy:.4g}')\n\n\n        if model_path !=  None:\n            stringa = ' dataset=' + str(dataset) + ' D=' + str(D) + ' gnn_depth=' + str(gnn_depth) + ' dense_depth=' + str(dense_depth) +' mixed_rollout='+str(mixed_rollout)+ 'dim_datasamples=' + str(dim_datasamples) + ' dim_dataset='+str(dim_dataset)+' root_graphs_per_iteration'+str(root_graphs_per_iteration)+ ' epochs='+str(epochs_roll_out)\n            save(cmp, model_path, timestamp,dataset,stringa)\n        cnt += 1\n\ndef save(cmp, out_file_path, timestamp,dataset,stringa=None):\n    filename = dataset + '_' + timestamp +'_param.pth'\n    path_ = os.path.join(out_file_path, filename)\n    torch.save(cmp.state_dict(), path_)\n\n    if stringa != None:\n        filename = dataset + '_' + timestamp + '_features.txt'\n        path_ = os.path.join(out_file_path, filename)\n        f = open(path_, 'w')\n        f.write(stringa)\n        f.close()\n\ndef load(cmp, file_path, device):\n    cmp.load_state_dict(torch.load(file_path, map_location=device))\n\n# ==========================================\n# File: MVC/model.py\n# Function/Context: Comparator\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport warnings\nimport sys\n\n\nclass Comparator(nn.Module):\n\n    def __init__(self, D, device, num_gnn_layers=3, num_dense_layers=3):\n        super(Comparator, self).__init__()\n        assert num_dense_layers > 2  # For skip connection\n\n        self.d = D\n\n        self.gnn_f_layers = nn.ModuleList([\n            nn.Linear(3 * D, D) for _ in range(num_gnn_layers)\n        ])\n\n        self.gnn_s_layers = nn.ModuleList([\n            nn.Linear(3 * D, D) for _ in range(num_gnn_layers)\n        ])\n\n        self.gnn_a_layers = nn.ModuleList([\n            nn.Linear(3 * D, D) for _ in range(num_gnn_layers)\n        ])\n\n        self.gnn_layer_norms = nn.ModuleList([\n            nn.LayerNorm(3 * D) for _ in range(num_gnn_layers)\n        ])\n\n        self.dense_layers = nn.ModuleList()\n        for i in range(num_dense_layers):\n            if i == 0:\n                self.dense_layers.append(nn.Linear(3 * D, D))\n            elif i + 1 == num_dense_layers:\n                self.dense_layers.append(nn.Linear(2 * D, 1))\n\n            else:\n                self.dense_layers.append(nn.Linear(D, D))\n\n        self.dense_layer_norms = nn.ModuleList([\n            nn.LayerNorm(D) for _ in range(num_dense_layers - 1)\n        ])\n\n        self.to(device)\n\n    # Takes the adjecancy matrx A of the graph and computes the D-dimensional embedding of the graph\n\n    def Embed(self, A, device):\n        GELU = nn.GELU()\n\n        # Computes the complement (including self-loops, which we need anyway)\n        B = 1 - A\n\n        # Add self loops to original adjacency matrix!!!\n        A = A.fill_diagonal_(1)\n\n        n = A.shape[0]\n        X = torch.zeros(n, self.d * 3).to(device)\n\n        for i in range(len(self.gnn_a_layers)):\n            Y = self.gnn_f_layers[i](X)\n            Z = self.gnn_s_layers[i](X)\n            W = self.gnn_a_layers[i](X)\n\n            X = GELU(torch.cat((Z, A @ Y, B @ W), dim=-1))\n            X = self.gnn_layer_norms[i](X)\n\n        # Global pooling\n        X = torch.mean(X, dim=0)\n\n        return X\n\n    def forward(self, A, B, device):\n        warnings.filterwarnings(\"ignore\")\n        A = A.to(device)  # Add self loops, important!\n        B = B.to(device)  # Add self loops, important!\n        RELU = nn.ReLU()\n\n        X = self.Embed(A, device)  # compute the embedding of A\n        Y = self.Embed(B, device)  # compute the embedding of B\n\n        for i in range(len(self.dense_layers)):\n            if i + 1 == len(self.dense_layers):\n\n                X = torch.cat((X, X_0), dim=-1)\n                Y = torch.cat((Y, Y_0), dim=-1)\n\n            X = self.dense_layers[i](X)\n            Y = self.dense_layers[i](Y)\n            if i + 1 < len(self.dense_layers):\n\n                X = self.dense_layer_norms[i](RELU(X))\n                Y = self.dense_layer_norms[i](RELU(Y))\n            if i == 0:\n                X_0, Y_0 = X, Y\n        Z = torch.cat((X, Y), dim=0).reshape(1, 2)\n\n        m = torch.nn.Softmax()\n        return m(Z)  # Return the input to the FeedForward Net\n\n# ==========================================\n# File: MVC/recursion_tree.py\n# Function/Context: \n# ==========================================\nimport torch\nimport sys\nimport random\n\nimport os\nimport scipy.special\nimport networkx as nx\nimport pickle\nimport numpy as np\n\nrandom.seed(33)\n\n\nclass datasample:\n    def __init__(self, G1, G2, device):\n        self.G1 = G1\n        self.G2 = G2\n        self.target = self.create_target(device)\n\n    def create_target(self, device):\n        if self.G1.nodes[0]['MVC'] > self.G2.nodes[0]['MVC']:\n            return (torch.tensor([[0.0, 1.0]])).to(device)\n        elif self.G1.nodes[0]['MVC'] < self.G2.nodes[0]['MVC']:\n            return (torch.tensor([[1.0, 0.0]])).to(device)\n        else:\n            return (torch.tensor([[0.5, 0.5]])).to(device)\n        \n\n\n\ndef build_tree(cmp,adj,device,flag_density = 'False'):\n    N = adj.shape[0] #original shape of the adjacency matrix\n    list_G = []\n    while(1):\n        #STEP 1: check if the while loop should stop and eventually pick a random node \n        list_nodes = node_list_for_MVC(adj,N)\n        if not list_nodes:\n            break\n        node = random.choice(list_nodes)\n\n        #STEP 2: create the two graphs (with 'node' and without 'node')\n\n        #create G1\n        G1 = np.pad(adj, ((0,1),(0,1)), mode='constant') #add new node\n        for i in range(N): #remove the edges \n            if G1[node][i] == 1:\n                G1[node][i] = 0\n                G1[i][node] = 0\n        G1[node][-1] = 1\n        G1[-1][node] = 1\n        grafo = nx.from_numpy_array(G1[:N,:N])\n        if flag_density == 'False':\n            list_G.append(grafo)\n        elif flag_density == 'True':\n            if nx.density(grafo) > 0.02:\n                list_G.append(grafo)\n        else:\n            sys.exit('flag_density has to be a string (not bool) and either True or False')\n        \n\n        #create G2\n        list_neighbors = np.nonzero(adj[node])[0].tolist()\n        G2 = np.copy(adj)\n        for n in list_neighbors:\n            G2 = np.pad(G2, ((0,1),(0,1)), mode='constant') #add new node\n            for i in range(N):\n                if G2[n][i] == 1:\n                    G2[n][i] = 0\n                    G2[i][n] = 0\n            G2[n][-1] = 1\n            G2[-1][n] = 1\n        grafo = nx.from_numpy_array(G2[:N,:N])\n        if flag_density == 'False':\n            list_G.append(grafo)\n        elif flag_density == 'True':\n            if nx.density(grafo) > 0.02:\n                list_G.append(grafo)\n        else:\n            sys.exit('flag_density has to be a string (not bool) and either True or False')\n        \n\n\n        #STEP 3: let the comparator choose\n        with torch.no_grad():\n            G1_torch = torch.tensor(G1.tolist(), dtype=torch.float).to(device)\n            G2_torch = torch.tensor(G2.tolist(), dtype=torch.float).to(device)\n            index = torch.argmax(cmp.forward(G1_torch, G2_torch, device)).item()\n\n\n        if index == 0:\n            adj = G1\n        else:\n            adj = G2\n\n    return list_G, np.sum(adj)/2\n\ndef find_MVC_value(cmp,adj,device):\n    N = adj.shape[0] #original shape of the adjacency matrix\n    while(1):\n        #STEP 1: check if the while loop should stop and eventually pick a random node \n        list_nodes = node_list_for_MVC(adj,N)\n        if not list_nodes:\n            break\n        node = random.choice(list_nodes)\n\n        #STEP 2: create the two graphs (with 'node' and without 'node')\n\n        #create G1\n        G1 = np.pad(adj, ((0,1),(0,1)), mode='constant') #add new node\n        for i in range(N): #remove the edges \n            if G1[node][i] == 1:\n                G1[node][i] = 0\n                G1[i][node] = 0\n        G1[node][-1] = 1\n        G1[-1][node] = 1\n\n        #create G2\n        list_neighbors = np.nonzero(adj[node])[0].tolist()\n        G2 = np.copy(adj)\n        for n in list_neighbors:\n            G2 = np.pad(G2, ((0,1),(0,1)), mode='constant') #add new node\n            for i in range(N):\n                if G2[n][i] == 1:\n                    G2[n][i] = 0\n                    G2[i][n] = 0\n            G2[n][-1] = 1\n            G2[-1][n] = 1\n\n        #STEP 3: let the comparator choose\n        with torch.no_grad():\n            G1_torch = torch.tensor(G1.tolist(), dtype=torch.float).to(device)\n            G2_torch = torch.tensor(G2.tolist(), dtype=torch.float).to(device)\n            index = torch.argmax(cmp.forward(G1_torch, G2_torch, device)).item()\n\n        if index == 0:\n            adj = G1\n        else:\n            adj = G2\n\n    return np.sum(adj)/2\n\ndef node_list_for_MVC(adj,N):\n    list_for_MVC = []\n    for i in range(N):\n        val = np.sum(adj[i])\n        if val >= 2:\n            list_for_MVC.append(i)\n        elif val == 1:\n            idx_neigh = np.argmax(adj[i])\n            val_neigh = np.sum(adj[idx_neigh])\n            if val_neigh > 1:\n                list_for_MVC.append(i)\n            elif val_neigh == 0:\n                sys.exit('Not possible for a node to have a neighbor without neighbors')\n    return list_for_MVC\n\n        \ndef run_several_times(list_graphs, times, cmp, device):\n    cnt = 0\n    for G in list_graphs:\n        min_val = G.number_of_nodes()*2\n\n        for n in range(times):\n            val = find_MVC_value(cmp, nx.to_numpy_array(G), device)\n            if val < min_val:\n                min_val = val\n        nx.set_node_attributes(G, min_val, 'MVC')\n        cnt += 1\n\n    return list_graphs\n\ndef create_dataset_buffer(cmp, device, times, dim_datasamples, list_G,show_graphs_stats = True,flag_density = 'False'):\n    list_val = []\n    list_dataset = []\n\n    num = find_max_list_graph(dim_datasamples)\n    if flag_density == 'True':\n        num = int(num*2)\n\n    for G in list_G:\n        if show_graphs_stats == True:\n            print(f'Density: {G.number_of_edges() / (G.number_of_nodes() * (G.number_of_nodes() - 1)) * 2:<.3f}', G,\n                    sep=' | ')\n\n\n        list_graphs, val = build_tree(cmp,nx.to_numpy_array(G),device,flag_density)  # the list contains all the intermediate graphs\n        list_val.append(val)\n\n        if len(list_graphs) > num:\n            list_graphs = list_graphs[0:num]\n\n        list_graphs = run_several_times(list_graphs, times, cmp, device)\n\n        samples = create_list_datasamples(list_graphs, device, dim_datasamples)\n\n        if len(samples) > dim_datasamples:\n            list_dataset = list_dataset + samples[0:dim_datasamples]\n        else:\n            list_dataset = list_dataset + samples\n\n\n    return list_dataset, list_val\n\n\ndef create_list_datasamples(list_graphs, device, num_samples):\n    list_datasamples = []\n    for i in range(len(list_graphs)):\n        G1 = list_graphs[i]\n        for j in range(i + 1, len(list_graphs)):\n            if len(list_datasamples) == num_samples:\n                return list_datasamples\n            G2 = list_graphs[j]\n            list_datasamples.append(datasample(G1, G2, device))\n\n    return list_datasamples\n\n\ndef find_max_list_graph(max_num):\n    i = 5\n    val = -1\n    while (val < max_num):\n        val = scipy.special.binom(i, 2)\n        i += 1\n    return i - 1\n\n# ==========================================\n# File: MVC/test.py\n# Function/Context: test_dataset\n# ==========================================\nimport matplotlib.pyplot as plt\nfrom ortools.linear_solver import pywraplp\nfrom tqdm import tqdm\n\nimport sys\nimport model\nimport networkx as nx\nimport datetime\nimport os\nimport numpy as np\nimport time\n\nfrom recursion_tree import find_MVC_value\nfrom heuristic_optimal_solvers import max_degree_heuristic\nfrom heuristic_optimal_solvers import solve_mvc_ilp\nfrom utils import load_benchmark_data\n\n\ndef test_dataset(cmp_buffer, device, D, dataset_name,optimum_mips=None, dataset_path=None):\n    idx0, idx1 = get_indeces_dataset(dataset_name)\n    list_G = load_benchmark_data(dataset_name, idxs=(idx0, idx1),dataset_path=dataset_path)\n\n    gur_solver = pywraplp.Solver.CreateSolver('GUROBI')\n\n    list_random = []\n    list_model_buffer = []\n    list_max_degree = []\n\n    list_ortools_1 = []\n    list_ortools_5 = []\n    list_gurobi_05 = []\n    list_gurobi_1 = []\n    list_gurobi_5 = []\n\n    list_time_random = []\n    list_time_buffer = []\n    list_time_max_degree = []\n\n    pbar = tqdm(enumerate(list_G))\n    for i, G in pbar:\n        pbar.set_description(f'|V|={G.number_of_nodes()}, |E|={G.number_of_edges()}')\n        start = time.time()\n        # FIND MVC FOR RANDOM COMPARATOR\n        cmp_random = model.Comparator(D, device)\n        val_random = find_MVC_value(cmp_random, nx.to_numpy_array(G), device)\n        end = time.time()\n        list_time_random.append(end - start)\n        \n        start = time.time()\n        # FIND MVC FOR MAX_DEGREE\n        val_max_degree = max_degree_heuristic(nx.to_numpy_array(G))\n        end = time.time()\n        list_time_max_degree.append((end - start))\n        \n        start = time.time()\n        # FIND MVC FOR THE MODEL_BUFFER\n        val_model_buffer = find_MVC_value(cmp_buffer, nx.to_numpy_array(G), device)\n        end = time.time()\n        list_time_buffer.append((end - start))\n        \n        \n        # FIND MVC FOR OR and GUROBI (FIX TIME)\n        or_1, _ = solve_mvc_ilp(G, time_limit_milliseconds=1000, mode='SCIP')\n        or_5, _ = solve_mvc_ilp(G, time_limit_milliseconds=5000, mode='SCIP')\n        gur_05, _ = solve_mvc_ilp(G, time_limit_milliseconds=500, solver=gur_solver)\n        gur_1, _ = solve_mvc_ilp(G, time_limit_milliseconds=1000, solver=gur_solver)\n        gur_5, _ = solve_mvc_ilp(G, time_limit_milliseconds=5000, solver=gur_solver)\n\n        if optimum_mips is not None:\n\n            val_random /= optimum_mips[i]\n            val_max_degree /= optimum_mips[i]\n            val_model_buffer /= optimum_mips[i]\n                        \n            or_1 /= optimum_mips[i]\n            or_5 /= optimum_mips[i]\n\n            gur_05 /= optimum_mips[i]\n            gur_1 /= optimum_mips[i]\n            gur_5 /= optimum_mips[i]\n\n        list_random.append(val_random)\n        list_max_degree.append(val_max_degree)\n        list_model_buffer.append(val_model_buffer)\n                \n        list_ortools_1.append(or_1)\n        list_ortools_5.append(or_5)\n        list_gurobi_05.append(gur_05)\n        list_gurobi_1.append(gur_1)\n        list_gurobi_5.append(gur_5)\n\n    print(f\"Mean ratio for random: {(np.array(list_random)).mean()} +/-  {(np.array(list_random)).std()}\")\n    print(f\"Mean ratio for max_degree: {(np.array(list_max_degree)).mean()} +/-  {(np.array(list_max_degree)).std()}\")\n    print(f\"Mean ratio for model_buffer: {(np.array(list_model_buffer)).mean()} +/-  {(np.array(list_model_buffer)).std()}\")\n\n\n    print(f\"Mean ratio for or_1: {(np.array(list_ortools_1)).mean()} +/-  {(np.array(list_ortools_1)).std()}\")\n    print(f\"Mean ratio for or_5: {(np.array(list_ortools_5)).mean()} +/-  {(np.array(list_ortools_5)).std()}\")\n\n    print(f\"Mean ratio for gur_05: {(np.array(list_gurobi_05)).mean()} +/-  {(np.array(list_gurobi_05)).std()}\")\n    print(f\"Mean ratio for gur_1: {(np.array(list_gurobi_1)).mean()} +/-  {(np.array(list_gurobi_1)).std()}\")\n    print(f\"Mean ratio for gur_5: {(np.array(list_gurobi_5)).mean()} +/-  {(np.array(list_gurobi_5)).std()}\")\n\n    print('random: (' + str(np.array(list_time_random).mean()) + 's/g)')\n    print('max_degree: (' + str(np.array(list_time_max_degree).mean()) + 's/g)')\n    print('buffer: (' + str(np.array(list_time_buffer).mean()) + 's/g)')\n\n\n\ndef get_indeces_dataset(dataset_name):\n    if dataset_name == 'RB200' or dataset_name == 'RB500':\n        idx0 = 0\n        idx1 = 100\n    else:\n        sys.exit('The provided dataset_name is not allowed')\n\n    return idx0,idx1",
  "description": "Combined Analysis:\n- [MIS/model.py]: This file implements the core GNN-based comparator function described in the paper. The Comparator class learns to predict which of two subgraphs has a larger Maximum Independent Set (MIS). It uses a graph neural network (GNN) architecture with three parallel linear transformations (f, s, a) to process node features, adjacency matrix, and complement adjacency matrix. The forward method takes two adjacency matrices (A, B) representing subgraphs and outputs a softmax probability indicating which graph likely has a larger MIS. This comparator is the key learned component used in the dynamic programming algorithm to recursively select vertices for the independent set.\n- [MIS/recursive_tree.py]: This file implements the core recursive dynamic programming algorithm for Maximum Independent Set (MIS) as described in the paper. The key functions are:\n1. build_tree(): Implements Algorithm 1's recursive process - at each step, selects a non-isolated node, creates two subgraphs (G1 without the node, G2 without its neighbors), uses a learned comparator (cmp) to choose which subgraph to pursue, and continues recursively until an independent set is found.\n2. find_MIS_value(): Similar recursive algorithm working directly on adjacency matrices.\n3. The datasample class creates training pairs by comparing MIS sizes of two graphs, enabling self-training.\n4. create_dataset_buffer() orchestrates the self-training process: generates subgraphs via build_tree(), estimates their MIS values via run_several_times(), and creates training pairs.\nThe implementation directly maps to the paper's mathematical model: the recursive branching corresponds to the DP formulation where at each step we choose between including/excluding a vertex (via removing its neighbors vs. removing the vertex itself). The comparator function (cmp.forward) learns to predict which branch leads to larger MIS, implementing the paper's key innovation of learned DP decisions.\n- [MIS/train.py]: This file implements the core self-training loop for the MIS comparator model. The train_dataset function orchestrates the iterative self-training process: (1) Generates training data via create_dataset_buffer (which implements the recursive DP algorithm from the paper), (2) Updates a replay buffer with self-generated labeled graph pairs, (3) Trains the comparator model on batches from this buffer to predict which subgraph has larger MIS. The code directly implements the self-training methodology where the model's own predictions during roll-outs generate training labels, avoiding ground-truth MIS annotations. Key functions: create_output_target prepares graph pairs for model inference, calculate_accuracy evaluates comparator predictions, and save/load handle model persistence.\n- [MVC/model.py]: This file implements the core GNN-based comparator function described in the paper's self-training framework for the Maximum Independent Set problem. The Comparator class learns to predict which of two subgraphs (A or B) has a larger MIS size by computing graph embeddings through a specialized GNN architecture (Embed method) and comparing them via a dense network with skip connections. This comparator is the key learned component that guides the dynamic programming-inspired recursive algorithm by deciding whether to remove a vertex or its neighbors at each step. The implementation matches the paper's description of using graph embeddings and a comparator network for self-training without ground-truth labels.\n- [MVC/recursion_tree.py]: This file implements the core recursive dynamic programming algorithm for the Minimum Vertex Cover (MVC) problem, which is complementary to the Maximum Independent Set (MIS) problem addressed in the paper. The algorithm mirrors the self-training DP approach described in the paper: at each step, a vertex is selected, and two subgraphs are created (representing different choices in the cover selection). A learned comparator function (cmp) is used to predict which subgraph has a smaller MVC value, guiding the recursion. Functions like 'build_tree' and 'find_MVC_value' encapsulate the recursive decision process, while helper functions manage data generation and training. The implementation adapts the MIS algorithm to MVC by comparing MVC sizes instead of MIS sizes, maintaining the same self-training structure through graph comparisons and recursive subgraph exploration.\n- [MVC/test.py]: This file implements the core testing logic for the Minimum Vertex Cover (MVC) problem, which is the complement of Maximum Independent Set (MIS). The test_dataset function evaluates the learned comparator model against multiple baselines: random comparator, max-degree heuristic, and exact solvers (OR-Tools/SCIP and Gurobi) with time limits. The key algorithm step is implemented via find_MVC_value which performs the recursive dynamic programming algorithm using the comparator to decide between subgraphs. This directly corresponds to Algorithm 1 in the paper, where at each recursion step the comparator chooses between removing a vertex or its neighbors. The file measures solution quality (normalized by optimal when available) and computation time, providing comprehensive evaluation metrics for the self-trained model.",
  "dependencies": [
    "build_tree",
    "find_MIS_value",
    "heuristic_optimal_solvers.max_degree_heuristic",
    "networkx",
    "scipy.special",
    "pickle",
    "sys",
    "utils.load_benchmark_data",
    "torch.optim",
    "tqdm.tqdm",
    "matplotlib.pyplot",
    "time",
    "recursive_tree.create_dataset_buffer",
    "get_non_independent_set",
    "torch_geometric.utils.to_networkx",
    "find_MVC_value",
    "numpy as np",
    "torch.nn.functional",
    "node_list_for_MVC",
    "warnings",
    "create_dataset_buffer",
    "tqdm",
    "math",
    "model",
    "os",
    "datetime",
    "datasample class",
    "torch_geometric.datasets.TUDataset",
    "random",
    "create_list_datasamples",
    "numpy",
    "torch_geometric.data.Data",
    "networkx as nx",
    "heuristic_optimal_solvers.solve_mvc_ilp",
    "find_max_list_graph",
    "torch.nn",
    "recursion_tree.find_MVC_value",
    "heuristic_optimal_solvers.greedy_MIS",
    "run_several_times",
    "torch",
    "buffer",
    "datasample",
    "ortools.linear_solver"
  ]
}