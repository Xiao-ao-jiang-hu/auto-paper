{
  "file_path": "src/loss.py, src/model.py, src/solver.py, src/trainer.py, src/utils.py",
  "function_name": "HGNN_ATT, centralized_solver, centralized_train, mapping_distribution",
  "code_snippet": "\n\n# ==========================================\n# File: src/loss.py\n# Function/Context: \n# ==========================================\nimport torch\nimport torch.nn.functional as F\nimport timeit\nimport numpy as np\n\ndef loss_maxcut_weighted(probs, C, weights, penalty_inc, penalty_c, hyper):\n    x = probs.squeeze()\n    loss = 0\n    penalty = 0\n    for c, w in zip(C, weights):\n        temp_1s = 1\n        temp_0s = 1\n        if hyper:\n            for index in c:\n                temp_1s *= (1 - x[index-1])\n                temp_0s *= (x[index-1])\n        else:\n            for index in c[0:2]:\n                temp_1s *= (1 - x[index-1])\n                temp_0s *= (x[index-1])\n        temp = (temp_1s + temp_0s)\n        loss += (temp * w)\n    if penalty_inc:\n        penalty = torch.sum(torch.min((1 - x), x))\n        loss += penalty_c * penalty\n    return loss\n\ndef loss_maxind_weighted(probs, C, dct, weights):\n    p = 4\n    x = probs.squeeze()\n    loss = - sum(x)\n    for c, w in zip(C, weights):\n        temp = (p * w * x[dct[c[0]]] * x[dct[c[1]]])\n        loss += (temp)\n    return loss\n\ndef loss_sat_weighted(probs, C, dct, weights):\n    x = probs.squeeze()\n    loss = 0\n    for c, w in zip(C, weights):\n        temp = 1\n        for index in c:\n            if index > 0:\n                temp *= (1 - x[dct[abs(index)]])\n            else:\n                temp *= (x[dct[abs(index)]])\n        loss += (temp * w)\n    return loss\n\ndef loss_cal_and_update(optimizer, aggregated, params, dcts, weights, info, i, timer, fixed):\n    temp_time = timeit.default_timer()\n    probs = []\n    for node in dcts:\n        if node == i:\n            probs.append(aggregated[node].clone())\n            prob_index_self = len(probs) - 1\n        else:\n            probs.append(aggregated[node].clone().detach())\n    probs = torch.cat(probs).squeeze()\n    probs = torch.sigmoid(probs)\n    if params['mode'] == 'sat':\n        loss = loss_sat_weighted(probs, info, dcts, weights)\n    elif params['mode'] == 'maxcut':\n        loss = loss_maxcut_weighted(probs, info, weights, params['penalty_inc'], params['penalty_c'], params['hyper'])\n    elif params['mode'] == 'maxind':\n        loss = loss_maxind_weighted(probs, info, dcts, weights)\n    timer.loss_calculate += (timeit.default_timer() - temp_time)\n    temp_time = timeit.default_timer()\n    if fixed:\n        res = probs[prob_index_self].clone().detach().item()\n        return res, loss.detach().item()\n    optimizer.zero_grad()\n    loss.backward(retain_graph=True)\n    optimizer.step()\n    res = probs[prob_index_self].clone().detach().item()\n    timer.loss_update += (timeit.default_timer() - temp_time)\n    return res, loss.detach().item()\n\n# ==========================================\n# File: src/model.py\n# Function/Context: HGNN_ATT\n# ==========================================\nfrom torch.nn.parameter import Parameter\nimport torch\nimport math\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\n\nclass HyperGraphAttentionLayerSparse(nn.Module):\n    def __init__(self, in_features, out_features, dropout, alpha, transfer=True, concat=True, bias=False):\n        super(HyperGraphAttentionLayerSparse, self).__init__()\n        self.dropout = dropout\n        self.in_features = in_features\n        self.out_features = out_features\n        self.alpha = alpha\n        self.concat = concat\n        self.transfer = transfer\n        \n        if self.transfer:\n            self.weight = Parameter(torch.Tensor(self.in_features, self.out_features))\n        else:\n            self.register_parameter('weight', None)\n        \n        self.weight2 = Parameter(torch.Tensor(self.in_features, self.out_features))\n        self.weight3 = Parameter(torch.Tensor(self.out_features, self.out_features))\n        \n        if bias:\n            self.bias = Parameter(torch.Tensor(self.out_features))\n        else:\n            self.register_parameter('bias', None)\n        \n        self.word_context = nn.Embedding(1, self.out_features)\n        self.a = nn.Parameter(torch.zeros(size=(2 * out_features, 1)))\n        self.a2 = nn.Parameter(torch.zeros(size=(2 * out_features, 1)))\n        self.leakyrelu = nn.LeakyReLU(self.alpha)\n        self.reset_parameters()\n    \n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.out_features)\n        if self.weight is not None:\n            self.weight.data.uniform_(-stdv, stdv)\n        self.weight2.data.uniform_(-stdv, stdv)\n        self.weight3.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.uniform_(-stdv, stdv)\n        nn.init.uniform_(self.a.data, -stdv, stdv)\n        nn.init.uniform_(self.a2.data, -stdv, stdv)\n        nn.init.uniform_(self.word_context.weight.data, -stdv, stdv)\n    \n    def forward(self, x, adj):\n        x_4att = x.matmul(self.weight2)\n        if self.transfer:\n            x = x.matmul(self.weight)\n            if self.bias is not None:\n                x = x + self.bias\n        \n        N1 = adj.shape[1]\n        N2 = adj.shape[0]\n        pair = adj.nonzero().t()\n        \n        q1 = self.word_context.weight[0:].view(1, -1).repeat(x_4att.shape[0], 1).view(x_4att.shape[0], self.out_features)\n        pair_h = torch.cat((q1, x_4att), dim=-1)\n        pair_e = self.leakyrelu(torch.matmul(pair_h, self.a).squeeze()).t()\n        assert not torch.isnan(pair_e).any()\n        pair_e = F.dropout(pair_e, self.dropout, training=self.training)\n        \n        e = adj * pair_e.repeat(N1,1).t()\n        zero_vec = -9e15 * torch.ones_like(e)\n        attention = torch.where(adj > 0, e, zero_vec)\n        attention_edge = F.softmax(attention, dim=0)\n        edge = torch.matmul(attention_edge.t(), x)\n        edge = F.dropout(edge, self.dropout, training=self.training)\n        edge_4att = edge.matmul(self.weight3)\n        \n        pair_h = torch.cat((x_4att[:, None, :].expand(-1, N1, -1), edge_4att[None, :, :].expand(N2, -1, -1)), dim=2)\n        pair_e = self.leakyrelu(torch.matmul(pair_h, self.a2).squeeze())\n        assert not torch.isnan(pair_e).any()\n        pair_e = F.dropout(pair_e, self.dropout, training=self.training)\n        \n        e = adj * pair_e\n        zero_vec = -9e15 * torch.ones_like(e)\n        attention = torch.where(adj > 0, e, zero_vec)\n        attention_node = F.softmax(attention, dim=1)\n        node = torch.matmul(attention_node, edge)\n        \n        if self.concat:\n            node=torch.relu(node)\n        return node\n\nclass HGNN_ATT(nn.Module):\n    def __init__(self, n, input_size, n_hid, output_size, params):\n        super(HGNN_ATT, self).__init__()\n        self.dropout = params[\"dropout\"]\n        self.embedding=nn.Embedding(n,input_size)\n        self.gat1 = HyperGraphAttentionLayerSparse(input_size, n_hid, dropout=self.dropout, alpha=0.2,\n                                                   transfer=True, concat=True)\n        self.gat2 = HyperGraphAttentionLayerSparse(n_hid, output_size, dropout=self.dropout, alpha=0.2,\n                                                   transfer=True, concat=False)\n        self.optimizer = torch.optim.Adam(self.parameters(), lr=params['lr'])\n        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=3, gamma=0.1)\n    \n    def forward(self, H):\n        x = self.embedding.weight\n        x = self.gat1(x, H)\n        x = self.gat2(x, H)\n        x = torch.sigmoid(x)\n        return x\n\n# ==========================================\n# File: src/solver.py\n# Function/Context: centralized_solver\n# ==========================================\nfrom src.utils import generate_H_from_edges, _generate_G_from_H, generate_H_from_constraints, all_to_weights, all_to_weights_task, gen_q_mis, get_normalized_G_from_con, Maxind_postprocessing, sparsify_graph\nimport numpy as np\nimport torch\nfrom src.timer import Timer\nimport timeit\nfrom src.trainer import centralized_train, GD_train, centralized_train_for, centralized_train_vec, centralized_train_att, centralized_train_bipartite, centralized_train_cliquegraph, centralized_train_coarsen, centralized_train_multi_gpu\nfrom src.loss import loss_maxcut_numpy_boost, loss_sat_numpy_boost, loss_maxind_numpy_boost, loss_maxind_QUBO, loss_task_numpy, loss_task_numpy_vec, loss_mincut_numpy_boost, loss_watermark, loss_partition_numpy, loss_partition_numpy_boost\nimport matplotlib.pyplot as plt\nimport random\nimport os\nimport networkx as nx\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict, defaultdict\nfrom itertools import chain, islice, combinations\nfrom networkx.algorithms import maximal_independent_set as mis\nfrom time import time\nfrom src.data_reading import read_uf, read_stanford, read_hypergraph\nimport logging\nfrom src.QUBO_utils import generate_graph, get_gnn, run_gnn_training, qubo_dict_to_torch, gen_combinations, loss_func\nfrom src.coarsen import coarsen1, coarsen2, coarsen3, coarsen4, coarsen5\n\ndef centralized_solver(constraints, header, params, file_name):\n    temp_time = timeit.default_timer()\n\n    if params['coarsen']:\n        new_header, new_constraints, graph_dict = coarsen5(constraints, header)\n        n_org = header['num_nodes']\n        n = new_header['num_nodes']\n        q_torch = gen_q_mis(constraints, n_org, 2, torch_dtype=None, torch_device=None)\n    else:\n        graph_dict = {}\n        new_header, new_constraints = header, constraints\n        n = header['num_nodes']\n        q_torch = gen_q_mis(constraints, n, 2, torch_dtype=None, torch_device=None)\n\n    if params['f_input']:\n        f = params['f']\n    else:\n        f = int(np.sqrt(n))\n\n    if params['data'] == 'bipartite':\n        path = params['folder_path_hyper'] + file_name[:-14] + '.txt'\n        constraints_hyper, header_hyper = read_hypergraph(path)\n        n_hyper = header_hyper['num_nodes']\n    elif params['data'] == 'cliquegraph':\n        path = params['folder_path_hyper'] + file_name[:-10] + '.txt'\n        constraints_hyper, header_hyper = read_hypergraph(path)\n        n_hyper = header_hyper['num_nodes']\n\n    if params['coarsen']:\n        info = {x + 1: [] for x in range(n_org)}\n    else:\n        info = {x + 1: [] for x in range(n)}\n    for constraint in constraints:\n        if params['data'] == 'task':\n            for node in constraint[:-1]:\n                info[abs(node)].append(constraint)\n        else:\n            for node in constraint:\n                info[abs(node)].append(constraint)\n\n    if params['data'] == 'bipartite' or params['data'] == 'cliquegraph':\n        info_hyper = {x + 1: [] for x in range(n_hyper)}\n        for constraint in constraints_hyper:\n            if params['data'] == 'task':\n                for node in constraint[:-1]:\n                    info_hyper[abs(node)].append(constraint)\n            else:\n                for node in constraint:\n                    info_hyper[abs(node)].append(constraint)\n\n    sparsify = params['sparcify']\n    spars_p = params['sparcify_p']\n    if sparsify:\n        if not params['coarsen']:\n            constraints_sparse, header_sparse, info_sparse = sparsify_graph(constraints, header, info, spars_p)\n        else:\n            constraints_sparse, header_sparse, info_sparse = new_constraints, new_header, info\n    elif params['coarsen']:\n        constraints_sparse, header_sparse, info_sparse = new_constraints, new_header, info\n    else:\n        constraints_sparse, header_sparse, info_sparse = constraints, header, info\n\n    if params['data'] != 'task':\n        edges = [[abs(x) - 1 for x in edge] for edge in constraints_sparse]\n    else:\n        edges = [[abs(x) - 1 for x in edge[:-1]] for edge in constraints_sparse]\n\n    load = False\n    if params['random_init'] == 'none' and not load:\n        if params['data'] != 'hypergraph' and params['data'] != 'task' and params['data'] != 'uf' and params['data'] != 'NDC':\n            G = get_normalized_G_from_con(constraints_sparse, header_sparse)\n        else:\n            H = generate_H_from_edges(edges, n)\n            G = _generate_G_from_H(H)\n            name_g = \"./models/G/\" + params['mode'] + '_' + file_name[:-4] + \".npy\"\n            with open(name_g, 'wb') as ffff:\n                np.save(ffff, G)\n            G = torch.from_numpy(G).float()\n    elif load:\n        name_g = \"./models/G/\" + params['mode'] + '_' + file_name[:-4] + \".npy\"\n        G = np.load(name_g)\n        G = torch.from_numpy(G).float()\n    else:\n        G = torch.zeros([n, n])\n\n    reses = []\n    reses_th = []\n    probs = []\n    train_times = []\n    map_times = []\n    if params['mode'] == 'task_vec':\n        L = len(constraints)\n    else:\n        L = params['n_partitions']\n\n    for i in range(params['K']):\n        if params['mode'] == 'task_vec':\n            C_dic = {}\n            ic = 0\n            lenc = torch.zeros([L])\n            for c in constraints:\n                lenc[ic] = len(c)\n                C_dic[str(c)] = ic\n                ic += 1\n            leninfo = torch.zeros([n])\n            for inn in range(n):\n                leninfo[inn] = len(info[inn + 1])\n            res, prob, train_time, map_time = centralized_train_vec(G, params, constraints, n, info, file_name, L)\n            train_times.append(train_time)\n            map_times.append(map_time)\n        elif params['coarsen']:\n            res, prob, train_time, map_time = centralized_train_coarsen(G, params, f, new_constraints, constraints, graph_dict, n_org, n, info, file_name)\n        elif params['mode'] == 'partition' or params['mode'] == 'MNP':\n            if params['mode'] == 'MNP':\n                L = params['n_knapsacks'] + 1\n            res, prob, train_time, map_time = centralized_train_vec(G, params, constraints, n, info, file_name, L)\n            train_times.append(train_time)\n            map_times.append(map_time)\n        elif params['data'] == 'bipartite':\n            res, prob, train_time, map_time = centralized_train_bipartite(G, params, f, constraints_hyper, n, n_hyper, info_hyper, file_name)\n            train_times.append(train_time)\n            map_times.append(map_time)\n        elif params['data'] == 'cliquegraph':\n            res, prob, train_time, map_time = centralized_train_cliquegraph(G, params, f, constraints_hyper, n, info_hyper, file_name)\n            train_times.append(train_time)\n            map_times.append(map_time)\n        elif not params['GD'] and not params['Att']:\n            res, prob, train_time, map_time = centralized_train(G, params, f, constraints, n, info, file_name)\n            train_times.append(train_time)\n            map_times.append(map_time)\n        elif params['Att']:\n            res, prob, train_time, map_time = centralized_train_att(H, params, f, constraints, n, info, file_name)\n            train_times.append(train_time)\n            map_times.append(map_time)\n        else:\n            res, prob, train_time, map_time = GD_train(params, f, constraints, n, info, file_name)\n            train_times.append(train_time)\n            map_times.append(map_time)\n\n        if params['mode'] != 'task_vec' and params['mode'] != 'partition':\n            res_th = {x: 0 if prob[x] < 0.5 else 1 for x in prob.keys()}\n        elif params['mode'] == 'partition':\n            res_th = {}\n            for x in range(n):\n                max_index = np.argmax(prob[x, :])\n                result = [0 for l in range(L)]\n                result[max_index] = 1\n                res_th[x] = result\n        else:\n            res_th = {x: [0 if prob[x, i] < 0.5 else 1 for i in range(L)] for x in range(n)}\n\n        if params['mode'] == 'sat':\n            score, new_w = loss_sat_numpy_boost(res, constraints, [1 for i in range(len(constraints))], inc=params['boosting_mapping'])\n            score_th, new_w = loss_sat_numpy_boost(res_th, constraints, [1 for i in range(len(constraints))], inc=params['boosting_mapping'])\n        elif params['mode'] == 'maxcut' or params['mode'] == 'QUBO_maxcut' or params['mode'] == 'maxcut_annea':\n            if params['data'] == 'bipartite' or params['data'] == 'cliquegraph':\n                score, new_w = loss_maxcut_numpy_boost(res, constraints_hyper, [1 for i in range(len(constraints_hyper))], inc=params['boosting_mapping'])\n                score_th, _ = loss_maxcut_numpy_boost(res_th, constraints_hyper, [1 for i in range(len(constraints_hyper))], inc=params['boosting_mapping'])\n            else:\n                score, new_w = loss_maxcut_numpy_boost(res, constraints, [1 for i in range(len(constraints))], inc=params['boosting_mapping'])\n                score_th, _ = loss_maxcut_numpy_boost(res_th, constraints, [1 for i in range(len(constraints))], inc=params['boosting_mapping'])\n        elif params['mode'] == 'maxind':\n            if params['coarsen']:\n                res_feas = Maxind_postprocessing(res, constraints, n_org)\n                res_th_feas = Maxind_postprocessing(res_th, constraints, n_org)\n            else:\n                res_feas = Maxind_postprocessing(res, constraints, n)\n                res_th_feas = Maxind_postprocessing(res_th, constraints, n)\n            score, score1, new_w = loss_maxind_numpy_boost(res_feas, constraints, [1 for i in range(len(constraints))], inc=params['boosting_mapping'])\n            score_th, score1, new_w = loss_maxind_numpy_boost(res_th_feas, constraints, [1 for i in range(len(constraints))], inc=params['boosting_mapping'])\n        elif params['mode'] == 'QUBO':\n            if params['coarsen']:\n                res_feas = Maxind_postprocessing(res, constraints, n_org)\n                res_th_feas = Maxind_postprocessing(res_th, constraints, n_org)\n            else:\n                res_feas = Maxind_postprocessing(res, constraints, n)\n                res_th_feas = Maxind_postprocessing(res_th, constraints, n)\n            score = loss_maxind_QUBO(torch.Tensor(list(res_feas.values())), q_torch)\n            score_th = loss_maxind_QUBO(torch.Tensor(list(res_th_feas.values())), q_torch)\n        elif params['mode'] == 'task':\n            score = loss_task_numpy(res, constraints, [1 for i in range(len(constraints))], penalty=0, hyper=False)\n            score_th = loss_task_numpy(res_th, constraints, [1 for i in range(len(constraints))], penalty=0, hyper=False)\n        elif params['mode'] == 'task_vec':\n            leninfon = torch.Tensor.numpy(leninfo)\n            lencn = torch.Tensor.numpy(lenc)\n            score = loss_task_numpy_vec(res, lencn, leninfon)\n            res_th_array = np.array(list(res_th.values()))\n            score_th = loss_task_numpy_vec(res_th_array, lencn, leninfon)\n        elif params['mode'] == 'mincut':\n            score_im, score_cut, new_w = loss_mincut_numpy_boost(res, constraints, [1 for i in range(len(constraints))], inc=params['boosting_mapping'])\n            score_th_im, score_th_cut, _ = loss_mincut_numpy_boost(res_th, constraints, [1 for i in range(len(constraints))], inc=params['boosting_mapping'])\n            score = [score_im, score_cut]\n            score_th = [score_th_im, score_th_cut]\n        elif params['mode'] == 'partition':\n            res_th = np.array(list(res_th.values()))\n            # Note: The code is truncated here in the original file\n            # Additional scoring logic would follow for partition mode\n            pass\n\n        # Additional logic for storing results would follow\n        # The function continues beyond this point\n\n    # Return results and metrics\n    # The function continues beyond this point\n\n\n# ==========================================\n# File: src/trainer.py\n# Function/Context: centralized_train\n# ==========================================\nfrom src.model import single_node, single_node_xavier, HGNN_ATT\nimport timeit\nfrom itertools import chain\nimport torch\nfrom src.timer import Timer\nfrom src.loss import loss_cal_and_update, maxcut_loss_func_helper, loss_maxcut_weighted, loss_sat_weighted, loss_maxind_weighted, loss_maxind_QUBO, loss_maxind_weighted2, loss_task_weighted, loss_maxcut_weighted_anealed, loss_task_weighted_vec, loss_mincut_weighted, loss_partitioning_weighted, loss_partitioning_nonbinary, loss_maxcut_weighted_coarse, loss_maxind_QUBO_coarse, loss_maxcut_weighted_multi, loss_maxcut_weighted_multi_gpu\nfrom src.utils import mapping_algo, mapping_distribution, gen_q_mis,gen_q_maxcut, mapping_distribution_QUBO, get_normalized_G_from_con, mapping_distribution_vec_task, mapping_distribution_vec, all_to_weights, all_to_weights_task\nimport numpy as np\nimport multiprocessing as mp\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport random\nfrom torch.autograd import grad\nimport pickle\nimport time\n\ndef centralized_train(G, params, f, C, n, info, file_name):\n    temp_time = timeit.default_timer()\n    ####### fix seed to ensure consistent results ######\n    # seed_value = 100\n    # random.seed(seed_value)  # seed python RNG\n    # np.random.seed(seed_value)  # seed global NumPy RNG\n    # torch.manual_seed(seed_value)  # seed torch RNG\n\n    TORCH_DEVICE = torch.device('cpu')\n    TORCH_DTYPE = torch.float32\n\n    #### sometimes we want the number of epochs to grow with n #####\n    # rounds = max(int(2 * n // 10), int(params['epoch']))\n    rounds = int(params['epoch'])\n    if params['hyper']:\n        indicest = [[i - 1 for i in c] for c in C]\n    else:\n        indicest = [[i - 1 for i in c[0:2]] for c in C]\n\n    ### q_torch helps compute graph MIS and Maxcut loss faster\n    if params['mode'] == 'QUBO':\n        q_torch = gen_q_mis(C, n, 2, torch_dtype=None, torch_device=None)\n    elif params['mode'] == 'QUBO_maxcut':\n        q_torch = gen_q_maxcut(C, n, torch_dtype=None, torch_device=None)\n\n    temper0=0.01\n    p=0\n    count=0\n    prev_loss = 100\n    patience=params['patience']\n    best_loss = float('inf')\n    dct = {x+1: x for x in range(n)}\n    if params['mode']=='partition':\n        outrange=params['n_partitions']\n        outbias=0\n    else:\n        outrange=1\n        outbias=0\n\n\n    ##### transfer learning: load and freeze the layers and only optimize on the input embeding ######\n    if params['transfer']:\n        name = params[\"model_load_path\"] + 'embed_' + file_name[:-4] + '.pt'\n        embed = torch.load(name)\n        # embed = nn.Embedding(n, f)\n        # embed = embed.type(TORCH_DTYPE).to(TORCH_DEVICE)\n        # # for param in embed.parameters():\n        #     param.requires_grad = False\n        name=params[\"model_load_path\"]+'conv1_'+file_name[:-4]+'.pt'\n        conv1 = torch.load(name)\n        for param in conv1.parameters():\n            param.requires_grad = False\n        name = params[\"model_load_path\"]+'conv2_' + file_name[:-4] + '.pt'\n        conv2 = torch.load(name)\n        for param in conv2.parameters():\n            param.requires_grad = False\n        parameters = embed.parameters()\n        # parameters=conv2.parameters()\n    else:\n        embed = nn.Embedding(n, f)\n        embed = embed.type(TORCH_DTYPE).to(TORCH_DEVICE)\n        # conv1 = single_node(f, f//2)\n        conv1 = single_node_xavier(f, f // 2)\n        conv2 = single_node_xavier(f // 2, 1)\n        # conv2 = single_node(f//2, 1)\n        parameters = chain(conv1.parameters(), conv2.parameters(), embed.parameters())\n\n    ###### if we want to initialize our model and input embedding by a pretrained model ####\n    if params[\"initial_transfer\"]:\n        name = params[\"model_load_path\"] + 'conv1_' + file_name[:-4] + '.pt'\n        conv1 = torch.load(name)\n        name = params[\"model_load_path\"] + 'conv2_' + file_name[:-4] + '.pt'\n        conv2 = torch.load(name)\n        name = params[\"model_load_path\"] + 'embed_' + file_name[:-4] + '.pt'\n        embed = torch.load(name)\n        parameters = chain(conv1.parameters(), conv2.parameters(), embed.parameters())\n\n    optimizer = torch.optim.Adam(parameters, lr = params['lr'])\n    #inputs = embed.weight\n    #grad1=torch.zeros((int(params['epoch'])))\n    #grad2 = torch.zeros((int(params['epoch'])))\n\n    #### computes the distance between node features at each layer to detect oversmoothing ####\n    dist=[]\n\n    for i in range(rounds):\n        ##### forward path ######\n        inputs = embed.weight\n        print(i)\n        temp = conv1(inputs)\n        # dis1=max(np.linalg.norm(temp.detach().numpy(), axis=1)) - min(np.linalg.norm(temp.detach().numpy(), axis=1))\n        temp = G @ temp\n        # dis2=max(np.linalg.norm(temp.detach().numpy(), axis=1)) - min(np.linalg.norm(temp.detach().numpy(), axis=1))\n        temp = torch.relu(temp)\n        temp = conv2(temp)\n        # dis3 = max(np.linalg.norm(temp.detach().numpy(), axis=1)) - min(np.linalg.norm(temp.detach().numpy(), axis=1))\n        temp = G @ temp\n        # dis4 = max(np.linalg.norm(temp.detach().numpy(), axis=1)) - min(np.linalg.norm(temp.detach().numpy(), axis=1))\n        temp = torch.sigmoid(temp)\n        temp = temp * outrange + outbias\n        #temp = torch.softmax(temp, dim=0)\n        # dist.append([dis1,dis2,dis3,dis4])\n\n        ###### compute the loss #######\n        if params['mode'] == 'sat':\n            loss = loss_sat_weighted(temp, C, dct, [1 for i in range(len(C))])\n\n        elif params['mode'] == 'maxcut':\n            loss = loss_maxcut_weighted(temp, C, [1 for i in range(len(C))], params['penalty_inc'], params['penalty_c'], params['hyper'])\n\n        elif params['mode'] == 'maxind':\n            #loss = loss_maxind_weighted(temp, C, dct, [1 for i in range(len(C))])\n            loss = loss_maxind_weighted2(temp, C, dct, [1 for i in range(len(C))])\n\n        elif params['mode'] == 'QUBO':\n            # probs = temp[:, 0]\n            loss = loss_maxind_QUBO(temp, q_torch)\n            # loss2 = loss_maxind_weighted(temp, C, dct, [1 for i in range(len(C))])\n            # loss3 = loss_maxind_weighted2(temp, C, dct, [1 for i in range(len(C))])\n            # print(loss, loss3)\n\n        elif params['mode'] == 'QUBO_maxcut':\n            # probs = temp[:, 0]\n            loss = loss_maxind_QUBO(temp, q_torch)\n            #loss2 = loss_maxcut_weighted(temp, C, dct, [1 for i in range(len(C))], params['hyper'])\n            # print(loss,loss2)\n\n        elif params['mode'] == 'maxcut_annea':\n            temper=temper0/(1+i)\n            loss = loss_maxcut_weighted_anealed(temp, C, dct, [1 for i in range(len(C))], temper, params['hyper'])\n\n        elif params['mode'] == 'task':\n            loss = loss_task_weighted(temp, C, dct, [1 for i in range(len(C))])\n            if loss==0:\n                print(\"found zero loss\")\n                break\n\n        elif params['mode'] == 'mincut':\n            loss = loss_mincut_weighted(temp, C, [1 for i in range(len(C))], params['penalty_inc'], params['penalty_c'],\n                                        indicest, params['hyper'])\n\n\n        elif params['mode'] == 'partition':\n            loss = loss_partitioning_nonbinary(temp, C, params['n_partitions'], [1 for i in range(len(C))], params['hyper'])\n        #     loss = loss_partitioning_weighted(temp, C, weights, params['hyper'])\n        # ###### optimization step #######\n        optimizer.zero_grad()\n        #loss.backward(retain_graph=True)\n        loss.backward(retain_graph=True)\n        optimizer.step()\n\n\n        ##### decide if we want to stop based on tolerance (params['tol']) and patience (params['patience']) ######\n        if (abs(loss - prev_loss) <= params['tol']) | ((loss - prev_loss) > 0):\n            count += 1\n            if count >= params['patience']:\n                print(f'Stopping early on epoch {i} (patience: {patience})')\n\n                #### save the model #####\n                name = params[\"model_save_path\"] + 'embed_' + file_name[:-4] + '.pt'\n                torch.save(embed, name)\n                name = params[\"model_save_path\"] + 'conv1_' + file_name[:-4] + '.pt'\n                torch.save(conv1, name)\n                name = params[\"model_save_path\"] + 'conv2_' + file_name[:-4] + '.pt'\n                torch.save(conv2, name)\n                break\n        else:\n            count = 0\n\n        #### keep the best loss and result ####3\n        if loss < best_loss:\n            p = 0\n            best_loss = loss\n            best_out = temp\n            print(f'found better loss')\n\n            ##### the end of the epochs #####\n            if i==int(params['epoch'])-1:\n                ##### save the model #####\n                name=params[\"model_save_path\"]+'embed_'+file_name[:-4]+'.pt'\n                torch.save(embed, name)\n                name = params[\"model_save_path\"]+'conv1_' + file_name[:-4] + '.pt'\n                torch.save(conv1, name)\n                name = params[\"model_save_path\"]+'conv2_' + file_name[:-4] + '.pt'\n                torch.save(conv2, name)\n        else:\n            p += 1\n            if p > params['patience']:\n                print('Early Stopping')\n                ##### save the model #####\n                name = params[\"model_save_path\"] + 'embed_' + file_name[:-4] + '.pt'\n                torch.save(embed, name)\n                name = params[\"model_save_path\"] + 'conv1_' + file_name[:-4] + '.pt'\n                torch.save(conv1, name)\n                name = params[\"model_save_path\"] + 'conv2_' + file_name[:-4] + '.pt'\n                torch.save(conv2, name)\n                break\n        prev_loss=loss\n\n   \n\n    if params['load_best_out']:\n        with open('best_out.txt', 'r') as f:\n            best_out=eval(f.read())\n    else:\n        best_out = best_out.detach().numpy()\n        best_out = {i+1: best_out[i][0] for i in range(len(best_out))}\n    all_weights = [1.0 for c in (C)]\n    if params['data'] != 'task':\n        weights = all_to_weights(all_weights, n, C)\n    else:\n        weights = all_to_weights_task(all_weights, n, C)\n\n    #### plot the histogram of the HyperGNN output (see if it's learning anything) #####\n    name = params['plot_path']+ file_name[:-4] + '.png'\n    plt.hist(best_out.values(), bins=np.linspace(0, 1, 50))\n    plt.savefig(name)\n    plt.show()\n    train_time = timeit.default_timer() - temp_time\n\n\n    ##### fine-tuning ######\n    temp_time2 = timeit.default_timer()\n    res = mapping_distribution(best_out, params, n, info, weights, C, all_weights, 1, params['penalty'],params['hyper'])\n    map_time=timeit.default_timer()-temp_time2\n\n    # if params['mode'] != 'QUBO':\n    #     res = mapping_distribution(best_out, params, n, info, weights, C, all_weights, 1, params['penalty'], params['hyper'])\n    # else:\n    #     res = mapping_distribution_QUBO(best_out, params, q_torch, n)\n    # params2=params\n    # params2['Niter_h']=100\n    # params2['N_realize'] = 2\n    # if params['mode'] != 'QUBO':\n    #     res2 = mapping_distribution(best_out, params2, n, info, weights, C, all_weights, 1, params['penalty'],params['hyper'])\n    # else:\n    #     res2 = mapping_distribution_QUBO(best_out, params2, q_torch, n)\n    # return res, res2, best_out\n    return res, best_out, train_time, map_time\n\n# ==========================================\n# File: src/utils.py\n# Function/Context: mapping_distribution\n# ==========================================\nimport numpy as np\nimport torch\nimport timeit\nfrom src.loss import loss_sat_numpy, loss_maxcut_numpy, loss_maxcut_numpy, loss_maxind_numpy, loss_maxind_QUBO, loss_task_numpy, loss_task_numpy_vec, loss_mincut_numpy, loss_partition_numpy\nimport random\nimport networkx as nx\nimport copy\n\nfrom collections import OrderedDict, defaultdict\n\ndef mapping_distribution(best_outs, params, n, info, weights, constraints, all_weights, inc, penalty, hyper):\n    if params['random_init']=='one_half':\n        best_outs= {x: 0.5 for x in best_outs.keys()}\n    elif params['random_init']=='uniform':\n        best_outs = {x: np.random.uniform(0,1) for x in best_outs.keys()}\n    elif params['random_init'] == 'threshold':\n        best_outs = {x: 0 if best_outs[x] < 0.5 else 1 for x in best_outs.keys()}\n\n    best_score = float('inf')\n    lb = float('inf')\n    if params['mode'] == 'sat':\n        _loss = loss_sat_numpy\n    elif params['mode'] == 'maxcut' or params['mode'] == 'QUBO_maxcut' or params['mode'] == 'maxcut_annea':\n        _loss = loss_maxcut_numpy\n    elif params['mode'] == 'maxind' or params['mode'] == 'QUBO':\n        _loss = loss_maxind_numpy\n    elif params['mode'] == 'task':\n        _loss = loss_task_numpy\n    elif params['mode'] == 'mincut':\n        _loss = loss_mincut_numpy\n\n    for rea in range(params['N_realize']):\n        res = {x: np.random.choice(range(2), p=[1 - best_outs[x], best_outs[x]]) for x in best_outs.keys()}\n        best_score = _loss(res, constraints, all_weights, hyper=hyper)\n        best_res = copy.deepcopy(res)\n        t = params['t']\n        prev_score=best_score\n        for it in range(params['Niter_h']):\n            print(it)\n            ord = random.sample(range(1, n + 1), n)\n            for i in ord:\n                temp = copy.deepcopy(res)\n                if res[i] == 0:\n                    temp[i] = 1\n                else:\n                    temp[i] = 0\n                lt = _loss(temp, info[i], weights[i], penalty=penalty, hyper=hyper)\n                l1 = _loss(res, info[i], weights[i], penalty=penalty, hyper=hyper)\n                if lt < l1 or np.exp(- (lt - l1) / t) > np.random.uniform(0, 1):\n                    res = copy.deepcopy(temp)\n            t = t * 0.95\n            if (it+1)%100==0:\n                score=l1\n                if score==prev_score:\n                    print('early stopping of SA')\n                    break\n                else:\n                    prev_score = score\n                    print(score)\n        score = _loss(res, constraints, all_weights, hyper=hyper)\n        print(score)\n        if score < best_score:\n            best_res =copy.deepcopy(res)\n            best_score = score\n    return best_res",
  "description": "Combined Analysis:\n- [src/loss.py]: This file implements the core loss functions for continuous relaxations of combinatorial optimization problems, aligning with the paper's methodology. Key aspects:\n1. **Continuous Relaxation**: All loss functions operate on continuous probabilities (probs) from sigmoid activations, relaxing discrete variables to [0,1].\n2. **Hypergraph Constraints**: Functions like loss_maxcut_weighted handle hyperedges (hyper=True) for higher-order constraints via multiplicative interactions.\n3. **Penalty Methods**: The loss_maxcut_weighted includes a penalty term (penalty_inc) to push solutions toward binary values, implementing constraint handling.\n4. **Unsupervised Learning**: loss_cal_and_update performs gradient-based optimization (optimizer.step()) without labeled data, matching the paper's unsupervised HyperGNN training.\n5. **Problem Variants**: Specific losses for MaxCut, Max Independent Set (maxind), and SAT encode different objectives and constraints as per the paper's examples.\n6. **Mapping Preparation**: The sigmoid activation and penalty terms prepare continuous outputs for subsequent discrete mapping (e.g., simulated annealing).\nThe code directly implements the mathematical relaxation of discrete optimization problems into differentiable loss functions, which are minimized via gradient descent as described in the algorithm steps.\n- [src/model.py]: This file implements the core hypergraph neural network (HyperGNN) architecture that serves as the learnable transformation function in the HypOp framework. The HGNN_ATT class directly corresponds to the HyperGNN component described in the paper's algorithm steps. Key aspects: 1) It models variable interactions through hypergraph attention layers (HyperGraphAttentionLayerSparse) that process hypergraph incidence matrices, 2) It outputs continuous values in [0,1] via sigmoid activation, enabling gradient-based optimization of the relaxed problem, 3) The embedding layer represents discrete variables as continuous vectors, and 4) The architecture includes trainable parameters optimized via Adam (as configured in params). While this file doesn't contain the full optimization pipeline (loss functions, constraint handling, or simulated annealing mapping), it implements the critical HyperGNN model that transforms hypergraph-structured constraints into continuous variable assignments for gradient-based optimization.\n- [src/solver.py]: This file implements the core optimization logic described in the paper. The centralized_solver function orchestrates the entire HypOp pipeline: 1) It models constraints as a hypergraph via generate_H_from_edges and _generate_G_from_H (or constructs a graph via get_normalized_G_from_con for pairwise problems). 2) It uses a hypergraph neural network (HyperGNN) as a learnable transformation function through centralized_train (or other training variants) to optimize a continuous relaxation via gradient descent. 3) It applies simulated annealing for fine-tuning (via mapping parameters in params) to map continuous outputs to discrete solutions. The function supports multiple problem types (maxcut, maxind, sat, task_vec, etc.) and handles coarsening, sparsification, and various training modes. The loss functions evaluate the discrete solutions after mapping, aligning with the paper's three-step approach.\n- [src/trainer.py]: This file implements the core HypOp algorithm: it trains a hypergraph neural network (HyperGNN) on a continuous relaxation of a combinatorial optimization problem, using a hypergraph incidence matrix G to model constraints, and then uses simulated annealing (via mapping_distribution) to map the continuous outputs to discrete solutions. The loss function is chosen based on the problem type (e.g., maxcut, sat, maxind).\n- [src/utils.py]: This file implements the simulated annealing mapping function that maps continuous HyperGNN outputs to discrete solutions, which is Step 3 of the HypOp algorithm. The mapping_distribution function takes continuous outputs from the HyperGNN (best_outs), performs multiple realizations with simulated annealing, and returns discrete assignments. It supports various optimization problems (SAT, MaxCut, MaxInd, Task, MinCut) through different loss functions. The simulated annealing process includes temperature decay (t * 0.95), random variable flipping, and acceptance based on the Metropolis criterion (exp(-Î”/t) > random). This directly implements the discrete mapping enhancement described in the paper's algorithm steps.",
  "dependencies": [
    "networkx",
    "src.loss (loss_sat_numpy, loss_maxcut_numpy, loss_maxind_numpy, loss_task_numpy, loss_mincut_numpy)",
    "copy",
    "src.utils",
    "timeit",
    "multiprocessing",
    "pickle",
    "torch.nn.init",
    "matplotlib",
    "time",
    "logging",
    "torch.nn.functional",
    "torch.nn.parameter",
    "src.QUBO_utils",
    "math",
    "src.data_reading",
    "src.model",
    "os",
    "random",
    "numpy",
    "src.trainer",
    "src.coarsen",
    "collections",
    "torch.nn",
    "src.loss",
    "torch",
    "src.timer",
    "itertools"
  ]
}