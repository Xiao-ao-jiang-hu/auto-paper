{
  "file_path": "models/GANN/graduated_assignment.py, models/GANN/model.py, src/lap_solvers/hungarian.py",
  "function_name": "GA_GM, Net.real_forward, hungarian",
  "code_snippet": "\n\n# ==========================================\n# File: models/GANN/graduated_assignment.py\n# Function/Context: GA_GM\n# ==========================================\nimport torch\nimport torch.nn as nn\nfrom src.lap_solvers.hungarian import hungarian\nfrom src.lap_solvers.sinkhorn import Sinkhorn\nfrom itertools import product\nfrom src.spectral_clustering import spectral_clustering\nfrom src.utils.pad_tensor import pad_tensor\n\nimport time\n\nclass Timer:\n    def __init__(self):\n        self.start_time = 0\n    def tic(self):\n        self.start_time = time.time()\n    def toc(self, str=\"\"):\n        print_helper('{:.5f}sec {}'.format(time.time()-self.start_time, str))\n\nDEBUG=False\n\ndef print_helper(*args):\n    if DEBUG:\n        print(*args)\n\n\nclass GA_GM(nn.Module):\n    \"\"\"\n    Graduated Assignment solver for\n     Graph Matching, Multi-Graph Matching and Multi-Graph Matching with a Mixture of Modes.\n\n    This operation does not support batched input, and all input tensors should not have the first batch dimension.\n\n    Parameter: maximum iteration mgm_iter\n               sinkhorn iteration sk_iter\n               initial sinkhorn regularization sk_tau0\n               sinkhorn regularization decaying factor sk_gamma\n               minimum tau value min_tau\n               convergence tolerance conv_tal\n    Input: multi-graph similarity matrix W\n           initial multi-matching matrix U0\n           number of nodes in each graph ms\n           size of universe n_univ\n           (optional) projector to doubly-stochastic matrix (sinkhorn) or permutation matrix (hungarian)\n    Output: multi-matching matrix U\n    \"\"\"\n    def __init__(self, mgm_iter=(200,), cluster_iter=10, sk_iter=20, sk_tau0=(0.5,), sk_gamma=0.5, cluster_beta=(1., 0.), converge_tol=1e-5, min_tau=(1e-2,), projector0=('sinkhorn',)):\n        super(GA_GM, self).__init__()\n        self.mgm_iter = mgm_iter\n        self.cluster_iter = cluster_iter\n        self.sk_iter = sk_iter\n        self.sk_tau0 = sk_tau0\n        self.sk_gamma = sk_gamma\n        self.cluster_beta = cluster_beta\n        self.converge_tol = converge_tol\n        self.min_tau = min_tau\n        self.projector0 = projector0\n\n    def forward(self, A, W, U0, ms, n_univ, quad_weight=1., cluster_quad_weight=1., num_clusters=2):\n        # gradient is not required for MGM module\n        W = W.detach()\n\n        num_graphs = ms.shape[0]\n        U = U0\n        m_indices = torch.cumsum(ms, dim=0)\n\n        Us = []\n        clusters = []\n\n        # initialize U with no clusters\n        cluster_M = torch.ones(num_graphs, num_graphs, device=A.device)\n        cluster_M01 = cluster_M\n\n        U = self.gagm(A, W, U, ms, n_univ, cluster_M, self.sk_tau0[0], self.min_tau[0], self.mgm_iter[0], self.projector0[0],\n                      quad_weight=quad_weight, hung_iter=(num_clusters == 1))\n        Us.append(U)\n\n        # MGM problem\n        if num_clusters == 1:\n            return U, torch.zeros(num_graphs, dtype=torch.int)\n\n        for beta, sk_tau0, min_tau, max_iter, projector0 in \\\n                zip(self.cluster_beta, self.sk_tau0, self.min_tau, self.mgm_iter, self.projector0):\n            for i in range(self.cluster_iter):\n                lastU = U\n\n                # clustering step\n                def get_alpha(scale=1., qw=1.):\n                    Alpha = torch.zeros(num_graphs, num_graphs, device=A.device)\n                    for idx1, idx2 in product(range(num_graphs), repeat=2):\n                        if idx1 == idx2:\n                            continue\n                        start_x = m_indices[idx1 - 1] if idx1 != 0 else 0\n                        end_x = m_indices[idx1]\n                        start_y = m_indices[idx2 - 1] if idx2 != 0 else 0\n                        end_y = m_indices[idx2]\n                        A_i = A[start_x:end_x, start_x:end_x]\n                        A_j = A[start_y:end_y, start_y:end_y]\n                        W_ij = W[start_x:end_x, start_y:end_y]\n                        U_i = U[start_x:end_x, :]\n                        U_j = U[start_y:end_y, :]\n                        X_ij = torch.mm(U_i, U_j.t())\n                        Alpha_ij = torch.sum(W_ij * X_ij) \\\n                                   + torch.exp(-torch.norm(torch.chain_matmul(X_ij.t(), A_i, X_ij) - A_j) / scale) * qw\n                        Alpha[idx1, idx2] = Alpha_ij\n                    return Alpha\n                Alpha = get_alpha(qw=cluster_quad_weight)\n\n                last_cluster_M01 = cluster_M01\n                cluster_v = spectral_clustering(Alpha, num_clusters, normalized=True)\n                cluster_M01 = (cluster_v.unsqueeze(0) == cluster_v.unsqueeze(1)).to(dtype=Alpha.dtype)\n                cluster_M = (1 - beta) * cluster_M01 + beta\n\n                if beta == self.cluster_beta[0] and i == 0:\n                    clusters.append(cluster_v)\n\n                # matching step\n                U = self.gagm(A, W, U, ms, n_univ, cluster_M, sk_tau0, min_tau, max_iter,\n                              projector='hungarian' if i != 0 else projector0, quad_weight=quad_weight,\n                              hung_iter=(beta == self.cluster_beta[-1]))\n\n                print_helper('beta = {:.2f}, delta U = {:.4f}, delta M = {:.4f}'.format(beta, torch.norm(lastU - U), torch.norm(last_cluster_M01 - cluster_M01)))\n\n                Us.append(U)\n                clusters.append(cluster_v)\n\n                if beta == 1:\n                    break\n\n                if torch.norm(lastU - U) < self.converge_tol and torch.norm(last_cluster_M01 - cluster_M01) < self.converge_tol:\n                    break\n\n        #return Us, clusters\n        return  U, cluster_v\n\n    def gagm(self, A, W, U0, ms, n_univ, cluster_M, init_tau, min_tau, max_iter, projector='sinkhorn', hung_iter=True, quad_weight=1.):\n        num_graphs = ms.shape[0]\n        U = U0\n        m_indices = torch.cumsum(ms, dim=0)\n\n        lastU = torch.zeros_like(U)\n\n        sinkhorn_tau = init_tau\n        #beta = 0.9\n        iter_flag = True\n\n        while iter_flag:\n            for i in range(max_iter):\n                lastU2 = lastU\n                lastU = U\n\n                # compact matrix form update of V\n                UUt = torch.mm(U, U.t())\n                cluster_weight = torch.repeat_interleave(cluster_M, ms.to(dtype=torch.long), dim=0)\n                cluster_weight = torch.repeat_interleave(cluster_weight, ms.to(dtype=torch.long), dim=1)\n                V = torch.chain_matmul(A, UUt * cluster_weight, A, U) * quad_weight * 2 + torch.mm(W * cluster_weight, U)\n                V /= num_graphs\n\n                U_list = []\n                if projector == 'hungarian':\n                    m_start = 0\n                    for m_end in m_indices:\n                        U_list.append(hungarian(V[m_start:m_end, :n_univ]))\n                        m_start = m_end\n                elif projector == 'sinkhorn':\n                    if torch.all(ms == ms[0]):\n                        if ms[0] <= n_univ:\n                            U_list.append(\n                                Sinkhorn(max_iter=self.sk_iter, tau=sinkhorn_tau, batched_operation=True) \\\n                                    (V.reshape(num_graphs, -1, n_univ), dummy_row=True).reshape(-1, n_univ))\n                        else:\n                            U_list.append(\n                                Sinkhorn(max_iter=self.sk_iter, tau=sinkhorn_tau, batched_operation=True) \\\n                                    (V.reshape(num_graphs, -1, n_univ).transpose(1, 2), dummy_row=True).transpose(1, 2).reshape(-1, n_univ))\n                    else:\n                        V_list = []\n                        n1 = []\n                        m_start = 0\n                        for m_end in m_indices:\n                            V_list.append(V[m_start:m_end, :n_univ])\n                            n1.append(m_end - m_start)\n                            m_start = m_end\n                        n1 = torch.tensor(n1)\n                        U = Sinkhorn(max_iter=self.sk_iter, tau=sinkhorn_tau, batched_operation=True) \\\n                            (torch.stack(pad_tensor(V_list), dim=0), n1, dummy_row=True)\n                        m_start = 0\n                        for idx, m_end in enumerate(m_indices):\n                            U_list.append(U[idx, :m_end - m_start, :])\n                            m_start = m_end\n                else:\n                    raise NameError('Unknown projecter name: {}'.format(projector))\n\n                U = torch.cat(U_list, dim=0)\n                if num_graphs == 2:\n                    U[:ms[0], :] = torch.eye(ms[0], n_univ, device=U.device)\n\n                if torch.norm(U - lastU) < self.converge_tol or torch.norm(U - lastU2) == 0:\n                    break\n\n            if i == max_iter - 1: # not converged\n                if hung_iter:\n                    pass\n                else:\n                    U_list = [hungarian(_) for _ in U_list]\n                    U = torch.cat(U_list, dim=0)\n                    print_helper(i, 'max iter')\n                    break\n\n            # projection control\n            if projector == 'hungarian':\n                print_helper(i, 'hungarian')\n                break\n            elif sinkhorn_tau > min_tau:\n                print_helper(i, sinkhorn_tau)\n                sinkhorn_tau *= self.sk_gamma\n            else:\n                print_helper(i, sinkhorn_tau)\n                if hung_iter:\n                    projector = 'hungarian'\n                else:\n                    U_list = [hungarian(_) for _ in U_list]\n                    U = torch.cat(U_list, dim=0)\n                    break\n\n        return U\n\n# ==========================================\n# File: models/GANN/model.py\n# Function/Context: Net.real_forward\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as functional\nimport numpy as np\nfrom src.lap_solvers.sinkhorn import Sinkhorn\nfrom src.feature_align import feature_align\nfrom models.PCA.affinity_layer import AffinityInp\nfrom models.GANN.graduated_assignment import GA_GM\nfrom src.lap_solvers.hungarian import hungarian\nfrom src.utils.pad_tensor import pad_tensor\nfrom itertools import combinations, product, chain\nfrom src.utils.config import cfg\nfrom src.backbone import *\nCNN = eval(cfg.BACKBONE)\n\nclass Net(CNN):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.affinity_layer = AffinityInp(cfg.GANN.FEATURE_CHANNEL)\n        self.tau = cfg.GANN.SK_TAU\n        self.sinkhorn = Sinkhorn(max_iter=cfg.GANN.SK_ITER_NUM,\n                                 tau=self.tau, epsilon=cfg.GANN.SK_EPSILON, batched_operation=False)\n        self.l2norm = nn.LocalResponseNorm(cfg.GANN.FEATURE_CHANNEL * 2, alpha=cfg.GANN.FEATURE_CHANNEL * 2, beta=0.5, k=0)\n        self.univ_size = torch.tensor(cfg.GANN.UNIV_SIZE)\n        self.quad_weight = cfg.GANN.QUAD_WEIGHT\n        self.cluster_quad_weight = cfg.GANN.CLUSTER_QUAD_WEIGHT\n        self.ga_mgmc = GA_GM(\n            mgm_iter=cfg.GANN.MGM_ITER, cluster_iter=cfg.GANN.CLUSTER_ITER,\n            sk_iter=cfg.GANN.SK_ITER_NUM, sk_tau0=cfg.GANN.INIT_TAU, sk_gamma=cfg.GANN.GAMMA,\n            cluster_beta=cfg.GANN.BETA,\n            converge_tol=cfg.GANN.CONVERGE_TOL, min_tau=cfg.GANN.MIN_TAU, projector0=cfg.GANN.PROJECTOR\n        )\n        self.rescale = cfg.PROBLEM.RESCALE\n\n    def real_forward(self, data_dict, num_clusters, **kwargs):\n        batch_size = data_dict['batch_size']\n        num_graphs = data_dict['num_graphs']\n        if 'images' in data_dict:\n            data = data_dict['images']\n            Ps = data_dict['Ps']\n            ns = data_dict['ns']\n            As_src = data_dict['As']\n            data_cat = torch.cat(data, dim=0)\n            P_cat = torch.cat(pad_tensor(Ps), dim=0)\n            n_cat = torch.cat(ns, dim=0)\n            node = self.node_layers(data_cat)\n            edge = self.edge_layers(node)\n            U = feature_align(node, P_cat, n_cat, self.rescale)\n            F = feature_align(edge, P_cat, n_cat, self.rescale)\n            feats = torch.cat((U, F), dim=1)\n            feats = self.l2norm(feats)\n            feats[torch.isnan(feats)] = 0.\n            feats = torch.split(feats, batch_size, dim=0)\n        else:\n            raise ValueError('Unknown data type for this model.')\n        feat_list = []\n        iterator = zip(feats, Ps, As_src, ns)\n        ms = torch.zeros(batch_size, num_graphs, dtype=torch.int, device=self.device)\n        for idx, (feat, P, As_src, n) in enumerate(iterator):\n            feat_list.append((idx, feat, P, As_src, n))\n            ms[:, idx] = n\n        msmax = torch.max(ms, dim=1).values\n        mscum = torch.cumsum(ms, dim=1)\n        mssum = mscum[:, -1]\n        A = [torch.zeros(m.item(), m.item(), device=self.device) for m in mssum]\n        for idx, feat, P, As_src, n in feat_list:\n            edge_lens = torch.sqrt(torch.sum((P.unsqueeze(1) - P.unsqueeze(2)) ** 2, dim=-1)) * As_src\n            median_lens = torch.median(torch.flatten(edge_lens, start_dim=-2), dim=-1).values\n            median_lens = median_lens.unsqueeze(-1).unsqueeze(-1)\n            A_ii = torch.exp(- edge_lens ** 2 / median_lens ** 2 / cfg.GANN.SCALE_FACTOR)\n            if cfg.GANN.NORM_QUAD_TERM:\n                A_ii = A_ii / n * self.univ_size\n            diag_A_ii = torch.diagonal(A_ii, dim1=-2, dim2=-1)\n            diag_A_ii[:] = 0\n            for b in range(batch_size):\n                start_idx = mscum[b, idx] - n[b]\n                end_idx = mscum[b, idx]\n                A[b][start_idx:end_idx, start_idx:end_idx] += A_ii[b, :n[b], :n[b]]\n        Wds = [torch.zeros(m.item(), m.item(), device=self.device) for m in mssum]\n        for src, tgt in product(feat_list, repeat=2):\n            src_idx, src_feat, P_src, A_src, n_src = src\n            tgt_idx, tgt_feat, P_tgt, A_tgt, n_tgt = tgt\n            if src_idx < tgt_idx:\n                continue\n            W_ij = self.affinity_layer(src_feat.transpose(1, 2), tgt_feat.transpose(1, 2))\n            for b in range(batch_size):\n                start_x = mscum[b, src_idx] - n_src[b]\n                end_x = mscum[b, src_idx]\n                start_y = mscum[b, tgt_idx] - n_tgt[b]\n                end_y = mscum[b, tgt_idx]\n                W_ijb = W_ij[b, :n_src[b], :n_tgt[b]]\n                if end_y - start_y >= end_x - start_x:\n                    W_ij_ds = self.sinkhorn(W_ijb, dummy_row=True)\n                else:\n                    W_ij_ds = self.sinkhorn(W_ijb.t(), dummy_row=True).t()\n                Wds[b][start_x:end_x, start_y:end_y] += W_ij_ds\n                if src_idx != tgt_idx:\n                    Wds[b][start_y:end_y, start_x:end_x] += W_ij_ds.t()\n        U = [[] for _ in range(batch_size)]\n        cluster_v = []\n        for b in range(batch_size):\n            if num_graphs == 2:\n                univ_size = max(feat_list[0][-1][b], feat_list[1][-1][b])\n            else:\n                univ_size = data_dict['univ_size'][b]\n            U0_b = torch.full((torch.sum(ms[b]), univ_size), 1 / univ_size.to(dtype=torch.float), device=self.device)\n            U0_b += torch.randn_like(U0_b) / 1000\n            U_b, cluster_v_b = self.ga_mgmc(A[b], Wds[b], U0_b, ms[b], univ_size, self.quad_weight, self.cluster_quad_weight, num_clusters)\n            cluster_v.append(cluster_v_b)\n            for i in range(num_graphs):\n                if i == 0:\n                    start_idx = 0\n                else:\n                    start_idx = mscum[b, i-1]\n                end_idx = mscum[b, i]\n                U[b].append(U_b[start_idx:end_idx, :])\n        cluster_v = torch.stack(cluster_v)\n        return U, cluster_v, Wds, mscum\n\n    @staticmethod\n    def collect_intra_class_matching_wrapper(U, Wds, mscum, cls_list):\n        pairwise_pred_s = []\n        pairwise_pred_x = []\n        mgm_pred_x = []\n        indices = []\n        unique_cls_list = set(cls_list)\n        intra_class_iterator = []\n        for cls in unique_cls_list:\n            idx_range = np.where(np.array(cls_list) == cls)[0]\n            intra_class_iterator.append(combinations(idx_range, 2))\n        intra_class_iterator = chain(*intra_class_iterator)\n        for idx1, idx2 in intra_class_iterator:\n            start_x = mscum[idx1 - 1] if idx1 != 0 else 0\n            end_x = mscum[idx1]\n            start_y = mscum[idx2 - 1] if idx2 != 0 else 0\n            end_y = mscum[idx2]\n            if end_y - start_y >= end_x - start_x:\n                s = Wds[start_x:end_x, start_y:end_y]\n            else:\n                s = Wds[start_y:end_y, start_x:end_x].t()\n            pairwise_pred_s.append(s.unsqueeze(0))\n            x = hungarian(s)\n            pairwise_pred_x.append(x.unsqueeze(0))\n            mgm_x = torch.mm(U[idx1], U[idx2].t())\n            mgm_pred_x.append(mgm_x.unsqueeze(0))\n            indices.append((idx1, idx2))\n        return pairwise_pred_s, pairwise_pred_x, mgm_pred_x, indices\n\n# ==========================================\n# File: src/lap_solvers/hungarian.py\n# Function/Context: hungarian\n# ==========================================\nimport torch\nimport scipy.optimize as opt\nimport numpy as np\nfrom multiprocessing import Pool\nfrom torch import Tensor\n\n\ndef hungarian(s: Tensor, n1: Tensor=None, n2: Tensor=None, nproc: int=1) -> Tensor:\n    r\"\"\"\n    Solve optimal LAP permutation by hungarian algorithm. The time cost is :math:`O(n^3)`.\n\n    :param s: :math:`(b\\times n_1 \\times n_2)` input 3d tensor. :math:`b`: batch size\n    :param n1: :math:`(b)` number of objects in dim1\n    :param n2: :math:`(b)` number of objects in dim2\n    :param nproc: number of parallel processes (default: ``nproc=1`` for no parallel)\n    :return: :math:`(b\\times n_1 \\times n_2)` optimal permutation matrix\n\n    .. note::\n        We support batched instances with different number of nodes, therefore ``n1`` and ``n2`` are\n        required to specify the exact number of objects of each dimension in the batch. If not specified, we assume\n        the batched matrices are not padded.\n    \"\"\"\n    if len(s.shape) == 2:\n        s = s.unsqueeze(0)\n        matrix_input = True\n    elif len(s.shape) == 3:\n        matrix_input = False\n    else:\n        raise ValueError('input data shape not understood: {}'.format(s.shape))\n\n    device = s.device\n    batch_num = s.shape[0]\n\n    perm_mat = s.cpu().detach().numpy() * -1\n    if n1 is not None:\n        n1 = n1.cpu().numpy()\n    else:\n        n1 = [None] * batch_num\n    if n2 is not None:\n        n2 = n2.cpu().numpy()\n    else:\n        n2 = [None] * batch_num\n\n    if nproc > 1:\n        with Pool(processes=nproc) as pool:\n            mapresult = pool.starmap_async(_hung_kernel, zip(perm_mat, n1, n2))\n            perm_mat = np.stack(mapresult.get())\n    else:\n        perm_mat = np.stack([_hung_kernel(perm_mat[b], n1[b], n2[b]) for b in range(batch_num)])\n\n    perm_mat = torch.from_numpy(perm_mat).to(device)\n\n    if matrix_input:\n        perm_mat.squeeze_(0)\n\n    return perm_mat\n\ndef _hung_kernel(s: torch.Tensor, n1=None, n2=None):\n    if n1 is None:\n        n1 = s.shape[0]\n    if n2 is None:\n        n2 = s.shape[1]\n    row, col = opt.linear_sum_assignment(s[:n1, :n2])\n    perm_mat = np.zeros_like(s)\n    perm_mat[row, col] = 1\n    return perm_mat",
  "description": "Combined Analysis:\n- [models/GANN/graduated_assignment.py]: This file implements the core optimization logic from the paper. The GA_GM class performs joint multi-graph matching and clustering (MGMC) using graduated assignment. Key implementations: 1) Alternating matching and clustering with annealing parameters (beta for clustering confidence, tau for Sinkhorn temperature). 2) Matching step uses first-order Taylor expansion of the multi-graph Koopmans-Beckmann QAP objective (V = A*(U*U^T)*cluster_weight*A*U*quad_weight*2 + W*cluster_weight*U). 3) Projection to doubly-stochastic matrices via Sinkhorn with decaying temperature, then Hungarian for discretization. 4) Clustering step computes graph-wise similarity Alpha_ij = tr(W_ij^T X_ij) + exp(-||X_ij^T A_i X_ij - A_j||)*qw and applies spectral clustering. 5) Enforces cycle-consistency through universe representation U_i (matching from graph i to universe). The implementation matches the paper's mathematical model and algorithm steps exactly.\n- [models/GANN/model.py]: This file implements the core optimization logic of the paper's GANN model. The key components are:\n1. **Objective Construction**: Computes adjacency matrices (A) for structural similarity and pairwise affinity matrices (W) via the AffinityInp layer, forming the quadratic and linear terms of the QAP objective.\n2. **Graduated Assignment Solver**: Delegates the joint optimization of matching and clustering to the GA_GM module, which implements the iterative graduated assignment algorithm with annealing parameters (tau, beta).\n3. **Cycle-Consistency Enforcement**: Uses universe representation (U) to ensure consistent matchings across graphs via matrix multiplication (U[i] @ U[j].T).\n4. **Clustering Integration**: The GA_GM module returns cluster assignments (cluster_v) alongside matching matrices, implementing the soft clustering component.\n5. **Linearization & Discretization**: Uses Sinkhorn for continuous relaxation (with annealing temperature) and Hungarian for discretization, matching the algorithm steps.\nThe code directly maps to the mathematical model: A corresponds to structural terms, W to feature similarities, and the GA_GM solver optimizes the combined objective with clustering constraints.\n- [src/lap_solvers/hungarian.py]: This file implements the Hungarian algorithm for solving Linear Assignment Problems (LAP), which is a core component of the graduated assignment procedure described in the paper. Specifically, after the Sinkhorn normalization step produces doubly-stochastic matrices, the Hungarian algorithm is used to discretize them into permutation matrices (binary 0-1 matrices) that satisfy the assignment constraints. The implementation supports batched processing and handles variable-sized graphs through padding masks (n1, n2 parameters). The function returns permutation matrices X_ij that satisfy the constraints X_ij ∈ {0,1}^{n_i×n_j}, X_ij1 = 1, and X_ij^T1 = 1, which are essential for the matching part of the joint MGMC optimization.",
  "dependencies": [
    "pad_tensor",
    "src.utils.pad_tensor",
    "scipy.optimize.linear_sum_assignment",
    "combinations",
    "multiprocessing.Pool",
    "time",
    "itertools.product",
    "torch.nn.functional",
    "src.lap_solvers.hungarian",
    "src.lap_solvers.sinkhorn",
    "AffinityInp",
    "_hung_kernel",
    "cfg",
    "product",
    "CNN backbone",
    "src.spectral_clustering",
    "numpy",
    "feature_align",
    "GA_GM",
    "hungarian",
    "torch.nn",
    "Sinkhorn",
    "chain",
    "torch"
  ]
}