{
  "paper_id": "DAGs_with_NO_TEARS_Continuous_Optimization_for_Structure_Lea",
  "title": "DAGs with NO TEARS: Continuous Optimization for Structure Learning",
  "abstract": "Estimating the structure of directed acyclic graphs (DAGs, also known as Bayesian networks) is a challenging problem since the search space of DAGs is combinatorial and scales superexponentially with the number of nodes. Existing approaches rely on various local heuristics for enforcing the acyclicity constraint. In this paper, we introduce a fundamentally different strategy: We formulate the structure learning problem as a purely continuous optimization problem over real matrices that avoids this combinatorial constraint entirely. This is achieved by a novel characterization of acyclicity that is not only smooth but also exact. The resulting problem can be efficiently solved by standard numerical algorithms, which also makes implementation effortless. The proposed method outperforms existing ones, without imposing any structural assumptions on the graph such as bounded treewidth or in-degree.",
  "problem_description_natural": "The paper addresses the problem of learning the structure of a directed acyclic graph (DAG) from observational data by minimizing a score function—specifically, a regularized least-squares loss—subject to the constraint that the learned graph must be acyclic. Traditionally, this is a combinatorial optimization problem due to the discrete nature of the acyclicity constraint. The authors reformulate it as a continuous optimization problem over the space of real-valued weighted adjacency matrices W ∈ ℝ^{d×d}, replacing the combinatorial DAG constraint with a smooth equality constraint h(W) = 0, where h is a novel function that exactly characterizes acyclicity (i.e., h(W) = 0 if and only if the graph induced by W is acyclic). The resulting problem is a nonconvex, equality-constrained continuous optimization problem that can be solved using standard numerical solvers like L-BFGS with augmented Lagrangian methods.",
  "problem_type": "continuous nonconvex optimization with smooth equality constraints",
  "datasets": [
    "Generated ER Graphs",
    "Generated SF Graphs",
    "Sachs et al. (2005) protein signaling dataset"
  ],
  "performance_metrics": [
    "Structural Hamming Distance (SHD)",
    "FDR",
    "TPR"
  ],
  "lp_model": {
    "objective": "$\\min_{W \\in \\mathbb{R}^{d \\times d}} \\left( \\frac{1}{2n} \\| \\mathbf{X} - \\mathbf{X}W \\|_F^2 + \\lambda \\|W\\|_1 \\right)$",
    "constraints": [
      "$\\operatorname{tr} \\left( e^{W \\circ W} \\right) - d = 0$"
    ],
    "variables": [
      "$W \\in \\mathbb{R}^{d \\times d}$: weighted adjacency matrix where $w_{ij}$ represents the weight of the edge from node $j$ to node $i$"
    ]
  },
  "raw_latex_model": "$$\\min_{W \\in \\mathbb{R}^{d \\times d}} \\quad \\frac{1}{2n} \\| \\mathbf{X} - \\mathbf{X}W \\|_F^2 + \\lambda \\|W\\|_1 \\quad \\text{subject to} \\quad \\operatorname{tr} \\left( e^{W \\circ W} \\right) - d = 0.$$",
  "algorithm_description": "The NOTEARS algorithm solves the equality-constrained program using an augmented Lagrangian method to handle the acyclicity constraint $h(W)=0$, with proximal quasi-Newton (PQN) for optimizing the unconstrained subproblems that include $\\ell_1$-regularization. This approach transforms the combinatorial DAG learning problem into a continuous optimization problem solvable by standard numerical methods."
}