{
  "paper_id": "Learning_Efficient_Online_3D_Bin_Packing_on_Packing_Configur",
  "title": "LEARNING EFFICIENT ONLINE 3D BIN PACKING ON PACKING CONFIGURATION TREES",
  "abstract": "Online 3D Bin Packing Problem (3D-BPP) has widespread applications in industrial automation and has aroused enthusiastic research interest recently. Existing methods usually solve the problem with limited resolution of spatial discretization, and/or cannot deal with complex practical constraints well. We propose to enhance the practical applicability of online 3D-BPP via learning on a novel hierarchical representation â€” packing configuration tree (PCT). PCT is a full-fledged description of the state and action space of bin packing which can support packing policy learning based on deep reinforcement learning (DRL). The size of the packing action space is proportional to the number of leaf nodes, i.e. candidate placements, making the DRL model easy to train and well-performing even with continuous solution space. During training, PCT expands based on heuristic rules, however, the DRL model learns a much more effective and robust packing policy than heuristic methods. Through extensive evaluation, we demonstrate that our method outperforms all existing online BPP methods and is versatile in terms of incorporating various practical constraints.",
  "problem_description_natural": "The online 3D Bin Packing Problem (3D-BPP) involves packing a sequence of cuboid-shaped items into bins of fixed dimensions, one item at a time, without knowledge of future items. Each item must be placed in an axis-aligned manner within the bin such that it does not overlap with previously packed items and remains fully contained within the bin. The goal is to minimize the number of bins used. Unlike the offline version where all items are known in advance, the online setting requires decisions to be made irrevocably as each item arrives. The problem becomes more challenging when incorporating real-world constraints such as packing stability, load balancing, and isle friendliness. The proposed method addresses these challenges using a hierarchical representation called the Packing Configuration Tree (PCT) combined with deep reinforcement learning to learn effective packing policies in continuous space.",
  "problem_type": "combinatorial optimization",
  "datasets": [
    "Discrete dataset from Zhao et al. (2021)",
    "Continuous dataset with S^d = 1",
    "Generated item set with |I| = 64"
  ],
  "performance_metrics": [
    "Space utilization (Uti.)",
    "Number of packed items (Num.)",
    "Variance of space utilization (Var.)",
    "Gap to best utilization",
    "Task-specific objective score (Obj.)"
  ],
  "lp_model": {
    "objective": "Maximize $\\sum_{i \\in \\mathcal{I}} s_{i,o_i}^x \\cdot s_{i,o_i}^y \\cdot s_{i,o_i}^z$",
    "constraints": [
      "$0 \\leq p_i^d \\leq S^d - s_{i,o_i}^d \\quad \\forall i \\in \\mathcal{I}, d \\in \\{x,y,z\\}$",
      "$p_i^d + s_{i,o_i}^d \\leq p_j^d + S^d(1 - e_{ij}^d) \\quad \\forall i \\neq j, i,j \\in \\mathcal{I}, d \\in \\{x,y,z\\}$"
    ],
    "variables": [
      "$p_i^d$: coordinate of item $i$ along axis $d$",
      "$e_{ij}^d$: binary variable, 1 if item $i$ precedes item $j$ along axis $d$",
      "$o_i$: orientation of item $i$, $o_i \\in \\mathbf{O}$"
    ]
  },
  "raw_latex_model": "$$\\text{Maximize } \\sum_{i \\in \\mathcal{I}} s_{i,o_i}^x \\cdot s_{i,o_i}^y \\cdot s_{i,o_i}^z$$ Subject to: $$0 \\leq p_i^d \\leq S^d - s_{i,o_i}^d \\quad \\forall i \\in \\mathcal{I}, d \\in \\{x,y,z\\}$$ $$p_i^d + s_{i,o_i}^d \\leq p_j^d + S^d(1 - e_{ij}^d) \\quad \\forall i \\neq j, i,j \\in \\mathcal{I}, d \\in \\{x,y,z\\}$$ with $e_{ij}^d \\in \\{0,1\\}$ and $o_i \\in \\mathbf{O}$.",
  "algorithm_description": "The paper uses Deep Reinforcement Learning (DRL) with a Packing Configuration Tree (PCT) representation. State features are extracted from PCT using Graph Attention Networks (GAT), and an actor-critic method (ACKTR) is employed to learn a policy that selects placement actions from candidate leaf nodes in the tree."
}