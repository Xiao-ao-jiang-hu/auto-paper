{
  "file_path": "Steiner-Tree/src/script/formulation.py, Steiner-Tree/src/script/functions.py",
  "function_name": "ILP, LP, solve_ILP, solve_LP, dataframe_generate",
  "code_snippet": "\n\n# ==========================================\n# File: Steiner-Tree/src/script/formulation.py\n# Function/Context: ILP, LP\n# ==========================================\nimport gurobipy as gp\nfrom gurobipy import GRB\n\ndef ILP(graph):\n    # set define\n    N, E, V = graph\n    root = V[0]\n    S = [each for each in N if (each not in V)]\n    V = V[1:]\n    # delete all the arcs that enter the source vertex\n    E = [arc for arc in E if not arc[0][1] == root]\n    # create the tuple dictionary of arcs\n    E_dict = gp.tupledict(E)\n    \n    # model creation\n    m = gp.Model(\"Steiner\")\n    \n    E_dict_keys = E_dict.keys()\n    X_dict = []\n    for k in V:\n        for arc in E_dict_keys:\n            X_dict.append((arc[0], arc[1], k))\n    \n    # add variables\n    x = m.addVars(X_dict,lb=0,vtype=GRB.INTEGER, name='x') # size: |V| * |E|\n    y = m.addVars(E_dict_keys,vtype=GRB.INTEGER, name='y') # size: |E|\n    \n    # set objective value 2.1\n    m.setObjective(gp.quicksum(E_dict[i, j] * y[i, j] for (i, j) in E_dict_keys), GRB.MINIMIZE)\n    \n    # add constraints\n    for i in N:\n        for k in V:\n            # constraint 2.2\n            if i == root:\n                m.addConstr(x.sum(i,'*',k) - x.sum('*',i,k) == 1)\n            elif i == k:\n                m.addConstr(x.sum(i,'*',k) - x.sum('*',i,k) == -1)\n            else:\n                m.addConstr(x.sum(i,'*',k) - x.sum('*',i,k) == 0)\n    \n    # constraint 2.3\n    for i,j,k in X_dict:\n        m.addConstr(x[i,j,k] <= y[i,j])\n        \n    # update the model\n    m.update()\n    \n    # optimize the model\n    m.optimize()\n    \n    # save the optimal solution\n    opt_cost = m.objVal\n    \n    opt_edges = []\n    \n    for v in m.getVars():\n        # save the vertices\n        if v.varName.startswith('y') and v.x != 0:\n            opt_edges.append((v.varName[2:-1], v.x))\n                \n    opt_runtime = m.Runtime\n    \n    return opt_edges, opt_cost, opt_runtime\n\ndef LP(graph):\n    # set define\n    N, E, V = graph\n    root = V[0]\n    S = [each for each in N if (each not in V)]\n    V = V[1:]\n    # delete all the arcs that enter the source vertex\n    E = [arc for arc in E if not arc[0][1] == root]\n    # create the tuple dictionary of arcs\n    E_dict = gp.tupledict(E)\n    \n    # model creation\n    m = gp.Model(\"Steiner\")\n    \n    E_dict_keys = E_dict.keys()\n    X_dict = []\n    for k in V:\n        for arc in E_dict_keys:\n            X_dict.append((arc[0], arc[1], k))\n    \n    # add variables\n    x = m.addVars(X_dict,lb=0,vtype=GRB.CONTINUOUS, name='x') # size: |V| * |E|\n    y = m.addVars(E_dict_keys,vtype=GRB.CONTINUOUS, name='y') # size: |E|\n    \n    # set objective value 2.1\n    m.setObjective(gp.quicksum(E_dict[i, j] * y[i, j] for (i, j) in E_dict_keys), GRB.MINIMIZE)\n    \n    # add constraints\n    for i in N:\n        for k in V:\n            # constraint 2.2\n            if i == root:\n                m.addConstr(x.sum(i,'*',k) - x.sum('*',i,k) == 1)\n            elif i == k:\n                m.addConstr(x.sum(i,'*',k) - x.sum('*',i,k) == -1)\n            else:\n                m.addConstr(x.sum(i,'*',k) - x.sum('*',i,k) == 0)\n    \n    # constraint 2.3\n    for i,j,k in X_dict:\n        m.addConstr(x[i,j,k] <= y[i,j])\n        \n    # update the model\n    m.update()\n    \n    # optimize the model\n    m.optimize()\n    \n    # save the optimal solution\n    opt_cost = m.objVal\n\n\n    opt_edges = []\n    \n    for v in m.getVars():\n        # save the vertices\n        if v.varName.startswith('y') and v.x != 0:\n            opt_edges.append((v.varName[2:-1], v.x))\n                \n    opt_runtime = m.Runtime\n    \n    return opt_edges, opt_cost, opt_runtime\n\n# ==========================================\n# File: Steiner-Tree/src/script/functions.py\n# Function/Context: solve_ILP, solve_LP, dataframe_generate\n# ==========================================\nimport pandas as pd\nimport numpy as np\nimport re\nimport os\nimport networkx as nx\nimport timeit\nfrom script.formulation import *\nfrom script.pruning import *\nfrom sklearn.model_selection import train_test_split\n\ndef read_graph_undirected(filename):\n    with open(filename) as f:\n        lines = f.readlines()\n        edges = []\n        for line in lines:\n            if line == '\\n': \n                continue\n            parts = line.split()\n            det = parts[0]\n            if det == 'Name':\n                name = parts[1]\n            elif det == 'Nodes':\n                n_vertices = int(parts[1])\n            elif det == 'Edges':\n                n_edges = int(parts[1])\n            elif det == 'E':\n                i = int(parts[1])\n                j = int(parts[2])\n                c = int(parts[3])\n                edge = ((i,j),c)\n                edges.append(edge)\n            elif det == 'Terminals':\n                n_terminals = int(parts[1])\n        vertices = np.arange(1, int(n_vertices)+1)\n        vertices = vertices.tolist()\n        terminals = np.arange(1, int(n_terminals)+1)\n        terminals = terminals.tolist()\n        assert(int(n_edges) == len(edges))\n    f.close()\n    return [vertices, edges, terminals]\n\ndef dataframe_generate(ds_filename, log_filename):\n    vertices, edges, terminals = read_graph_undirected(ds_filename)\n    series = []\n    df = pd.DataFrame(columns = ['Node 1', 'Node 2', 'Weight'])\n    for edge in edges:\n        nodes = edge[0]\n        series.append({'Node 1' : nodes[0] , 'Node 2' : nodes[1], 'Weight' : edge[1]})\n    df = pd.DataFrame(columns = ['Node 1', 'Node 2', 'Weight'], data=series)\n    log = read_log(log_filename)\n\n    # Feature: LP Value\n    lp = log['lp_sol']\n    ilp = log['ilp_sol']\n    df['ILP'] = df.apply(\n        lambda row : get_ILP(row['Node 1'], row['Node 2'], ilp),\n        axis=1\n    )\n\n    df['LP'] = normalize(df.apply(\n        lambda row : get_LP(row['Node 1'], row['Node 2'], lp),\n        axis=1\n    ), True)\n\n    df['LP_bool'] = df.apply(\n        lambda row : 1 if get_LP(row['Node 1'], row['Node 2'], lp) > 0 else 0,\n        axis=1\n    )\n\n    start = timeit.default_timer()\n    # Feature: Normalized Weight\n    df['Normalized Weight Std'] = normalize(df['Weight'], True)\n\n    # Feature: Local Rank\n    df['Local Rank Max'] = normalize(df.apply(\n        lambda row : localrank(\n            row['Node 1'], row['Node 2'], df[['Node 1', 'Node 2', 'Weight']], 'min'),\n        axis=1\n    ), True)\n    \n    df['Local Rank Min'] = normalize(df.apply(\n        lambda row : localrank(\n            row['Node 1'], row['Node 2'], df[['Node 1', 'Node 2', 'Weight']],'max'),\n        axis=1\n    ), True)\n\n    df['Local Rank Product'] = normalize(df.apply(\n        lambda row : row['Local Rank Min']*row['Local Rank Max'],\n        axis=1\n    ), True)\n\n    df['Edges Connected'] = normalize(df.apply(\n        lambda row : get_connected(df, row['Node 1'], row['Node 2']),\n        axis=1\n    ), True)\n\n    # Create Graph object\n    G = nx.Graph()\n    for index, row in df.iterrows():\n        G.add_edge(row['Node 1'],row['Node 2'])\n\n    # Feature: Degree Centrality\n    d_cen = nx.degree_centrality(G)\n    df['Degree Centrality Max'] = normalize(df.apply(\n        lambda row : max(d_cen[row['Node 1']], d_cen[row['Node 2']]), axis=1\n    ), True)\n\n    df['Degree Centrality Min'] = normalize(df.apply(\n        lambda row : min(d_cen[row['Node 1']], d_cen[row['Node 2']]), axis=1\n    ), True)\n\n    df['Degree Centrality Product'] = normalize(df.apply(\n        lambda row : row['Degree Centrality Min']*row['Degree Centrality Max'],\n        axis=1\n    ), True)\n\n    # Feature: Betweenness Centrality\n    b_cen = nx.betweenness_centrality(G)\n    df['Betweenness Centrality Max'] = normalize(df.apply(\n        lambda row : max(b_cen[row['Node 1']], b_cen[row['Node 2']]), axis=1\n    ), True)\n\n    df['Betweenness Centrality Min'] = normalize(df.apply(\n        lambda row : min(b_cen[row['Node 1']], b_cen[row['Node 2']]), axis=1\n    ), True)\n\n    df['Betweenness Centrality Product'] = normalize(df.apply(\n        lambda row : row['Betweenness Centrality Min']*row['Betweenness Centrality Max'],\n        axis=1\n    ), True)\n\n    stop = timeit.default_timer()\n    runtime = stop - start + float(log[\"lp_rt\"])\n    return df, runtime\n\ndef get_LP(i,j,lp):\n    for each in lp:\n        if i == int(each[0]) and j == int(each[1]) : return float(each[2])\n        if i == int(each[1]) and j == int(each[0]) : return float(each[2])\n    return 0\n\ndef get_ILP(i,j,ilp):\n    for each in ilp:\n        if i == int(each[0]) and j == int(each[1]) : return float(each[2])\n        if i == int(each[1]) and j == int(each[0]) : return float(each[2])\n    return 0\n\ndef solve_LP(ds_path, log_path):\n    graph = read_graph_undirected(ds_path)\n    print(\"Graph readed.\")\n    print(\"Calculating features...\")\n    df, fe_rt = dataframe_generate(ds_path, log_path)\n    print(\"Feature calculation runtime (LP not included): \", fe_rt)\n    df_pruned_lp = prune_lp(df)\n    print(\"Pruning finished.\")\n    graph_lp = reconstruct(df_pruned_lp, graph[2])\n    print(\"Pruning Method: LP\")   \n    print(\"Solving...\")\n    sol, obj_lp, rt = ILP(graph_lp)\n    print(\"Solved.\")\n    print(\"The OBJ is: \", obj_lp)\n    print(\"The Runtime is: \", rt)\n    return obj_lp\n\ndef solve_ILP(clf, ds_path, log_path, pr):\n    graph = read_graph_undirected(ds_path)\n    print(\"Graph readed.\")\n    print(\"Calculating features...\")\n    df, fe_rt = dataframe_generate(ds_path, log_path)\n    print(\"Feature calculation runtime (LP not included): \", fe_rt)\n    df_pruned_ml = prune_ml(clf, df, pr)\n    print(\"Pruning finished.\")\n    graph_ml = reconstruct(df_pruned_ml, graph[2])\n    print(\"Pruning Method: ML\")\n    print(\"Solving...\")\n    sol, obj_ilp, rt = ILP(graph_ml)\n    print(\"Solved.\")\n    print(\"The OBJ is: \", obj_ilp)\n    print(\"The Runtime is: \", rt)\n    return obj_ilp",
  "description": "Combined Analysis:\n- [Steiner-Tree/src/script/formulation.py]: This file implements the core optimization model for the Steiner Tree problem as described in the paper. The ILP function corresponds to the integer linear programming formulation with binary y variables (implicitly enforced via INTEGER type and constraints). The LP function provides the linear programming relaxation (continuous variables). Both functions implement the objective (minimizing weighted sum of selected edges) and constraints (flow conservation for each terminal and linking constraints between flow and edge selection variables). The code directly maps to the mathematical model: objective (2.1), constraints (2.2 and 2.3), with appropriate variable definitions. However, the learning-to-prune framework (feature extraction, ML model, edge pruning) is not included in this file; it solely provides the exact ILP solver and LP relaxation used within the broader algorithm.\n- [Steiner-Tree/src/script/functions.py]: This file implements the core learning-to-prune framework for the Steiner Tree Problem. Key components:\n1. Feature Engineering: The 'dataframe_generate' function creates 13 normalized features for each edge (LP value, weight statistics, local ranks, degree/betweenness centrality) as described in the paper.\n2. Pruning Pipeline: 'solve_ILP' and 'solve_LP' implement the complete algorithm: read graph → compute features → prune edges → reconstruct reduced graph → solve with exact ILP.\n3. ML Integration: 'solve_ILP' takes a trained classifier (clf) to predict edges to prune, implementing the supervised learning approach.\n4. Optimization Interface: Functions call 'ILP()' from formulation module (exact solver) and 'prune_ml()'/'prune_lp()' from pruning module (edge selection).\n5. Data Processing: Includes graph reading, log parsing, and feature normalization utilities.\nThe code directly implements Algorithm 1 from the paper: feature extraction → ML prediction → instance reduction → exact solving.",
  "dependencies": [
    "numpy",
    "networkx",
    "gurobipy",
    "re",
    "timeit",
    "localrank",
    "os",
    "get_connected",
    "normalize",
    "ILP",
    "read_log",
    "pandas",
    "script.formulation",
    "reconstruct",
    "script.pruning",
    "sklearn.model_selection",
    "prune_ml",
    "prune_lp"
  ]
}