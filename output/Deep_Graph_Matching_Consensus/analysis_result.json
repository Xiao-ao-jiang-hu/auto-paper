{
  "paper_id": "Deep_Graph_Matching_Consensus",
  "title": "DEEP GRAPH MATCHING CONSENSUS",
  "abstract": "This work presents a two-stage neural architecture for learning and refining structural correspondences between graphs. First, we use localized node embeddings computed by a graph neural network to obtain an initial ranking of soft correspondences between nodes. Secondly, we employ synchronous message passing networks to iteratively re-rank the soft correspondences to reach a matching consensus in local neighborhoods between graphs. We show, theoretically and empirically, that our message passing scheme computes a well-founded measure of consensus for corresponding neighborhoods, which is then used to guide the iterative re-ranking process. Our purely local and sparsity-aware architecture scales well to large, real-world inputs while still being able to recover global correspondences consistently. We demonstrate the practical effectiveness of our method on real-world tasks from the fields of computer vision and entity alignment between knowledge graphs, on which we improve upon the current state-of-the-art.",
  "problem_description_natural": "The paper addresses the problem of graph matching, which involves establishing injective node correspondences between a source graph and a target graph by maximizing structural similarityâ€”specifically, by preserving edge relationships between matched nodes. This is traditionally formulated as a quadratic assignment problem that maximizes the number of aligned edges under one-to-one mapping constraints. The authors aim to solve this in a data-driven, differentiable manner using deep learning, avoiding NP-hard combinatorial optimization during inference. Their approach uses an initial soft correspondence matrix derived from node embeddings, then iteratively refines it by enforcing neighborhood consensus: ensuring that neighbors of matched nodes are also consistently matched. The goal is to produce a correspondence matrix that reflects both local node similarity and global structural coherence without solving an explicit optimization problem at test time.",
  "problem_type": "Graph Matching",
  "datasets": [
    "Generated ER Graphs",
    "PASCALVOC",
    "WILLOW-OBJECTCLASS",
    "PASCALPF",
    "DBP15K"
  ],
  "performance_metrics": [
    "Hits@1",
    "Hits@10"
  ],
  "lp_model": {
    "objective": "$\\max_{\\boldsymbol{S}} \\sum_{\\substack{i,i' \\in \\mathcal{V}_s \\\\ j,j' \\in \\mathcal{V}_t}} A^{(s)}_{i,i'} A^{(t)}_{j,j'} S_{i,j} S_{i',j'}$",
    "constraints": [
      "$\\sum_{j \\in \\mathcal{V}_t} S_{i,j} = 1 \\quad \\forall i \\in \\mathcal{V}_s$",
      "$\\sum_{i \\in \\mathcal{V}_s} S_{i,j} \\leq 1 \\quad \\forall j \\in \\mathcal{V}_t$",
      "$S_{i,j} \\in \\{0,1\\} \\quad \\forall i \\in \\mathcal{V}_s, j \\in \\mathcal{V}_t$"
    ],
    "variables": [
      "$S_{i,j}$: binary decision variable indicating if node $i$ in source graph $\\mathcal{G}_s$ is matched to node $j$ in target graph $\\mathcal{G}_t$"
    ]
  },
  "raw_latex_model": "$$\\begin{aligned} \\max_{\\boldsymbol{S}} & \\sum_{\\substack{i,i' \\in \\mathcal{V}_s \\\\ j,j' \\in \\mathcal{V}_t}} A^{(s)}_{i,i'} A^{(t)}_{j,j'} S_{i,j} S_{i',j'} \\\\ \\text{s.t.} \\quad & \\sum_{j \\in \\mathcal{V}_t} S_{i,j} = 1, \\quad \\forall i \\in \\mathcal{V}_s, \\\\ & \\sum_{i \\in \\mathcal{V}_s} S_{i,j} \\leq 1, \\quad \\forall j \\in \\mathcal{V}_t, \\\\ & S_{i,j} \\in \\{0,1\\}, \\quad \\forall i \\in \\mathcal{V}_s, j \\in \\mathcal{V}_t. \\end{aligned}$$",
  "algorithm_description": "The paper proposes a two-stage neural architecture: (1) A local feature matching stage using a Graph Neural Network (GNN) to compute node embeddings and initial soft correspondences via inner product and Sinkhorn/softmax normalization. (2) An iterative refinement stage using synchronous message passing (another GNN) to enforce neighborhood consensus by mapping node indicator functions between graphs, comparing the resulting distributed features, and updating correspondence scores via a trainable MLP. The method is trained end-to-end with a supervised loss combining initial and refined matching errors."
}