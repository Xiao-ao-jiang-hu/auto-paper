{
  "file_path": "NHDE-M/CVRP/source/MODEL__Actor/grouped_actors.py, NHDE-M/KP/source/MODEL__Actor/grouped_actors.py, NHDE-M/KP/source/mo_knapsack_problem.py, NHDE-M/TSP-3O/source/mo_travelling_saleman_problem.py, NHDE-M/TSP/source/MODEL__Actor/grouped_actors.py, NHDE-P/MOCVRP/POMO/MOCVRPEnv.py, NHDE-P/MOCVRP/POMO/MOCVRPModel.py, NHDE-P/MOKP/POMO/MOKPModel.py, NHDE-P/MOTSP/POMO/MOTSPEnv.py, NHDE-P/MOTSP/POMO/MOTSPModel.py, NHDE-P/MOTSP_3obj/POMO/MOTSPEnv_3obj.py, NHDE-P/MOTSP_3obj/POMO/MOTSPModel_3obj.py",
  "function_name": "ACTOR, ACTOR, GROUP_STATE, GROUP_ENVIRONMENT, GROUP_ENVIRONMENT._get_group_travel_distance, ACTOR, Encoder, Next_Node_Probability_Calculator_for_group, CVRPEnv, CVRPModel, KPModel, TSPEnv, TSPModel, TSPEnv, TSPModel",
  "code_snippet": "\n\n# ==========================================\n# File: NHDE-M/CVRP/source/MODEL__Actor/grouped_actors.py\n# Function/Context: ACTOR\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\n\n# For debugging\nfrom IPython.core.debugger import set_trace\n\n# Hyper Parameters\nfrom HYPER_PARAMS import *\nfrom TORCH_OBJECTS import *\n\n\n########################################\n# ACTOR\n########################################\n\nclass ACTOR(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n\n        self.encoder = Encoder()\n        self.node_prob_calculator = Next_Node_Probability_Calculator_for_group()\n\n        self.batch_s = None\n        self.encoded_nodes = None\n        self.encoded_graph = None\n\n    def reset(self, group_state, sols, sols_mask):\n        self.batch_s = group_state.data.size(0)\n        self.encoded_nodes = self.encoder(group_state.data, sols, sols_mask)\n        # shape = (batch, problem+1, EMBEDDING_DIM)\n        self.encoded_graph = self.encoded_nodes.mean(dim=1, keepdim=True)\n        # shape = (batch, 1, EMBEDDING_DIM)\n\n        self.node_prob_calculator.reset(self.encoded_nodes)\n\n    def get_action_probabilities(self, group_state, sols_mask_pomo):\n        encoded_LAST_NODES = pick_nodes_for_each_group(self.encoded_nodes, group_state.current_node)\n        # shape = (batch, group, EMBEDDING_DIM)\n        remaining_loaded = group_state.loaded[:, :, None]\n        # shape = (batch, group, 1)\n\n        item_select_probabilities = self.node_prob_calculator(self.encoded_graph, encoded_LAST_NODES,\n                                                              remaining_loaded, sols_mask_pomo, ninf_mask=group_state.ninf_mask)\n        # shape = (batch, group, problem+1)\n\n        return item_select_probabilities\n\n\n########################################\n# ACTOR_SUB_NN : ENCODER\n########################################\n\nclass Encoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.embedding_depot = nn.Linear(2, EMBEDDING_DIM)\n        self.embedding_node = nn.Linear(3, EMBEDDING_DIM)\n        self.sols_embedding = nn.Linear(2, EMBEDDING_DIM)\n        self.layers = nn.ModuleList([Encoder_Layer() for _ in range(ENCODER_LAYER_NUM)])\n\n    def forward(self, data, sols, sols_mask):\n        # data.shape = (batch, problem+1, 3)\n\n        depot_xy = data[:, [0], 0:2]\n        # shape = (batch, 1, 2)\n        node_xy_demand = data[:, 1:, 0:3]\n        # shape = (batch, problem, 3)\n\n        embedded_depot = self.embedding_depot(depot_xy)\n        # shape = (batch, 1, EMBEDDING_DIM)\n        embedded_node = self.embedding_node(node_xy_demand)\n        # shape = (batch, problem, EMBEDDING_DIM)\n\n        sols_embedded_input = self.sols_embedding(sols)\n        # shape: (batch, sols, embedding)\n\n        # out = torch.cat((embedded_depot, embedded_node), dim=1)\n        # shape = (batch, problem+1, EMBEDDING_DIM)\n        out = torch.cat((embedded_depot, embedded_node, sols_embedded_input), dim=1)\n\n        for layer in self.layers:\n            out = layer(out, sols_mask)\n\n        return out\n\n\nclass Encoder_Layer(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.Wq = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wk = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wv = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.multi_head_combine = nn.Linear(HEAD_NUM * KEY_DIM, EMBEDDING_DIM)\n\n        self.Wq_s = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wk_s = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wv_s = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.multi_head_combine_s = nn.Linear(HEAD_NUM * KEY_DIM, EMBEDDING_DIM)\n\n        self.addAndNormalization1 = Add_And_Normalization_Module()\n        self.feedForward = Feed_Forward_Module()\n        self.addAndNormalization2 = Add_And_Normalization_Module()\n\n    def forward(self, input1, sols_mask):\n        # input.shape = (batch, problem, EMBEDDING_DIM)\n\n        q_n = reshape_by_heads(self.Wq(input1[:, :NODE_SIZE]), head_num=HEAD_NUM)\n        k_n = reshape_by_heads(self.Wk(input1[:, :NODE_SIZE]), head_num=HEAD_NUM)\n        v_n = reshape_by_heads(self.Wv(input1[:, :NODE_SIZE]), head_num=HEAD_NUM)\n        # q shape = (batch, HEAD_NUM, problem, KEY_DIM)\n\n        q_s = reshape_by_heads(self.Wq(input1[:, NODE_SIZE:]), head_num=HEAD_NUM)\n        k_s = reshape_by_heads(self.Wk(input1[:, NODE_SIZE:]), head_num=HEAD_NUM)\n        v_s = reshape_by_heads(self.Wv(input1[:, NODE_SIZE:]), head_num=HEAD_NUM)\n        # q shape = (batch, HEAD_NUM, problem, KEY_DIM)\n\n        out_concat_nn = multi_head_attention(q_n, k_n, v_n)\n        # shape: (batch, problem, HEAD_NUM*KEY_DIM)\n\n        out_concat_ns = multi_head_attention(q_n, k_s, v_s, ninf_mask=sols_mask)\n        # shape: (batch, problem, HEAD_NUM*KEY_DIM)\n\n        multi_head_out_n = self.multi_head_combine(out_concat_nn + out_concat_ns)\n        # shape: (batch, problem, EMBEDDING_DIM)\n\n        out_concat_sn = multi_head_attention(q_s, k_n, v_n)\n        # shape: (batch, problem, HEAD_NUM*KEY_DIM)\n\n        multi_head_out_s = self.multi_head_combine_s(out_concat_sn)\n        # shape: (batch, problem, EMBEDDING_DIM)\n\n        multi_head_out = torch.cat((multi_head_out_n, multi_head_out_s), dim=1)\n\n        out1 = self.addAndNormalization1(input1, multi_head_out)\n        out2 = self.feedForward(out1)\n        out3 = self.addAndNormalization2(out1, out2)\n\n        return out3\n\n\n########################################\n# ACTOR_SUB_NN : Next_Node_Probability_Calculator\n########################################\n\nclass Next_Node_Probability_Calculator_for_group(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.Wq = nn.Linear(2*EMBEDDING_DIM+1, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wk = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wv = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n\n        self.multi_head_combine = nn.Linear(HEAD_NUM * KEY_DIM, EMBEDDING_DIM)\n\n        self.Wk_s = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wv_s = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n\n        self.Wk_logit = nn.Linear(EMBEDDING_DIM, EMBEDDING_DIM, bias=False)\n\n        self.k = None  # saved key, for multi-head attention\n        self.v = None  # saved value, for multi-head_attention\n        self.single_head_key = None  # saved, for single-head attention\n\n    def reset(self, encoded_nodes):\n        # encoded_nodes.shape = (batch, problem+1, EMBEDDING_DIM)\n\n        self.k = reshape_by_heads(self.Wk(encoded_nodes[:, :NODE_SIZE]), head_num=HEAD_NUM)\n        self.v = reshape_by_heads(self.Wv(encoded_nodes[:, :NODE_SIZE]), head_num=HEAD_NUM)\n        self.k_s = reshape_by_heads(self.Wk(encoded_nodes[:, NODE_SIZE:]), head_num=HEAD_NUM)\n        self.v_s = reshape_by_heads(self.Wv(encoded_nodes[:, NODE_SIZE:]), head_num=HEAD_NUM)\n        # shape = (batch, HEAD_NUM, problem+1, KEY_DIM)\n        # self.single_head_key = encoded_nodes.transpose(1, 2)\n        self.single_head_key = self.Wk_logit(encoded_nodes[:, :NODE_SIZE]).transpose(1, 2)\n        # shape = (batch, EMBEDDING_DIM, problem+1)\n\n    def forward(self, input1, input2, remaining_loaded, sols_mask_pomo, ninf_mask=None):\n        # input1.shape = (batch, 1, EMBEDDING_DIM)\n        # input2.shape = (batch, group, EMBEDDING_DIM)\n        # remaining_loaded.shape = (batch, group, 1)\n        # ninf_mask.shape = (batch, group, problem+1)\n\n        group_s = input2.size(1)\n\n        #  Multi-Head Attention\n        #######################################################\n        input_cat = torch.cat((input1.expand(-1, group_s, -1), input2, remaining_loaded), dim=2)\n        # shape = (batch, group, 2*EMBEDDING_DIM+1)\n\n        q = reshape_by_heads(self.Wq(input_cat), head_num=HEAD_NUM)\n        # shape = (batch, HEAD_NUM, group, KEY_DIM)\n\n        out_concat_n = multi_head_attention(q, self.k, self.v, group_ninf_mask=ninf_mask)\n        out_concat_s = multi_head_attention(q, self.k_s, self.v_s, group_ninf_mask=sols_mask_pomo)\n        out_concat = out_concat_n + out_concat_s\n        # shape = (batch, n, HEAD_NUM*KEY_DIM)\n\n        mh_atten_out = self.multi_head_combine(out_concat)\n        # shape = (batch, n, EMBEDDING_DIM)\n\n        #  Single-Head Attention, for probability calculation\n        #######################################################\n        score = torch.matmul(mh_atten_out, self.single_head_key)\n        # shape = (batch, n, problem+1)\n\n        score_scaled = score / np.sqrt(EMBEDDING_DIM)\n        # shape = (batch_s, group, problem+1)\n\n        score_clipped = LOGIT_CLIPPING * torch.tanh(score_scaled)\n\n        if ninf_mask is None:\n            score_masked = score_clipped\n        else:\n            score_masked = score_clipped + ninf_mask\n\n        probs = F.softmax(score_masked, dim=2)\n        # shape = (batch, group, problem+1)\n\n        return probs\n\n\n########################################\n# NN SUB CLASS / FUNCTIONS\n########################################\n\ndef pick_nodes_for_each_group(encoded_nodes, node_index_to_pick):\n    # encoded_nodes.shape = (batch, problem, EMBEDDING_DIM)\n    # node_index_to_pick.shape = (batch, group)\n\n    gathering_index = node_index_to_pick[:, :, None].expand(-1, -1, EMBEDDING_DIM)\n    # shape = (batch, group, EMBEDDING_DIM)\n\n    picked_nodes = encoded_nodes.gather(dim=1, index=gathering_index)\n    # shape = (batch, group, EMBEDDING_DIM)\n\n    return picked_nodes\n\n\ndef reshape_by_heads(qkv, head_num):\n    # q.shape = (batch, C, head_num*key_dim)\n\n    batch_s = qkv.size(0)\n    C = qkv.size(1)\n\n    q_reshaped = qkv.reshape(batch_s, C, head_num, -1)\n    # shape = (batch, C, head_num, key_dim)\n\n    q_transposed = q_reshaped.transpose(1, 2)\n    # shape = (batch, head_num, C, key_dim)\n\n    return q_transposed\n\n\ndef multi_head_attention(q, k, v, ninf_mask=None, group_ninf_mask=None):\n    # q shape = (batch, head_num, n, key_dim)   : n can be either 1 or group\n    # k,v shape = (batch, head_num, problem, key_dim)\n    # ninf_mask.shape = (batch, group, problem)\n\n    batch_s = q.size(0)\n    head_num = q.size(1)\n    n = q.size(2)\n    key_dim = q.size(3)\n    problem_s = k.size(2)\n\n    score = torch.matmul(q, k.transpose(2, 3))\n    # shape = (batch, head_num, n, problem)\n\n    score_scaled = score / np.sqrt(key_dim)\n    if ninf_mask is not None:\n        score_scaled = score_scaled + ninf_mask[:, None, None, :].expand(batch_s, head_num, n, problem_s)\n    if group_ninf_mask is not None:\n        score_scaled = score_scaled + group_ninf_mask[:, None, :, :].expand(batch_s, head_num, n, problem_s)\n\n    weights = nn.Softmax(dim=3)(score_scaled)\n    # shape = (batch, head_num, n, problem)\n\n    out = torch.matmul(weights, v)\n    # shape = (batch, head_num, n, key_dim)\n\n    out_transposed = out.transpose(1, 2)\n    # shape = (batch, n, head_num, key_dim)\n\n    out_concat = out_transposed.reshape(batch_s, n, head_num * key_dim)\n    # shape = (batch, n, head_num*key_dim)\n\n    return out_concat\n\n\nclass Add_And_Normalization_Module(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.norm_by_EMB = nn.BatchNorm1d(EMBEDDING_DIM, affine=True)\n        # 'Funny' Batch_Norm, as it will normalized by EMB dim\n\n    def forward(self, input1, input2):\n        # input.shape = (batch, problem, EMBEDDING_DIM)\n        batch_s = input1.size(0)\n        problem_s = input1.size(1)\n\n        added = input1 + input2\n        normalized = self.norm_by_EMB(added.reshape(batch_s * problem_s, EMBEDDING_DIM))\n\n        return normalized.reshape(batch_s, problem_s, EMBEDDING_DIM)\n\n\nclass Feed_Forward_Module(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.W1 = nn.Linear(EMBEDDING_DIM, FF_HIDDEN_DIM)\n        self.W2 = nn.Linear(FF_HIDDEN_DIM, EMBEDDING_DIM)\n\n    def forward(self, input1):\n        # input.shape = (batch, problem, EMBEDDING_DIM)\n\n        return self.W2(F.relu(self.W1(input1)))\n\n# ==========================================\n# File: NHDE-M/KP/source/MODEL__Actor/grouped_actors.py\n# Function/Context: ACTOR\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\n\n# For debugging\nfrom IPython.core.debugger import set_trace\n\n# Hyper Parameters\nfrom HYPER_PARAMS import *\nfrom TORCH_OBJECTS import *\n\n########################################\n# ACTOR\n########################################\n\nclass ACTOR(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n\n        self.encoder = Encoder()\n        self.node_prob_calculator = Next_Node_Probability_Calculator_for_group()\n\n        self.batch_s = None\n        self.encoded_nodes_and_dummy = None\n        self.encoded_nodes = None\n        self.encoded_graph = None\n\n    def reset(self, group_state, sols, sols_mask):\n        self.batch_s = group_state.item_data.size(0)\n        # self.encoded_nodes_and_dummy = Tensor(np.zeros((self.batch_s, PROBLEM_SIZE+1, EMBEDDING_DIM)))\n        # self.encoded_nodes_and_dummy[:, :PROBLEM_SIZE, :] = self.encoder(group_state.item_data)\n        # self.encoded_nodes = self.encoded_nodes_and_dummy[:, :PROBLEM_SIZE, :]\n        # # shape = (batch, problem, EMBEDDING_DIM)\n\n        self.encoded_nodes = self.encoder(group_state.item_data, sols, sols_mask)\n        self.encoded_graph = self.encoded_nodes.mean(dim=1, keepdim=True)\n        # shape = (batch, 1, EMBEDDING_DIM)\n\n        self.node_prob_calculator.reset(self.encoded_nodes)\n\n    def get_action_probabilities(self, group_state, sols_mask_pomo):\n\n        probs = self.node_prob_calculator(graph=self.encoded_graph, capacity=group_state.capacity, sols_mask_pomo=sols_mask_pomo,\n                                          ninf_mask=group_state.fit_ninf_mask)\n        # shape = (batch, group, problem)\n\n        return probs\n\n\n########################################\n# ACTOR_SUB_NN : ENCODER\n########################################\n\nclass Encoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.embedding = nn.Linear(3, EMBEDDING_DIM)\n        self.sols_embedding = nn.Linear(2, EMBEDDING_DIM)\n        self.layers = nn.ModuleList([Encoder_Layer() for _ in range(ENCODER_LAYER_NUM)])\n\n    def forward(self, item_data, sols, sols_mask):\n        # item_data.shape = (batch, problem, 2)\n\n        embedded_input = self.embedding(item_data)\n        # shape = (batch, problem, EMBEDDING_DIM)\n\n        sols_embedded_input = self.sols_embedding(sols)\n        # shape: (batch, sols, embedding)\n\n        # out = embedded_input\n        out = torch.cat((embedded_input, sols_embedded_input), dim=1)\n        for layer in self.layers:\n            out = layer(out, sols_mask)\n\n        return out\n\n\nclass Encoder_Layer(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.Wq = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wk = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wv = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.multi_head_combine = nn.Linear(HEAD_NUM * KEY_DIM, EMBEDDING_DIM)\n\n        self.Wq_s = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wk_s = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wv_s = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.multi_head_combine_s = nn.Linear(HEAD_NUM * KEY_DIM, EMBEDDING_DIM)\n\n        self.addAndNormalization1 = Add_And_Normalization_Module()\n        self.feedForward = Feed_Forward_Module()\n        self.addAndNormalization2 = Add_And_Normalization_Module()\n\n    def forward(self, input1, sols_mask):\n        # input.shape = (batch, problem, EMBEDDING_DIM)\n\n        q_n = reshape_by_heads(self.Wq(input1[:, :NODE_SIZE]), head_num=HEAD_NUM)\n        k_n = reshape_by_heads(self.Wk(input1[:, :NODE_SIZE]), head_num=HEAD_NUM)\n        v_n = reshape_by_heads(self.Wv(input1[:, :NODE_SIZE]), head_num=HEAD_NUM)\n        # q shape = (batch, HEAD_NUM, problem, KEY_DIM)\n\n        q_s = reshape_by_heads(self.Wq(input1[:, NODE_SIZE:]), head_num=HEAD_NUM)\n        k_s = reshape_by_heads(self.Wk(input1[:, NODE_SIZE:]), head_num=HEAD_NUM)\n        v_s = reshape_by_heads(self.Wv(input1[:, NODE_SIZE:]), head_num=HEAD_NUM)\n        # q shape = (batch, HEAD_NUM, problem, KEY_DIM)\n\n        out_concat_nn = multi_head_attention(q_n, k_n, v_n)\n        # shape: (batch, problem, HEAD_NUM*KEY_DIM)\n\n        out_concat_ns = multi_head_attention(q_n, k_s, v_s, ninf_mask=sols_mask)\n        # shape: (batch, problem, HEAD_NUM*KEY_DIM)\n\n        multi_head_out_n = self.multi_head_combine(out_concat_nn + out_concat_ns)\n        # shape: (batch, problem, EMBEDDING_DIM)\n\n        out_concat_sn = multi_head_attention(q_s, k_n, v_n)\n        # shape: (batch, problem, HEAD_NUM*KEY_DIM)\n\n        multi_head_out_s = self.multi_head_combine_s(out_concat_sn)\n        # shape: (batch, problem, EMBEDDING_DIM)\n\n        multi_head_out = torch.cat((multi_head_out_n, multi_head_out_s), dim=1)\n\n        out1 = self.addAndNormalization1(input1, multi_head_out)\n        out2 = self.feedForward(out1)\n        out3 = self.addAndNormalization2(out1, out2)\n\n        return out3\n\n\n########################################\n# ACTOR_SUB_NN : Next_Node_Probability_Calculator\n########################################\n\nclass Next_Node_Probability_Calculator_for_group(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.Wq = nn.Linear(1+EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wk = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wv = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n\n        self.multi_head_combine = nn.Linear(HEAD_NUM * KEY_DIM, EMBEDDING_DIM)\n\n        self.Wk_s = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wv_s = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n\n        self.Wk_logit = nn.Linear(EMBEDDING_DIM, EMBEDDING_DIM, bias=False)\n\n        self.k = None  # saved key, for multi-head attention\n        self.v = None  # saved value, for multi-head_attention\n        self.single_head_key = None  # saved, for single-head attention\n\n    def reset(self, encoded_nodes):\n        # encoded_nodes.shape = (batch, problem, EMBEDDING_DIM)\n\n        self.k = reshape_by_heads(self.Wk(encoded_nodes[:, :NODE_SIZE]), head_num=HEAD_NUM)\n        self.v = reshape_by_heads(self.Wv(encoded_nodes[:, :NODE_SIZE]), head_num=HEAD_NUM)\n        self.k_s = reshape_by_heads(self.Wk(encoded_nodes[:, NODE_SIZE:]), head_num=HEAD_NUM)\n        self.v_s = reshape_by_heads(self.Wv(encoded_nodes[:, NODE_SIZE:]), head_num=HEAD_NUM)\n        # shape = (batch, HEAD_NUM, problem, KEY_DIM)\n\n        # self.single_head_key = encoded_nodes.transpose(1, 2)\n        self.single_head_key = self.Wk_logit(encoded_nodes[:, :NODE_SIZE]).transpose(1, 2)\n        # shape = (batch, EMBEDDING_DIM, problem)\n\n    def forward(self, graph, capacity, sols_mask_pomo, ninf_mask=None):\n        # graph.shape = (batch, 1, EMBEDDING_DIM)\n        # capacity.shape = (batch, group)\n        #  ninf_mask.shape = (batch, group, problem)\n\n        batch_s = capacity.size(0)\n        group_s = capacity.size(1)\n\n        #  Multi-Head Attention\n        #######################################################\n        input1 = graph.expand(batch_s, group_s, EMBEDDING_DIM)\n        input2 = capacity[:, :, None]\n        input_cat = torch.cat((input1, input2), dim=2)\n        # shape = (batch, group, 1+EMBEDDING_DIM)\n\n        q = reshape_by_heads(self.Wq(input_cat), head_num=HEAD_NUM)\n        # shape = (batch, HEAD_NUM, group, KEY_DIM)\n\n        out_concat_n = multi_head_attention(q, self.k, self.v, group_ninf_mask=ninf_mask)\n        out_concat_s = multi_head_attention(q, self.k_s, self.v_s, group_ninf_mask=sols_mask_pomo)\n        out_concat = out_concat_n + out_concat_s\n        # shape = (batch, group, HEAD_NUM*KEY_DIM)\n\n        mh_atten_out = self.multi_head_combine(out_concat)\n        # shape = (batch, group, EMBEDDING_DIM)\n\n        #  Single-Head Attention, for probability calculation\n        #######################################################      \n        score = torch.matmul(mh_atten_out, self.single_head_key)\n        # shape = (batch, group, problem)\n\n        score_scaled = score / np.sqrt(EMBEDDING_DIM)\n        # shape = (batch, group, problem)\n\n        score_clipped = LOGIT_CLIPPING * torch.tanh(score_scaled)\n\n        if ninf_mask is None:\n            score_masked = score_clipped\n        else:\n            score_masked = score_clipped + ninf_mask\n\n        probs = F.softmax(score_masked, dim=2)\n        # shape = (batch, group, problem)\n\n        return probs\n\n\n########################################\n# NN SUB CLASS / FUNCTIONS\n########################################\n\ndef pick_nodes_for_each_group(encoded_nodes, node_index_to_pick):\n    # encoded_nodes.shape = (batch, problem, EMBEDDING_DIM)\n    # node_index_to_pick.shape = (batch, group_s)\n    batch_s = node_index_to_pick.size(0)\n    group_s = node_index_to_pick.size(1)\n\n    gathering_index = node_index_to_pick[:, :, None].expand(batch_s, group_s, EMBEDDING_DIM)\n    # shape = (batch, group, EMBEDDING_DIM)\n\n    picked_nodes = encoded_nodes.gather(dim=1, index=gathering_index)\n    # shape = (batch, group, EMBEDDING_DIM)\n\n    return picked_nodes\n\n\ndef reshape_by_heads(qkv, head_num):\n    # q.shape = (batch, C, head_num*key_dim)\n\n    batch_s = qkv.size(0)\n    C = qkv.size(1)\n\n    q_reshaped = qkv.reshape(batch_s, C, head_num, -1)\n    # shape = (batch, C, head_num, key_dim)\n\n    q_transposed = q_reshaped.transpose(1, 2)\n    # shape = (batch, head_num, C, key_dim)\n\n    return q_transposed\n\n\ndef multi_head_attention(q, k, v, ninf_mask=None, group_ninf_mask=None):\n    # q shape = (batch, head_num, n, key_dim)   : n can be either 1 or group\n    # k,v shape = (batch, head_num, problem, key_dim)\n    # ninf_mask.shape = (batch, group, problem)\n\n    batch_s = q.size(0)\n    head_num = q.size(1)\n    n = q.size(2)\n    key_dim = q.size(3)\n    problem_s = k.size(2)\n\n    score = torch.matmul(q, k.transpose(2, 3))\n    # shape = (batch, head_num, n, TSP_SIZE)\n\n    score_scaled = score / np.sqrt(key_dim)\n    if ninf_mask is not None:\n        score_scaled = score_scaled + ninf_mask[:, None, None, :].expand(batch_s, head_num, n, problem_s)\n    if group_ninf_mask is not None:\n        score_scaled = score_scaled + group_ninf_mask[:, None, :, :].expand(batch_s, head_num, n, problem_s)\n\n    weights = nn.Softmax(dim=3)(score_scaled)\n    # shape = (batch, head_num, n, problem)\n\n    out = torch.matmul(weights, v)\n    # shape = (batch, head_num, n, key_dim)\n\n    out_transposed = out.transpose(1, 2)\n    # shape = (batch, n, head_num, key_dim)\n\n    out_concat = out_transposed.reshape(batch_s, n, head_num * key_dim)\n    # shape = (batch, n, head_num*key_dim)\n\n    return out_concat\n\n\nclass Add_And_Normalization_Module(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.norm_by_EMB = nn.BatchNorm1d(EMBEDDING_DIM, affine=True)\n        # 'Funny' Batch_Norm, as it will normalized by EMB dim\n\n    def forward(self, input1, input2):\n        # input.shape = (batch, problem, EMBEDDING_DIM)\n\n        batch_s = input1.size(0)\n        problem_s = input1.size(1)\n\n        added = input1 + input2\n        normalized = self.norm_by_EMB(added.reshape(batch_s * problem_s, EMBEDDING_DIM))\n\n        return normalized.reshape(batch_s, problem_s, EMBEDDING_DIM)\n\n\nclass Feed_Forward_Module(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.W1 = nn.Linear(EMBEDDING_DIM, FF_HIDDEN_DIM)\n        self.W2 = nn.Linear(FF_HIDDEN_DIM, EMBEDDING_DIM)\n\n    def forward(self, input1):\n        # input.shape = (batch, problem, EMBEDDING_DIM)\n\n        return self.W2(F.relu(self.W1(input1)))\n\n# ==========================================\n# File: NHDE-M/KP/source/mo_knapsack_problem.py\n# Function/Context: GROUP_STATE, GROUP_ENVIRONMENT\n# ==========================================\n####################################\n# EXTERNAL LIBRARY\n####################################\nimport torch\nimport numpy as np\n\n# For debugging\nfrom IPython.core.debugger import set_trace\n\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\n\n####################################\n# PROJECT VARIABLES\n####################################\nfrom HYPER_PARAMS import *\nfrom TORCH_OBJECTS import *\n\n\n####################################\n# DATA\n####################################\ndef KNAPSACK_DATA_LOADER__RANDOM(num_sample, num_items, batch_size):\n    dataset = KnapSack_Dataset__Random(num_sample=num_sample, num_items=num_items)\n    data_loader = DataLoader(dataset=dataset,\n                             batch_size=batch_size,\n                             shuffle=False,\n                             num_workers=0,\n                             collate_fn=knapsack_collate_fn)\n    return data_loader\n\n\nclass KnapSack_Dataset__Random(Dataset):\n    def __init__(self, num_sample, num_items):\n        self.num_sample = num_sample\n        self.num_items = num_items\n        self.data = np.random.rand(num_sample, num_items, 3)\n\n    def __getitem__(self, index):\n        # data = np.random.rand(self.num_items, 3)\n        data = self.data[index]\n\n        return data\n\n    def __len__(self):\n        return self.num_sample\n\n\ndef knapsack_collate_fn(batch):\n    return Tensor(batch)\n\n\n####################################\n# STATE\n####################################\nclass STATE:\n\n    def __init__(self, item_data, capacity):\n        self.batch_s = item_data.size(0)\n        self.items_and_a_dummy = Tensor(np.zeros((self.batch_s, PROBLEM_SIZE+1, 2)))\n        self.items_and_a_dummy[:, :PROBLEM_SIZE, :] = item_data\n        self.item_data = self.items_and_a_dummy[:, :PROBLEM_SIZE, :]\n        # shape = (batch, problem, 2)\n\n        # History\n        ####################################\n        self.current_node = None\n        self.selected_count = 0\n        self.selected_node_list = LongTensor(np.zeros((self.batch_s, 0)))\n        # shape = (batch, selected_count)\n\n        # Status\n        ####################################\n        self.accumulated_value = Tensor(np.zeros((self.batch_s,)))\n        # shape = (batch,)\n        self.capacity = Tensor(np.ones((self.batch_s,))) * capacity\n        # shape = (batch,)\n        self.ninf_mask_w_dummy = Tensor(np.zeros((self.batch_s, PROBLEM_SIZE+1)))\n        self.ninf_mask = self.ninf_mask_w_dummy[:, :PROBLEM_SIZE]\n        # shape = (batch, problem)\n        self.fit_ninf_mask = None\n        self.finished = BoolTensor(np.zeros((self.batch_s,)))\n        # shape = (batch,)\n\n    def move_to(self, selected_item_idx):\n        # selected_item_idx.shape = (batch,)\n\n        # History\n        ####################################\n        self.current_node = selected_item_idx\n        self.selected_count += 1\n        self.selected_node_list = torch.cat((self.selected_node_list, selected_item_idx[:, None]), dim=1)\n\n        # Status\n        ####################################\n        gathering_index = selected_item_idx[:, None, None].expand(self.batch_s, 1, 2)\n        selected_item = self.items_and_a_dummy.gather(dim=1, index=gathering_index).squeeze(dim=1)\n        # shape = (batch, 2)\n\n        self.accumulated_value += selected_item[:, 1]\n        self.capacity -= selected_item[:, 0]\n\n        self.ninf_mask_w_dummy[torch.arange(self.batch_s), selected_item_idx] = -np.inf\n        unfit_bool = (self.capacity[:, None] - self.item_data[:, :, 0]) < 0\n        # shape = (batch, problem)\n        self.fit_ninf_mask = self.ninf_mask.clone()\n        self.fit_ninf_mask[unfit_bool] = -np.inf\n\n        self.finished = (self.fit_ninf_mask == -np.inf).all(dim=1)\n        # shape = (batch,)\n        self.fit_ninf_mask[self.finished[:, None].expand(self.batch_s, PROBLEM_SIZE)] = 0  # do not mask finished epi.\n\n\nclass GROUP_STATE:\n\n    def __init__(self, group_size, item_data, capacity):\n        self.batch_s = item_data.size(0)\n        self.group_s = group_size\n        self.items_and_a_dummy = Tensor(np.zeros((self.batch_s, PROBLEM_SIZE+1, 3)))\n        self.items_and_a_dummy[:, :PROBLEM_SIZE, :] = item_data\n        self.item_data = self.items_and_a_dummy[:, :PROBLEM_SIZE, :]\n        # shape = (batch, problem, 2)\n\n        # History\n        ####################################\n        self.current_node = None\n        # shape = (batch_s, group)\n        self.selected_count = 0\n        self.selected_node_list = LongTensor(np.zeros((self.batch_s, self.group_s, 0)))\n        # shape = (batch_s, group, selected_count)\n\n        # Status\n        ####################################\n        self.accumulated_value_obj1 = Tensor(np.zeros((self.batch_s, self.group_s)))\n        self.accumulated_value_obj2 = Tensor(np.zeros((self.batch_s, self.group_s)))\n        # shape = (batch, group)\n        self.capacity = Tensor(np.ones((self.batch_s, self.group_s))) * capacity\n        # shape = (batch, group)\n        self.ninf_mask_w_dummy = Tensor(np.zeros((self.batch_s, self.group_s, PROBLEM_SIZE+1)))\n        self.ninf_mask = self.ninf_mask_w_dummy[:, :, :PROBLEM_SIZE]\n        # shape = (batch, group, problem)\n        self.fit_ninf_mask = None\n        self.finished = BoolTensor(np.zeros((self.batch_s, self.group_s)))\n        # shape = (batch, group)\n\n\n    def move_to(self, selected_idx_mat):\n        # selected_idx_mat.shape = (batch, group)\n\n        # History\n        ####################################\n        self.current_node = selected_idx_mat\n        self.selected_count += 1\n        self.selected_node_list = torch.cat((self.selected_node_list, selected_idx_mat[:, :, None]), dim=2)\n\n        # Status\n        ####################################\n        items_mat = self.items_and_a_dummy[:, None, :, :].expand(self.batch_s, self.group_s, PROBLEM_SIZE+1, 3)\n        gathering_index = selected_idx_mat[:, :, None, None].expand(self.batch_s, self.group_s, 1, 3)\n        selected_item = items_mat.gather(dim=2, index=gathering_index).squeeze(dim=2)\n        # shape = (batch, group, 2)\n\n        self.accumulated_value_obj1 += selected_item[:, :, 1]\n        self.accumulated_value_obj2 += selected_item[:, :, 2]\n        self.capacity -= selected_item[:, :, 0]\n\n        batch_idx_mat = torch.arange(self.batch_s)[:, None].expand(self.batch_s, self.group_s)\n        group_idx_mat = torch.arange(self.group_s)[None, :].expand(self.batch_s, self.group_s)\n        self.ninf_mask_w_dummy[batch_idx_mat, group_idx_mat, selected_idx_mat] = -np.inf\n\n        unfit_bool = (self.capacity[:, :, None] - self.item_data[:, None, :, 0]) < 0\n        # shape = (batch, group, problem)\n        self.fit_ninf_mask = self.ninf_mask.clone()\n        self.fit_ninf_mask[unfit_bool] = -np.inf\n\n        self.finished = (self.fit_ninf_mask == -np.inf).all(dim=2)\n        # shape = (batch, group)\n        self.fit_ninf_mask[self.finished[:, :, None].expand(self.batch_s, self.group_s, PROBLEM_SIZE)] = 0\n        # do not mask finished episode\n\n\n\n\n\n\n####################################\n# ENVIRONMENT\n####################################\nclass ENVIRONMENT:\n\n    def __init__(self, item_data):\n        # item_data.shape = (batch, problem, 2)\n\n        self.item_data = item_data\n        self.batch_s = item_data.size(0)\n        self.state = None\n\n    def reset(self):\n        if PROBLEM_SIZE == 50:\n            capacity = 12.5\n        elif PROBLEM_SIZE == 100:\n            capacity = 25\n        elif PROBLEM_SIZE == 200:\n            capacity = 25\n        else:\n            raise NotImplementedError\n\n        self.state = STATE(item_data=self.item_data, capacity=capacity)\n\n        reward = None\n        done = False\n        return self.state, reward, done\n\n    def step(self, selected_item_idx):\n        # selected_node_idx.shape = (batch,)\n\n        # move state\n        self.state.move_to(selected_item_idx)\n\n        # returning values\n        done = self.state.finished.all()\n        if done:\n            reward = self.state.accumulated_value\n        else:\n            reward = None\n\n        return self.state, reward, done\n\n\nclass GROUP_ENVIRONMENT:\n\n    def __init__(self, item_data):\n        # seq.shape = (batch, problem, 2)\n\n        self.item_data = item_data\n        self.batch_s = item_data.size(0)\n        self.group_state = None\n\n    def reset(self, group_size):\n        if PROBLEM_SIZE == 50:\n            capacity = 12.5\n        elif PROBLEM_SIZE == 100:\n            capacity = 25\n        elif PROBLEM_SIZE == 200:\n            capacity = 25\n        else:\n            raise NotImplementedError\n\n        self.group_state = GROUP_STATE(group_size=group_size,\n                                       item_data=self.item_data, capacity=capacity)\n\n        reward = None\n        done = False\n        return self.group_state, reward, done\n\n    def step(self, selected_idx_mat):\n        # selected_idx_mat.shape = (batch, group)\n\n        # move state\n        self.group_state.move_to(selected_idx_mat)\n\n        # returning values\n        reward = None\n        done = self.group_state.finished.all()  # state.finished.shape = (batch, group)\n        if done:\n            # reward = self.group_state.accumulated_value\n            reward = torch.stack([self.group_state.accumulated_value_obj1,self.group_state.accumulated_value_obj2],axis = 2)\n        else:\n            reward = None\n\n        return self.group_state, reward, done\n\n# ==========================================\n# File: NHDE-M/TSP-3O/source/mo_travelling_saleman_problem.py\n# Function/Context: GROUP_ENVIRONMENT._get_group_travel_distance\n# ==========================================\nimport torch\nimport numpy as np\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom HYPER_PARAMS import *\nfrom TORCH_OBJECTS import *\n\nclass GROUP_ENVIRONMENT:\n\n    def __init__(self, data):\n       \n\n        self.data = data\n        self.batch_s = data.size(0)\n        self.group_s = None\n        self.group_state = None\n\n    def reset(self, group_size):\n        self.group_s = group_size\n        self.group_state = GROUP_STATE(group_size=group_size, data=self.data)\n        reward = None\n        done = False\n        return self.group_state, reward, done\n\n    def step(self, selected_idx_mat):\n        # selected_idx_mat.shape = (batch, group)\n\n        # move state\n        self.group_state.move_to(selected_idx_mat)\n\n        # returning values\n        done = (self.group_state.selected_count == TSP_SIZE)\n        if done:\n            reward = -self._get_group_travel_distance()  # note the minus sign!\n        else:\n            reward = None\n        return self.group_state, reward, done\n\n    def _get_group_travel_distance(self):\n\n        gathering_index = self.group_state.selected_node_list.unsqueeze(3).expand(self.batch_s, -1, TSP_SIZE, 6)\n\n\n        seq_expanded = self.data[:, None, :, :].expand(self.batch_s, self.group_s, TSP_SIZE, 6)\n\n        ordered_seq = seq_expanded.gather(dim=2, index=gathering_index)\n\n\n        rolled_seq = ordered_seq.roll(dims=2, shifts=-1)\n\n        segment_lengths_obj1 = ((ordered_seq[:, :, :, :2]-rolled_seq[:, :, :, :2])**2).sum(3).sqrt()\n        segment_lengths_obj2 = ((ordered_seq[:, :, :, 2:4]-rolled_seq[:, :, :, 2:4])**2).sum(3).sqrt()\n        segment_lengths_obj3 = ((ordered_seq[:, :, :, 4:] - rolled_seq[:, :, :, 4:]) ** 2).sum(3).sqrt()\n\n        group_travel_distances_obj1 = segment_lengths_obj1.sum(2)\n        group_travel_distances_obj2 = segment_lengths_obj2.sum(2)\n        group_travel_distances_obj3 = segment_lengths_obj3.sum(2)\n\n        group_travel_distances_combined = torch.stack([group_travel_distances_obj1,group_travel_distances_obj2,group_travel_distances_obj3],axis = 2)\n\n\n        return group_travel_distances_combined\n\n# ==========================================\n# File: NHDE-M/TSP/source/MODEL__Actor/grouped_actors.py\n# Function/Context: ACTOR, Encoder, Next_Node_Probability_Calculator_for_group\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom IPython.core.debugger import set_trace\nfrom HYPER_PARAMS import *\nfrom TORCH_OBJECTS import *\n\nclass ACTOR(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.box_select_probabilities = None\n        self.encoder = Encoder()\n        self.node_prob_calculator = Next_Node_Probability_Calculator_for_group()\n        self.batch_s = None\n        self.encoded_nodes = None\n\n    def reset(self, group_state, sols, sols_mask, sols_mask_pomo):\n        self.batch_s = group_state.data.size(0)\n        self.encoded_nodes = self.encoder(group_state.data, sols, sols_mask)\n        self.node_prob_calculator.reset(self.encoded_nodes, sols_mask_pomo, group_ninf_mask=group_state.ninf_mask)\n\n    def soft_reset(self, group_state):\n        self.node_prob_calculator.reset(self.encoded_nodes, group_ninf_mask=group_state.ninf_mask)\n\n    def update(self, group_state):\n        encoded_LAST_NODES = pick_nodes_for_each_group(self.encoded_nodes, group_state.current_node)\n        probs = self.node_prob_calculator(encoded_LAST_NODES)\n        self.box_select_probabilities = probs\n\n    def get_action_probabilities(self):\n        return self.box_select_probabilities\n\nclass Encoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.embedding = nn.Linear(4, EMBEDDING_DIM)\n        self.sols_embedding = nn.Linear(2, EMBEDDING_DIM)\n        self.layers = nn.ModuleList([Encoder_Layer() for _ in range(ENCODER_LAYER_NUM)])\n\n    def forward(self, data, sols, sols_mask):\n        embedded_input = self.embedding(data)\n        sols_embedded_input = self.sols_embedding(sols)\n        out = torch.cat((embedded_input, sols_embedded_input), dim=1)\n        for layer in self.layers:\n            out = layer(out, sols_mask)\n        return out\n\nclass Encoder_Layer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.Wq = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wk = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wv = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.multi_head_combine = nn.Linear(HEAD_NUM * KEY_DIM, EMBEDDING_DIM)\n        self.Wq_s = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wk_s = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wv_s = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.multi_head_combine_s = nn.Linear(HEAD_NUM * KEY_DIM, EMBEDDING_DIM)\n        self.addAndNormalization1 = Add_And_Normalization_Module()\n        self.feedForward = Feed_Forward_Module()\n        self.addAndNormalization2 = Add_And_Normalization_Module()\n\n    def forward(self, input1, sols_mask):\n        q_n = reshape_by_heads(self.Wq(input1[:, :NODE_SIZE]), head_num=HEAD_NUM)\n        k_n = reshape_by_heads(self.Wk(input1[:, :NODE_SIZE]), head_num=HEAD_NUM)\n        v_n = reshape_by_heads(self.Wv(input1[:, :NODE_SIZE]), head_num=HEAD_NUM)\n        q_s = reshape_by_heads(self.Wq(input1[:, NODE_SIZE:]), head_num=HEAD_NUM)\n        k_s = reshape_by_heads(self.Wk(input1[:, NODE_SIZE:]), head_num=HEAD_NUM)\n        v_s = reshape_by_heads(self.Wv(input1[:, NODE_SIZE:]), head_num=HEAD_NUM)\n        out_concat_nn = multi_head_attention(q_n, k_n, v_n)\n        out_concat_ns = multi_head_attention(q_n, k_s, v_s, ninf_mask=sols_mask)\n        multi_head_out_n = self.multi_head_combine(out_concat_nn + out_concat_ns)\n        out_concat_sn = multi_head_attention(q_s, k_n, v_n)\n        multi_head_out_s = self.multi_head_combine_s(out_concat_sn)\n        multi_head_out = torch.cat((multi_head_out_n, multi_head_out_s), dim=1)\n        out1 = self.addAndNormalization1(input1, multi_head_out)\n        out2 = self.feedForward(out1)\n        out3 = self.addAndNormalization2(out1, out2)\n        return out3\n\nclass Next_Node_Probability_Calculator_for_group(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.Wq_graph = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wq_first = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wq_last = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wk = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wv = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.multi_head_combine = nn.Linear(HEAD_NUM * KEY_DIM, EMBEDDING_DIM)\n        self.Wk_s = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wv_s = nn.Linear(EMBEDDING_DIM, HEAD_NUM * KEY_DIM, bias=False)\n        self.Wk_logit = nn.Linear(EMBEDDING_DIM, EMBEDDING_DIM, bias=False)\n        self.q_graph = None\n        self.q_first = None\n        self.k = None\n        self.v = None\n        self.k_s = None\n        self.v_s = None\n        self.single_head_key = None\n        self.group_ninf_mask = None\n        self.sols_mask_pomo = None\n\n    def reset(self, encoded_nodes, sols_mask_pomo, group_ninf_mask):\n        encoded_graph = encoded_nodes.mean(dim=1, keepdim=True)\n        self.q_graph = reshape_by_heads(self.Wq_graph(encoded_graph), head_num=HEAD_NUM)\n        self.q_first = None\n        self.k = reshape_by_heads(self.Wk(encoded_nodes[:, :NODE_SIZE]), head_num=HEAD_NUM)\n        self.v = reshape_by_heads(self.Wv(encoded_nodes[:, :NODE_SIZE]), head_num=HEAD_NUM)\n        self.k_s = reshape_by_heads(self.Wk_s(encoded_nodes[:, NODE_SIZE:]), head_num=HEAD_NUM)\n        self.v_s = reshape_by_heads(self.Wv_s(encoded_nodes[:, NODE_SIZE:]), head_num=HEAD_NUM)\n        self.single_head_key = self.Wk_logit(encoded_nodes[:, :NODE_SIZE]).transpose(1, 2)\n        self.group_ninf_mask = group_ninf_mask\n        self.sols_mask_pomo = sols_mask_pomo\n\n    def forward(self, encoded_LAST_NODE):\n        if self.q_first is None:\n            self.q_first = reshape_by_heads(self.Wq_first(encoded_LAST_NODE), head_num=HEAD_NUM)\n        q_last = reshape_by_heads(self.Wq_last(encoded_LAST_NODE), head_num=HEAD_NUM)\n        q = self.q_graph + self.q_first + q_last\n        out_concat_n = multi_head_attention(q, self.k, self.v, group_ninf_mask=self.group_ninf_mask)\n        out_concat_s = multi_head_attention(q, self.k_s, self.v_s, group_ninf_mask=self.sols_mask_pomo)\n        out_concat = out_concat_n + out_concat_s\n        mh_atten_out = self.multi_head_combine(out_concat)\n        score = torch.matmul(mh_atten_out, self.single_head_key)\n        score_scaled = score / np.sqrt(EMBEDDING_DIM)\n        score_clipped = LOGIT_CLIPPING * torch.tanh(score_scaled)\n        score_masked = score_clipped + self.group_ninf_mask.clone()\n        probs = F.softmax(score_masked, dim=2)\n        return probs\n\ndef pick_nodes_for_each_group(encoded_nodes, node_index_to_pick):\n    batch_s = node_index_to_pick.size(0)\n    group_s = node_index_to_pick.size(1)\n    gathering_index = node_index_to_pick[:, :, None].expand(batch_s, group_s, EMBEDDING_DIM)\n    picked_nodes = encoded_nodes.gather(dim=1, index=gathering_index)\n    return picked_nodes\n\ndef reshape_by_heads(qkv, head_num):\n    batch_s = qkv.size(0)\n    C = qkv.size(1)\n    q_reshaped = qkv.reshape(batch_s, C, head_num, -1)\n    q_transposed = q_reshaped.transpose(1, 2)\n    return q_transposed\n\ndef multi_head_attention(q, k, v, ninf_mask=None, group_ninf_mask=None):\n    batch_s = q.size(0)\n    head_num = q.size(1)\n    n = q.size(2)\n    key_dim = q.size(3)\n    input_s = k.size(2)\n    score = torch.matmul(q, k.transpose(2, 3))\n    score_scaled = score / np.sqrt(key_dim)\n    if ninf_mask is not None:\n        score_scaled = score_scaled + ninf_mask[:, None, None, :].expand(batch_s, head_num, n, input_s)\n    if group_ninf_mask is not None:\n        score_scaled = score_scaled + group_ninf_mask[:, None, :, :].expand(batch_s, head_num, n, input_s)\n    weights = nn.Softmax(dim=3)(score_scaled)\n    out = torch.matmul(weights, v)\n    out_transposed = out.transpose(1, 2)\n    out_concat = out_transposed.reshape(batch_s, n, head_num * key_dim)\n    return out_concat\n\nclass Add_And_Normalization_Module(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.norm_by_EMB = nn.BatchNorm1d(EMBEDDING_DIM, affine=True)\n\n    def forward(self, input1, input2):\n        batch_s = input1.size(0)\n        problem_s = input1.size(1)\n        added = input1 + input2\n        normalized = self.norm_by_EMB(added.reshape(batch_s * problem_s, EMBEDDING_DIM))\n        return normalized.reshape(batch_s, problem_s, EMBEDDING_DIM)\n\nclass Feed_Forward_Module(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.W1 = nn.Linear(EMBEDDING_DIM, FF_HIDDEN_DIM)\n        self.W2 = nn.Linear(FF_HIDDEN_DIM, EMBEDDING_DIM)\n\n    def forward(self, input1):\n        return self.W2(F.relu(self.W1(input1)))\n\n# ==========================================\n# File: NHDE-P/MOCVRP/POMO/MOCVRPEnv.py\n# Function/Context: CVRPEnv\n# ==========================================\nfrom dataclasses import dataclass\nimport torch\n\nfrom MOCVRPProblemDef import get_random_problems, augment_xy_data_by_8_fold\n\n\n@dataclass\nclass Reset_State:\n    depot_xy: torch.Tensor = None\n    node_xy: torch.Tensor = None\n    \n    node_demand: torch.Tensor = None\n    # shape: (batch, problem)\n\n\n@dataclass\nclass Step_State:\n    BATCH_IDX: torch.Tensor = None\n    POMO_IDX: torch.Tensor = None\n    # shape: (batch, pomo)\n    selected_count: int = None\n    load: torch.Tensor = None\n    current_node: torch.Tensor = None\n    # shape: (batch, pomo)\n    ninf_mask: torch.Tensor = None\n    # shape: (batch, pomo, problem+1)\n    finished: torch.Tensor = None\n    # shape: (batch, pomo)\n\n\nclass CVRPEnv:\n    def __init__(self, **env_params):\n\n        # Const @INIT\n        ####################################\n        self.env_params = env_params\n        self.problem_size = env_params['problem_size']\n        self.pomo_size = env_params['pomo_size']\n\n        # Const @Load_Problem\n        ####################################\n        self.batch_size = None\n        self.BATCH_IDX = None\n        self.POMO_IDX = None\n        # IDX.shape: (batch, pomo)\n        self.depot_node_xy = None\n        # shape: (batch, problem+1, 2)\n        self.depot_node_demand = None\n        # shape: (batch, problem+1)\n\n        # Dynamic-1\n        ####################################\n        self.selected_count = None\n        self.current_node = None\n        # shape: (batch, pomo)\n        self.selected_node_list = None\n        # shape: (batch, pomo, 0~)\n\n        # Dynamic-2\n        ####################################\n        self.at_the_depot = None\n        # shape: (batch, pomo)\n        self.load = None\n        # shape: (batch, pomo)\n        self.visited_ninf_flag = None\n        # shape: (batch, pomo, problem+1)\n        self.ninf_mask = None\n        # shape: (batch, pomo, problem+1)\n        self.finished = None\n        # shape: (batch, pomo)\n\n        # states to return\n        ####################################\n        self.reset_state = Reset_State()\n        self.step_state = Step_State()\n\n\n    def load_problems(self, batch_size, aug_factor=1, problems=None):\n        self.batch_size = batch_size\n\n        if problems is not None:\n            depot_xy, node_xy, node_demand = problems\n        else:\n            depot_xy, node_xy, node_demand = get_random_problems(batch_size, self.problem_size)\n\n        if aug_factor > 1:\n            if aug_factor == 8:\n                self.batch_size = self.batch_size * 8\n                depot_xy = augment_xy_data_by_8_fold(depot_xy)\n                node_xy = augment_xy_data_by_8_fold(node_xy)\n                node_demand = node_demand.repeat(8, 1)\n            else:\n                raise NotImplementedError\n\n        self.depot_node_xy = torch.cat((depot_xy, node_xy), dim=1)\n        # shape: (batch, problem+1, 2)\n        depot_demand = torch.zeros(size=(self.batch_size, 1))\n        # shape: (batch, 1)\n        self.depot_node_demand = torch.cat((depot_demand, node_demand), dim=1)\n        # shape: (batch, problem+1)\n\n\n        self.BATCH_IDX = torch.arange(self.batch_size)[:, None].expand(self.batch_size, self.pomo_size)\n        self.POMO_IDX = torch.arange(self.pomo_size)[None, :].expand(self.batch_size, self.pomo_size)\n\n        self.reset_state.depot_xy = depot_xy\n        self.reset_state.node_xy = node_xy\n        self.reset_state.node_demand = node_demand\n\n        self.step_state.BATCH_IDX = self.BATCH_IDX\n        self.step_state.POMO_IDX = self.POMO_IDX\n\n    def reset(self):\n        self.selected_count = 0\n        self.current_node = None\n        # shape: (batch, pomo)\n        self.selected_node_list = torch.zeros((self.batch_size, self.pomo_size, 0), dtype=torch.long)\n        # shape: (batch, pomo, 0~)\n\n        self.at_the_depot = torch.ones(size=(self.batch_size, self.pomo_size), dtype=torch.bool)\n        # shape: (batch, pomo)\n        self.load = torch.ones(size=(self.batch_size, self.pomo_size))\n        # shape: (batch, pomo)\n        self.visited_ninf_flag = torch.zeros(size=(self.batch_size, self.pomo_size, self.problem_size+1))\n        # shape: (batch, pomo, problem+1)\n        self.ninf_mask = torch.zeros(size=(self.batch_size, self.pomo_size, self.problem_size+1))\n        # shape: (batch, pomo, problem+1)\n        self.finished = torch.zeros(size=(self.batch_size, self.pomo_size), dtype=torch.bool)\n        # shape: (batch, pomo)\n\n        reward = None\n        done = False\n        return self.reset_state, reward, done\n\n    def pre_step(self):\n        self.step_state.selected_count = self.selected_count\n        self.step_state.load = self.load\n        self.step_state.current_node = self.current_node\n        self.step_state.ninf_mask = self.ninf_mask\n        self.step_state.finished = self.finished\n\n        reward = None\n        done = False\n        return self.step_state, reward, done\n\n    def step(self, selected):\n        # selected.shape: (batch, pomo)\n\n        # Dynamic-1\n        ####################################\n        self.selected_count += 1\n        self.current_node = selected\n        # shape: (batch, pomo)\n        self.selected_node_list = torch.cat((self.selected_node_list, self.current_node[:, :, None]), dim=2)\n        # shape: (batch, pomo, 0~)\n\n        # Dynamic-2\n        ####################################\n        self.at_the_depot = (selected == 0)\n\n        demand_list = self.depot_node_demand[:, None, :].expand(self.batch_size, self.pomo_size, -1)\n        # shape: (batch, pomo, problem+1)\n        gathering_index = selected[:, :, None]\n        # shape: (batch, pomo, 1)\n        selected_demand = demand_list.gather(dim=2, index=gathering_index).squeeze(dim=2)\n        # shape: (batch, pomo)\n        self.load -= selected_demand\n        self.load[self.at_the_depot] = 1 # refill loaded at the depot\n\n        self.visited_ninf_flag[self.BATCH_IDX, self.POMO_IDX, selected] = float('-inf')\n        # shape: (batch, pomo, problem+1)\n        self.visited_ninf_flag[:, :, 0][~self.at_the_depot] = 0  # depot is considered unvisited, unless you are AT the depot\n\n        self.ninf_mask = self.visited_ninf_flag.clone()\n        round_error_epsilon = 0.00001\n        demand_too_large = self.load[:, :, None] + round_error_epsilon < demand_list\n        # shape: (batch, pomo, problem+1)\n        self.ninf_mask[demand_too_large] = float('-inf')\n        # shape: (batch, pomo, problem+1)\n\n        newly_finished = (self.visited_ninf_flag == float('-inf')).all(dim=2)\n        # shape: (batch, pomo)\n        self.finished = self.finished + newly_finished\n        # shape: (batch, pomo)\n\n        # do not mask depot for finished episode.\n        self.ninf_mask[:, :, 0][self.finished] = 0\n\n        self.step_state.selected_count = self.selected_count\n        self.step_state.load = self.load\n        self.step_state.current_node = self.current_node\n        self.step_state.ninf_mask = self.ninf_mask\n        self.step_state.finished = self.finished\n\n        # returning values\n        done = self.finished.all()\n        if done:\n            reward = -self._get_travel_distance()  # note the minus sign!\n        else:\n            reward = None\n\n        return self.step_state, reward, done\n\n    def _get_travel_distance(self):\n        gathering_index = self.selected_node_list[:, :, :, None].expand(-1, -1, -1, 2)\n        # shape: (batch, pomo, selected_list_length, 2)\n        all_xy = self.depot_node_xy[:, None, :, :].expand(-1, self.pomo_size, -1, -1)\n        # shape: (batch, pomo, problem+1, 2)\n        \n        # obj1: travel_distances\n        ordered_seq = all_xy.gather(dim=2, index=gathering_index)\n        # shape: (batch, pomo, selected_list_length, 2)\n\n        rolled_seq = ordered_seq.roll(dims=2, shifts=-1)\n        segment_lengths = ((ordered_seq-rolled_seq)**2).sum(3).sqrt()\n        # shape: (batch, pomo, selected_list_length)\n\n        travel_distances = segment_lengths.sum(2)\n        # shape: (batch, pomo)\n        \n        # obj2: makespans\n        not_idx = (gathering_index[:, :, :, 0] > 0).roll(dims=2, shifts=-1)\n        cum_lengths = torch.cumsum(segment_lengths, dim = 2)\n      \n        # cum_lengths[not_idx] = 0\n        cum_lengths[not_idx] = 0\n        sorted_cum_lengths, _ = cum_lengths.sort(axis = 2)\n      \n        rolled_sorted_cum_lengths = sorted_cum_lengths.roll(dims=2, shifts = 1)\n        diff_mat = sorted_cum_lengths - rolled_sorted_cum_lengths\n        diff_mat[diff_mat < 0] = 0\n       \n        makespans, _ = torch.max(diff_mat,dim = 2)\n        \n        objs = torch.stack([travel_distances,makespans],axis = 2)\n        \n        return objs\n\n# ==========================================\n# File: NHDE-P/MOCVRP/POMO/MOCVRPModel.py\n# Function/Context: CVRPModel\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass CVRPModel(nn.Module):\n\n    def __init__(self, **model_params):\n        super().__init__()\n        self.model_params = model_params\n\n        self.encoder = CVRP_Encoder(**model_params)\n        self.decoder = CVRP_Decoder(**model_params)\n        self.encoded_nodes = None\n        # shape: (batch, problem+1, EMBEDDING_DIM)\n\n    def pre_forward(self, reset_state, sols, sols_mask):\n        depot_xy = reset_state.depot_xy\n        # shape: (batch, 1, 2)\n        node_xy = reset_state.node_xy\n        # shape: (batch, problem, 2)\n        node_demand = reset_state.node_demand\n        # shape: (batch, problem)\n        node_xy_demand = torch.cat((node_xy, node_demand[:, :, None]), dim=2)\n        # shape: (batch, problem, 3)\n\n        self.encoded_nodes = self.encoder(depot_xy, node_xy_demand, sols, sols_mask)\n        # shape: (batch, problem+1, embedding)\n        self.decoder.set_kv(self.encoded_nodes)\n\n    def forward(self, state, sols_mask_pomo):\n        batch_size = state.BATCH_IDX.size(0)\n        pomo_size = state.BATCH_IDX.size(1)\n\n\n        if state.selected_count == 0:  # First Move, depot\n            selected = torch.zeros(size=(batch_size, pomo_size), dtype=torch.long)\n            prob = torch.ones(size=(batch_size, pomo_size))\n\n        elif state.selected_count == 1:  # Second Move, POMO\n            selected = torch.arange(start=1, end=pomo_size+1)[None, :].expand(batch_size, pomo_size)\n            prob = torch.ones(size=(batch_size, pomo_size))\n\n        else:\n            encoded_last_node = _get_encoding(self.encoded_nodes, state.current_node)\n            # shape: (batch, pomo, embedding)\n            probs = self.decoder(encoded_last_node, state.load, sols_mask_pomo, ninf_mask=state.ninf_mask)\n            # shape: (batch, pomo, problem+1)\n\n            if self.training or self.model_params['eval_type'] == 'softmax':\n                while True:  # to fix pytorch.multinomial bug on selecting 0 probability elements\n                    with torch.no_grad():\n                        selected = probs.reshape(batch_size * pomo_size, -1).multinomial(1) \\\n                            .squeeze(dim=1).reshape(batch_size, pomo_size)\n                    # shape: (batch, pomo)\n                    prob = probs[state.BATCH_IDX, state.POMO_IDX, selected].reshape(batch_size, pomo_size)\n                    # shape: (batch, pomo)\n                    if (prob != 0).all():\n                        break\n\n            else:\n                selected = probs.argmax(dim=2)\n                # shape: (batch, pomo)\n                prob = None  # value not needed. Can be anything.\n\n        return selected, prob\n\n\ndef _get_encoding(encoded_nodes, node_index_to_pick):\n    # encoded_nodes.shape: (batch, problem, embedding)\n    # node_index_to_pick.shape: (batch, pomo)\n\n    batch_size = node_index_to_pick.size(0)\n    pomo_size = node_index_to_pick.size(1)\n    embedding_dim = encoded_nodes.size(2)\n\n    gathering_index = node_index_to_pick[:, :, None].expand(batch_size, pomo_size, embedding_dim)\n    # shape: (batch, pomo, embedding)\n\n    picked_nodes = encoded_nodes.gather(dim=1, index=gathering_index)\n    # shape: (batch, pomo, embedding)\n\n    return picked_nodes\n\n\n########################################\n# ENCODER\n########################################\n\nclass CVRP_Encoder(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n        self.model_params = model_params\n        embedding_dim = self.model_params['embedding_dim']\n        encoder_layer_num = self.model_params['encoder_layer_num']\n\n        self.embedding_depot = nn.Linear(2, embedding_dim)\n        self.embedding_node = nn.Linear(3, embedding_dim)\n        self.sols_embedding = nn.Linear(2, embedding_dim)\n        self.layers = nn.ModuleList([EncoderLayer(**model_params) for _ in range(encoder_layer_num)])\n\n    def forward(self, depot_xy, node_xy_demand, sols, sols_mask):\n        # depot_xy.shape: (batch, 1, 2)\n        # node_xy_demand.shape: (batch, problem, 3)\n\n        embedded_depot = self.embedding_depot(depot_xy)\n        # shape: (batch, 1, embedding)\n        embedded_node = self.embedding_node(node_xy_demand)\n        # shape: (batch, problem, embedding)\n        sols_embedded_input = self.sols_embedding(sols)\n        # shape: (batch, sols, embedding)\n\n        # out = torch.cat((embedded_depot, embedded_node), dim=1)\n        out = torch.cat((embedded_depot, embedded_node, sols_embedded_input), dim=1)\n        # shape: (batch, problem+1, embedding)\n\n        for layer in self.layers:\n            out = layer(out, sols_mask)\n\n        return out\n        # shape: (batch, problem+1, embedding)\n\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n        self.model_params = model_params\n        embedding_dim = self.model_params['embedding_dim']\n        head_num = self.model_params['head_num']\n        qkv_dim = self.model_params['qkv_dim']\n\n        self.Wq = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.Wk = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.Wv = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.multi_head_combine = nn.Linear(head_num * qkv_dim, embedding_dim)\n\n        self.Wq_s = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.Wk_s = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.Wv_s = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.multi_head_combine_s = nn.Linear(head_num * qkv_dim, embedding_dim)\n\n        self.add_n_normalization_1 = AddAndInstanceNormalization(**model_params)\n        self.feed_forward = FeedForward(**model_params)\n        self.add_n_normalization_2 = AddAndInstanceNormalization(**model_params)\n\n    def forward(self, input1, sols_mask):\n        # input1.shape: (batch, problem+1, embedding)\n        head_num = self.model_params['head_num']\n\n        q_n = reshape_by_heads(self.Wq(input1[:, :self.model_params['node_size']]), head_num=head_num)\n        k_n = reshape_by_heads(self.Wk(input1[:, :self.model_params['node_size']]), head_num=head_num)\n        v_n = reshape_by_heads(self.Wv(input1[:, :self.model_params['node_size']]), head_num=head_num)\n        # q shape: (batch, HEAD_NUM, problem, KEY_DIM)\n\n        q_s = reshape_by_heads(self.Wq_s(input1[:, self.model_params['node_size']:]), head_num=head_num)\n        k_s = reshape_by_heads(self.Wk_s(input1[:, self.model_params['node_size']:]), head_num=head_num)\n        v_s = reshape_by_heads(self.Wv_s(input1[:, self.model_params['node_size']:]), head_num=head_num)\n        # q shape: (batch, HEAD_NUM, problem, KEY_DIM)\n\n        out_concat_nn = multi_head_attention(q_n, k_n, v_n)\n        # shape: (batch, problem, HEAD_NUM*KEY_DIM)\n\n        out_concat_ns = multi_head_attention(q_n, k_s, v_s, rank2_ninf_mask=sols_mask)\n        # shape: (batch, problem, HEAD_NUM*KEY_DIM)\n\n        multi_head_out_n = self.multi_head_combine(out_concat_nn + out_concat_ns)\n        # shape: (batch, problem, EMBEDDING_DIM)\n\n        out_concat_sn = multi_head_attention(q_s, k_n, v_n)\n        # shape: (batch, problem, HEAD_NUM*KEY_DIM)\n\n        multi_head_out_s = self.multi_head_combine_s(out_concat_sn)\n        # shape: (batch, problem, EMBEDDING_DIM)\n\n        multi_head_out = torch.cat((multi_head_out_n, multi_head_out_s), dim=1)\n\n        out1 = self.add_n_normalization_1(input1, multi_head_out)\n        out2 = self.feed_forward(out1)\n        out3 = self.add_n_normalization_2(out1, out2)\n\n        return out3\n        # shape: (batch, problem, embedding)\n\n\n########################################\n# DECODER\n########################################\n\nclass CVRP_Decoder(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n        self.model_params = model_params\n        embedding_dim = self.model_params['embedding_dim']\n        head_num = self.model_params['head_num']\n        qkv_dim = self.model_params['qkv_dim']\n        \n        hyper_input_dim = 2 + 2\n        hyper_hidden_embd_dim = self.model_params['hyper_hidden_dim']\n        self.embd_dim = 2 + 2\n        self.hyper_output_dim = 6 * self.embd_dim\n        \n        self.hyper_fc1 = nn.Linear(hyper_input_dim, hyper_hidden_embd_dim, bias=True)\n        self.hyper_fc2 = nn.Linear(hyper_hidden_embd_dim, hyper_hidden_embd_dim, bias=True)\n        self.hyper_fc3 = nn.Linear(hyper_hidden_embd_dim, self.hyper_output_dim, bias=True)\n        \n        self.hyper_Wq_last = nn.Linear(self.embd_dim, (embedding_dim + 1) * head_num * qkv_dim, bias=False)\n        self.hyper_Wk = nn.Linear(self.embd_dim, embedding_dim * head_num * qkv_dim, bias=False)\n        self.hyper_Wv = nn.Linear(self.embd_dim, embedding_dim * head_num * qkv_dim, bias=False)\n        self.hyper_multi_head_combine = nn.Linear(self.embd_dim, head_num * qkv_dim * embedding_dim, bias=False)\n        self.hyper_Wk_s = nn.Linear(self.embd_dim, embedding_dim * head_num * qkv_dim, bias=False)\n        self.hyper_Wv_s = nn.Linear(self.embd_dim, embedding_dim * head_num * qkv_dim, bias=False)\n\n        self.Wq_last_para = None\n        self.multi_head_combine_para = None\n\n\n        self.k = None  # saved key, for multi-head attention\n        self.v = None  # saved value, for multi-head_attention\n        self.single_head_key = None  # saved, for single-head attention\n     \n    def assign(self, pref):\n        \n        embedding_dim = self.model_params['embedding_dim']\n        head_num = self.model_params['head_num']\n        qkv_dim = self.model_params['qkv_dim']\n        \n        hyper_embd = self.hyper_fc1(pref)\n        hyper_embd = self.hyper_fc2(hyper_embd)\n        mid_embd = self.hyper_fc3(hyper_embd)\n        \n        self.Wq_last_para = self.hyper_Wq_last(mid_embd[:1 * self.embd_dim]).reshape(head_num * qkv_dim, embedding_dim + 1)\n        self.Wk_para = self.hyper_Wk(mid_embd[1 * self.embd_dim: 2 * self.embd_dim]).reshape(head_num * qkv_dim, embedding_dim)\n        self.Wv_para = self.hyper_Wv(mid_embd[2 * self.embd_dim: 3 * self.embd_dim]).reshape(head_num * qkv_dim, embedding_dim)\n        self.multi_head_combine_para = self.hyper_multi_head_combine(mid_embd[3 * self.embd_dim: 4 * self.embd_dim]).reshape(head_num * qkv_dim, embedding_dim)\n        self.Wk_para_s = self.hyper_Wk_s(mid_embd[4 * self.embd_dim: 5 * self.embd_dim]).reshape(embedding_dim,\n                                                                                                 head_num * qkv_dim)\n        self.Wv_para_s = self.hyper_Wv_s(mid_embd[5 * self.embd_dim: 6 * self.embd_dim]).reshape(embedding_dim,\n                                                                                                 head_num * qkv_dim)\n        \n\n    def set_kv(self, encoded_nodes):\n        # encoded_nodes.shape: (batch, problem+1, embedding)\n        head_num = self.model_params['head_num']\n\n        self.k = reshape_by_heads(F.linear(encoded_nodes[:, :self.model_params['node_size']], self.Wk_para),\n                                  head_num=head_num)\n        self.v = reshape_by_heads(F.linear(encoded_nodes[:, :self.model_params['node_size']], self.Wv_para),\n                                  head_num=head_num)\n\n        self.k_s = reshape_by_heads(F.linear(encoded_nodes[:, self.model_params['node_size']:], self.Wk_para_s),\n                                    head_num=head_num)\n        self.v_s = reshape_by_heads(F.linear(encoded_nodes[:, self.model_params['node_size']:], self.Wv_para_s),\n                                    head_num=head_num)\n        \n        self.single_head_key = encoded_nodes[:, :self.model_params['node_size']].transpose(1, 2)\n       \n \n    def forward(self, encoded_last_node, load, sols_mask_pomo, ninf_mask):\n        # encoded_last_node.shape: (batch, pomo, embedding)\n        # ninf_mask.shape: (batch, pomo, problem)\n\n        embedding_dim = self.model_params['embedding_dim']\n        head_num = self.model_params['head_num']\n        qkv_dim = self.model_params['qkv_dim']\n        \n        head_num = self.model_params['head_num']\n\n        #  Multi-Head Attention\n        #######################################################\n        input_cat = torch.cat((encoded_last_node, load[:, :, None]), dim=2)\n\n        # shape = (batch, group, EMBEDDING_DIM+1)\n        q_last = reshape_by_heads(F.linear(input_cat, self.Wq_last_para), head_num = head_num)\n        q = q_last\n\n        out_concat_n = multi_head_attention(q, self.k, self.v, rank3_ninf_mask=ninf_mask)\n        out_concat_s = multi_head_attention(q, self.k_s, self.v_s, rank3_ninf_mask=sols_mask_pomo)\n        out_concat = out_concat_n + out_concat_s\n        # shape: (batch, pomo, head_num*qkv_dim)\n\n        mh_atten_out = F.linear(out_concat, self.multi_head_combine_para)\n        # shape: (batch, pomo, embedding)\n\n        #  Single-Head Attention, for probability calculation\n        #######################################################\n        score = torch.matmul(mh_atten_out, self.single_head_key)\n        # shape: (batch, pomo, problem)\n\n        sqrt_embedding_dim = self.model_params['sqrt_embedding_dim']\n        logit_clipping = self.model_params['logit_clipping']\n\n        score_scaled = score / sqrt_embedding_dim\n        # shape: (batch, pomo, problem)\n\n        score_clipped = logit_clipping * torch.tanh(score_scaled)\n\n        score_masked = score_clipped + ninf_mask\n\n        probs = F.softmax(score_masked, dim=2)\n        # shape: (batch, pomo, problem)\n\n        return probs\n\n\n########################################\n# NN SUB CLASS / FUNCTIONS\n########################################\n\ndef reshape_by_heads(qkv, head_num):\n    # q.shape: (batch, n, head_num*key_dim)   : n can be either 1 or PROBLEM_SIZE\n\n    batch_s = qkv.size(0)\n    n = qkv.size(1)\n\n    q_reshaped = qkv.reshape(batch_s, n, head_num, -1)\n    # shape: (batch, n, head_num, key_dim)\n\n    q_transposed = q_reshaped.transpose(1, 2)\n    # shape: (batch, head_num, n, key_dim)\n\n    return q_transposed\n\n\ndef multi_head_attention(q, k, v, rank2_ninf_mask=None, rank3_ninf_mask=None):\n    # q shape: (batch, head_num, n, key_dim)   : n can be either 1 or PROBLEM_SIZE\n    # k,v shape: (batch, head_num, problem, key_dim)\n    # rank2_ninf_mask.shape: (batch, problem)\n    # rank3_ninf_mask.shape: (batch, group, problem)\n\n    batch_s = q.size(0)\n    head_num = q.size(1)\n    n = q.size(2)\n    key_dim = q.size(3)\n\n    input_s = k.size(2)\n\n    score = torch.matmul(q, k.transpose(2, 3))\n    # shape: (batch, head_num, n, problem)\n\n    score_scaled = score / torch.sqrt(torch.tensor(key_dim, dtype=torch.float))\n    if rank2_ninf_mask is not None:\n        score_scaled = score_scaled + rank2_ninf_mask[:, None, None, :].expand(batch_s, head_num, n, input_s)\n    if rank3_ninf_mask is not None:\n        score_scaled = score_scaled + rank3_ninf_mask[:, None, :, :].expand(batch_s, head_num, n, input_s)\n\n    weights = nn.Softmax(dim=3)(score_scaled)\n    # shape: (batch, head_num, n, problem)\n\n    out = torch.matmul(weights, v)\n    # shape: (batch, head_num, n, key_dim)\n\n    out_transposed = out.transpose(1, 2)\n    # shape: (batch, n, head_num, key_dim)\n\n    out_concat = out_transposed.reshape(batch_s, n, head_num * key_dim)\n    # shape: (batch, n, head_num*key_dim)\n\n    return out_concat\n\n# ==========================================\n# File: NHDE-P/MOKP/POMO/MOKPModel.py\n# Function/Context: KPModel\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nclass KPModel(nn.Module):\n\n    def __init__(self, **model_params):\n        super().__init__()\n        self.model_params = model_params\n\n        self.encoder = KP_Encoder(**model_params)\n        self.decoder = KP_Decoder(**model_params)\n        self.encoded_nodes_and_dummy = None\n        self.encoded_nodes = None\n        self.encoded_graph = None\n        # shape: (batch, problem, EMBEDDING_DIM)\n\n    def pre_forward(self, reset_state, sols, sols_mask):\n        #self.encoded_nodes = self.encoder(reset_state.problems)\n        # shape: (batch, problem, EMBEDDING_DIM)\n\n        self.encoded_nodes = self.encoder(reset_state.problems, sols, sols_mask)\n        \n        self.encoded_graph = self.encoded_nodes.mean(dim=1, keepdim=True)\n        \n        self.decoder.set_kv(self.encoded_nodes)\n\n    def forward(self, state, sols_mask_pomo):\n        batch_size = state.BATCH_IDX.size(0)\n        pomo_size = state.BATCH_IDX.size(1)\n        \n        # shape: (batch, pomo, embedding)\n        probs = self.decoder(self.encoded_graph, capacity = state.capacity, sols_mask_pomo=sols_mask_pomo, ninf_mask=state.ninf_mask)\n        # shape: (batch, pomo, problem)\n\n        if self.training or self.model_params['eval_type'] == 'softmax':\n            selected = probs.reshape(batch_size * pomo_size, -1).multinomial(1) \\\n                .squeeze(dim=1).reshape(batch_size, pomo_size)\n            # shape: (batch, pomo)\n\n            prob = probs[state.BATCH_IDX, state.POMO_IDX, selected] \\\n                .reshape(batch_size, pomo_size)\n            # shape: (batch, pomo)\n\n        else:\n            selected = probs.argmax(dim=2)\n            # shape: (batch, pomo)\n            prob = None\n\n\n        return selected, prob\n\n\ndef _get_encoding(encoded_nodes, node_index_to_pick):\n    # encoded_nodes.shape: (batch, problem, embedding)\n    # node_index_to_pick.shape: (batch, pomo)\n\n    batch_size = node_index_to_pick.size(0)\n    pomo_size = node_index_to_pick.size(1)\n    embedding_dim = encoded_nodes.size(2)\n\n    gathering_index = node_index_to_pick[:, :, None].expand(batch_size, pomo_size, embedding_dim)\n    # shape: (batch, pomo, embedding)\n\n    picked_nodes = encoded_nodes.gather(dim=1, index=gathering_index)\n    # shape: (batch, pomo, embedding)\n\n    return picked_nodes\n\n\n########################################\n# ENCODER\n########################################\n\nclass KP_Encoder(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n        self.model_params = model_params\n        embedding_dim = self.model_params['embedding_dim']\n        encoder_layer_num = self.model_params['encoder_layer_num']\n\n        self.embedding = nn.Linear(3, embedding_dim)\n        self.sols_embedding = nn.Linear(2, embedding_dim)\n        self.layers = nn.ModuleList([EncoderLayer(**model_params) for _ in range(encoder_layer_num)])\n\n    def forward(self, data, sols, sols_mask):\n        # data.shape: (batch, problem, 2)\n\n        embedded_input = self.embedding(data)\n        # shape: (batch, problem, embedding)\n\n        embedded_input_s = self.sols_embedding(sols)\n        # shape: (batch, sols, embedding)\n\n        # out = embedded_input\n        out = torch.cat((embedded_input, embedded_input_s), dim=1)\n        for layer in self.layers:\n            out = layer(out, sols_mask)\n\n        return out\n\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n        self.model_params = model_params\n        embedding_dim = self.model_params['embedding_dim']\n        head_num = self.model_params['head_num']\n        qkv_dim = self.model_params['qkv_dim']\n\n        self.Wq = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.Wk = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.Wv = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.multi_head_combine = nn.Linear(head_num * qkv_dim, embedding_dim)\n\n        self.Wq_s = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.Wk_s = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.Wv_s = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.multi_head_combine_s = nn.Linear(head_num * qkv_dim, embedding_dim)\n\n        self.addAndNormalization1 = Add_And_Normalization_Module(**model_params)\n        self.feedForward = Feed_Forward_Module(**model_params)\n        self.addAndNormalization2 = Add_And_Normalization_Module(**model_params)\n\n    def forward(self, input1, sols_mask):\n        # input.shape: (batch, problem, EMBEDDING_DIM)\n        head_num = self.model_params['head_num']\n\n        q_n = reshape_by_heads(self.Wq(input1[:, :self.model_params['node_size']]), head_num=head_num)\n        k_n = reshape_by_heads(self.Wk(input1[:, :self.model_params['node_size']]), head_num=head_num)\n        v_n = reshape_by_heads(self.Wv(input1[:, :self.model_params['node_size']]), head_num=head_num)\n        # q shape: (batch, HEAD_NUM, problem, KEY_DIM)\n\n        q_s = reshape_by_heads(self.Wq_s(input1[:, self.model_params['node_size']:]), head_num=head_num)\n        k_s = reshape_by_heads(self.Wk_s(input1[:, self.model_params['node_size']:]), head_num=head_num)\n        v_s = reshape_by_heads(self.Wv_s(input1[:, self.model_params['node_size']:]), head_num=head_num)\n        # q shape: (batch, HEAD_NUM, problem, KEY_DIM)\n\n        out_concat_nn = multi_head_attention(q_n, k_n, v_n)\n        # shape: (batch, problem, HEAD_NUM*KEY_DIM)\n\n        out_concat_ns = multi_head_attention(q_n, k_s, v_s, rank2_ninf_mask=sols_mask)\n        # shape: (batch, problem, HEAD_NUM*KEY_DIM)\n\n        multi_head_out_n = self.multi_head_combine(out_concat_nn + out_concat_ns)\n        # shape: (batch, problem, EMBEDDING_DIM)\n\n        out_concat_sn = multi_head_attention(q_s, k_n, v_n)\n        # shape: (batch, problem, HEAD_NUM*KEY_DIM)\n\n        multi_head_out_s = self.multi_head_combine_s(out_concat_sn)\n        # shape: (batch, problem, EMBEDDING_DIM)\n\n        multi_head_out = torch.cat((multi_head_out_n, multi_head_out_s), dim=1)\n\n        out1 = self.addAndNormalization1(input1, multi_head_out)\n        out2 = self.feedForward(out1)\n        out3 = self.addAndNormalization2(out1, out2)\n\n        return out3\n        # shape: (batch, problem, EMBEDDING_DIM)\n\n\n########################################\n# DECODER\n########################################\n\nclass KP_Decoder(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n        self.model_params = model_params\n        embedding_dim = self.model_params['embedding_dim']\n        head_num = self.model_params['head_num']\n        qkv_dim = self.model_params['qkv_dim']\n        \n        hyper_input_dim = 2 + 2\n        hyper_hidden_embd_dim = self.model_params['hyper_hidden_dim']\n        self.embd_dim = 2 + 2\n        self.hyper_output_dim = 6 * self.embd_dim\n        \n        self.hyper_fc1 = nn.Linear(hyper_input_dim, hyper_hidden_embd_dim, bias=True)\n        self.hyper_fc2 = nn.Linear(hyper_hidden_embd_dim, hyper_hidden_embd_dim, bias=True)\n        self.hyper_fc3 = nn.Linear(hyper_hidden_embd_dim, self.hyper_output_dim, bias=True)\n        \n        self.hyper_Wq = nn.Linear(self.embd_dim, (1 + embedding_dim) * head_num * qkv_dim, bias=False)\n        self.hyper_Wk = nn.Linear(self.embd_dim, embedding_dim * head_num * qkv_dim, bias=False)\n        self.hyper_Wv = nn.Linear(self.embd_dim, embedding_dim * head_num * qkv_dim, bias=False)\n        self.hyper_multi_head_combine = nn.Linear(self.embd_dim, head_num * qkv_dim * embedding_dim, bias=False)\n        self.hyper_Wk_s = nn.Linear(self.embd_dim, embedding_dim * head_num * qkv_dim, bias=False)\n        self.hyper_Wv_s = nn.Linear(self.embd_dim, embedding_dim * head_num * qkv_dim, bias=False)\n\n        self.Wq_para = None\n        self.multi_head_combine_para = None\n        \n        self.k = None  # saved key, for multi-head attention\n        self.v = None  # saved value, for multi-head_attention\n        self.single_head_key = None  # saved, for single-head attention\n        \n    def assign(self, pref):\n        \n        embedding_dim = self.model_params['embedding_dim']\n        head_num = self.model_params['head_num']\n        qkv_dim = self.model_params['qkv_dim']\n        \n        hyper_embd = self.hyper_fc1(pref)\n        hyper_embd = self.hyper_fc2(hyper_embd)\n        mid_embd = self.hyper_fc3(hyper_embd)\n        \n        self.Wq_para = self.hyper_Wq(mid_embd[:self.embd_dim]).reshape(head_num * qkv_dim, (1 + embedding_dim))\n        self.Wk_para = self.hyper_Wk(mid_embd[1 * self.embd_dim: 2 * self.embd_dim]).reshape(head_num * qkv_dim, embedding_dim)\n        self.Wv_para = self.hyper_Wv(mid_embd[2 * self.embd_dim: 3 * self.embd_dim]).reshape(head_num * qkv_dim, embedding_dim)\n        self.multi_head_combine_para = self.hyper_multi_head_combine(mid_embd[3 * self.embd_dim: 4 * self.embd_dim]).reshape(head_num * qkv_dim, embedding_dim)\n        self.Wk_para_s = self.hyper_Wk_s(mid_embd[4 * self.embd_dim: 5 * self.embd_dim]).reshape(embedding_dim,\n                                                                                                 head_num * qkv_dim)\n        self.Wv_para_s = self.hyper_Wv_s(mid_embd[5 * self.embd_dim: 6 * self.embd_dim]).reshape(embedding_dim,\n                                                                                                 head_num * qkv_dim)\n        \n        \n    def set_kv(self, encoded_nodes):\n        # encoded_nodes.shape: (batch, problem, embedding)\n        head_num = self.model_params['head_num']\n\n        self.k = reshape_by_heads(F.linear(encoded_nodes[:, :self.model_params['node_size']], self.Wk_para),\n                                  head_num=head_num)\n        self.v = reshape_by_heads(F.linear(encoded_nodes[:, :self.model_params['node_size']], self.Wv_para),\n                                  head_num=head_num)\n\n        self.k_s = reshape_by_heads(F.linear(encoded_nodes[:, self.model_params['node_size']:], self.Wk_para_s),\n                                    head_num=head_num)\n        self.v_s = reshape_by_heads(F.linear(encoded_nodes[:, self.model_params['node_size']:], self.Wv_para_s),\n                                    head_num=head_num)\n\n        self.single_head_key = encoded_nodes[:, :self.model_params['node_size']].transpose(1, 2)\n     \n    def forward(self, graph, capacity, sols_mask_pomo, ninf_mask):\n        # encoded_last_node.shape: (batch, pomo, embedding)\n        # ninf_mask.shape: (batch, pomo, problem)\n        embedding_dim = self.model_params['embedding_dim']\n        head_num = self.model_params['head_num']\n        qkv_dim = self.model_params['qkv_dim']\n        \n        batch_size = capacity.size(0)\n        group_size = capacity.size(1)\n\n        #  Multi-Head Attention\n        #######################################################\n        input1 = graph.expand(batch_size, group_size, embedding_dim)\n        input2 = capacity[:, :, None]\n        input_cat = torch.cat((input1, input2), dim=2)\n        \n        #  Multi-Head Attention\n        #######################################################\n        q = reshape_by_heads(F.linear(input_cat, self.Wq_para), head_num = head_num)\n\n        out_concat_n = multi_head_attention(q, self.k, self.v, ninf_mask=ninf_mask)\n        out_concat_s = multi_head_attention(q, self.k_s, self.v_s, ninf_mask=sols_mask_pomo)\n        out_concat = out_concat_n + out_concat_s\n       \n        mh_atten_out = F.linear(out_concat, self.multi_head_combine_para)\n        # shape: (batch, pomo, embedding)\n\n        #  Single-Head Attention, for probability calculation\n        #######################################################\n        score = torch.matmul(mh_atten_out, self.single_head_key)\n        # shape: (batch, pomo, problem)\n\n        sqrt_embedding_dim = self.model_params['sqrt_embedding_dim']\n        logit_clipping = self.model_params['logit_clipping']\n\n        score_scaled = score / sqrt_embedding_dim\n        # shape: (batch, pomo, problem)\n\n        score_clipped = logit_clipping * torch.tanh(score_scaled)\n\n        #score_masked = score_clipped + ninf_mask\n        if ninf_mask is None:\n            score_masked = score_clipped\n        else:\n            score_masked = score_clipped + ninf_mask\n\n        probs = F.softmax(score_masked, dim=2)\n        # shape: (batch, pomo, problem)\n\n        return probs\n\n\n########################################\n# NN SUB CLASS / FUNCTIONS\n########################################\n\ndef reshape_by_heads(qkv, head_num):\n    # q.shape: (batch, n, head_num*key_dim)   : n can be either 1 or PROBLEM_SIZE\n\n    batch_s = qkv.size(0)\n    n = qkv.size(1)\n\n    q_reshaped = qkv.reshape(batch_s, n, head_num, -1)\n    # shape: (batch, n, head_num, key_dim)\n\n    q_transposed = q_reshaped.transpose(1, 2)\n    # shape: (batch, head_num, n, key_dim)\n\n    return q_transposed\n\ndef multi_head_attention(q, k, v, rank2_ninf_mask=None, ninf_mask=None):\n    # q shape = (batch, head_num, n, key_dim)   : n can be either 1 or group\n    # k,v shape = (batch, head_num, problem, key_dim)\n    # rank2_ninf_mask.shape: (batch, problem)\n    # ninf_mask.shape = (batch, group, problem)\n\n    batch_s = q.size(0)\n    head_num = q.size(1)\n    n = q.size(2)\n    key_dim = q.size(3)\n    problem_s = k.size(2)\n\n    score = torch.matmul(q, k.transpose(2, 3))\n    # shape = (batch, head_num, n, TSP_SIZE)\n\n    score_scaled = score / torch.sqrt(torch.tensor(key_dim, dtype=torch.float))\n    if rank2_ninf_mask is not None:\n        score_scaled = score_scaled + rank2_ninf_mask[:, None, None, :].expand(batch_s, head_num, n, problem_s)\n    if ninf_mask is not None:\n        score_scaled = score_scaled + ninf_mask[:, None, :, :].expand(batch_s, head_num, n, problem_s)\n\n    weights = nn.Softmax(dim=3)(score_scaled)\n    # shape = (batch, head_num, n, problem)\n\n    out = torch.matmul(weights, v)\n    # shape = (batch, head_num, n, key_dim)\n\n    out_transposed = out.transpose(1, 2)\n    # shape = (batch, n, head_num, key_dim)\n\n    out_concat = out_transposed.reshape(batch_s, n, head_num * key_dim)\n    # shape = (batch, n, head_num*key_dim)\n\n    return out_concat\n\n\nclass Add_And_Normalization_Module(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n        embedding_dim = model_params['embedding_dim']\n        self.norm = nn.InstanceNorm1d(embedding_dim, affine=True, track_running_stats=False)\n\n    def forward(self, input1, input2):\n        # input.shape: (batch, problem, embedding)\n\n        added = input1 + input2\n        # shape: (batch, problem, embedding)\n\n        transposed = added.transpose(1, 2)\n        # shape: (batch, embedding, problem)\n\n        normalized = self.norm(transposed)\n        # shape: (batch, embedding, problem)\n\n        back_trans = normalized.transpose(1, 2)\n        # shape: (batch, problem, embedding)\n\n        return back_trans\n\n\nclass Feed_Forward_Module(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n\n# ==========================================\n# File: NHDE-P/MOTSP/POMO/MOTSPEnv.py\n# Function/Context: TSPEnv\n# ==========================================\nfrom dataclasses import dataclass\nimport torch\n\n# from MOTSProblemDef import get_random_problems, augment_xy_data_by_64_fold_2obj\nfrom MOTSP.MOTSProblemDef import get_random_problems, augment_xy_data_by_64_fold_2obj\n\n\n@dataclass\nclass Reset_State:\n    problems: torch.Tensor\n    # shape: (batch, problem, 2)\n\n@dataclass\nclass Step_State:\n    BATCH_IDX: torch.Tensor\n    POMO_IDX: torch.Tensor\n    # shape: (batch, pomo)\n    current_node: torch.Tensor = None\n    # shape: (batch, pomo)\n    ninf_mask: torch.Tensor = None\n    # shape: (batch, pomo, node)\n\n\nclass TSPEnv:\n    def __init__(self, **env_params):\n\n        # Const @INIT\n        ####################################\n        self.env_params = env_params\n        self.problem_size = env_params['problem_size']\n        self.pomo_size = env_params['pomo_size']\n\n        # Const @Load_Problem\n        ####################################\n        self.batch_size = None\n        self.BATCH_IDX = None\n        self.POMO_IDX = None\n        # IDX.shape: (batch, pomo)\n        self.problems = None\n        # shape: (batch, node, node)\n\n        # Dynamic\n        ####################################\n        self.selected_count = None\n        self.current_node = None\n        # shape: (batch, pomo)\n        self.selected_node_list = None\n        # shape: (batch, pomo, 0~problem)\n\n    def load_problems(self, batch_size, aug_factor=1, problems=None):\n        self.batch_size = batch_size\n        if problems is not None:\n            self.problems = problems\n        else:\n            self.problems = get_random_problems(batch_size, self.problem_size)\n        # problems.shape: (batch, problem, 2)\n        if aug_factor > 1:\n            if aug_factor == 64:\n                self.batch_size = self.batch_size * 64\n                self.problems = augment_xy_data_by_64_fold_2obj(self.problems)\n            else:\n                raise NotImplementedError\n\n        self.BATCH_IDX = torch.arange(self.batch_size)[:, None].expand(self.batch_size, self.pomo_size)\n        self.POMO_IDX = torch.arange(self.pomo_size)[None, :].expand(self.batch_size, self.pomo_size)\n\n    def reset(self):\n        self.selected_count = 0\n        self.current_node = None\n        # shape: (batch, pomo)\n        self.selected_node_list = torch.zeros((self.batch_size, self.pomo_size, 0), dtype=torch.long)\n        # shape: (batch, pomo, 0~problem)\n\n        # CREATE STEP STATE\n        self.step_state = Step_State(BATCH_IDX=self.BATCH_IDX, POMO_IDX=self.POMO_IDX)\n        self.step_state.ninf_mask = torch.zeros((self.batch_size, self.pomo_size, self.problem_size))\n        # shape: (batch, pomo, problem)\n\n        reward = None\n        done = False\n        return Reset_State(self.problems), reward, done\n\n    def pre_step(self):\n        reward = None\n        done = False\n        return self.step_state, reward, done\n\n    def step(self, selected):\n        # selected.shape: (batch, pomo)\n\n        self.selected_count += 1\n        self.current_node = selected\n        # shape: (batch, pomo)\n        self.selected_node_list = torch.cat((self.selected_node_list, self.current_node[:, :, None]), dim=2)\n        # shape: (batch, pomo, 0~problem)\n\n        # UPDATE STEP STATE\n        self.step_state.current_node = self.current_node\n        # shape: (batch, pomo)\n        self.step_state.ninf_mask[self.BATCH_IDX, self.POMO_IDX, self.current_node] = float('-inf')\n        # shape: (batch, pomo, node)\n\n        # returning values\n        done = (self.selected_count == self.problem_size)\n        if done:\n            reward = -self._get_travel_distance()  # note the minus sign!\n        else:\n            reward = None\n\n        return self.step_state, reward, done\n\n    def _get_travel_distance(self):\n       \n        gathering_index = self.selected_node_list.unsqueeze(3).expand(self.batch_size, -1, self.problem_size, 4)\n        # shape: (batch, pomo, problem, 4)\n        seq_expanded = self.problems[:, None, :, :].expand(self.batch_size, self.pomo_size, self.problem_size, 4)\n\n        ordered_seq = seq_expanded.gather(dim=2, index=gathering_index)\n        # shape: (batch, pomo, problem, 2)\n        rolled_seq = ordered_seq.roll(dims=2, shifts=-1)\n        \n        segment_lengths_obj1 = ((ordered_seq[:, :, :, :2]-rolled_seq[:, :, :, :2])**2).sum(3).sqrt()\n        segment_lengths_obj2 = ((ordered_seq[:, :, :, 2:]-rolled_seq[:, :, :, 2:])**2).sum(3).sqrt()\n\n        travel_distances_obj1 = segment_lengths_obj1.sum(2)\n        travel_distances_obj2 = segment_lengths_obj2.sum(2)\n    \n        travel_distances_vec = torch.stack([travel_distances_obj1,travel_distances_obj2],axis = 2)\n        \n        # shape: (batch, pomo)\n        return travel_distances_vec\n\n# ==========================================\n# File: NHDE-P/MOTSP/POMO/MOTSPModel.py\n# Function/Context: TSPModel\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass TSPModel(nn.Module):\n\n    def __init__(self, **model_params):\n        super().__init__()\n        self.model_params = model_params\n\n        self.encoder = TSP_Encoder(**model_params)\n        self.decoder = TSP_Decoder(**model_params)\n        self.encoded_nodes = None\n        # shape: (batch, problem, EMBEDDING_DIM)\n\n    def pre_forward(self, reset_state, sols, sols_mask):\n        self.encoded_nodes = self.encoder(reset_state.problems, sols, sols_mask)\n        # shape: (batch, problem, EMBEDDING_DIM)\n        self.decoder.set_kv(self.encoded_nodes)\n\n    def forward(self, state, sols_mask_pomo):\n        batch_size = state.BATCH_IDX.size(0)\n        pomo_size = state.BATCH_IDX.size(1)\n        \n        if state.current_node is None:\n            selected = torch.arange(pomo_size)[None, :].expand(batch_size, pomo_size)\n            prob = torch.ones(size=(batch_size, pomo_size))\n\n            encoded_first_node = _get_encoding(self.encoded_nodes, selected)\n            # shape: (batch, pomo, embedding)\n            self.decoder.set_q1(encoded_first_node)\n\n        else:\n            encoded_last_node = _get_encoding(self.encoded_nodes, state.current_node)\n            # shape: (batch, pomo, embedding)\n            probs = self.decoder(encoded_last_node, sols_mask_pomo, ninf_mask=state.ninf_mask)\n            # shape: (batch, pomo, problem)\n\n            if self.training or self.model_params['eval_type'] == 'softmax':\n                selected = probs.reshape(batch_size * pomo_size, -1).multinomial(1) \\\n                    .squeeze(dim=1).reshape(batch_size, pomo_size)\n                # shape: (batch, pomo)\n\n                prob = probs[state.BATCH_IDX, state.POMO_IDX, selected] \\\n                    .reshape(batch_size, pomo_size)\n                # shape: (batch, pomo)\n\n            else:\n                selected = probs.argmax(dim=2)\n                # shape: (batch, pomo)\n                prob = None\n                \n        return selected, prob\n\ndef _get_encoding(encoded_nodes, node_index_to_pick):\n    # encoded_nodes.shape: (batch, problem, embedding)\n    # node_index_to_pick.shape: (batch, pomo)\n\n    batch_size = node_index_to_pick.size(0)\n    pomo_size = node_index_to_pick.size(1)\n    embedding_dim = encoded_nodes.size(2)\n\n    gathering_index = node_index_to_pick[:, :, None].expand(batch_size, pomo_size, embedding_dim)\n    # shape: (batch, pomo, embedding)\n\n    picked_nodes = encoded_nodes.gather(dim=1, index=gathering_index)\n    # shape: (batch, pomo, embedding)\n\n    return picked_nodes\n\n\n########################################\n# ENCODER\n########################################\n\nclass TSP_Encoder(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n        self.model_params = model_params\n        embedding_dim = self.model_params['embedding_dim']\n        encoder_layer_num = self.model_params['encoder_layer_num']\n\n        self.embedding = nn.Linear(4, embedding_dim)\n        self.sols_embedding = nn.Linear(2, embedding_dim)\n        self.layers = nn.ModuleList([EncoderLayer(**model_params) for _ in range(encoder_layer_num)])\n\n    def forward(self, data, sols, sols_mask):\n        # data.shape: (batch, problem, 2)\n\n        embedded_input = self.embedding(data)\n        # shape: (batch, problem, embedding)\n\n        embedded_input_s = self.sols_embedding(sols)\n        # shape: (batch, sols, embedding)\n\n        # out = embedded_input\n        out = torch.cat((embedded_input, embedded_input_s), dim=1)\n        for layer in self.layers:\n            out = layer(out, sols_mask)\n\n        return out\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n        self.model_params = model_params\n        embedding_dim = self.model_params['embedding_dim']\n        head_num = self.model_params['head_num']\n        qkv_dim = self.model_params['qkv_dim']\n\n        self.Wq = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.Wk = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.Wv = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.multi_head_combine = nn.Linear(head_num * qkv_dim, embedding_dim)\n\n        self.Wq_s = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.Wk_s = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.Wv_s = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.multi_head_combine_s = nn.Linear(head_num * qkv_dim, embedding_dim)\n\n        self.addAndNormalization1 = Add_And_Normalization_Module(**model_params)\n        self.feedForward = Feed_Forward_Module(**model_params)\n        self.addAndNormalization2 = Add_And_Normalization_Module(**model_params)\n\n    def forward(self, input1, sols_mask):\n        # input.shape: (batch, problem, EMBEDDING_DIM)\n        head_num = self.model_params['head_num']\n\n        q_n = reshape_by_heads(self.Wq(input1[:, :self.model_params['node_size']]), head_num=head_num)\n        k_n = reshape_by_heads(self.Wk(input1[:, :self.model_params['node_size']]), head_num=head_num)\n        v_n = reshape_by_heads(self.Wv(input1[:, :self.model_params['node_size']]), head_num=head_num)\n        # q shape: (batch, HEAD_NUM, problem, KEY_DIM)\n\n        q_s = reshape_by_heads(self.Wq_s(input1[:, self.model_params['node_size']:]), head_num=head_num)\n        k_s = reshape_by_heads(self.Wk_s(input1[:, self.model_params['node_size']:]), head_num=head_num)\n        v_s = reshape_by_heads(self.Wv_s(input1[:, self.model_params['node_size']:]), head_num=head_num)\n        # q shape: (batch, HEAD_NUM, problem, KEY_DIM)\n\n        out_concat_nn = multi_head_attention(q_n, k_n, v_n)\n        # shape: (batch, problem, HEAD_NUM*KEY_DIM)\n\n        out_concat_ns = multi_head_attention(q_n, k_s, v_s, rank2_ninf_mask=sols_mask)\n        # shape: (batch, problem, HEAD_NUM*KEY_DIM)\n\n        multi_head_out_n = self.multi_head_combine(out_concat_nn + out_concat_ns)\n        # shape: (batch, problem, EMBEDDING_DIM)\n\n        out_concat_sn = multi_head_attention(q_s, k_n, v_n)\n        # shape: (batch, problem, HEAD_NUM*KEY_DIM)\n\n        multi_head_out_s = self.multi_head_combine_s(out_concat_sn)\n        # shape: (batch, problem, EMBEDDING_DIM)\n\n        multi_head_out = torch.cat((multi_head_out_n, multi_head_out_s), dim=1)\n\n        out1 = self.addAndNormalization1(input1, multi_head_out)\n        out2 = self.feedForward(out1)\n        out3 = self.addAndNormalization2(out1, out2)\n\n        return out3\n        # shape: (batch, problem, EMBEDDING_DIM)\n\n\n########################################\n# DECODER\n########################################\n\nclass TSP_Decoder(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n        self.model_params = model_params\n        embedding_dim = self.model_params['embedding_dim']\n        head_num = self.model_params['head_num']\n        qkv_dim = self.model_params['qkv_dim']\n        \n        hyper_input_dim = 2 + 2\n        hyper_hidden_embd_dim = self.model_params['hyper_hidden_dim']\n        self.embd_dim = 2 + 2\n        self.hyper_output_dim = 7 * self.embd_dim\n        \n        self.hyper_fc1 = nn.Linear(hyper_input_dim, hyper_hidden_embd_dim, bias=True)\n        self.hyper_fc2 = nn.Linear(hyper_hidden_embd_dim, hyper_hidden_embd_dim, bias=True)\n        self.hyper_fc3 = nn.Linear(hyper_hidden_embd_dim, self.hyper_output_dim, bias=True)\n        \n        self.hyper_Wq_first = nn.Linear(self.embd_dim, embedding_dim * head_num * qkv_dim, bias=False)\n        self.hyper_Wq_last = nn.Linear(self.embd_dim, embedding_dim * head_num * qkv_dim, bias=False)\n        self.hyper_Wk = nn.Linear(self.embd_dim, embedding_dim * head_num * qkv_dim, bias=False)\n        self.hyper_Wv = nn.Linear(self.embd_dim, embedding_dim * head_num * qkv_dim, bias=False)\n        self.hyper_multi_head_combine = nn.Linear(self.embd_dim, head_num * qkv_dim * embedding_dim, bias=False)\n        self.hyper_Wk_s = nn.Linear(self.embd_dim, embedding_dim * head_num * qkv_dim, bias=False)\n        self.hyper_Wv_s = nn.Linear(self.embd_dim, embedding_dim * head_num * qkv_dim, bias=False)\n\n        self.Wq_last_para = None\n        self.multi_head_combine_para = None\n\n        self.k = None  # saved key, for multi-head attention\n        self.v = None  # saved value, for multi-head_attention\n        self.single_head_key = None  # saved, for single-head attention\n        self.q_first = None  # saved q1, for multi-head attention\n        \n    def assign(self, pref):\n        \n        embedding_dim = self.model_params['embedding_dim']\n        head_num = self.model_params['head_num']\n        qkv_dim = self.model_params['qkv_dim']\n        \n        hyper_embd = self.hyper_fc1(pref)\n        hyper_embd = self.hyper_fc2(hyper_embd)\n        mid_embd = self.hyper_fc3(hyper_embd)\n        \n        self.Wq_first_para = self.hyper_Wq_first(mid_embd[:self.embd_dim]).reshape(embedding_dim, head_num * qkv_dim)\n        self.Wq_last_para = self.hyper_Wq_last(mid_embd[self.embd_dim:2 * self.embd_dim]).reshape(embedding_dim, head_num * qkv_dim)\n        self.Wk_para = self.hyper_Wk(mid_embd[2 * self.embd_dim: 3 * self.embd_dim]).reshape(embedding_dim, head_num * qkv_dim)\n        self.Wv_para = self.hyper_Wv(mid_embd[3 * self.embd_dim: 4 * self.embd_dim]).reshape(embedding_dim, head_num * qkv_dim)\n        self.multi_head_combine_para = self.hyper_multi_head_combine(mid_embd[4 * self.embd_dim: 5 * self.embd_dim]).reshape(head_num * qkv_dim, embedding_dim)\n        self.Wk_para_s = self.hyper_Wk_s(mid_embd[5 * self.embd_dim: 6 * self.embd_dim]).reshape(embedding_dim,\n                                                                                             head_num * qkv_dim)\n        self.Wv_para_s = self.hyper_Wv_s(mid_embd[6 * self.embd_dim: 7 * self.embd_dim]).reshape(embedding_dim,\n                                                                                             head_num * qkv_dim)\n        \n        \n    def set_kv(self, encoded_nodes):\n        # encoded_nodes.shape: (batch, problem, embedding)\n        head_num = self.model_params['head_num']\n\n        self.k = reshape_by_heads(F.linear(encoded_nodes[:, :self.model_params['node_size']], self.Wk_para), head_num=head_num)\n        self.v = reshape_by_heads(F.linear(encoded_nodes[:, :self.model_params['node_size']], self.Wv_para), head_num=head_num)\n\n        self.k_s = reshape_by_heads(F.linear(encoded_nodes[:, self.model_params['node_size']:], self.Wk_para_s),\n                                  head_num=head_num)\n        self.v_s = reshape_by_heads(F.linear(encoded_nodes[:, self.model_params['node_size']:], self.Wv_para_s),\n                                  head_num=head_num)\n        \n        # shape: (batch, head_num, pomo, qkv_dim)\n        self.single_head_key = encoded_nodes[:, :self.model_params['node_size']].transpose(1, 2)\n        # shape: (batch, embedding, problem)\n\n    def set_q1(self, encoded_q1):\n        # encoded_q.shape: (batch, n, embedding)  # n can be 1 or pomo\n        head_num = self.model_params['head_num']\n\n        self.q_first = reshape_by_heads(F.linear(encoded_q1, self.Wq_first_para), head_num=head_num)\n        # shape: (batch, head_num, n, qkv_dim)\n\n    def forward(self, encoded_last_node, sols_mask_pomo, ninf_mask):\n        # encoded_last_node.shape: (batch, pomo, embedding)\n        # ninf_mask.shape: (batch, pomo, problem)\n        \n        embedding_dim = self.model_params['embedding_dim']\n        head_num = self.model_params['head_num']\n        qkv_dim = self.model_params['qkv_dim']\n    \n        head_num = self.model_params['head_num']\n\n        #  Multi-Head Attention\n        #######################################################\n        q_last = reshape_by_heads(F.linear(encoded_last_node, self.Wq_last_para), head_num = head_num)\n      \n        q = self.q_first + q_last \n        # shape: (batch, head_num, pomo, qkv_dim)\n\n        out_concat_n = multi_head_attention(q, self.k, self.v, rank3_ninf_mask=ninf_mask)\n        out_concat_s = multi_head_attention(q, self.k_s, self.v_s, rank3_ninf_mask=sols_mask_pomo)\n        out_concat = out_concat_n + out_concat_s\n        # shape: (batch, pomo, head_num*qkv_dim)\n\n        mh_atten_out = F.linear(out_concat, self.multi_head_combine_para)\n        # shape: (batch, pomo, embedding)\n\n        #  Single-Head Attention, for probability calculation\n        #######################################################\n        score = torch.matmul(mh_atten_out, self.single_head_key)\n        # shape: (batch, pomo, problem)\n\n        sqrt_embedding_dim = self.model_params['sqrt_embedding_dim']\n        logit_clipping = self.model_params['logit_clipping']\n\n        score_scaled = score / sqrt_embedding_dim\n        # shape: (batch, pomo, problem)\n\n        score_clipped = logit_clipping * torch.tanh(score_scaled)\n\n        score_masked = score_clipped + ninf_mask\n\n        probs = F.softmax(score_masked, dim=2)\n        # shape: (batch, pomo, problem)\n\n        return probs\n\n\n########################################\n# NN SUB CLASS / FUNCTIONS\n########################################\n\ndef reshape_by_heads(qkv, head_num):\n    # q.shape: (batch, n, head_num*key_dim)   : n can be either 1 or PROBLEM_SIZE\n\n    batch_s = qkv.size(0)\n    n = qkv.size(1)\n\n    q_reshaped = qkv.reshape(batch_s, n, head_num, -1)\n    # shape: (batch, n, head_num, key_dim)\n\n    q_transposed = q_reshaped.transpose(1, 2)\n    # shape: (batch, head_num, n, key_dim)\n\n    return q_transposed\n\n\ndef multi_head_attention(q, k, v, rank2_ninf_mask=None, rank3_ninf_mask=None):\n    # q shape: (batch, head_num, n, key_dim)   : n can be either 1 or PROBLEM_SIZE\n    # k,v shape: (batch, head_num, problem, key_dim)\n    # rank2_ninf_mask.shape: (batch, problem)\n    # rank3_ninf_mask.shape: (batch, group, problem)\n\n    batch_s = q.size(0)\n    head_num = q.size(1)\n    n = q.size(2)\n    key_dim = q.size(3)\n\n    input_s = k.size(2)\n\n    score = torch.matmul(q, k.transpose(2, 3))\n    # shape: (batch, head_num, n, problem)\n\n    score_scaled = score / torch.sqrt(torch.tensor(key_dim, dtype=torch.float))\n    if rank2_ninf_mask is not None:\n        score_scaled = score_scaled + rank2_ninf_mask[:, None, None, :].expand(batch_s, head_num, n, input_s)\n    if rank3_ninf_mask is not None:\n        score_scaled = score_scaled + rank3_ninf_mask[:, None, :, :].expand(batch_s, head_num, n, input_s)\n\n    weights = nn.Softmax(dim=3)(score_scaled)\n    # shape: (batch, head_num, n, problem)\n\n    out = torch.matmul(weights, v)\n    # shape: (batch, head_num, n, key_dim)\n\n    out_transposed = out.transpose(1, 2)\n    # shape: (batch, n, head_num, key_dim)\n\n    out_concat = out_transposed.reshape(batch_s, n, head_num * key_dim)\n    # shape: (batch, n, head_num*key_dim)\n\n    return out_concat\n\n\nclass Add_And_Normalization_Module(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n        embedding_dim = model_params['embedding_dim']\n        self.norm = nn.InstanceNorm1d(embedding_dim, affine=True, track_running_stats=False)\n\n    def forward(self, input1, input2):\n        # input.shape: (batch, problem, embedding)\n        added = input1 + input2\n        transposed = added.transpose(1, 2)\n        # shape: (batch, embedding, problem)\n        normalized = self.norm(transposed)\n        # shape: (batch, embedding, problem)\n        back_trans = normalized.transpose(1, 2)\n        # shape: (batch, problem, embedding)\n        return back_trans\n\n\nclass Feed_Forward_Module(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n        embedding_dim = model_params['embedding_dim']\n        ff_hidden_dim = model_params['ff_hidden_dim']\n\n        self.W1 = nn.Linear(embedding_dim, ff_hidden_dim)\n        self.W2 = nn.Linear(ff_hidden_dim, embedding_dim)\n\n    def forward(self, input1):\n        # input.shape: (batch, problem, embedding)\n        return self.W2(F.relu(self.W1(input1)))\n\n# ==========================================\n# File: NHDE-P/MOTSP_3obj/POMO/MOTSPEnv_3obj.py\n# Function/Context: TSPEnv\n# ==========================================\nfrom dataclasses import dataclass\nimport torch\n\nfrom MOTSProblemDef_3obj import get_random_problems, augment_xy_data_by_n_fold_3obj\n\n@dataclass\nclass Reset_State:\n    problems: torch.Tensor\n    # shape: (batch, problem, 2)\n\n@dataclass\nclass Step_State:\n    BATCH_IDX: torch.Tensor\n    POMO_IDX: torch.Tensor\n    # shape: (batch, pomo)\n    current_node: torch.Tensor = None\n    # shape: (batch, pomo)\n    ninf_mask: torch.Tensor = None\n    # shape: (batch, pomo, node)\n\n\nclass TSPEnv:\n    def __init__(self, **env_params):\n\n        # Const @INIT\n        ####################################\n        self.env_params = env_params\n        self.problem_size = env_params['problem_size']\n        self.pomo_size = env_params['pomo_size']\n\n        # Const @Load_Problem\n        ####################################\n        self.batch_size = None\n        self.BATCH_IDX = None\n        self.POMO_IDX = None\n        # IDX.shape: (batch, pomo)\n        self.problems = None\n        # shape: (batch, node, node)\n\n        # Dynamic\n        ####################################\n        self.selected_count = None\n        self.current_node = None\n        # shape: (batch, pomo)\n        self.selected_node_list = None\n        # shape: (batch, pomo, 0~problem)\n\n    def load_problems(self, batch_size, aug_factor=1):\n        self.batch_size = batch_size\n\n        self.problems = get_random_problems(batch_size, self.problem_size)\n        # problems.shape: (batch, problem, 2)\n        if aug_factor > 1:\n            if aug_factor <= 512:\n                self.batch_size = self.batch_size * aug_factor\n                self.problems = augment_xy_data_by_n_fold_3obj(self.problems, aug_factor)\n            else:\n                raise NotImplementedError\n\n        self.BATCH_IDX = torch.arange(self.batch_size)[:, None].expand(self.batch_size, self.pomo_size)\n        self.POMO_IDX = torch.arange(self.pomo_size)[None, :].expand(self.batch_size, self.pomo_size)\n\n    def reset(self):\n        self.selected_count = 0\n        self.current_node = None\n        # shape: (batch, pomo)\n        self.selected_node_list = torch.zeros((self.batch_size, self.pomo_size, 0), dtype=torch.long)\n        # shape: (batch, pomo, 0~problem)\n\n        # CREATE STEP STATE\n        self.step_state = Step_State(BATCH_IDX=self.BATCH_IDX, POMO_IDX=self.POMO_IDX)\n        self.step_state.ninf_mask = torch.zeros((self.batch_size, self.pomo_size, self.problem_size))\n        # shape: (batch, pomo, problem)\n\n        reward = None\n        done = False\n        return Reset_State(self.problems), reward, done\n\n    def pre_step(self):\n        reward = None\n        done = False\n        return self.step_state, reward, done\n\n    def step(self, selected):\n        # selected.shape: (batch, pomo)\n\n        self.selected_count += 1\n        self.current_node = selected\n        # shape: (batch, pomo)\n        self.selected_node_list = torch.cat((self.selected_node_list, self.current_node[:, :, None]), dim=2)\n        # shape: (batch, pomo, 0~problem)\n\n        # UPDATE STEP STATE\n        self.step_state.current_node = self.current_node\n        # shape: (batch, pomo)\n        self.step_state.ninf_mask[self.BATCH_IDX, self.POMO_IDX, self.current_node] = float('-inf')\n        # shape: (batch, pomo, node)\n\n        # returning values\n        done = (self.selected_count == self.problem_size)\n        if done:\n            reward = -self._get_travel_distance()  # note the minus sign!\n        else:\n            reward = None\n\n        return self.step_state, reward, done\n\n    def _get_travel_distance(self):\n      \n        gathering_index = self.selected_node_list.unsqueeze(3).expand(self.batch_size, -1, self.problem_size, 6)\n        # shape: (batch, pomo, problem, 6)\n        seq_expanded = self.problems[:, None, :, :].expand(self.batch_size, self.pomo_size, self.problem_size, 6)\n\n        ordered_seq = seq_expanded.gather(dim=2, index=gathering_index)\n        # shape: (batch, pomo, problem, 2)\n        rolled_seq = ordered_seq.roll(dims=2, shifts=-1)\n        \n        segment_lengths_obj1 = ((ordered_seq[:, :, :, :2]-rolled_seq[:, :, :, :2])**2).sum(3).sqrt()\n        segment_lengths_obj2 = ((ordered_seq[:, :, :, 2:4]-rolled_seq[:, :, :, 2:4])**2).sum(3).sqrt()\n        segment_lengths_obj3 = ((ordered_seq[:, :, :, 4:]-rolled_seq[:, :, :, 4:])**2).sum(3).sqrt()\n\n        travel_distances_obj1 = segment_lengths_obj1.sum(2)\n        travel_distances_obj2 = segment_lengths_obj2.sum(2)\n        travel_distances_obj3 = segment_lengths_obj3.sum(2)\n    \n        travel_distances_vec = torch.stack([travel_distances_obj1,travel_distances_obj2,travel_distances_obj3],axis = 2)\n        \n        # shape: (batch, pomo)\n        return travel_distances_vec\n\n# ==========================================\n# File: NHDE-P/MOTSP_3obj/POMO/MOTSPModel_3obj.py\n# Function/Context: TSPModel\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass TSPModel(nn.Module):\n\n    def __init__(self, **model_params):\n        super().__init__()\n        self.model_params = model_params\n\n        self.encoder = TSP_Encoder(**model_params)\n        self.decoder = TSP_Decoder(**model_params)\n        self.encoded_nodes = None\n        # shape: (batch, problem, EMBEDDING_DIM)\n\n    def pre_forward(self, reset_state, sols, sols_mask):\n        self.encoded_nodes = self.encoder(reset_state.problems, sols, sols_mask)\n        # shape: (batch, problem, EMBEDDING_DIM)\n        self.decoder.set_kv(self.encoded_nodes)\n\n    def forward(self, state, sols_mask_pomo):\n        batch_size = state.BATCH_IDX.size(0)\n        pomo_size = state.BATCH_IDX.size(1)\n        \n        if state.current_node is None:\n            selected = torch.arange(pomo_size)[None, :].expand(batch_size, pomo_size)\n            prob = torch.ones(size=(batch_size, pomo_size))\n\n            encoded_first_node = _get_encoding(self.encoded_nodes, selected)\n            # shape: (batch, pomo, embedding)\n            self.decoder.set_q1(encoded_first_node)\n\n        else:\n            encoded_last_node = _get_encoding(self.encoded_nodes, state.current_node)\n            # shape: (batch, pomo, embedding)\n            probs = self.decoder(encoded_last_node, sols_mask_pomo, ninf_mask=state.ninf_mask)\n            # shape: (batch, pomo, problem)\n\n            if self.training or self.model_params['eval_type'] == 'softmax':\n                selected = probs.reshape(batch_size * pomo_size, -1).multinomial(1) \\\n                    .squeeze(dim=1).reshape(batch_size, pomo_size)\n                # shape: (batch, pomo)\n\n                prob = probs[state.BATCH_IDX, state.POMO_IDX, selected] \\\n                    .reshape(batch_size, pomo_size)\n                # shape: (batch, pomo)\n\n            else:\n                selected = probs.argmax(dim=2)\n                # shape: (batch, pomo)\n                prob = None\n\n\n        return selected, prob\n\n\ndef _get_encoding(encoded_nodes, node_index_to_pick):\n    # encoded_nodes.shape: (batch, problem, embedding)\n    # node_index_to_pick.shape: (batch, pomo)\n\n    batch_size = node_index_to_pick.size(0)\n    pomo_size = node_index_to_pick.size(1)\n    embedding_dim = encoded_nodes.size(2)\n\n    gathering_index = node_index_to_pick[:, :, None].expand(batch_size, pomo_size, embedding_dim)\n    # shape: (batch, pomo, embedding)\n\n    picked_nodes = encoded_nodes.gather(dim=1, index=gathering_index)\n    # shape: (batch, pomo, embedding)\n\n    return picked_nodes\n\n\n########################################\n# ENCODER\n########################################\n\nclass TSP_Encoder(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n        self.model_params = model_params\n        embedding_dim = self.model_params['embedding_dim']\n        encoder_layer_num = self.model_params['encoder_layer_num']\n\n        self.embedding = nn.Linear(6, embedding_dim)\n        self.sols_embedding = nn.Linear(3, embedding_dim)\n        self.layers = nn.ModuleList([EncoderLayer(**model_params) for _ in range(encoder_layer_num)])\n\n    def forward(self, data, sols, sols_mask):\n        # data.shape: (batch, problem, 2)\n\n        embedded_input = self.embedding(data)\n        # shape: (batch, problem, embedding)\n\n        embedded_input_s = self.sols_embedding(sols)\n        # shape: (batch, sols, embedding)\n\n        # out = embedded_input\n        out = torch.cat((embedded_input, embedded_input_s), dim=1)\n        for layer in self.layers:\n            out = layer(out, sols_mask)\n\n        return out\n\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n        self.model_params = model_params\n        embedding_dim = self.model_params['embedding_dim']\n        head_num = self.model_params['head_num']\n        qkv_dim = self.model_params['qkv_dim']\n\n        self.Wq = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.Wk = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.Wv = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.multi_head_combine = nn.Linear(head_num * qkv_dim, embedding_dim)\n\n        self.Wq_s = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.Wk_s = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.Wv_s = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.multi_head_combine_s = nn.Linear(head_num * qkv_dim, embedding_dim)\n\n        self.addAndNormalization1 = Add_And_Normalization_Module(**model_params)\n        self.feedForward = Feed_Forward_Module(**model_params)\n        self.addAndNormalization2 = Add_And_Normalization_Module(**model_params)\n\n    def forward(self, input1, sols_mask):\n        # input.shape: (batch, problem, EMBEDDING_DIM)\n        head_num = self.model_params['head_num']\n\n        q_n = reshape_by_heads(self.Wq(input1[:, :self.model_params['node_size']]), head_num=head_num)\n        k_n = reshape_by_heads(self.Wk(input1[:, :self.model_params['node_size']]), head_num=head_num)\n        v_n = reshape_by_heads(self.Wv(input1[:, :self.model_params['node_size']]), head_num=head_num)\n        # q shape: (batch, HEAD_NUM, problem, KEY_DIM)\n\n        q_s = reshape_by_heads(self.Wq_s(input1[:, self.model_params['node_size']:]), head_num=head_num)\n        k_s = reshape_by_heads(self.Wk_s(input1[:, self.model_params['node_size']:]), head_num=head_num)\n        v_s = reshape_by_heads(self.Wv_s(input1[:, self.model_params['node_size']:]), head_num=head_num)\n        # q shape: (batch, HEAD_NUM, problem, KEY_DIM)\n\n        out_concat_nn = multi_head_attention(q_n, k_n, v_n)\n        # shape: (batch, problem, HEAD_NUM*KEY_DIM)\n\n        out_concat_ns = multi_head_attention(q_n, k_s, v_s, rank2_ninf_mask=sols_mask)\n        # shape: (batch, problem, HEAD_NUM*KEY_DIM)\n\n        multi_head_out_n = self.multi_head_combine(out_concat_nn + out_concat_ns)\n        # shape: (batch, problem, EMBEDDING_DIM)\n\n        out_concat_sn = multi_head_attention(q_s, k_n, v_n)\n        # shape: (batch, problem, HEAD_NUM*KEY_DIM)\n\n        multi_head_out_s = self.multi_head_combine_s(out_concat_sn)\n        # shape: (batch, problem, EMBEDDING_DIM)\n\n        multi_head_out = torch.cat((multi_head_out_n, multi_head_out_s), dim=1)\n\n        out1 = self.addAndNormalization1(input1, multi_head_out)\n        out2 = self.feedForward(out1)\n        out3 = self.addAndNormalization2(out1, out2)\n\n        return out3\n        # shape: (batch, problem, EMBEDDING_DIM)\n\n\n########################################\n# DECODER\n########################################\n\nclass TSP_Decoder(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n        self.model_params = model_params\n        embedding_dim = self.model_params['embedding_dim']\n        head_num = self.model_params['head_num']\n        qkv_dim = self.model_params['qkv_dim']\n        \n        hyper_input_dim = 3 + 2\n        hyper_hidden_embd_dim = self.model_params['hyper_hidden_dim']\n        self.embd_dim = 3 + 2\n        self.hyper_output_dim = 7 * self.embd_dim\n        \n        self.hyper_fc1 = nn.Linear(hyper_input_dim, hyper_hidden_embd_dim, bias=True)\n        self.hyper_fc2 = nn.Linear(hyper_hidden_embd_dim, hyper_hidden_embd_dim, bias=True)\n        self.hyper_fc3 = nn.Linear(hyper_hidden_embd_dim, self.hyper_output_dim, bias=True)\n        \n        self.hyper_Wq_first = nn.Linear(self.embd_dim, embedding_dim * head_num * qkv_dim, bias=False)\n        self.hyper_Wq_last = nn.Linear(self.embd_dim, embedding_dim * head_num * qkv_dim, bias=False)\n        self.hyper_Wk = nn.Linear(self.embd_dim, embedding_dim * head_num * qkv_dim, bias=False)\n        self.hyper_Wv = nn.Linear(self.embd_dim, embedding_dim * head_num * qkv_dim, bias=False)\n        self.hyper_multi_head_combine = nn.Linear(self.embd_dim, head_num * qkv_dim * embedding_dim, bias=False)\n        self.hyper_Wk_s = nn.Linear(self.embd_dim, embedding_dim * head_num * qkv_dim, bias=False)\n        self.hyper_Wv_s = nn.Linear(self.embd_dim, embedding_dim * head_num * qkv_dim, bias=False)\n\n        self.Wq_last_para = None\n        self.multi_head_combine_para = None\n\n        self.k = None  # saved key, for multi-head attention\n        self.v = None  # saved value, for multi-head_attention\n        self.single_head_key = None  # saved, for single-head attention\n        self.q_first = None  # saved q1, for multi-head attention\n        \n    def assign(self, pref):\n        \n        embedding_dim = self.model_params['embedding_dim']\n        head_num = self.model_params['head_num']\n        qkv_dim = self.model_params['qkv_dim']\n        \n        hyper_embd = self.hyper_fc1(pref)\n        hyper_embd = self.hyper_fc2(hyper_embd)\n        mid_embd = self.hyper_fc3(hyper_embd)\n        \n        self.Wq_first_para = self.hyper_Wq_first(mid_embd[:self.embd_dim]).reshape(embedding_dim, head_num * qkv_dim)\n        self.Wq_last_para = self.hyper_Wq_last(mid_embd[self.embd_dim:2 * self.embd_dim]).reshape(embedding_dim, head_num * qkv_dim)\n        self.Wk_para = self.hyper_Wk(mid_embd[2 * self.embd_dim: 3 * self.embd_dim]).reshape(embedding_dim, head_num * qkv_dim)\n        self.Wv_para = self.hyper_Wv(mid_embd[3 * self.embd_dim: 4 * self.embd_dim]).reshape(embedding_dim, head_num * qkv_dim)\n        self.multi_head_combine_para = self.hyper_multi_head_combine(mid_embd[4 * self.embd_dim: 5 * self.embd_dim]).reshape(head_num * qkv_dim, embedding_dim)\n        self.Wk_para_s = self.hyper_Wk_s(mid_embd[5 * self.embd_dim: 6 * self.embd_dim]).reshape(embedding_dim,\n                                                                                                 head_num * qkv_dim)\n        self.Wv_para_s = self.hyper_Wv_s(mid_embd[6 * self.embd_dim: 7 * self.embd_dim]).reshape(embedding_dim,\n                                                                                                 head_num * qkv_dim)\n        \n        \n    def set_kv(self, encoded_nodes):\n        # encoded_nodes.shape: (batch, problem, embedding)\n        head_num = self.model_params['head_num']\n\n        self.k = reshape_by_heads(F.linear(encoded_nodes[:, :self.model_params['node_size']], self.Wk_para),\n                                  head_num=head_num)\n        self.v = reshape_by_heads(F.linear(encoded_nodes[:, :self.model_params['node_size']], self.Wv_para),\n                                  head_num=head_num)\n\n        self.k_s = reshape_by_heads(F.linear(encoded_nodes[:, self.model_params['node_size']:], self.Wk_para_s),\n                                    head_num=head_num)\n        self.v_s = reshape_by_heads(F.linear(encoded_nodes[:, self.model_params['node_size']:], self.Wv_para_s),\n                                    head_num=head_num)\n        \n        self.single_head_key = encoded_nodes[:, :self.model_params['node_size']].transpose(1, 2)\n     \n    def set_q1(self, encoded_q1):\n        # encoded_q.shape: (batch, n, embedding)  # n can be 1 or pomo\n        head_num = self.model_params['head_num']\n\n        self.q_first = reshape_by_heads(F.linear(encoded_q1, self.Wq_first_para), head_num=head_num)\n        # shape: (batch, head_num, n, qkv_dim)\n\n    def forward(self, encoded_last_node, sols_mask_pomo, ninf_mask):\n        # encoded_last_node.shape: (batch, pomo, embedding)\n        # ninf_mask.shape: (batch, pomo, problem)\n        \n        embedding_dim = self.model_params['embedding_dim']\n        head_num = self.model_params['head_num']\n        qkv_dim = self.model_params['qkv_dim']\n        \n\n        head_num = self.model_params['head_num']\n\n        #  Multi-Head Attention\n        #######################################################\n        q_last = reshape_by_heads(F.linear(encoded_last_node, self.Wq_last_para), head_num = head_num)\n       \n        q = self.q_first + q_last \n        # shape: (batch, head_num, pomo, qkv_dim)\n\n        out_concat_n = multi_head_attention(q, self.k, self.v, rank3_ninf_mask=ninf_mask)\n        out_concat_s = multi_head_attention(q, self.k_s, self.v_s, rank3_ninf_mask=sols_mask_pomo)\n        out_concat = out_concat_n + out_concat_s\n        # shape: (batch, pomo, head_num*qkv_dim)\n\n        mh_atten_out = F.linear(out_concat, self.multi_head_combine_para)\n        # shape: (batch, pomo, embedding)\n\n        #  Single-Head Attention, for probability calculation\n        #######################################################\n        score = torch.matmul(mh_atten_out, self.single_head_key)\n        # shape: (batch, pomo, problem)\n\n        sqrt_embedding_dim = self.model_params['sqrt_embedding_dim']\n        logit_clipping = self.model_params['logit_clipping']\n\n        score_scaled = score / sqrt_embedding_dim\n        # shape: (batch, pomo, problem)\n\n        score_clipped = logit_clipping * torch.tanh(score_scaled)\n\n        score_masked = score_clipped + ninf_mask\n\n        probs = F.softmax(score_masked, dim=2)\n        # shape: (batch, pomo, problem)\n\n        return probs\n\n\n########################################\n# NN SUB CLASS / FUNCTIONS\n########################################\n\ndef reshape_by_heads(qkv, head_num):\n    # q.shape: (batch, n, head_num*key_dim)   : n can be either 1 or PROBLEM_SIZE\n\n    batch_s = qkv.size(0)\n    n = qkv.size(1)\n\n    q_reshaped = qkv.reshape(batch_s, n, head_num, -1)\n    # shape: (batch, n, head_num, key_dim)\n\n    q_transposed = q_reshaped.transpose(1, 2)\n    # shape: (batch, head_num, n, key_dim)\n\n    return q_transposed\n\n\ndef multi_head_attention(q, k, v, rank2_ninf_mask=None, rank3_ninf_mask=None):\n    # q shape: (batch, head_num, n, key_dim)   : n can be either 1 or PROBLEM_SIZE\n    # k,v shape: (batch, head_num, problem, key_dim)\n    # rank2_ninf_mask.shape: (batch, problem)\n    # rank3_ninf_mask.shape: (batch, group, problem)\n\n    batch_s = q.size(0)\n    head_num = q.size(1)\n    n = q.size(2)\n    key_dim = q.size(3)\n\n    input_s = k.size(2)\n\n    score = torch.matmul(q, k.transpose(2, 3))\n    # shape: (batch, head_num, n, problem)\n\n    score_scaled = score / torch.sqrt(torch.tensor(key_dim, dtype=torch.float))\n    if rank2_ninf_mask is not None:\n        score_scaled = score_scaled + rank2_ninf_mask[:, None, None, :].expand(batch_s, head_num, n, input_s)\n    if rank3_ninf_mask is not None:\n        score_scaled = score_scaled + rank3_ninf_mask[:, None, :, :].expand(batch_s, head_num, n, input_s)\n\n    weights = nn.Softmax(dim=3)(score_scaled)\n    # shape: (batch, head_num, n, problem)\n\n    out = torch.matmul(weights, v)\n    # shape: (batch, head_num, n, key_dim)\n\n    out_transposed = out.transpose(1, 2)\n    # shape: (batch, n, head_num, key_dim)\n\n    out_concat = out_transposed.reshape(batch_s, n, head_num * key_dim)\n    # shape: (batch, n, head_num*key_dim)\n\n    return out_concat\n\n\nclass Add_And_Normalization_Module(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n        embedding_dim = model_params['embedding_dim']\n        self.norm = nn.InstanceNorm1d(embedding_dim, affine=True)",
  "description": "Combined Analysis:\n- [NHDE-M/CVRP/source/MODEL__Actor/grouped_actors.py]: This file implements the heterogeneous graph attention mechanism described in the NHDE paper. The ACTOR class uses an Encoder that jointly processes both the problem instance (depot and customer nodes) and the current Pareto front solutions (sols) through separate attention heads. This corresponds to the paper's 'heterogeneous graph attention mechanism to capture relations between instance and Pareto front graphs'. The Next_Node_Probability_Calculator_for_group computes action probabilities for constructing CVRP routes, supporting the 'multiple candidate solutions per subproblem' strategy through grouped processing. The architecture directly implements the neural heuristic component that guides solution construction for decomposed subproblems.\n- [NHDE-M/KP/source/MODEL__Actor/grouped_actors.py]: This file implements the core heterogeneous graph attention mechanism from the NHDE paper. The ACTOR class integrates both problem instance data (items) and current solution set (Pareto front approximation) through a dual-stream encoder. The Encoder_Layer performs cross-attention between nodes (items) and solutions (sols), modeling their interactions as described in the paper's heterogeneous graph attention. The Next_Node_Probability_Calculator_for_group then uses this joint representation to compute action probabilities for constructing multiple solutions (grouped) per subproblem, enabling the multiple Pareto optima strategy. This directly corresponds to the paper's algorithm of using a heterogeneous attention mechanism to jointly model the problem instance and evolving Pareto front while sampling multiple candidate solutions per subproblem.\n- [NHDE-M/KP/source/mo_knapsack_problem.py]: This file implements the core environment and state management for the Multi-Objective Knapsack Problem (MOKP) as part of the NHDE framework. The key components are:\n1. **GROUP_STATE**: Manages the state for multiple parallel solutions (group_size) with two objectives (obj1 and obj2). It tracks selected items, accumulated values for both objectives, remaining capacity, and feasibility constraints via masking.\n2. **GROUP_ENVIRONMENT**: Provides the RL environment interface with reset() and step() methods, returning bi-objective rewards when episodes complete.\n3. **Mathematical Model Implementation**: \n   - Objective: Maximizes accumulated values for two objectives (f and f) via accumulated_value_obj1 and accumulated_value_obj2.\n   - Constraint: Capacity constraint enforced through capacity tracking and infeasibility masking (fit_ninf_mask).\n   - Feasible Set : All item subsets not exceeding knapsack capacity, implemented via dynamic masking of items that would exceed capacity.\n4. **Algorithm Alignment**: Supports the paper's multiple Pareto optima strategy by maintaining group_s parallel solution constructions, enabling diversity through simultaneous exploration of different subproblems (weighted sum scalarizations). The environment returns Pareto-relevant bi-objective rewards for RL training.\n5. **Data Handling**: Includes data loaders for random instance generation with 3-dimensional items (weight, value_obj1, value_obj2).\n\nThis file provides the foundational combinatorial optimization environment upon which the neural heuristic (NHDE) operates, implementing the exact constraint satisfaction and objective accumulation required for MOKP.\n- [NHDE-M/TSP-3O/source/mo_travelling_saleman_problem.py]: This file implements the core multi-objective optimization model for MOTSP with 3 objectives. The GROUP_ENVIRONMENT class specifically implements the mathematical objective function: min f(x) = (f(x), f(x), f(x)) where each f(x) represents the total Euclidean travel distance for objective i. The _get_group_travel_distance method computes all three objective values simultaneously by: 1) Reconstructing the tour sequence from selected nodes, 2) Computing Euclidean distances between consecutive nodes for each objective separately (using different coordinate dimensions), 3) Summing distances per tour per objective, 4) Stacking results into a 3D tensor (batch  group  objectives). This directly implements the multi-objective combinatorial optimization model described in the paper.\n- [NHDE-M/TSP/source/MODEL__Actor/grouped_actors.py]: This file implements the core neural architecture for NHDE's heterogeneous graph attention mechanism. The ACTOR class integrates an Encoder that jointly processes problem instance nodes (TSP coordinates) and solution representations (Pareto front) using separate attention heads, exactly as described in the paper's heterogeneous graph modeling. The Next_Node_Probability_Calculator_for_group implements the multiple Pareto optima strategy by maintaining separate attention states for each group (subproblem), enabling simultaneous construction of diverse solutions. The architecture uses multi-head attention between instance and solution graphs, followed by probability calculation with logit clipping, directly implementing the neural heuristic construction process for decomposed subproblems.\n- [NHDE-P/MOCVRP/POMO/MOCVRPEnv.py]: This file implements the core environment for Multi-Objective Capacitated Vehicle Routing Problem (MOCVRP) as described in the paper. It directly maps to the optimization model: 1) It defines two objectives (travel distance and makespan) in _get_travel_distance() method, corresponding to the vector objective function f(x). 2) It enforces combinatorial constraints through state management: vehicle capacity constraints via load tracking and demand checking (lines with demand_too_large), depot constraints via at_the_depot flags, and route feasibility via visited_ninf_mask. 3) The step() method implements the solution construction process where actions (node selections) must satisfy the constraints, and the final reward returns negative objective values for minimization. 4) The environment supports batch processing and multiple parallel instances (POMO) for efficient neural network training, which aligns with the algorithm's need for sampling multiple candidate solutions per subproblem.\n- [NHDE-P/MOCVRP/POMO/MOCVRPModel.py]: This file implements the core neural network architecture for the NHDE-P algorithm applied to MOCVRP. It directly implements the heterogeneous graph attention mechanism described in the paper: 1) The encoder (CVRP_Encoder) jointly embeds problem instance nodes (depot, customer coordinates/demand) and Pareto front solutions (sols) using separate linear layers, then processes them through attention layers that capture cross-attention between instance and solution graphs. 2) The decoder (CVRP_Decoder) uses a hypernetwork that takes preference vectors (for weighted sum scalarization) to dynamically generate attention parameters, enabling solution construction conditioned on different objective weightings. 3) The forward method implements the autoregressive solution construction process with POMO-style parallel rollouts. This matches the paper's key algorithmic components: indicator-enhanced DRL (via preference-conditioned decoding), heterogeneous attention between instance and Pareto front, and multiple solution sampling per subproblem.\n- [NHDE-P/MOKP/POMO/MOKPModel.py]: This file implements the core neural architecture for the NHDE-P algorithm applied to Multi-Objective Knapsack Problem (MOKP). It contains:\n1. KPModel: Main model class coordinating encoder-decoder flow\n2. KP_Encoder: Heterogeneous graph attention encoder that jointly processes problem items (node_size) and current Pareto front solutions (sols)\n3. KP_Decoder: Hypernetwork-based decoder that generates attention parameters conditioned on preference vectors (pref) for weighted sum scalarization\n4. Key algorithm components:\n   - Encoder uses separate attention heads for item-item and item-solution interactions (heterogeneous attention)\n   - Decoder's assign() method implements preference-conditioned parameter generation via hypernetwork\n   - Forward pass constructs solutions sequentially while respecting capacity constraints (ninf_mask)\n   - Supports multiple parallel solutions (POMO) for diversity enhancement\n5. Matches paper's description of: heterogeneous attention mechanism, preference-based decomposition, multiple solution sampling\n- [NHDE-P/MOTSP/POMO/MOTSPEnv.py]: This file implements the core environment for Multi-Objective Traveling Salesman Problem (MOTSP) within the NHDE framework. It directly implements the optimization model constraints and objective evaluation for MOTSP:\n\n1. **Constraints Implementation**: The environment enforces Hamiltonian cycle constraints through the `ninf_mask` mechanism that prevents revisiting nodes (line 78: `self.step_state.ninf_mask[self.BATCH_IDX, self.POMO_IDX, self.current_node] = float('-inf')`). The `selected_node_list` tracks the sequence of visited nodes, ensuring each node is visited exactly once.\n\n2. **Objective Evaluation**: The `_get_travel_distance()` method computes both objective values (travel distances for two objectives) for complete tours. It calculates Euclidean distances between consecutive nodes in the tour for both objectives separately (lines 97-103), then returns a vector of both objectives (line 105).\n\n3. **Multi-Objective Representation**: The environment handles 4-dimensional problem data (last dimension of size 4 in `self.problems`), representing coordinates for two objectives. The reward is a vector of negative travel distances (line 81: `reward = -self._get_travel_distance()`), aligning with the paper's minimization formulation.\n\n4. **Algorithm Integration**: While this file doesn't implement the full NHDE algorithm (heterogeneous graph attention, hypervolume indicators, etc.), it provides the essential environment for solution construction and evaluation that the NHDE algorithm interacts with. The POMO (Policy Optimization with Multiple Optima) parallelization supports the multiple candidate solutions per subproblem strategy mentioned in the paper.\n\n5. **Problem Instance Handling**: The environment supports problem augmentation (64-fold augmentation via `augment_xy_data_by_64_fold_2obj`) and batch processing, which are crucial for the neural approach's efficiency and the diversity enhancement techniques.\n- [NHDE-P/MOTSP/POMO/MOTSPModel.py]: This file implements the core neural network architecture for NHDE-P's MOTSP solver. It features: 1) A heterogeneous encoder that processes both problem nodes and existing solutions through separate attention mechanisms, 2) A hypernetwork-based decoder that generates attention parameters conditioned on preference vectors (for weighted sum scalarization), 3) Multi-head attention that jointly attends to both problem nodes and solution embeddings, enabling the model to learn relationships between the instance and Pareto front. The architecture directly implements the paper's key innovation of using heterogeneous graph attention to model both problem instance and evolving solution set.\n- [NHDE-P/MOTSP_3obj/POMO/MOTSPEnv_3obj.py]: This file implements the core environment for Multi-Objective Traveling Salesman Problem (MOTSP) with 3 objectives. It directly implements the mathematical optimization model by:\n1. Representing the multi-objective minimization problem through three Euclidean distance calculations in _get_travel_distance()\n2. Enforcing Hamiltonian cycle constraints via node selection masking (ninf_mask) that prevents revisiting nodes\n3. Implementing the combinatorial decision space through sequential node selection (selected_node_list)\n4. Computing vectorized rewards as negative travel distances for all three objectives\n5. Supporting batch processing and POMO (Policy Optimization with Multiple Optima) sampling strategy for diversity\n\nThe environment provides the fundamental infrastructure for the NHDE algorithm's solution construction phase, handling the combinatorial constraints and multi-objective evaluation that are central to the paper's optimization model.\n- [NHDE-P/MOTSP_3obj/POMO/MOTSPModel_3obj.py]: This file implements the core neural architecture for the NHDE-P algorithm for Multi-Objective TSP (MOTSP). It contains the heterogeneous graph attention mechanism that jointly models the problem instance (node coordinates) and the Pareto front (solutions). The TSP_Encoder processes both node features (6D input) and solution features (3 objectives) through separate attention heads, enabling interaction between the instance graph and Pareto front graph. The TSP_Decoder uses a hypernetwork that takes preference vectors (3+2 dimensions) to generate attention parameters, implementing the weighted sum scalarization approach. The model supports POMO (Policy Optimization with Multiple Optima) for sampling multiple candidate solutions per subproblem, aligning with the paper's multiple Pareto optima strategy.",
  "dependencies": [
    "MOTSP.MOTSProblemDef.get_random_problems",
    "torch.utils.data.Dataset",
    "TORCH_OBJECTS",
    "torch.utils.data.DataLoader",
    "torch.nn.functional",
    "MOTSProblemDef_3obj.augment_xy_data_by_n_fold_3obj",
    "AddAndInstanceNormalization",
    "ENVIRONMENT",
    "FeedForward",
    "dataclasses.dataclass",
    "MOCVRPProblemDef.augment_xy_data_by_8_fold",
    "Add_And_Normalization_Module",
    "KnapSack_Dataset__Random",
    "MOCVRPProblemDef.get_random_problems",
    "GROUP_STATE",
    "TSP_Encoder",
    "IPython.core.debugger",
    "TSP_Decoder",
    "knapsack_collate_fn",
    "multi_head_attention",
    "_get_encoding",
    "dataclasses",
    "numpy",
    "STATE",
    "reshape_by_heads",
    "KNAPSACK_DATA_LOADER__RANDOM",
    "MOTSProblemDef_3obj.get_random_problems",
    "EncoderLayer",
    "HYPER_PARAMS",
    "torch.nn",
    "MOTSP.MOTSProblemDef.augment_xy_data_by_64_fold_2obj",
    "pick_nodes_for_each_group",
    "Feed_Forward_Module",
    "GROUP_ENVIRONMENT",
    "torch"
  ]
}