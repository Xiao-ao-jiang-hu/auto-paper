{
  "file_path": "blackbox_backprop/mAP.py, blackbox_backprop/ranking.py, blackbox_backprop/recall.py, solvers_to_visualize.py",
  "function_name": "MapLoss.raw_map_computation, TrueRanker, RecallLoss, RankingBBSolver",
  "code_snippet": "\n\n# ==========================================\n# File: blackbox_backprop/mAP.py\n# Function/Context: MapLoss.raw_map_computation\n# ==========================================\nfrom collections import deque\n\nimport torch\n\nfrom blackbox_backprop.ranking import TrueRanker, rank_normalised\n\n\nclass MapLoss(torch.nn.Module):\n    \"\"\" Torch module for computing recall-based loss as in 'Blackbox differentiation of Ranking-based Metrics' \"\"\"\n    def __init__(self,\n                 lambda_val,\n                 margin,\n                 interclass_coef,\n                 batch_memory,\n                 ):\n        \"\"\"\n        :param lambda_val:  hyperparameter of black-box backprop\n        :param margin: margin to be enforced between positives and negatives (alpha in the paper)\n        :param interclass_coef: coefficient for interclass loss (beta in paper)\n        :param batch_memory: how many batches should be in memory\n        \"\"\"\n        super().__init__()\n        self.batch_memory = batch_memory\n        self.margin = margin\n        self.lambda_val = lambda_val\n        self.interclass_coef = interclass_coef\n\n        self.storage = deque()\n\n    def raw_map_computation(self, scores, targets):\n        \"\"\"\n                :param scores: [batch_size, num_classes] predicted relevance scores\n                :param targets: [batch_size, num_classes] ground truth relevances\n        \"\"\"\n        # Compute map\n        HIGH_CONSTANT = 2.0\n        epsilon = 1e-5\n        transposed_scores = scores.transpose(0, 1)\n        transposed_targets = targets.transpose(0, 1)\n        deviations = torch.abs(torch.randn_like(transposed_targets)) * (transposed_targets - 0.5)\n\n        transposed_scores = transposed_scores - self.margin * deviations\n        ranks_of_positive = TrueRanker.apply(transposed_scores, self.lambda_val)\n        scores_for_ranking_positives = -ranks_of_positive + HIGH_CONSTANT * transposed_targets\n        ranks_within_positive = rank_normalised(scores_for_ranking_positives)\n        ranks_within_positive.requires_grad = False\n        assert torch.all(ranks_within_positive * transposed_targets < ranks_of_positive * transposed_targets + epsilon)\n\n        sum_of_precisions_at_j_per_class = ((ranks_within_positive / ranks_of_positive) * transposed_targets).sum(dim=1)\n        precisions_per_class = sum_of_precisions_at_j_per_class / (transposed_targets.sum(dim=1) + epsilon)\n\n        present_class_mask = targets.sum(axis=0) != 0\n        return 1.0 - precisions_per_class[present_class_mask].mean()\n\n\n    def forward(self, output, target):\n\n        current_storage = list(self.storage)\n        long_output = torch.cat([output] + [x[0] for x in current_storage], dim=0)\n        long_target = torch.cat([target] + [x[1] for x in current_storage], dim=0)\n\n        assert long_output.shape[0] == long_target.shape[0]  # even in multi-gpu setups\n        cross_batch_loss = self.raw_map_computation(long_output, long_target)\n\n        output_flat = output.reshape((-1, 1))\n        target_flat = target.reshape((-1, 1))\n        interclass_loss = self.raw_map_computation(output_flat, target_flat)\n\n        while len(self.storage) >= self.batch_memory:\n            self.storage.popleft()\n\n        self.storage.append([output.detach(), target.detach()])\n\n        loss = (1.0 - self.interclass_coef) * cross_batch_loss + self.interclass_coef * interclass_loss\n        return loss\n\n# ==========================================\n# File: blackbox_backprop/ranking.py\n# Function/Context: TrueRanker\n# ==========================================\nimport torch\n\n\ndef rank(seq):\n    return torch.argsort(torch.argsort(seq).flip(1))\n\n\ndef rank_normalised(seq):\n    return (rank(seq) + 1).float() / seq.size()[1]\n\n\nclass TrueRanker(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, sequence, lambda_val):\n        rank = rank_normalised(sequence)\n        ctx.lambda_val = lambda_val\n        ctx.save_for_backward(sequence, rank)\n        return rank\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        sequence, rank = ctx.saved_tensors\n        assert grad_output.shape == rank.shape\n        sequence_prime = sequence + ctx.lambda_val * grad_output\n        rank_prime = rank_normalised(sequence_prime)\n        gradient = -(rank - rank_prime) / (ctx.lambda_val + 1e-8)\n        return gradient, None\n\n# ==========================================\n# File: blackbox_backprop/recall.py\n# Function/Context: RecallLoss\n# ==========================================\nfrom collections import deque\n\nimport torch\n\nfrom .ranking import TrueRanker, rank_normalised\n\n\nclass RecallLoss(torch.nn.Module):\n    def __init__(self, lambda_val, margin, weight_fn):\n        \"\"\"\n        Torch module for computing recall-based loss as in \"Blackbox differentiation of Ranking-based Metrics\"\n        :param lambda_val:  hyperparameter of black-box backprop\n        :param margin: margin to be enforced between positives and negatives (alpha in the paper)\n        :param weight_fn: callable torch.Tensor -> torch.Tensor (such as log(1+x) or log(1+log(1+x)) etc.)\n        \"\"\"\n        super().__init__()\n        self.sorter = TrueRanker()\n        self.margin = margin\n        self.lambda_val = lambda_val\n        self.weight_fn = weight_fn\n\n    def forward(self, score_sequences, gt_relevance_sequences):\n        \"\"\"\n        :param score_sequences: [num_sequences, len_of_sequence] scores of images (floats in [-1,1])\n        :param gt_relevance_sequences: [num_sequences, len_of_sequence] of booleans relevant/irrelevant\n        \"\"\"\n        HIGH_CONSTANT = 2.0  # This is actually high enough as normalised ranks live in [0,1].\n        TINY_CONSTANT = 1e-5\n        length = score_sequences.shape[0]\n        device = score_sequences.device\n\n        deviations = (gt_relevance_sequences - 0.5).to(device)\n        score_sequences = score_sequences - self.margin * deviations\n\n        ranks_among_all = TrueRanker.apply(score_sequences, self.lambda_val)\n        scores_among_positive = -ranks_among_all + HIGH_CONSTANT * gt_relevance_sequences\n        scores_among_positive = scores_among_positive.to(device)\n        ranks_among_positive = rank_normalised(scores_among_positive)\n        ranks_among_positive.require_grad = False\n\n        ranks_for_queries = (ranks_among_all - ranks_among_positive) * gt_relevance_sequences\n\n        assert torch.all(ranks_for_queries > -TINY_CONSTANT)\n\n        # denormalize ranks\n        ranks_for_queries = ranks_for_queries * length\n        recall = self.weight_fn(ranks_for_queries * gt_relevance_sequences).sum() / gt_relevance_sequences.sum()\n        return recall\n\n\nclass BatchMemoryRecallLoss(torch.nn.Module):\n    \"\"\"\n    A wrapper around a rank-based loss that allows batch memory.\n    \"\"\"\n    def __init__(self, batch_memory, **kwargs):\n        \"\"\"\n        :param batch_memory: How many batches should be in memory\n        :param kwargs: arguments of the underlying loss\n        \"\"\"\n        super().__init__()\n        self.batch_memory = batch_memory\n        self.loss = RecallLoss(**kwargs)\n\n        self.batch_storage = deque()\n        self.labels_storage = deque()\n\n    def reset(self):\n        self.batch_storage.clear()\n        self.labels_storage.clear()\n\n\n    def forward(self, score_sequences, gt_relevance_sequences):\n        if self.batch_memory > 0:\n            all_score_sequences = torch.cat((score_sequences,) + tuple(self.batch_storage), dim=0)\n            all_relevance_sequences = torch.cat((gt_relevance_sequences,) + tuple(self.labels_storage), dim=0)\n            result = self.loss(all_score_sequences, all_relevance_sequences)\n\n            if len(self.batch_storage) == self.batch_memory:\n                self.batch_storage.popleft()\n\n            self.batch_storage.append(score_sequences.detach())\n\n            if len(self.labels_storage) == self.batch_memory:\n                self.labels_storage.popleft()\n\n            self.labels_storage.append(gt_relevance_sequences.detach())\n        else:\n            result = self.loss(score_sequences, gt_relevance_sequences)\n        return result\n\n# ==========================================\n# File: solvers_to_visualize.py\n# Function/Context: RankingBBSolver\n# ==========================================\nfrom visualization_utils import BlackboxSolverAbstract, gen_w_and_y_grad, gen_edges\n\ntry:\n    from lpmp_py import gm_solver\n    from lpmp_py import mgm_solver\nexcept ImportError:\n    print(\"lpmp_py missing. Install it separately for using (multi)graph matching solvers\")\n\nfrom blackbox_backprop.travelling_salesman import gurobi_tsp\nfrom blackbox_backprop.shortest_path import dijkstra\n\nimport numpy as np\nimport itertools as it\n\nclass RankingBBSolver(BlackboxSolverAbstract):\n    \"\"\"\n    Ranking solver.\n    \"\"\"\n\n    @staticmethod\n    def ranks_normal(sequence):\n        return (np.argsort(np.argsort(sequence)[::-1]) + 1) / float(len(sequence))\n\n    @staticmethod\n    def solver(inputs):\n        sequence = inputs[0]\n        s = RankingBBSolver.ranks_normal(sequence)\n        return [s]\n\n    @staticmethod\n    def gen_input(sequence_length, seed, w_normal_factor=1, w_normal_addend=0, w_shift_factor=1, w_shift_addend=0, y_factor=1, \n                  y_addend=0):\n        w_slice_l, y_grad_l = gen_w_and_y_grad(\n            seed=seed,\n            params=[dict(shape=[sequence_length],\n                         w_slice_par=dict(mode='slice_random', normal_factor=w_normal_factor, normal_addend=w_normal_addend, \n                                              shift_factor=w_shift_factor, shift_addend=w_shift_addend),\n                         y_grad_par=dict(mode='random', factor=y_factor, addend=y_addend))])\n\n        solver_config = dict()\n        return w_slice_l, y_grad_l, solver_config",
  "description": "Combined Analysis:\n- [blackbox_backprop/mAP.py]: The file implements the core algorithm for optimizing rank-based metrics (specifically mAP) using blackbox differentiation. The key steps are:\n1. It uses TrueRanker.apply() to compute ranks via the blackbox combinatorial solver (argsort) with hyperparameter lambda_val.\n2. It constructs a modified scoring function (scores_for_ranking_positives) to compute ranks within positive examples.\n3. It computes Average Precision per class using the formula: sum((ranks_within_positive / ranks_of_positive) * targets) / sum(targets).\n4. The loss is defined as 1 - mean precision across classes, enabling gradient-based optimization.\n5. The implementation follows the RaMBO method from the paper, where the ranking problem is formulated as minimizing a linear objective over permutations, and blackbox backpropagation provides gradients through the non-differentiable solver.\n- [blackbox_backprop/ranking.py]: This file implements the core blackbox differentiation logic for ranking operations as described in the paper. The TrueRanker class uses torch.autograd.Function to define a custom forward and backward pass. The forward pass computes the normalized rank of the input sequence via argsort (the blackbox combinatorial solver for ranking). The backward pass applies the blackbox differentiation technique: it perturbs the input sequence by adding a scaled gradient, recomputes the ranking on the perturbed input, and estimates the gradient via finite differences. This matches the RaMBO method's approach to enabling gradient-based optimization of rank-based metrics. The optimization model's objective (minimizing a linear function over permutations) is implicitly solved by argsort, and the constraints are enforced by the ranking operation itself.\n- [blackbox_backprop/recall.py]: This file implements the core algorithm for optimizing rank-based metrics via blackbox differentiation. The RecallLoss class directly uses the TrueRanker (which solves the combinatorial ranking optimization) through TrueRanker.apply() with the lambda hyperparameter for blackbox backprop. The forward method computes recall loss by: 1) adjusting scores with margin, 2) obtaining ranks via the blackbox solver, 3) computing positive-only ranks, and 4) applying a weight function to compute recall. BatchMemoryRecallLoss extends this with memory across batches. The implementation matches the paper's RaMBO method for end-to-end training with rank-based metrics.\n- [solvers_to_visualize.py]: The RankingBBSolver class directly implements the core ranking optimization logic from the paper. The key method 'ranks_normal' computes the ranking by: 1) argsorting the sequence in descending order ([::-1]), 2) argsorting again to get ranks, 3) normalizing to [0,1]. This corresponds to solving the permutation optimization problem min_{π∈Π_n} ∑ y_i π(i) where the solution is the ranking of scores. The 'solver' method wraps this ranking function for blackbox differentiation, and 'gen_input' generates synthetic data for testing. This aligns with the paper's approach of treating ranking as a combinatorial optimization problem and applying blackbox backpropagation through the ranking function.",
  "dependencies": [
    "blackbox_backprop.ranking.TrueRanker",
    "numpy",
    "visualization_utils.gen_w_and_y_grad",
    "visualization_utils.BlackboxSolverAbstract",
    "blackbox_backprop (parent module)",
    "blackbox_backprop.ranking.rank_normalised",
    "collections.deque",
    "torch"
  ]
}