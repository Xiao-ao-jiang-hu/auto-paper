{
  "paper_id": "Controlling_Graph_Dynamics_with_Reinforcement_Learning_and_G",
  "title": "Controlling Graph Dynamics with Reinforcement Learning and Graph Neural Networks",
  "abstract": "We consider the problem of controlling a partially-observed dynamic process on a graph by a limited number of interventions. This problem naturally arises in contexts such as scheduling virus tests to curb an epidemic; targeted marketing in order to promote a product; and manually inspecting posts to detect fake news spreading on social networks. We formulate this setup as a sequential decision problem over a temporal graph process. In face of an exponential state space, combinatorial action space and partial observability, we design a novel tractable scheme to control dynamical processes on temporal graphs. We successfully apply our approach to two popular problems that fall into our framework: prioritizing which nodes should be tested in order to curb the spread of an epidemic, and influence maximization on a graph.",
  "problem_description_natural": "The paper addresses the challenge of controlling diffusive processes (e.g., epidemics or information cascades) on time-evolving graphs through a limited number of nodal interventions. At each time step, the agent observes only partial information about node states and must select a small subset of nodes (e.g., for testing or seeding) to influence the global dynamics. The goal is to optimize a long-term objective—such as minimizing the total number of infected individuals over time or maximizing the number of influenced nodes—despite partial observability, a combinatorial action space (choosing subsets of nodes), and complex spatiotemporal dependencies in the graph dynamics.",
  "problem_type": "combinatorial optimization",
  "datasets": [
    "CA-GrQc",
    "Montreal",
    "Portland",
    "Email",
    "GEMSEC-RO",
    "CA-HEPTh",
    "Preferential attachment (PA) networks",
    "Stochastic Block Model (SBM) networks",
    "Contact-tracing (CT) networks"
  ],
  "performance_metrics": [
    "Percent of healthy nodes",
    "Percent of influenced nodes",
    "Contained epidemic fraction"
  ],
  "lp_model": {
    "objective": "$\\max \\sum_t \\gamma^{t-t_0} g(c_1(t), c_2(t), \\dots)$ (or $\\min$ for epidemic test prioritization, where $g$ is the negative of the number of infected nodes)",
    "constraints": [
      "$\\sum_{v \\in \\mathcal{V}} x_v(t) = k$ for each time step $t$",
      "$x_v(t) \\in \\{0,1\\}$ for all $v \\in \\mathcal{V}$ and $t$"
    ],
    "variables": [
      "$x_v(t)$: binary decision variable indicating whether node $v$ is selected for intervention at time $t$",
      "$c_i(t)$: number of nodes in state $y_i$ at time $t$ (state variable, not directly controlled)"
    ]
  },
  "raw_latex_model": "$$\\begin{aligned} \\text{Maximize } & \\sum_{t} \\gamma^{t-t_0} g(c_1(t), c_2(t), \\dots) \\\\ \\text{subject to } & \\sum_{v \\in \\mathcal{V}} x_v(t) = k, \\quad \\forall t \\\\ & x_v(t) \\in \\{0,1\\}, \\quad \\forall v \\in \\mathcal{V}, \\forall t \\\\ & \\text{State dynamics: } ST_v(t) \\text{ evolves stochastically based on graph interactions and interventions.} \\end{aligned}$$",
  "algorithm_description": "We propose a reinforcement learning (RL) framework using graph neural networks (GNNs) and proximal policy optimization (PPO) to learn a policy for sequential node selection. The ranking module employs two GNNs: one for local diffusion modeling and another for long-range information propagation. This approach is applied to two specific problems: epidemic test prioritization (minimizing infected nodes via testing) and influence maximization (maximizing influenced nodes via seeding)."
}