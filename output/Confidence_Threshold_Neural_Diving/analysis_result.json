{
  "paper_id": "Confidence_Threshold_Neural_Diving",
  "title": "Confidence Threshold Neural Diving",
  "abstract": "Finding a better feasible solution in a shorter time is an integral part of solving Mixed Integer Programs. We present a post-hoc method based on Neural Diving [1] to build heuristics more flexibly. We hypothesize that variables with higher confidence scores are more definite to be included in the optimal solution. For our hypothesis, we provide empirical evidence that confidence threshold technique produces partial solutions leading to final solutions with better primal objective values. Our method won 2nd place in the primal task on the NeurIPS 2021 ML4CO competition. Also, our method shows the best score among other learning-based methods in the competition.",
  "problem_description_natural": "The paper addresses the primal task in Mixed Integer Linear Programming (MILP), where the goal is to find high-quality feasible solutions quickly to minimize the primal integral—a measure of how fast good primal bounds are found over time. Given MILP instances of the form min c^T x subject to A^T x ≤ b and x ∈ ℤ^p × ℝ^{n−p}, the method generates partial variable assignments using a trained graph neural network and fixes only those variables for which the model’s prediction confidence exceeds a chosen threshold. The resulting partial solution is then completed by a MIP solver (SCIP). The approach aims to accelerate the discovery of better primal solutions compared to standard solver heuristics.",
  "problem_type": "MILP",
  "datasets": [
    "item placement",
    "load balancing",
    "anonymous",
    "MIPLIB 2010"
  ],
  "performance_metrics": [
    "primal integral",
    "cumulative reward",
    "feasibility",
    "primal bound"
  ],
  "lp_model": {
    "objective": "\\min \\mathbf{c}^\\top \\mathbf{x}",
    "constraints": [
      "\\mathbf{A}^\\top \\mathbf{x} \\leq \\mathbf{b}"
    ],
    "variables": [
      "\\mathbf{x} \\in \\mathbb{Z}^p \\times \\mathbb{R}^{n-p} - vector of variables with p integer and n-p continuous variables"
    ]
  },
  "raw_latex_model": "\\begin{aligned}\n& \\arg\\min_{\\mathbf{x}} \\mathbf{c}^\\top \\mathbf{x} \\\\\n& \\text{subject to } \\mathbf{A}^\\top \\mathbf{x} \\leq \\mathbf{b}, \\\\\n& \\quad \\mathbf{x} \\in \\mathbb{Z}^p \\times \\mathbb{R}^{n-p},\n\\end{aligned}",
  "algorithm_description": "Step 1: Collect training solutions by solving MIP instances with a solver (e.g., SCIP) under specified time limits and heuristic emphasis (aggressive). For data-scarce datasets, augment with additional problems like MIPLIB 2010.\nStep 2: Train a Graph Convolutional Neural Network (GCNN) model using a mini-batch loss function that normalizes by the number of nodes per graph to predict variable assignments.\nStep 3: For inference on new instances, apply the trained model to obtain confidence scores for each variable. Set a confidence threshold; variables with scores above the threshold are fixed to their predicted values, forming a partial solution.\nStep 4: If the partial solution is feasible, use it to warm-start the solver; if infeasible, unfix variables and rely solely on the solver. Determine the optimal confidence threshold via grid search on a validation set to minimize primal integral.\nStep 5: Evaluate the method by measuring primal integral on test instances. For specific datasets like item placement, tune SCIP parameters instead of using the neural network model."
}