{
  "file_path": "VAG_CO/EnergyFunctions/MIS.py, VAG_CO/Networks/AutoregressiveModels/GNN_PPO_spin_drop_vectorised.py, VAG_CO/Samplers/PPOSampler.py, VAG_CO/trainPPO/IGraphVecEnv/GraphEnv_train_configuration.py, VAG_CO/trainPPO/trainPPO_configuration.py",
  "function_name": "MIS_Energy, GNN_PPO_SpinDrop, GNN_actor_critic, PPOSampler, IGraphEnv, HiVNAPPo.train",
  "code_snippet": "\n\n# ==========================================\n# File: VAG_CO/EnergyFunctions/MIS.py\n# Function/Context: MIS_Energy\n# ==========================================\nimport jraph\nimport jax\nimport jax.numpy as jnp\nimport jax.tree_util as tree\n\ndef MIS_Energy(H_graph, bins, A = 1., B = 1.1):\n    nodes = H_graph.nodes\n    n_node = H_graph.n_node\n    n_graph = n_node.shape[0]\n    graph_idx = jnp.arange(n_graph)\n    sum_n_node = tree.tree_leaves(nodes)[0].shape[0]\n    node_gr_idx = jnp.repeat(\n        graph_idx, n_node, axis=0, total_repeat_length=sum_n_node)\n    total_num_nodes = tree.tree_leaves(nodes)[0].shape[0]\n\n    adjacency = jnp.ones_like(H_graph.edges)\n\n    ### normalise through average number of nodes in dataset\n    A = A\n    B = B\n\n    raveled_bins = jnp.reshape(bins, (bins.shape[0], 1))\n    Energy_messages = adjacency * (raveled_bins[H_graph.senders]) * (raveled_bins[H_graph.receivers])\n    Energy_per_node = 0.5 * jax.ops.segment_sum(Energy_messages, H_graph.receivers, total_num_nodes)\n\n    Hb = B * (jax.ops.segment_sum(Energy_per_node, node_gr_idx, n_graph))\n    Ha = A * (jax.ops.segment_sum(raveled_bins, node_gr_idx, n_graph))\n\n    Energy = - Ha + Hb\n\n    return Energy\n\n# ==========================================\n# File: VAG_CO/Networks/AutoregressiveModels/GNN_PPO_spin_drop_vectorised.py\n# Function/Context: GNN_PPO_SpinDrop, GNN_actor_critic\n# ==========================================\nfrom ..BuildingBlocks.EncodeProcessDecodeGNNs import EncodeProcess, EncodeProcessNew\nfrom ..BuildingBlocks.GNNetworks import ProbMLP, ValueMLP\nimport flax.linen as nn\nimport jax.numpy as jnp\nimport jax\nimport jax.tree_util as tree\nfrom jraph_utils import utils as jutils\n\n### TODO add observations\nclass GNN_PPO_SpinDrop(nn.Module):\n    spiral_transform: list\n    cfg: dict\n    SampleMode: str = \"sample\"\n    training: bool = False\n    weight_tied: bool = False\n\n    def setup(self):\n\n        self.message_MLP_features = self.cfg.Network_params.GNNs.message_MLP_features\n        self.node_MLP_features = self.cfg.Network_params.GNNs.node_MLP_features\n        self.edge_MLP_features = self.cfg.Network_params.GNNs.edge_MLP_features\n        self.encode_node_features = self.cfg.Network_params.GNNs.encode_node_features\n        self.encode_edge_features = self.cfg.Network_params.GNNs.encode_edge_features\n        self.n_GNN_layers = self.cfg.Network_params.GNNs.n_GNN_layers\n        self.policy_MLP_features = self.cfg.Network_params.policy_MLP_features\n        self.value_MLP_features = self.cfg.Network_params.value_MLP_features\n\n        self.policy_global_features = self.cfg.Network_params.GNNs.policy_global_features\n        self.value_global_features = self.cfg.Network_params.GNNs.value_global_features\n\n        self.EnergyFunction = self.cfg.Ising_params.EnergyFunction\n\n        self.layer_norm = self.cfg.Network_params.layer_norm\n        self.edge_updates = self.cfg.Network_params.GNNs.edge_updates\n        self.GNN_name = self.cfg.Network_params.GNNs.GNN_name\n\n        self.lam = self.cfg.Train_params.PPO.lam\n        self.GNN_mode = self.cfg.Network_params.GNNs.mode\n        #RNN = lambda x: GRNN_inside_loop(SampleMode = self.SampleMode,out_features = self.out_features, hidden_RNN_features = self.hidden_RNN_features, n_RNN_layers = self.n_RNN_layers, training = self.training)\n        self.cell = nn.scan(\n            nn.jit(GNN_actor_critic),\n            variable_broadcast=\"params\",\n            split_rngs={\"params\": False})\n\n        self.apply_cell = self.cell(self.spiral_transform, self.message_MLP_features, self.node_MLP_features, self.edge_MLP_features, self.encode_node_features, self.encode_edge_features, self.n_GNN_layers,\n                                    self.policy_MLP_features, self.value_MLP_features, self.EnergyFunction, self.policy_global_features, self.value_global_features, GNN_name = self.GNN_name,\n                                    layer_norm = self.layer_norm, edge_updates = self.edge_updates, SampleMode = self.SampleMode, training = self.training)\n\n        self.gamma = 1.\n\n\n    @nn.compact\n    def __call__(self, H_graph, compl_H_graph, Ext_fields, ones, key, T):\n        if (self.SampleMode == \"sample\"):\n            return self.sample(H_graph, compl_H_graph, Ext_fields, key)\n        elif(self.SampleMode == \"sample_unpadded\"):\n            actions = ones\n            return self.sample_unpadded(H_graph, compl_H_graph, Ext_fields, key)\n        elif(self.SampleMode == \"eval_spin\"):\n            actions = ones\n            return self.eval_spin(H_graph, compl_H_graph, Ext_fields, actions)\n        elif(self.SampleMode == \"sample_sparse\"):\n            return self.sample(H_graph, compl_H_graph, Ext_fields, key)\n        elif(self.SampleMode == \"eval_spin_sparse\"):\n            actions = ones\n            return self.eval_spin_padded(H_graph, compl_H_graph, Ext_fields, actions)#\n\n    def sample(self, H_graphs, compl_H_graph, Ext_fields, key):\n        return self.apply_cell.sample(H_graphs, compl_H_graph,Ext_fields, key)\n\n    def sample_unpadded(self, H_graphs, compl_H_graph, Ext_fields, key):\n        return self.apply_cell.sample_unpadded(H_graphs, compl_H_graph, Ext_fields, key)\n\n    def eval_spin(self, H_graph, compl_H_graph, Ext_fields, actions):\n        values, log_probs = self.apply_cell.eval(H_graph, compl_H_graph, Ext_fields, actions)\n\n        return values, log_probs\n\n    def eval_spin_padded(self, H_graph, compl_H_graph, Ext_fields, actions):\n        values, log_probs = self.apply_cell.eval_padded(H_graph, compl_H_graph, Ext_fields, actions)\n\n        return values, log_probs\n\n\nclass GNN_actor_critic(nn.Module):\n    spiral_transform: list\n    message_MLP_features: list\n    node_MLP_features: list\n    edge_MLP_features: list\n    encode_node_features: list\n    encode_edge_features: list\n    n_GNN_layers: int\n    policy_MLP_features: list\n    value_MLP_features: list\n\n    EnergyFunction: str\n    value_global_features: bool\n    policy_global_features: bool\n\n    layer_norm: bool = True\n    edge_updates: bool = True\n    SampleMode: str = \"sample\"\n    training: bool = False\n    GNN_mode: str = \"non_linear\"\n    GNN_name: str = \"newEncoder\"\n\n    def setup(self):\n        # self.value_GNN = EncodeProcessDecode(self.GNN_MLP_features, n_layers = self.n_GNN_layers,\n        #                                message_passing = self.GNN_mode, training = self.training, weight_tied = False)\n        if(self.GNN_name == \"newEncoder\"):\n            self.prob_GNN = EncodeProcessNew(self.message_MLP_features, self.node_MLP_features, self.edge_MLP_features, self.encode_node_features, self.encode_edge_features, layer_norm = self.layer_norm,\n                                               edge_updates = self.edge_updates,  n_layers = self.n_GNN_layers,\n                                               message_passing = self.GNN_mode, training = self.training, weight_tied = False)\n        else:\n            self.prob_GNN = EncodeProcess(self.message_MLP_features, self.node_MLP_features, self.edge_MLP_features, self.encode_node_features, self.encode_edge_features, layer_norm = self.layer_norm,\n                                               edge_updates = self.edge_updates,  n_layers = self.n_GNN_layers,\n                                               message_passing = self.GNN_mode, training = self.training, weight_tied = False)\n\n        self.probMLP = ProbMLP(features= self.policy_MLP_features, training = self.training)\n        self.valueMLP = ValueMLP(features= self.value_MLP_features, training = self.training)\n\n    def __call__(self, carry, xs):\n        if(self.SampleMode == \"sample\"):\n            return self.sample(carry, xs)\n        elif (self.SampleMode == \"sample_sparse\"):\n            return self.sample_batched_graphs(carry, xs)\n\n    def forward_padded(self, H_graph, compl_H_graph, spin_sites):\n        GNN_embeddings = self.prob_GNN(H_graph, compl_H_graph)\n        node_embedding = GNN_embeddings[spin_sites]\n\n        ### TODO add global features\n        if(self.value_global_features or self.policy_global_features):\n            nodes = H_graph.nodes\n            n_node = H_graph.n_node\n            n_graph = n_node.shape[0]\n            graph_idx = jnp.arange(n_graph)\n            sum_n_node = tree.tree_leaves(nodes)[0].shape[0]\n            node_gr_idx = jnp.repeat(\n                graph_idx, n_node, axis=0, total_repeat_length=sum_n_node)\n\n            print(jax.ops.segment_sum(GNN_embeddings, node_gr_idx, n_graph).shape)\n            sum_embedding = jax.ops.segment_sum(GNN_embeddings, node_gr_idx, n_graph)\n            print(\"node embedding shape\", node_embedding.shape)\n            print(GNN_embeddings.shape)\n            print(\"sum embedding shape\", sum_embedding.shape)\n            concat_embedding = jnp.concatenate([sum_embedding, node_embedding], axis = -1)\n\n        if(self.value_global_features):\n            values = self.valueMLP(concat_embedding)\n        else:\n            values = self.valueMLP(node_embedding)\n\n        if(self.policy_global_features):\n            log_prob = self.probMLP(concat_embedding)\n        else:\n            log_prob = self.probMLP(node_embedding)\n\n        return values, log_prob\n\n    def forward(self, H_graph, compl_H_graph, spin_sites):\n        GNN_embeddings = self.prob_GNN(H_graph, compl_H_graph)\n        node_embedding = jnp.squeeze(GNN_embeddings[spin_sites], axis = -2)\n\n        ### TODO add global features\n        if(self.value_global_features or self.policy_global_features):\n            nodes = H_graph.nodes\n            n_node = H_graph.n_node\n            n_graph = n_node.shape[0]\n            graph_idx = jnp.arange(n_graph)\n            sum_n_node = tree.tree_leaves(nodes)[0].shape[0]\n            node_gr_idx = jnp.repeat(\n                graph_idx, n_node, axis=0, total_repeat_length=sum_n_node)\n\n            print(jax.ops.segment_sum(GNN_embeddings, node_gr_idx, n_graph).shape)\n            sum_embedding = jnp.squeeze(jax.ops.segment_sum(GNN_embeddings, node_gr_idx, n_graph), axis = -2)\n            print(\"node embedding shape\", node_embedding.shape)\n            print(GNN_embeddings.shape)\n            print(\"sum embedding shape\", sum_embedding.shape)\n            concat_embedding = jnp.concatenate([sum_embedding, node_embedding], axis = -1)\n\n        if(self.value_global_features):\n            values = self.valueMLP(concat_embedding)\n        else:\n            values = self.valueMLP(node_embedding)\n\n        if(self.policy_global_features):\n            log_prob = self.probMLP(concat_embedding)\n        else:\n            log_prob = self.probMLP(node_embedding)\n\n        return values, log_prob\n\n\n    def sample_unpadded(self, H_graph, compl_H_graph, Ext_fields, key):\n        n_node = H_graph.n_node\n        spin_site = jutils.get_first_node_idxs(n_node)#jnp.cumsum(graph_idx * n_node)\n\n        rand_nodes = H_graph.nodes\n        Spin_identifier = jnp.zeros(rand_nodes.shape[0])\n        Spin_identifier = Spin_identifier.at[spin_site].set(jnp.ones_like(spin_site))\n        one_hot_identifier = jax.nn.one_hot(Spin_identifier, num_classes=2)\n        H_embedding = jnp.concatenate([Ext_fields, rand_nodes, one_hot_identifier], axis = -1)\n\n        print(H_embedding)\n\n        H_graph = H_graph._replace(nodes=H_embedding)\n\n        values, log_probs = self.forward(H_graph, compl_H_graph, spin_site)\n\n        key, subkey = jax.random.split(key)\n\n        sampled_bin_values = jax.random.categorical(subkey, log_probs, axis=-1)\n\n        one_hot_spins = jax.nn.one_hot(sampled_bin_values, num_classes=2)\n\n        spin_log_prob = jnp.sum(log_probs * one_hot_spins, axis=-1)\n\n        return values, spin_log_prob, jnp.zeros_like(spin_site), sampled_bin_values, key\n\n    def sample(self, H_graph, compl_H_graph, Ext_fields, key):\n        n_node = H_graph.n_node\n        spin_site = jutils.get_first_node_idxs(n_node)\n\n        rand_nodes = H_graph.nodes\n        Spin_identifier = jnp.zeros(rand_nodes.shape[0])\n        Spin_identifier = Spin_identifier.at[spin_site].set(jnp.ones_like(spin_site))\n        one_hot_identifier = jax.nn.one_hot(Spin_identifier, num_classes=2)\n        H_embedding = jnp.concatenate([Ext_fields, rand_nodes, one_hot_identifier], axis = -1)\n\n        H_graph = H_graph._replace(nodes=H_embedding)\n        values, log_probs = self.forward_padded(H_graph, compl_H_graph, spin_site)\n\n        key, subkey = jax.random.split(key)\n\n        sampled_bin_values = jax.random.categorical(subkey, log_probs, axis=-1)\n\n        one_hot_spins = jax.nn.one_hot(sampled_bin_values, num_classes=2)\n\n        spin_log_prob = jnp.sum(log_probs * one_hot_spins, axis=-1)\n\n        return values[:-1], spin_log_prob[:-1], jnp.zeros_like(spin_site[:-1]), sampled_bin_values[:-1], key\n\n    def eval_padded(self, H_graph, compl_H_graph, Ext_fields, actions):\n        ### TODO add embedding for next spin which has to be generated\n        spin_values = actions[:,0]\n\n        n_node = H_graph.n_node\n\n        spin_site = jutils.get_first_node_idxs(n_node)\n\n        rand_nodes = H_graph.nodes\n        Spin_identifier = jnp.zeros(rand_nodes.shape[0])\n        Spin_identifier = Spin_identifier.at[spin_site].set(jnp.ones_like(spin_site))\n        one_hot_identifier = jax.nn.one_hot(Spin_identifier, num_classes=2)\n        H_embedding = jnp.concatenate([Ext_fields, rand_nodes, one_hot_identifier], axis = -1)\n\n        H_graph = H_graph._replace(nodes=H_embedding)\n\n        values, log_probs = self.forward_padded(H_graph, compl_H_graph, spin_site)\n\n        one_hot_spins = jax.nn.one_hot(spin_values, num_classes=2)\n\n        spin_log_prob = jnp.sum(log_probs[:-1] * one_hot_spins, axis=-1)\n\n        return values[:-1], spin_log_prob\n\n    def eval(self, H_graph, compl_H_graph, Ext_fields, actions):\n        print(\"unpadded\")\n        print(actions.shape)\n        spin_values = actions[:, 0]\n\n        n_node = H_graph.n_node\n\n        spin_site = jutils.get_first_node_idxs(n_node)\n\n        rand_nodes = H_graph.nodes\n        Spin_identifier = jnp.zeros(rand_nodes.shape[0])\n        Spin_identifier = Spin_identifier.at[spin_site].set(jnp.ones_like(spin_site))\n        one_hot_identifier = jax.nn.one_hot(Spin_identifier, num_classes=2)\n        H_embedding = jnp.concatenate([Ext_fields, rand_nodes, one_hot_identifier], axis = -1)\n        print(\"spin site\", spin_site)\n        print(\"H_embedding\", H_embedding)\n        H_graph = H_graph._replace(nodes=H_embedding)\n\n        values, log_probs = self.forward(H_graph, compl_H_graph, spin_site)\n\n        one_hot_spins = jax.nn.one_hot(spin_values, num_classes=2)\n\n        spin_log_prob = jnp.sum(log_probs * one_hot_spins, axis=-1)\n        return values, spin_log_prob\n\n# ==========================================\n# File: VAG_CO/Samplers/PPOSampler.py\n# Function/Context: PPOSampler\n# ==========================================\nimport jax\nimport jax.numpy as jnp\nimport jax.lax\nimport optax\n\nfrom ..Networks.AutoregressiveModels.GNN_PPO_spin_selector import GNN_PPO_SpinSelector\nfrom ..Networks.AutoregressiveModels.GNN_PPO_spin_drop import GNN_PPO_SpinDrop\nfrom .BaseSamplers.BaseSampler import BaseRNNSampler\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom functools import partial\n\n### TODO pay attention this currently does not work with external fields\nclass PPOSampler(BaseRNNSampler):\n\n    def __init__(self, GraphDataLoader, epoch_dict, cfg, sparse_graphs = True):\n\n        self.sparse_graphs = sparse_graphs\n\n        self.clip = cfg.Train_params.PPO.clip_value\n        self.GraphDataLoader = GraphDataLoader\n        self.n_nodes = self.GraphDataLoader.n_nodes\n        self.x = GraphDataLoader.x\n        self.y = GraphDataLoader.y\n\n        self.n_basis_states = cfg.Train_params.n_basis_states\n        self.num_classes = 2\n\n        self.init_key = jax.random.PRNGKey(cfg.Train_params.seed)\n        self.key, self.dropout_key = jax.random.split(self.init_key, num=2)\n\n        self.cfg = cfg\n\n        self.lr = cfg.Train_params.lr\n        self.lr_alpha = cfg.Train_params.lr_alpha\n\n        self.N_warmup = epoch_dict[\"N_warmup\"]*epoch_dict[\"batch_epochs\"]\n        self.N_anneal = epoch_dict[\"N_anneal\"]*epoch_dict[\"batch_epochs\"]\n        self.N_equil = epoch_dict[\"N_equil\"]*epoch_dict[\"batch_epochs\"]\n        self.epochs = self.N_warmup + self.N_anneal + self.N_equil\n        print(\"epoch_dict\", epoch_dict)\n\n        self.network_type = cfg.Network_params.network_type\n\n        ### INITIALIZE PROB AND PHASE NETWORK\n        self.back_transform = jnp.array(self.GraphDataLoader.back_transform)\n        self.spiral_transform = jnp.array(self.GraphDataLoader.spiral_transform)\n        print(\"back_transform\", self.back_transform)\n        print(\"spiral_transform\", self.spiral_transform)\n        self.schedule = cfg.Train_params.lr_schedule\n        self.initialize_networks()\n\n    def initialize_networks(self):\n        print(\"init \", self.network_type)\n        if(self.network_type == \"GNN_SpinSelector\"):\n            print(\"GNN is used\")\n            net = GNN_PPO_SpinSelector\n        elif (self.network_type == \"GNN_SpinDrop\"):\n            print(\"GNN is used\")\n            net = GNN_PPO_SpinDrop\n        else:\n            ValueError(f\"Network type {self.network_type} is not valid.\")\n\n        if(self.sparse_graphs):\n            self.SampleGRNN = net(self.spiral_transform, self.cfg, SampleMode = \"sample_sparse\")\n            self.EvalSpinGRNN = net(self.spiral_transform, self.cfg, SampleMode = \"eval_spin_sparse\")\n            self.SampleUnpaddedGRNN = net(self.spiral_transform, self.cfg, SampleMode = \"sample_unpadded\")\n        else:\n            self.SampleGRNN = net(self.spiral_transform, self.cfg, SampleMode = \"sample\")\n            self.EvalSpinGRNN = net(self.spiral_transform, self.cfg, SampleMode = \"eval_spin\")\n        self.initEvalSpinGRNN = net(self.spiral_transform, self.cfg, SampleMode=\"eval_spin\")\n\n        self.init_params()\n\n        print(\"Networks are initialized\")\n\n        ### INITIALIZE OPTAX\n        print(\"learning rate scheduler, lr , lr_alpha\", self.lr, self.lr_alpha, self.schedule, self.epochs)\n\n        if(self.schedule == \"cosine_warmup\"):\n            warmup_steps = 500\n            self.lr_scheduler = optax.warmup_cosine_decay_schedule(init_value=0., peak_value=10 * self.lr,\n                                                             end_value=self.lr_alpha * self.lr,\n                                                             warmup_steps=warmup_steps,\n                                                             decay_steps=self.epochs - warmup_steps)\n        elif(self.schedule ==  \"cosine\"):\n            self.lr_scheduler = optax.cosine_decay_schedule(self.lr, self.epochs, alpha=self.lr_alpha)\n        elif(self.schedule ==  \"cosine_restart\"):\n            min_lr = self.lr * self.lr_alpha\n            mean_lr = (self.lr + min_lr) / 2\n            n = 2\n            peak_factor = 2\n            if(self.N_warmup > 0):\n                # for freq in range(n)\n                lrs = [self.lr,self.lr/n]\n                lrs_end = [self.lr/n, self.lr]\n                warmup_cosine_decay_scheduler_list = [optax.warmup_cosine_decay_schedule(init_value=lrs[i], peak_value=peak_factor * self.lr,\n                                                   warmup_steps=int(0.2 * self.N_warmup / n),\n                                                   decay_steps=int(self.N_warmup / n),\n                                                   end_value=lrs_end[i]) for i in range(n)]\n                boundaries = [int(self.N_warmup / n), 2*int(self.N_warmup / n)]\n            else:\n                boundaries = []\n                warmup_cosine_decay_scheduler_list = []\n\n            if(self.N_anneal > 0):\n                freq_width = int(0.1 * self.N_anneal)\n                n_cycles = int(self.N_anneal / freq_width)\n                print(\"cosine with restarts is used with \", n_cycles, \" frequencies\")\n                cos_anneal = lambda x: (self.lr - mean_lr) * np.cos(np.pi * x / n_cycles) + mean_lr\n                lrs = [cos_anneal(0)]\n                lrs.extend([ cos_anneal(freq)/n for freq in range(n_cycles)])\n                warmup_cosine_decay_scheduler_list.extend([optax.warmup_cosine_decay_schedule(init_value=lrs[freq], peak_value= peak_factor*cos_anneal(freq),\n                                                                                   warmup_steps=int(0.2*freq_width),\n                                                                                   decay_steps=freq_width,\n                                                                                   end_value=lrs[freq + 1]) for freq in range(n_cycles)])\n                boundaries.extend([self.N_warmup + freq_width * (i + 1) for i in range(len(warmup_cosine_decay_scheduler_list) - 1 - len(boundaries))])\n            if(self.N_equil > 0):\n                freq_width = int(0.1 * self.N_equil)\n                n_cycles = int(self.N_equil / freq_width)\n                print(\"cosine with restarts is used with \", n_cycles, \" frequencies\")\n                cos_anneal = lambda x: min_lr\n                lrs = [cos_anneal(0)]\n                lrs.extend([ cos_anneal(freq)/n for freq in range(n_cycles)])\n                warmup_cosine_decay_scheduler_list.extend([optax.warmup_cosine_decay_schedule(init_value=lrs[freq], peak_value= peak_factor*cos_anneal(freq),\n                                                                                   warmup_steps=int(0.2*freq_width),\n                                                                                   decay_steps=freq_width,\n                                                                                   end_value=lrs[freq + 1]) for freq in range(n_cycles)])\n                boundaries.extend([self.N_warmup + self.N_anneal + freq_width * (i + 1) for i in range(len(warmup_cosine_decay_scheduler_list) - 1 - len(boundaries))])\n\n            self.lr_scheduler = optax.join_schedules(warmup_cosine_decay_scheduler_list, boundaries=boundaries)\n\n            if(False):\n                lrs = [self.lr_scheduler(i) for i in range(self.epochs)]\n                print(\"finished\")\n                #print(lrs)\n                plt.figure()\n                plt.scatter(range(self.epochs), lrs)\n                plt.title(\"SGD With Warm Restarts Scheduler\")\n                plt.ylabel(\"Learning Rate\")\n                plt.xlabel(\"Epochs/Steps\")\n                plt.show()\n\n        self.optimizer = optax.chain(optax.clip_by_global_norm(1.0), optax.scale_by_radam(), optax.scale(-self.lr))\n        self.opt_state = self.optimizer.init(self.params)\n\n        # opt_init, self.opt_update = optax.radam(learning_rate=self.lr_scheduler)\n        # self.opt_state = opt_init(self.params)\n\n        self.init_Nb_sample()\n        #self.init_Nb_eval()\n        self.init_Hb_Nb_sample()\n        #self.init_Hb_Nb_eval()\n        #self.init_Nb_eval_spin()\n        #self.init_Hb_Nb_eval_spin()\n\n        self.init_idxb_eval()\n        self.init_Nb_idxb_eval()\n        self.init_Hb_Nb_idxb_eval()\n        self.init_backward_Hb_Nb_loss()\n        self.init_Nb_sample_sparse()\n        self.init_Nb_sample_reduce()\n        #self.init_Nb_sparse_eval()\n        self.init_idxb_sparse_eval()\n        self.init_Nb_idxb_sparse_eval()\n        self.init_backward_sparse_reduced_Hb_Nb_loss()\n        self.init_backward_sparse_Hb_Nb_loss()\n\n        jitting = True\n        update_params = lambda par, grad, opt_state: self.update_params(par, grad, opt_state)\n\n        if (jitting):\n            update_params = jax.jit(update_params)\n        self.update_params_jit = update_params\n\n\n    def init_params(self):\n        self.key, subkey = jax.random.split(self.key)\n        H_graph = self.GraphDataLoader.H_graph\n        actions = jnp.ones((self.n_nodes,2), dtype = jnp.int32)\n        self.params = self.initEvalSpinGRNN.init({\"params\": subkey}, H_graph, actions, None, None)\n\n    def return_batched_key(self, batch_size):\n        subkeys = jax.random.split(self.key, batch_size)\n        return subkeys\n\n    ### TODO add this to BaseSampler\n    def get_params(self):\n        return self.params\n\n    def get_opt_states(self):\n        return self.opt_state\n\n    def save_opt_state(self, opt_state):\n        self.opt_state = opt_state\n\n    ### TODO add this to BaseSampler\n    def update_optimizer(self, lr, params):\n        self.curr_lr = lr\n        #self.optimizer = optax.radam(learning_rate=self.curr_lr)\n        self.optimizer = optax.chain(optax.clip_by_global_norm(1.0), optax.scale_by_radam(),\n                                              optax.scale(-self.curr_lr))\n        opt_state = self.optimizer.init(params)\n        return opt_state\n\n    ### TODO add this to BaseSampler\n    def update_params(self, params, grads, opt_state):\n        grad_update, opt_state = self.optimizer.update(grads, opt_state)\n        params = optax.apply_updates(params, grad_update)\n        return params, opt_state\n\n    def Hb_Nb_loss(self, params, Hb_Nb_binaries, Hb_Nb_observations, Hb_H_graph, Hb_Nb_idx, Hb_Nb_A_k, Hb_Nb_log_probs, Hb_Nb_rtg, c1 = 0.5):\n\n        Hb_Nb_Values, curr_Hb_Nb_log_probs = self.Hb_Nb_idxb_eval(params, Hb_H_graph, Hb_Nb_binaries, Hb_Nb_observations, Hb_Nb_idx)\n        ratios = jnp.exp(curr_Hb_Nb_log_probs - Hb_Nb_log_probs)\n\n        surr1 = ratios * Hb_Nb_A_k\n        surr2 = jnp.clip(ratios, 1 - self.clip, 1 + self.clip) * Hb_Nb_A_k\n\n        actor_loss = jnp.mean((-jnp.minimum(surr1, surr2)))\n        critic_loss = jnp.mean((Hb_Nb_Values - Hb_Nb_rtg) ** 2)\n        overall_loss = (1-c1) * actor_loss + c1 *critic_loss\n        return overall_loss, (actor_loss, critic_loss)\n\n# ==========================================\n# File: VAG_CO/trainPPO/IGraphVecEnv/GraphEnv_train_configuration.py\n# Function/Context: IGraphEnv\n# ==========================================\nimport gym\nimport igraph\nimport numpy as np\nfrom numpy import random\nfrom ..OwnVecEnv import SubprocVecEnv\nimport time\nimport igraph as ig\nimport jraph\nimport random\nimport copy\nfrom loadGraphDatasets import GetDataLoaders\nfrom . import DataContainer\nfrom jraph_utils import utils as jutils\nimport itertools\n\nfloat_type = np.float32\nint_type = np.int32\n\nclass IGraphEnv(gym.Env):\n    ### use jax random keys?\n    \"\"\"Custom Environment that follows gym interface\"\"\"\n    metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n\n    ### TODO since always the first node is selected actually one igraph is enough! it will make the code much faster!\n    def __init__(self,cfg, H_seed, mode = \"train\"):\n        super(IGraphEnv, self).__init__()\n        random.seed(H_seed)\n        self.cfg = cfg\n        self.node_embedding_type = cfg[\"Ising_params\"][\"node_embedding_type\"]\n        self.edge_embedding_type = cfg[\"Ising_params\"][\"edge_embedding_type\"]\n        self.H_seed = H_seed\n        self.mode = mode\n        self.Nb = cfg[\"Train_params\"][\"n_basis_states\"]\n        self.EnergyFunction = cfg[\"Ising_params\"][\"EnergyFunction\"]\n        self.dataset_name = cfg[\"Ising_params\"][\"IsingMode\"]\n        self.ordering = cfg[\"Ising_params\"][\"ordering\"]\n        self.n_rand_nodes = cfg[\"Ising_params\"][\"n_rand_nodes\"]\n\n        self.gamma = 1.\n        self.lam = cfg[\"Train_params\"][\"PPO\"][\"lam\"]\n\n        self.time_horizon = cfg[\"Train_params\"][\"PPO\"][\"time_horizon\"]\n\n        ### TODO align code with ReplayBuffer_vectorised\n        #print(\"init spin env\", mode)\n        #start = time.time()\n        Dataset_dict = GetDataLoaders.init_Dataset(cfg, H_idx = H_seed, mode = mode)\n        #end = time.time()\n        #print(\"init time needed\", end-start)\n\n        self.num_graphs = Dataset_dict[\"num_graphs\"]\n        print(\"num_Graphs =\", self.num_graphs)\n\n        self.next_graph_func = Dataset_dict[\"loader_func\"]\n        self.loader = Dataset_dict[\"loader\"]\n        self.finished = False\n        self.global_reset = False\n\n        self.order_func = self.init_graph_ordering_func()\n\n        self.mean_energy = Dataset_dict[\"mean_energy\"]\n        self.std_energy = Dataset_dict[\"std_energy\"]\n\n        self.policy_global_features = cfg[\"Network_params\"][\"policy_MLP_features\"]\n        self.n_classes = self.policy_global_features[-1]\n        self.n_sampled_sites = int(np.log2(self.n_classes))\n        self.sampling_mode = \"normal\"\n\n        self.mov_reward = [0.,1.]\n        self.seed = cfg[\"Ising_params\"][\"shuffle_seed\"]\n        self.rng = np.random.default_rng(self.seed *self.H_seed + self.H_seed)\n        self.masking = cfg[\"Train_params\"][\"masking\"]\n        self.pruning = cfg[\"Train_params\"][\"pruning\"]\n\n        self.reversed_disjoint_graph_ordering = cfg[\"Ising_params\"][\"reversed_disjoint_graph_ordering\"]\n        self.centrality = cfg[\"Ising_params\"][\"centrality\"]\n\n    def normalize_Energy(self, Energy_arr):\n        return (Energy_arr-self.mean_energy)/self.std_energy\n\n    def compute_unormalized_Energy(self, Energy_arr):\n        return (Energy_arr * self.std_energy + self.mean_energy)\n\n    def convert_to_jraph(self, igraph):\n        couplings = np.array(igraph.es[\"couplings\"] , dtype = float_type)\n        edge_list = igraph.get_edgelist()\n        edge_arr = np.array(edge_list, dtype = int_type)\n\n        #print(\"ecount\", igraph.ecount())\n        #print(edge_list)\n        if (igraph.ecount() > 0):\n            receivers = edge_arr[:, 0]\n            senders = edge_arr[:, 1]\n            edges = couplings\n        else:\n            receivers = np.zeros((0,), dtype=int_type)\n            senders = receivers\n            edges = np.ones((0, 1))\n            edges = edges\n\n        N = igraph.vcount()\n\n        node_features = np.array(igraph.vs[\"node_features\"], dtype = float_type)\n\n        jgraph = jraph.GraphsTuple(nodes=node_features, edges=edges, receivers=receivers,\n                                              senders=senders,\n                                              n_node= np.array([N], dtype = int_type),\n                                              n_edge=np.array([senders.shape[0]], dtype = int_type), globals= np.array([N], dtype = int_type))\n\n        return jgraph\n\n    def _add_empty_nodes(self, Igraph, orig_Igraph, gs):\n        num_nodes = Igraph.vcount()\n        mod = num_nodes%self.n_sampled_sites\n        new_gs = gs\n        if(mod != 0):\n            additional_nodes = self.n_sampled_sites - mod\n            self.additional_nodes = additional_nodes\n            external_fields = Igraph.vs[\"ext_fields\"]\n            external_fields.extend([np.array([0.]) for i in range(additional_nodes)])\n            Igraph.add_vertices(additional_nodes)\n            Igraph.vs[\"ext_fields\"] = external_fields\n\n            if(orig_Igraph != None):\n                orig_Igraph.add_vertices(additional_nodes)\n\n            new_gs = np.ones((num_nodes+additional_nodes,1))\n            new_gs[:num_nodes,0] = gs\n        #print(\"ext field output\", np.array(Igraph.vs[\"ext_fields\"]).shape)\n\n        return Igraph, orig_Igraph, new_gs\n\n    def init_graph_func(self):\n        i_graph, orig_igraph, self.gs, self.finished, self.loader = self.next_graph_func(self.global_reset, self.finished, self.loader)\n        self.orig_n_nodes = i_graph.vcount()\n        self.global_reset = False\n        i_graph, orig_igraph, self.gs = self._add_empty_nodes(i_graph, orig_igraph, self.gs)\n        self.N_spin = i_graph.vcount()\n        return i_graph, orig_igraph\n\n    def shuffle_graph(self, Igraph):\n        perm = np.arange(0, Igraph.vcount())\n        np.random.shuffle(perm)\n        Igraph = Igraph.permute_vertices(list(perm))\n        return Igraph, perm\n\n    def _test_Graph_centralities(self, g):\n        s1 = time.time()\n        # Degree Centrality\n        degree_centrality = g.degree()\n        most_central_node_degree = degree_centrality.index(max(degree_centrality))\n        s2= time.time()\n        # Closeness Centrality\n        closeness_centrality = g.closeness()\n        most_central_node_closeness = closeness_centrality.index(max(closeness_centrality))\n        s3 = time.time()\n        # Betweenness Centrality\n        betweenness_centrality = g.betweenness()\n        most_central_node_betweenness = betweenness_centrality.index(max(betweenness_centrality))\n        s4 = time.time()\n        # Eigenvector Centrality\n        eigenvector_centrality = g.eigenvector_centrality()\n        most_central_node_eigenvector = eigenvector_centrality.index(max(eigenvector_centrality))\n        s5 = time.time()\n        print(\"time\", s2 - s1 , s3-s2, s4 - s3, s5 - s4)\n\n    def bfs_order_graph(self, Igraph):\n        Igraph.vs[\"index\"] = np.arange(0,  Igraph.vcount())\n\n        ### TODO padded nodes are apparently always first, maybe make them last\n        ### TODO randomize the order of disjoint graphs\n        disjoint_graphs = Igraph.decompose()\n        ordered_subgraphs = []\n\n\n        if(self.reversed_disjoint_graph_ordering):\n            disjoint_graphs = reversed(disjoint_graphs)\n        else:\n            pass\n\n        for subgraph in disjoint_graphs:\n            if(self.centrality):\n                eigenvector_centrality = subgraph.eigenvector_centrality()\n                most_central_node_eigenvector = eigenvector_centrality.index(max(eigenvector_centrality))\n                idx = most_central_node_eigenvector\n            else:\n                vc = subgraph.vcount()\n                idx = np.random.randint(0, high=vc)\n\n            res = subgraph.bfs(idx)\n            order = res[0]\n            subgraph = subgraph.permute_vertices(list(order))\n            ordered_subgraphs.append(subgraph)\n\n        ordered_IGraph = igraph.disjoint_union(ordered_subgraphs)\n        return ordered_IGraph, ordered_IGraph.vs[\"index\"]\n\n    def dfs_order_graph(self, Igraph):\n        Igraph.vs[\"index\"] = np.arange(0,  Igraph.vcount())\n        disjoint_graphs = Igraph.decompose()\n        ordered_subgraphs = []\n\n        if (self.reversed_disjoint_graph_ordering):\n            disjoint_graphs = reversed(disjoint_graphs)\n        else:\n            pass\n\n        for subgraph in disjoint_graphs:\n            vc = subgraph.vcount()\n            idx = np.random.randint(0, high=vc)\n            res = subgraph.dfs(idx)\n            order = res[0]\n            subgraph = subgraph.permute_vertices(list(order))\n            ordered_subgraphs.append(subgraph)\n\n        ordered_IGraph = ig.disjoint_union(ordered_subgraphs)\n\n        return ordered_IGraph, ordered_IGraph.vs[\"index\"]\n\n    def init_graph_ordering_func(self):\n        if(self.ordering == \"None\" ):\n            func = lambda x: self.shuffle_graph(x)\n        elif(self.ordering == \"BFS\"):\n            func = lambda x: self.bfs_order_graph(x)\n        elif(self.ordering == \"DFS\"):\n            func = lambda x: self.dfs_order_graph(x)\n        else:\n            ValueError(\"Non Valid graph ordering function\")\n        return func\n\n    def from_Hb_Igraph_to_Nb_Hb_Jgraph(self, Igraph):\n        Igraph, self.perm = self.order_func(Igraph)\n\n        if(self.node_embedding_type == \"random\"):\n            Igraph.vs[\"node_features\"] = np.random.normal(0,1, size = (Igraph.vcount(), self.n_rand_nodes))\n        else:\n            Igraph.vs[\"node_features\"] = np.zeros((Igraph.vcount(), 1))\n\n        self.EnergyIgraph = Igraph\n\n        if(\"MaxCl_compl\" in self.EnergyFunction or not self.pruning):\n            ### here ext fields are spins\n            ext_fields = np.array(Igraph.vs[\"ext_fields\"], dtype = float_type)\n            Nb_ext_fields = np.repeat(ext_fields[np.newaxis,:], self.Nb, axis = 0)\n            Nb_spins = copy.copy(self.Nb_spins)\n            Nb_ext_fields = np.concatenate([Nb_ext_fields, Nb_spins], axis = -1)\n        else:\n            ext_fields = np.array(Igraph.vs[\"ext_fields\"], dtype = float_type)\n            Nb_ext_fields = np.repeat(ext_fields[np.newaxis,:], self.Nb, axis = 0)\n\n        #convert_start = time.time()\n        Jgraph = self.convert_to_jraph(self.EnergyIgraph)\n        #convert_end = time.time()\n        #print(\"convert time\", convert_end-convert_start)\n\n        return Jgraph, Nb_ext_fields\n\n    def reset(self):\n        # start = time.time()\n        self.dEnergies = []\n        self.env_step = 0\n        self.time_horizon_step = -1\n\n        # start_loading = time.time()\n        Igraph, self.orig_Igraph = self.init_graph_func()\n        # end_loading = time.time()\n\n        self.gt_Energy = Igraph[\"gt_Energy\"]\n        self.original_Energy = Igraph[\"original_Energy\"]\n        self.self_loop_Energy = Igraph[\"self_loop_Energy\"]\n        self.Nb_spins = np.zeros((self.Nb, self.N_spin, 1), dtype = float_type)\n        # start_H_graph = time.time()\n        self.EnergyJgraph, self.Nb_external_fields = self.from_Hb_Igraph_to_Nb_Hb_Jgraph(Igraph)\n        # end_H_graph = time.time()\n\n        # start_compl_graph = time.time()\n        self.Jgraph, self.Igraph = self._get_compl_graph(self.EnergyJgraph, self.EnergyIgraph)\n        self.num_edges = self.Igraph.ecount()\n        self.return_Jgraph = self.Jgraph\n        # end_compl_graph = time.time()\n\n        # start_init_graph = time.time()\n        self.init_EnergyJgraph = copy.deepcopy(self.EnergyJgraph)\n        self.init_Nb_external_fields = copy.copy(self.Nb_external_fields)\n        # end_init_graph = time.time()\n\n        self.DataContainer = DataContainer.DataContainer(self.cfg, self.time_horizon, self.Nb, self.N_spin, mov_reward = self.mov_reward, lam = self.lam, n_sampled_spins=self.n_sampled_sites)\n\n        # end = time.time()\n        # print(\"init graph\", end_init_graph-start_init_graph)\n        # print(\"compl graph\",end_compl_graph - start_compl_graph)\n        # print(\"H_graph\", end_H_graph - start_H_graph)\n        # print(\"load graph time\", end_loading-start_loading)\n        # print(\"reset time\", end-start)\n        #jutils.check_number_of_edge_occurances(self.Jgraph)\n        #jutils.check_number_of_edge_occurances(self.EnergyJgraph)\n        return {\"H_graph\": {\"Nb_ext_fields\": self.Nb_external_fields, \"jgraph\": self.return_Jgraph}}\n\n\n    def compute_laplace_embeding_on_edges(self, eigenvectors, H_graph):\n        ev_dist = np.abs((eigenvectors[H_graph.receivers, :]- eigenvectors[H_graph.senders, :]))\n        ev_mul = eigenvectors[H_graph.receivers, :]*eigenvectors[H_graph.senders, :]\n        edge_features = np.concatenate([ev_dist, ev_mul], axis = -1)\n        return edge_features\n\n    def compute_Energy_cropped_graph_np(self, Nb_external_fields, Nb_spins, self_senders, self_receivers, self_loop_weight):\n        ### TODO this could be made more efficient by only considering the relevant subgraph\n        Nb_ext_fields_edges = Nb_spins[:,self_senders]*Nb_spins[:,self_receivers] * self_loop_weight[np.newaxis,:]\n        Nb_energy_from_self_loops = np.sum(Nb_ext_fields_edges, axis = -2)\n\n        Nb_Energy_per_node = Nb_external_fields * Nb_spins\n        Nb_Energy = np.sum(Nb_Energy_per_node,axis = -2) + Nb_energy_from_self_loops\n\n        return Nb_Energy\n\n    def _remove_violations(self, Nt_spins, H_graph):\n\n        iH_graph = jutils.from_jgraph_to_igraph(H_graph)\n        indices = list(self.env_step + np.arange(0,self.n_sampled_sites))\n        indices = [i for i in indices if i < iH_graph.vcount()]\n        #print(indices, iH_graph.vcount(), self.env_step, H_graph.nodes.shape[0])\n        subgraph = iH_graph.subgraph(indices)\n\n        H_graph = jutils.from_igraph_to_jgraph(subgraph) ## TODO kae to dir to make it fster\n\n\n        if(self.EnergyFunction == \"MIS\" or \"MaxCl\" in self.EnergyFunction):\n            Nb_bins = (Nt_spins + 1) / 2\n            Nb_ext_fields_edges = Nb_bins[:,H_graph.senders]*Nb_bins[:,H_graph.receivers]\n        elif(self.EnergyFunction == \"MVC\"):\n            Nb_bins = (Nt_spins + 1) / 2\n            Nb_ext_fields_edges = (1-Nb_bins[:, H_graph.senders]) * (1-Nb_bins[:, H_graph.receivers])\n        Nb_energy_from_self_loops = np.sum(Nb_ext_fields_edges, axis = -2)\n        return Nb_energy_from_self_loops\n\n    def _compute_violations(self, Nb_spins, H_graph):\n        #start_prepare_graphs = time.time()\n        iH_graph = jutils.from_jgraph_to_igraph(H_graph)\n        iH_graph.vs[\"spins_Nb\"] = np.swapaxes(Nb_spins,0,1)\n        indices = list(self.env_step + np.arange(0,self.n_sampled_sites))\n        indices = [i for i in indices if i < iH_graph.vcount()]\n        list_of_neighbors = iH_graph.neighborhood(indices, order=1)\n        merged_neighborhood = list(itertools.chain(*list_of_neighbors))\n        all_relevant_indices = set(indices + merged_neighborhood)\n        all_relevant_indices = [i for i in all_relevant_indices if i <= max(indices)]\n        if(len(all_relevant_indices) > 0):\n            subgraph = iH_graph.subgraph(all_relevant_indices)\n\n            #H_graph = jutils.from_igraph_to_dir_jgraph(subgraph)\n            subH_graph = jutils.from_igraph_to_dir_jgraph(subgraph)\n            #sub_Nb_spins = Nb_spins[:,all_relevant_indices]\n            sub_Nb_spins = np.swapaxes(subgraph.vs[\"spins_Nb\"], 0,1)\n            senders = subH_graph.senders\n            receivers = subH_graph.receivers\n            \n            if(self.EnergyFunction == \"MIS\" or \"MaxCl\" in self.EnergyFunction):\n                Nb_bins = (sub_Nb_spins + 1) / 2\n                Nb_ext_fields_edges = Nb_bins[:,senders]*Nb_bins[:,receivers]\n            elif(self.EnergyFunction == \"MVC\"):\n                Nb_bins = (sub_Nb_spins + 1) / 2\n                Nb_ext_fields_edges = (1-Nb_bins[:, senders]) * (1-Nb_bins[:, receivers])\n            Nb_energy_from_self_loops = np.sum(Nb_ext_fields_edges, axis = -2)\n        else:\n            Nb_energy_from_self_loops = np.zeros((self.Nb, 1))\n        #end_prepare_graphs = time.time()\n        #print(\"prepare graphs time\", end_prepare_graphs-start_prepare_graphs)\n        return Nb_energy_from_self_loops\n\n# ==========================================\n# File: VAG_CO/trainPPO/trainPPO_configuration.py\n# Function/Context: HiVNAPPo.train\n# ==========================================\nimport copy\nimport pickle\nimport jax\nimport jax.random\nimport jraph\nimport wandb\nimport jax.numpy as jnp\nfrom train.ConfigureRun import configure_run_ReplayBuffer\nfrom train import LoadUtils as LoadUtils\nfrom train.LogMetrics import SaveModel\nfrom tqdm import tqdm\nfrom functools import partial\nfrom trainPPO.OwnVecEnv import SubprocVecEnv\n\nfrom trainPPO.IGraphVecEnv.GraphEnv_train_configuration import IGraphEnv as IGraphEnv_train\nfrom trainPPO.IGraphVecEnv.GraphEnv_eval_configuration import IGraphEnv as IGraphEnv_eval\nimport numpy as np\nfrom jraph_utils import utils as jutils\nfrom collections import Counter\nimport time\nfrom jax import config\nfrom matplotlib import pyplot as plt\nfrom train.LRSchedule.Schedules import cosine_warmup as LRCosineSchedule\nfrom trainPPO.DataContainer_Dataloader import ContainerDataset, concatenate_arrays\nimport os\nfrom torch.utils.data import DataLoader\nfrom utils.moving_averages import MovingAverage\n\nconfig.update('jax_enable_x64', False)\n\n### todo add dE caluclation!\nclass HiVNAPPo:\n    def __init__(self):\n        self.gamma = 1.\n\n        self.occured_padded_graphs_env = []\n        self.occured_padded_graphs_backward = []\n\n        self.vmapped_forward_env = jax.vmap(self.forward_env, in_axes = (None, None, None, 0,0), out_axes = (0,0,0,0,0, 0))\n        self.jitted_vmapped_forward_env = jax.jit(self.vmapped_forward_env)\n\n        self.jitted_forward_env = jax.jit(partial(self.forward_env))\n        self.jitted_forward_env_unpadded = jax.jit(partial(self.forward_env_unpadded))\n        self.edge_list = []\n        self.edge_list_backward = []\n        self.tracked_jit_tuples = {}\n        self.tracked_jit_tuples[\"eval\"] = {}\n        self.tracked_jit_tuples[\"forw\"] = {}\n        self.tracked_jit_tuples[\"backw\"] = {}\n        self.mov_avrg_step = 0\n\n\n    def track_jitting_forward(self, padded_minib_H_graph, padded_minib_compl_H_graph):\n        self.edge_list.append((padded_minib_H_graph.nodes.shape[0], padded_minib_H_graph.edges.shape[0], padded_minib_compl_H_graph.nodes.shape[0], padded_minib_compl_H_graph.edges.shape[0]))\n\n        if(padded_minib_H_graph.nodes.shape[0] != padded_minib_compl_H_graph.nodes.shape[0]):\n            print((padded_minib_H_graph.nodes.shape[0], padded_minib_H_graph.edges.shape[0], padded_minib_compl_H_graph.nodes.shape[0], padded_minib_compl_H_graph.edges.shape[0]))\n            ValueError(\"shapes do not match\")\n        print(\"number of occurances forward\")\n        print(Counter(self.edge_list).keys())\n        print(Counter(self.edge_list).values())\n\n    def track_jitting_backward(self, padded_minib_H_graph, padded_minib_compl_H_graph):\n        self.edge_list_backward.append((padded_minib_H_graph.nodes.shape[0], padded_minib_H_graph.edges.shape[0], padded_minib_compl_H_graph.nodes.shape[0], padded_minib_compl_H_graph.edges.shape[0]))\n\n        if(padded_minib_H_graph.nodes.shape[0] != padded_minib_compl_H_graph.nodes.shape[0]):\n            print((padded_minib_H_graph.nodes.shape[0], padded_minib_H_graph.edges.shape[0], padded_minib_compl_H_graph.nodes.shape[0], padded_minib_compl_H_graph.edges.shape[0]))\n            ValueError(\"shapes do not match\")\n\n        print(\"number of occurances\")\n        print(Counter(self.edge_list).keys())\n        print(Counter(self.edge_list).values())\n\n    def _get_closest_larger_value(self, number, values):\n        closest_larger_value = None\n        closest_distance = float('inf')\n        for value in values:\n            if value > number and value - number < closest_distance:\n                closest_larger_value = value\n                closest_distance = value - number\n        return closest_larger_value\n\n    def track_compl_graph_edges(self, padded_minib_H_graph, padded_minib_compl_H_graph, k = 1.4, mode = \"eval\"):\n        H_graph_edges = padded_minib_H_graph.edges.shape[0]\n        H_graph_nodes = padded_minib_H_graph.nodes.shape[0]\n\n        compl_H_graph_edges = padded_minib_compl_H_graph.edges.shape[0]\n        nearest_number_edges = jutils._nearest_bigger_power_of_k(compl_H_graph_edges, k=k)\n\n        H_graph_tuple = (H_graph_nodes, H_graph_edges)\n\n        if(H_graph_tuple in self.tracked_jit_tuples[mode]):\n            if(nearest_number_edges > max(self.tracked_jit_tuples[mode][H_graph_tuple])):\n                print(\"new n_edges are added\")\n                self.tracked_jit_tuples[mode][H_graph_tuple].append(nearest_number_edges)\n            else:\n                print(\"closest jit function can be taken\")\n                nearest_number_edges = self._get_closest_larger_value(nearest_number_edges, self.tracked_jit_tuples[mode][H_graph_tuple])\n        else:\n            print(\"jit function is initialised\")\n            self.tracked_jit_tuples[mode][H_graph_tuple] = [nearest_number_edges]\n        return nearest_number_edges\n\n    def check_if_jitted(self, graph_info_list, H_graph, mode = \"env\"):\n        num_nodes = cpu_np.sum(H_graph.n_node)\n        num_edges = cpu_np.sum(H_graph.n_edge)\n        check_tuple = (num_nodes, num_edges)\n        if(check_tuple not in graph_info_list):\n            graph_info_list.append(check_tuple)\n\n        print(\"there are \",len(graph_info_list), f\"jitted {mode} graph functions\")\n        print(\"nodes\", cpu_np.sum(H_graph.n_node), H_graph.nodes.shape)\n        print(\"edges\", cpu_np.sum(H_graph.n_edge), H_graph.edges.shape)\n        print(\"receivers\", H_graph.receivers.shape)\n        print(\"senders\", H_graph.senders.shape)\n        print(\"globals\", H_graph.globals.shape)\n\n    def calc_traces(self, rewards, values, not_dones):\n        advantage = cpu_np.zeros_like(values)\n        for t in reversed(range(self.time_horizon)):\n            delta = rewards[t] + self.gamma * not_dones[t+1]*values[t+1] - values[t]\n            advantage[t] = delta + self.gamma*self.lam *not_dones[t+1]*advantage[t+1]\n\n        value_target = (advantage + values)[0:self.time_horizon]\n        return value_target, advantage[0:self.time_horizon]\n\n    def collate_data_dict(self, Hb_data_dict):\n\n        Hb_ext_field_arr = np.concatenate([data_dict[\"external_fields\"] for data_dict in Hb_data_dict], axis=1)\n        minib_H_graphs = jraph.batch_np([data_dict[\"graphs\"] for data_dict in Hb_data_dict])\n        array_list = [data_dict[\"arrays\"] for data_dict in Hb_data_dict]\n\n        Hb_array_dict = concatenate_arrays(array_list)\n\n        minib_value_target = Hb_array_dict[\"value_targets\"]\n        minib_A_k = Hb_array_dict[\"advantage\"]\n        minib_actions = Hb_array_dict[\"actions\"]\n        minib_log_probs = Hb_array_dict[\"log_probs\"]\n        masks = Hb_array_dict[\"masks\"]\n\n        minib_A_k = (minib_A_k- np.mean(minib_A_k))/(np.std(minib_A_k) + 1e-10)\n\n        return minib_H_graphs, masks, minib_actions, minib_A_k, minib_log_probs, minib_value_target, Hb_ext_field_arr\n\n    def init_attributes(self, config):\n        self.cfg = config\n        self.n_test_graphs = config[\"Test_params\"][\"n_test_graphs\"]\n        self.updates_per_iterations = config[\"Train_params\"][\"PPO\"][\"updates_per_iteration\"]\n        self.clip = config[\"Train_params\"][\"PPO\"][\"clip_value\"]\n        self.mini_Nb = config[\"Train_params\"][\"PPO\"][\"mini_Nb\"]\n        self.mini_Hb = config[\"Train_params\"][\"PPO\"][\"mini_Hb\"]\n        self.mini_Sb = config[\"Train_params\"][\"PPO\"][\"mini_Sb\"]\n\n        self.lam = config[\"Train_params\"][\"PPO\"][\"lam\"]\n        self.alpha = config[\"Train_params\"][\"PPO\"][\"alpha\"]\n        self.Nb = config[\"Train_params\"][\"n_basis_states\"]\n        self.Hb = config[\"Train_params\"][\"H_batch_size\"]\n        self.time_horizon = config[\"Train_params\"][\"PPO\"][\"time_horizon\"]\n\n        self.EnergyFunction = config[\"Ising_params\"][\"EnergyFunction\"]\n\n        self.graph_padding_factor = config[\"Ising_params\"][\"graph_padding_factor\"]\n        self.compl_graph_padding_factor = config[\"Ising_params\"][\"compl_graph_padding_factor\"]\n\n        self.policy_global_features = config[\"Network_params\"][\"policy_MLP_features\"]\n        self.n_classes = self.policy_global_features[-1]\n        self.n_sampled_sites = int(np.log2(self.n_classes))\n\n        self.graph_padding_factor = config[\"Ising_params\"][\"graph_padding_factor\"]\n\n        if(\"pruning\" not in config[\"Train_params\"].keys()):\n            config[\"Train_params\"][\"pruning\"] = True\n        if(\"masking\" not in config[\"Train_params\"].keys()):\n            config[\"Train_params\"][\"masking\"] = False\n        if(\"reversed_disjoint_graph_ordering\" not in config[\"Ising_params\"].keys()):\n            config[\"Ising_params\"][\"reversed_disjoint_graph_ordering\"] = False\n        if(\"self_loops\" not in config[\"Ising_params\"].keys()):\n            config[\"Ising_params\"][\"self_loops\"] = False\n        if (\"centrality\" not in config[\"Ising_params\"].keys()):\n            config[\"Ising_params\"][\"centrality\"] = False\n\n\n    ### TODO check if graph generation is correct\n    def train(self, config, load_path = None):\n\n        ### Flags\n        #os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n        #os.environ[\"WANDB_MODE\"] = \"offline\"\n        self.cfg = config\n        config[\"sparse_graphs\"] = True\n        path = config[\"Paths\"][\"path\"]\n        group = config[\"group\"]\n        job_type = config[\"job_type\"]\n        N_warmup = config[\"Anneal_params\"][\"N_warmup\"]\n        N_anneal = config[\"Anneal_params\"][\"N_anneal\"]\n        N_equil = config[\"Anneal_params\"][\"N_equil\"]\n        seed = config[\"Train_params\"][\"seed\"]\n        batch_epochs = config[\"Train_params\"][\"batch_epochs\"]\n        self.n_test_graphs = config[\"Test_params\"][\"n_test_graphs\"]\n        T = config[\"Anneal_params\"][\"Temperature\"]\n        self.updates_per_iterations = config[\"Train_params\"][\"PPO\"][\"updates_per_iteration\"]\n        self.clip = config[\"Train_params\"][\"PPO\"][\"clip_value\"]\n        self.mini_Nb = config[\"Train_params\"][\"PPO\"][\"mini_Nb\"]\n        self.mini_Hb = config[\"Train_params\"][\"PPO\"][\"mini_Hb\"]\n        self.mini_Sb = config[\"Train_params\"][\"PPO\"][\"mini_Sb\"]\n\n        self.lam = config[\"Train_params\"][\"PPO\"][\"lam\"]\n        self.alpha = config[\"Train_params\"][\"PPO\"][\"alpha\"]\n        self.Nb = config[\"Train_params\"][\"n_basis_states\"]\n        self.Hb = config[\"Train_params\"][\"H_batch_size\"]\n\n        self.lr = config[\"Train_params\"][\"lr\"]\n        self.lr_min = self.lr*config[\"Train_params\"][\"lr_alpha\"]\n\n        self.time_horizon = config[\"Train_params\"][\"PPO\"][\"time_horizon\"]\n        self.EnergyFunction = config[\"Ising_params\"][\"EnergyFunction\"]\n\n        self.graph_padding_factor = config[\"Ising_params\"][\"graph_padding_factor\"]\n        self.compl_graph_padding_factor = config[\"Ising_params\"][\"compl_graph_padding_factor\"]\n        self.policy_global_features = config[\"Network_params\"][\"policy_MLP_features\"]\n        self.n_classes = self.policy_global_features[-1]\n        self.n_sampled_sites = int(np.log2(self.n_classes))\n        self.config = config\n        alpha = config[\"Train_params\"][\"PPO\"][\"mov_avrg\"]\n        self.MovingAverage = MovingAverage(alpha, alpha)\n\n        epochs = int(N_warmup + N_anneal + N_equil)\n\n        cosine_schedule_func = lambda epoch: LRCosineSchedule(epoch,  epochs, N_warmup, N_equil, N_anneal, self.lr, lr_min = self.lr_min)\n        os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n        wandb.login()\n        project_name = config[\"project\"]\n        run = wandb.init(mode = \"online\", project=project_name, reinit=True, group=group, job_type=job_type, config=config, settings=wandb.Settings(_service_wait=300))\n        run.name = wandb.run.id\n\n\n        wandb.define_metric(\"val/step\")\n        wandb.define_metric(\"val/*\", step_metric=\"val/step\")\n\n        wandb.define_metric(\"train/step\")\n        wandb.define_metric(\"train/*\", step_metric=\"train/step\")\n        wandb.define_metric(\"schedules/*\", step_metric=\"train/step\")\n\n        print(\"saving is\", self.cfg[\"Save_settings\"][\"save_params\"])\n        if(self.cfg[\"Save_settings\"][\"save_params\"] == True):\n            run_path = SaveModel.create_save_path(project_name, \"\", N_anneal, run, path, config)\n\n        if(load_path != \"None\"):\n            params, opt_state, epoch = LoadUtils.checkpoint(load_path)\n            print(\"Load model from checkpoint at epoch\", epoch)\n            params = jax.tree_util.tree_map(lambda x: jnp.array(x), params)\n            ### TODO overwrite anneal scheduler\n            RNN, ICGenerator, anneal_scheduler = configure_run_ReplayBuffer(config, sparse_graphs=True)\n\n            self.RNN = RNN\n            self.ICGenerator = ICGenerator\n\n            RNN.key = jax.random.PRNGKey(seed + 10)\n            RNN_key = RNN.key\n            Nb_RNN_key = jax.random.split(RNN_key, self.Nb)\n            epoch_arr = np.arange(epoch, epochs)\n            self.train_loop(RNN, params, opt_state, Nb_RNN_key, epoch_arr, epochs, batch_epochs, N_warmup, N_equil, N_anneal, anneal_scheduler, T, run_path)\n        else:\n            print(\"Reinitialize Model\")\n            RNN, ICGenerator, anneal_scheduler = configure_run_ReplayBuffer(config, sparse_graphs=True)\n            self.RNN = RNN\n            self.ICGenerator = ICGenerator\n\n            RNN_key = RNN.key\n            Nb_RNN_key = jax.random.split(RNN_key, self.Nb)\n\n            params = RNN.get_params()\n            opt_state = RNN.get_opt_states()\n\n            if(config[\"Anneal_params\"][\"schedule\"] == \"cosine_frac\"):\n                epoch_arr = np.arange(0, config[\"Anneal_params\"][\"reps\"] *epochs)\n            else:\n                epoch_arr = np.arange(0, epochs)\n            self.train_loop(RNN, params, opt_state, Nb_RNN_key,epoch_arr, epochs, batch_epochs, N_warmup, N_equil, N_anneal, anneal_scheduler, T, run_path)",
  "description": "Combined Analysis:\n- [VAG_CO/EnergyFunctions/MIS.py]: The file implements the core energy function for the Maximum Independent Set (MIS) problem using the Ising model formulation. The MIS_Energy function directly maps to the optimization model: Objective: maximize sum of selected nodes (minimize negative sum) with constraint: no two adjacent nodes selected. The energy E = -A*q_i + B*_{(i,j)E} q_i*q_j where q_i  {0,1} are binary variables (bins). The first term (-A*Ha) encourages node selection (maximization), while the second term (B*Hb) penalizes edge violations (constraint satisfaction). This matches the paper's approach of framing combinatorial optimization as energy minimization over binary variables.\n- [VAG_CO/Networks/AutoregressiveModels/GNN_PPO_spin_drop_vectorised.py]: This file implements the core autoregressive GNN architecture for VAG-CO's PPO-based optimization. The GNN_PPO_SpinDrop class uses a scanned GNN_actor_critic cell to sequentially sample/evaluate spin variables in an autoregressive manner. Key components: 1) Graph neural network (EncodeProcess/EncodeProcessNew) processes Ising problem graph structure; 2) Actor-critic architecture with separate policy (ProbMLP) and value (ValueMLP) networks; 3) Autoregressive sampling via jax.random.categorical over log probabilities; 4) Support for both padded and unpadded graph representations. The implementation directly corresponds to the paper's autoregressive modeling approach using GNNs and PPO for combinatorial optimization, though the training loop and annealing mechanisms are implemented elsewhere.\n- [VAG_CO/Samplers/PPOSampler.py]: This file implements the core PPO training logic for VAG-CO's autoregressive GNN policy. The PPOSampler class initializes GNN networks for spin selection/dropping, sets up learning rate schedules with annealing (matching the variational annealing concept), and defines the PPO loss function (Hb_Nb_loss) that combines policy gradient with clipping and value function updates. The update_params method performs gradient updates using the RAdam optimizer. This directly corresponds to the algorithm's reinforcement learning training step where the policy is optimized to minimize energy (via reward) with proximal policy updates.\n- [VAG_CO/trainPPO/IGraphVecEnv/GraphEnv_train_configuration.py]: This file implements the core environment for the VAG-CO algorithm, which is a reinforcement learning environment for combinatorial optimization problems on graphs. The IGraphEnv class handles graph loading, preprocessing, energy computation, and constraint checking for problems like MIS, MVC, MaxCl, and MaxCut. It converts graphs to jraph format for GNN processing, implements graph ordering (BFS/DFS/random), and computes energy functions with local fields and pairwise interactions. The environment supports multiple basis states (Nb) for variational annealing and includes subgraph tokenization through n_sampled_sites. Key methods include energy computation (compute_Energy_cropped_graph_np), constraint violation checking (_compute_violations), and graph preprocessing for autoregressive generation.\n- [VAG_CO/trainPPO/trainPPO_configuration.py]: This file implements the core training loop for the VAG-CO algorithm using Proximal Policy Optimization (PPO). The HiVNAPPo class manages the training configuration, environment interactions, and the annealing schedule. Key steps include: 1) Initializing the neural network (RNN) and environment, 2) Collecting trajectories via autoregressive actions on graph-structured problems, 3) Computing advantages and value targets for PPO updates, 4) Applying a cosine annealing schedule for temperature (entropy regularization) as per variational annealing. The code directly maps to the VAG-CO algorithm's use of GNNs for autoregressive solution generation and PPO for policy optimization with annealed entropy.",
  "dependencies": [
    "jraph_utils.utils",
    "jraph",
    "jax.random",
    "copy",
    "trainPPO.IGraphVecEnv.GraphEnv_eval_configuration.IGraphEnv",
    "optax",
    "matplotlib.pyplot",
    "train.LoadUtils",
    "trainPPO.IGraphVecEnv.GraphEnv_train_configuration.IGraphEnv",
    "jax.lax",
    "torch.utils.data.DataLoader",
    "igraph",
    "time",
    "trainPPO.DataContainer_Dataloader.ContainerDataset",
    "utils.moving_averages.MovingAverage",
    "itertools",
    "GNN_PPO_SpinDrop",
    "jax.tree_util",
    "jutils",
    "ValueMLP",
    "jax.ops.segment_sum",
    "tqdm",
    "gym",
    "train.ConfigureRun.configure_run_ReplayBuffer",
    "jax.numpy",
    "collections.Counter",
    "EncodeProcess",
    "BaseRNNSampler",
    "wandb",
    "os",
    "trainPPO.DataContainer_Dataloader.concatenate_arrays",
    "GNN_PPO_SpinSelector",
    "ProbMLP",
    "functools.partial",
    "loadGraphDatasets.GetDataLoaders",
    "trainPPO.OwnVecEnv.SubprocVecEnv",
    "train.LogMetrics.SaveModel",
    "random",
    "numpy",
    "flax.linen",
    "jraph_utils",
    "jax",
    "DataContainer",
    "train.LRSchedule.Schedules.cosine_warmup",
    "EncodeProcessNew"
  ]
}