{
  "file_path": "algorithms.py, cutsel_agent_parallel.py, parallel_reinforce_algorithm.py, pointer_net.py, pointer_net_end_token.py",
  "function_name": "ReinforceBaselineAlg, HierarchyCutSelectAgent.cutselselect, generate_hierarchy_samples, evaluate_hierarchy, test_hierarchy, Decoder.forward, PointerNetworkEndToken",
  "code_snippet": "\n\n# ==========================================\n# File: algorithms.py\n# Function/Context: ReinforceBaselineAlg\n# ==========================================\nimport argparse\nimport os\nfrom sqlalchemy import all_\nfrom tqdm import tqdm \nimport torch\nimport numpy as np\nimport json \nimport copy \nimport os.path as osp\nimport math\n\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\n\nfrom cutsel_agent_parallel import CutSelectAgent, HierarchyCutSelectAgent\nfrom logger import logger\n\nfrom utils import setup_logger, create_stats_ordered_dict, set_global_seed\nfrom utilss.mean_std import RunningMeanStd\n\nclass ReinforceBaselineAlg():\n    def __init__(\n        self,\n        env,\n        pointer_net, # policy net in cutsel_agent\n        value_net,\n        sel_cuts_percent,\n        device,\n        evaluate_freq=1,\n        evaluate_samples=1,\n        optimizer_class='Adam',\n        actor_net_lr=1e-4,\n        critic_net_lr=1e-4,\n        reward_scale=1,\n        num_epochs=100,\n        max_grad_norm=2.0,\n        batch_size=32,\n        train_decode_type='stochastic',\n        evaluate_decode_type='greedy',\n        reward_type='solving_time',\n        baseline_type=\"no_baseline\", # ['no_baseline', 'simple', 'net']\n        critic_beta=0.9,\n        train_steps_per_epoch=1,\n        lr_decay=False,\n        lr_decay_step=5,\n        lr_decay_rate=0.96,\n        normalize=False,\n        normalize_reward=True\n    ):\n        self.env = env\n        self.pointer_net = pointer_net\n        self.sel_cuts_percent = sel_cuts_percent\n        self.value_net = value_net\n        self.actor_net_lr = actor_net_lr\n        self.critic_net_lr = critic_net_lr\n        self.reward_scale = reward_scale\n        self.num_epochs = num_epochs\n        self.max_grad_norm = max_grad_norm\n        self.batch_size = batch_size\n        self.device = device\n        self.reward_type = reward_type\n        self.train_steps_per_epoch = train_steps_per_epoch\n\n        # decode type\n        self.train_decode_type = train_decode_type\n        self.evaluate_decode_type = evaluate_decode_type\n\n        # evaluate \n        self.evaluate_freq = evaluate_freq\n        self.evaluate_samples = evaluate_samples\n        \n        # optimizer\n        if isinstance(optimizer_class, str):\n            optimizer_class = eval('optim.'+optimizer_class)\n            self.optimizer_class = optimizer_class\n        \n        self.policy_optimizer = optimizer_class(\n            self.pointer_net.parameters(),\n            lr=self.actor_net_lr\n        )\n\n        self.baseline_type = baseline_type\n        self.critic_beta = critic_beta\n        if self.baseline_type == 'net':\n            # using critic net as a baseline function\n            self.value_optimizer = optimizer_class(\n                self.value_net.parameters(),\n                lr=self.critic_net_lr\n            )\n            self.critic_mse = torch.nn.MSELoss()\n        elif self.baseline_type == 'simple':\n            self.critic_exp_mvg_avg = torch.zeros(1)\n            # .to(self.device)\n        \n        # lr scheduler\n        self.lr_decay = lr_decay\n        self.lr_decay_step = lr_decay_step\n        self.lr_decay_rate = lr_decay_rate\n        if self.lr_decay:\n            self.policy_lr_scheduler = lr_scheduler.StepLR(\n                self.policy_optimizer,\n                self.lr_decay_step,\n                gamma=self.lr_decay_rate\n            )\n        \n        # normalizer\n        self.normalize = normalize\n        if normalize:\n            feature_shape = (self.pointer_net.embedding_dim,)\n            self.mean_std = RunningMeanStd(feature_shape)\n\n        self.normalize_reward = normalize_reward\n\n    def _prob_to_logp(self, prob):\n        logprob = 0\n        for p in prob:\n            logp = torch.log(p)\n            logprob += logp\n        # logprob[(logprob < -10000).detach()] = 0.\n        \n        return logprob\n\n    def evaluate(self, epoch):\n        logger.log(f\"evaluating...  epoch: {epoch}\")\n        neg_solving_time = np.zeros((1, self.evaluate_samples))\n        neg_total_nodes = np.zeros((1, self.evaluate_samples))\n        for i in range(self.evaluate_samples):\n            self.env.reset()\n            cutsel_agent = CutSelectAgent(\n                self.env.m,\n                self.pointer_net,\n                self.value_net,\n                self.sel_cuts_percent,\n                self.device,\n                self.evaluate_decode_type,\n                self.baseline_type\n            )\n            env_step_info = self.env.step(cutsel_agent)\n            neg_solving_time[:,i] = env_step_info['solving_time']\n            neg_total_nodes[:,i] = env_step_info['ntotal_nodes']\n        logger.record_tabular('evaluating/Neg Solving time', np.mean(neg_solving_time))\n        logger.record_tabular('evaluating/Neg Total Nodes', np.mean(neg_total_nodes))\n\n    def save_checkpoint(self, epoch):\n        state_dict = {}\n        state_dict['pointer_net'] = self.pointer_net.state_dict()\n        if self.normalize:\n            state_dict['mean'] = self.mean_std.mean\n            state_dict['std'] = self.mean_std.std\n            state_dict['epsilon'] = self.mean_std.epsilon\n\n        logger.save_itr_params(epoch, state_dict)\n\n    def _process_env_info(self, env_step_infos):\n        env_info = {}\n        for k in env_step_infos[0].keys():\n            env_info[k] = []\n            for info in env_step_infos:\n                env_info[k].extend(info[k])\n\n        return env_info\n\n    def _normalize_state(self, state):\n        mean = self.mean_std.mean\n        std = self.mean_std.std\n        epsilon = self.mean_std.epsilon\n\n        return (state - mean) / (std+epsilon)\n\n    def _process_data(self, raw_results):\n        env_step_infos = [result[0] for result in raw_results]\n        training_datasets = [result[1] for result in raw_results] # list of dict \n        states = []\n        actions = []\n        sel_cuts_nums = []\n        neg_rewards = []\n        new_step_infos = {}\n        for dict_data in training_datasets:\n            states.extend(dict_data['state']) # list of numpy\n            actions.extend(dict_data['action']) # list of list\n            sel_cuts_nums.extend(dict_data['sel_cuts_num'])\n            neg_rewards.extend(dict_data['neg_reward'])\n        print(f\"debug log neg_rewards: {neg_rewards}\")\n        print(f\"debug log states len: {len(states)}\")\n        neg_rewards = np.vstack(neg_rewards)\n        neg_rewards = self.reward_scale * neg_rewards\n        if self.normalize_reward:\n            # log raw neg rewards\n            logger.record_dict(create_stats_ordered_dict('training/Nonnormalize Neg Reward', neg_rewards))\n            neg_rewards_mean = np.mean(neg_rewards)\n            neg_rewards_std = np.std(neg_rewards)\n            neg_rewards = (neg_rewards - neg_rewards_mean) / (neg_rewards_std + 1e-3)\n\n        for k in env_step_infos[0].keys():\n            new_step_infos[k] = []\n        for step_info in env_step_infos:\n            for k in step_info.keys():\n                new_step_infos[k].extend(step_info[k])\n        if self.normalize:\n            # update mean_std\n            logger.log(\"normalizing data .....\")\n            stack_states = np.vstack(states)\n            self.mean_std.update(stack_states)\n            # log non-normalize states\n            feature_len = stack_states.shape[1]\n            for i in range(feature_len):\n                logger.record_dict(create_stats_ordered_dict(f'training/cut {i+1} th non-normalize feature', stack_states[:,i:i+1]))\n            # normalize states\n            normalize_states = [self._normalize_state(state) for state in states]\n            return neg_rewards, normalize_states, actions, sel_cuts_nums, new_step_infos\n\n        return neg_rewards, states, actions, sel_cuts_nums, new_step_infos\n\n    def _compute_sm_entropy(self, probs):\n        probs = torch.squeeze(probs)\n        entropy = 0\n        for prob in probs:\n            entropy -= prob * torch.log(prob)\n        return entropy\n\n    def train(self, raw_results, epoch):\n        neg_rewards, states, actions, sel_cuts_nums, env_step_infos = self._process_data(raw_results)\n        # states to torch\n        ### compute policy gradient \n        # compute baseline function \n        total_num_samples = len(states)\n        if total_num_samples < self.batch_size:\n            train_loop = 1\n        elif total_num_samples % self.batch_size == 0:\n            train_loop =  int(total_num_samples / self.batch_size)\n        else:\n            train_loop =  int(total_num_samples / self.batch_size) + 1\n\n        if self.baseline_type == 'simple':\n            if epoch == 1:\n                self.critic_exp_mvg_avg = neg_rewards.mean()\n            else:\n                self.critic_exp_mvg_avg = (self.critic_exp_mvg_avg * self.critic_beta) + ((1. - self.critic_beta) * neg_rewards.mean())\n\n        for i in range(train_loop):\n            if i == (train_loop - 1):\n                batch_size = len(states[i*self.batch_size:])\n            else:\n                batch_size = self.batch_size\n            logger.log(f\"training epoch: {epoch}, training loop: {train_loop}\")\n            log_prefix = f\"training epoch: {epoch}, training loop: {train_loop}\"\n            logprobs = torch.zeros((batch_size, 1)).to(self.device)\n            if self.baseline_type == 'net':\n                neg_baseline_value = torch.zeros((batch_size, 1)).to(self.device)\n            for j in range(batch_size):\n                logger.log(f\"{log_prefix} step {j}: cuda memory: {torch.cuda.memory_allocated(0)/1024**3} GB\")\n                logger.log(f\"{log_prefix} step {j}: cuda cached: {torch.cuda.memory_cached(0)/1024**3} GB\")\n                if (torch.cuda.memory_cached(0)/1024**3) > 4.5:\n                    # empty torch cache\n                    torch.cuda.empty_cache()\n                cur_index = int(i * self.batch_size + j)\n                state = torch.from_numpy(states[cur_index]).float().to(self.device)\n                state = state.reshape(state.shape[0], 1, state.shape[1])\n                action = actions[cur_index]\n                sel_cuts_num = sel_cuts_nums[cur_index]\n                pointer_probs, logprob = self.pointer_net.logprobs(\n                    state, sel_cuts_num, action\n                )\n                if logprob.item() < -4000:\n                    logprobs[j,:] = logprob.detach()\n                    logger.log('warning: logprob too small, we drop it!!!')\n                else:\n                    logprobs[j,:] = logprob\n                if self.baseline_type == 'net':\n                    neg_baseline_value[j,:] = self.value_net(state).squeeze()\n                if i == 0 and j == 0:\n                    # log tensorboard \n                    logger.tb_logger.add_histogram(\"selected_idxes\", np.array(action), global_step=epoch)\n                    for pos, prob_distribution in enumerate(pointer_probs):   \n                        logger.tb_logger.add_histogram(f\"position {pos} probability distribution\", prob_distribution, global_step=epoch)\n            pos_1_entropy = self._compute_sm_entropy(pointer_probs[0])\n            logger.record_tabular('pos_1_entropy', pos_1_entropy.item())\n\n            if i == (train_loop - 1) :\n                minibatch_neg_rewards = torch.from_numpy(neg_rewards[i*self.batch_size:]).float().to(self.device)\n            else:\n                minibatch_neg_rewards = torch.from_numpy(neg_rewards[i*self.batch_size:(i+1)*self.batch_size]).float().to(self.device)\n            \n            if self.baseline_type == 'simple':\n                neg_advantage = minibatch_neg_rewards - torch.tensor([self.critic_exp_mvg_avg], dtype=torch.float, device=self.device)\n            elif self.baseline_type == 'no_baseline':\n                neg_advantage = minibatch_neg_rewards\n            elif self.baseline_type == 'net':\n                neg_advantage = minibatch_neg_rewards - neg_baseline_value.detach()\n            # compute policy loss\n            reinforce_loss = (neg_advantage * logprobs).mean()\n            self.policy_optimizer.zero_grad()\n            reinforce_loss.backward() # compute gradient\n            # clip gradient norms\n            torch.nn.utils.clip_grad_norm(self.pointer_net.parameters(),\n                    float(self.max_grad_norm), norm_type=2)            \n            self.policy_optimizer.step()\n            # compute value loss \n            if self.baseline_type == 'net':\n                critic_loss = self.critic_mse(neg_baseline_value, minibatch_neg_rewards)\n                self.value_optimizer.zero_grad()\n                critic_loss.backward()\n                torch.nn.utils.clip_grad_norm(self.value_net.parameters(),\n                        float(self.max_grad_norm), norm_type=2)  \n                self.value_optimizer.step()\n\n            torch.cuda.empty_cache()\n        # lr update\n        if self.lr_decay:\n            self.policy_lr_scheduler.step()\n            logger.log(f\"epoch: {epoch}, policy optimizer lr: {self.policy_optimizer.param_groups[0]['lr']}\")\n        # save model            \n        self.save_checkpoint(epoch)\n\n        # log data\n        logger.tb_logger.add_histogram(\"neg_rewards\", neg_rewards, global_step=epoch)\n        logger.record_tabular('Epoch', epoch)\n        logger.record_dict(create_stats_ordered_dict('training/Neg Reward', neg_rewards))\n        # logger.record_tabular('training/Neg Reward', neg_rewards.mean().item())\n        logger.record_tabular('training/Neg Advantage', neg_advantage.mean().item())\n        logger.record_dict(create_stats_ordered_dict('training/logprobs',logprobs.cpu().detach().numpy()))\n        logger.record_tabular('training/reinforce loss', reinforce_loss.item())\n        logger.record_tabular('training/Critic Value', self.critic_exp_mvg_avg.item())\n        if self.baseline_type == 'net':\n            logger.record_tabular('training/Critic Loss', critic_loss.item())\n            logger.record_tabular('training/Critic Value', neg_baseline_value.mean().item())\n        \n        # log states\n        logger.record_dict(create_stats_ordered_dict('training/len cuts', [len(state) for state in states]))\n        feature_len = states[0].shape[1]\n        for i in range(feature_len):\n            logger.record_dict(create_stats_ordered_dict(f'training/cut {i+1} th feature', np.vstack([state[:,i:i+1] for state in states])))\n        logger.record_dict(create_stats_ordered_dict('training/sel cuts num', sel_cuts_nums))\n\n        stats = {}\n        for k in env_step_infos:\n            stats.update(create_stats_ordered_dict(\n                'training/'+k,\n                env_step_infos[k]\n            ))\n        logger.record_dict(stats)\n\n    def log_evaluate_stats(self, evaluate_results):\n        neg_solving_time = np.vstack([result[0] for result in evaluate_results])\n        neg_total_nodes = np.vstack([result[1] for result in evaluate_results])\n        primaldualintegral = np.vstack([result[2] for result in evaluate_results])\n        primal_dual_gap = np.vstack([result[4] for result in evaluate_results])\n        stats = {}\n        stats.update(\n            create_stats_ordered_dict('evaluating/Neg Solving time', neg_solving_time)\n        )\n        stats.update(\n            create_stats_ordered_dict('evaluating/Neg Total Nodes', neg_total_nodes)\n        )\n        stats.update(\n            create_stats_ordered_dict('evaluating/Primal Dual Integral', primaldualintegral)\n        )\n        stats.update(\n            create_stats_ordered_dict('evaluating/Primal Dual Gap', primal_dual_gap)\n        )\n        logger.record_dict(stats)\n\n# ==========================================\n# File: cutsel_agent_parallel.py\n# Function/Context: HierarchyCutSelectAgent.cutselselect\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.autograd as autograd\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport math\nimport numpy as np\nimport time\nimport pyscipopt as scip\nfrom pyscipopt import SCIP_RESULT\n\n# from beam_search import Beam\nfrom utils import cut_feature_generator, advanced_cut_feature_generator\n# from utils_fix_isp_bug import cut_feature_generator\nfrom logger import logger\n\nclass CutSelectAgent(scip.Cutsel):\n    def __init__(\n        self,\n        scip_model,\n        pointer_net,\n        value_net,\n        sel_cuts_percent,\n        device,\n        decode_type,\n        mean_std,\n        policy_type\n    ):\n        super().__init__()\n        self.scip_model = scip_model\n        self.policy = pointer_net\n        self.value = value_net\n        self.sel_cuts_percent = sel_cuts_percent\n        self.device = device\n        self.decode_type = decode_type\n        self.policy_type = policy_type\n\n        self.data = {}\n        # self.cuts_info ={}\n        self.lp_info = {\n            \"lp_solution_value\": [],\n            \"lp_solution_integer_var_value\": []\n        }\n        self.mean_std = mean_std\n\n    def _normalize(self, cuts_features):\n        # print(f\"debug log mean: {self.mean_std.mean}, std: {self.mean_std.std}\")\n        return (cuts_features-self.mean_std.mean) / (self.mean_std.std + self.mean_std.epsilon)\n    \n    def cutselselect(self, cuts, forcedcuts, root, maxnselectedcuts):\n        if self.policy_type == 'with_token':\n            cuts_dict = self._cutselselect_with_token(cuts, forcedcuts, root, maxnselectedcuts)\n        else:\n            cuts_dict = self._cutselselect(cuts, forcedcuts, root, maxnselectedcuts)  \n\n        return cuts_dict \n    \n    def _cutselselect(self, cuts, forcedcuts, root, maxnselectedcuts):\n        '''first method called in each iteration in the main solving loop. '''\n        # this method needs to be implemented by the user\n        logger.log(\"cut selection policy without token!!!\")\n        logger.log(f\"forcedcuts length: {len(forcedcuts)}\")\n        logger.log(f\"len cuts: {len(cuts)}\")\n        num_cuts = len(cuts)\n        # cur_lp_info = self._get_lp_info()\n        # for k in cur_lp_info.keys():\n        #     self.lp_info[k].append(cur_lp_info[k])\n        if num_cuts <= 1:\n            return {\n                'cuts': cuts, # selected sorted cuts\n                'nselectedcuts': 1, # num of selected cuts\n                'result': SCIP_RESULT.SUCCESS\n            }            \n        sel_cuts_num = min(int(num_cuts * self.sel_cuts_percent), int(maxnselectedcuts))\n        sel_cuts_num = max(sel_cuts_num, 2)\n        st_before_input = time.time()\n        cuts_features = advanced_cut_feature_generator(self.scip_model, cuts)\n        et_feature_extractor = time.time()\n        if self.mean_std is not None:\n            # normalize cut features\n            normalize_cut_features = self._normalize(cuts_features)\n            input_cuts = torch.from_numpy(normalize_cut_features).to(self.device)\n        else:\n            input_cuts = torch.from_numpy(cuts_features).to(self.device)\n        \n        input_cuts = input_cuts.reshape(input_cuts.shape[0], 1, input_cuts.shape[1])\n        st_end_input = time.time()\n        # 只做选择动作的功能，不做计算梯度的功能\n        with torch.no_grad():\n            _, input_idxs =  self.policy(input_cuts.float(), sel_cuts_num, self.decode_type) # (list of tensor, list of tensor)\n        st_end_inference = time.time()\n        print(f\"process input time: {st_end_input-st_before_input} s\")\n        print(f\"input feature extractor time: {et_feature_extractor-st_before_input} s\")\n        print(f\"input cpu data to gpu time: {st_end_input-et_feature_extractor} s\")\n        print(f\"pointer net inference time: {st_end_inference - st_end_input} s\")\n        idxes = [input.cpu().detach().item() for input in input_idxs]\n        assert len(set(idxes))==len(idxes) # 保证选择的idxes 没有重复的！\n        all_idxes = list(range(num_cuts))\n        not_sel_idxes = list(set(all_idxes).difference(idxes))\n        sorted_cuts = [cuts[idx] for idx in idxes]\n        not_sel_cuts = [cuts[n_idx] for n_idx in not_sel_idxes]\n        sorted_cuts.extend(not_sel_cuts)\n        # debug\n        # sorted_cuts = cuts\n        # 只log 第一次cut 处的state 和 action\n        if not self.data:\n            self.data = {\n                \"state\": cuts_features,\n                \"action\": idxes,\n                \"sel_cuts_num\": sel_cuts_num,\n            }\n            # self.cuts_info = {\n            #     \"length_cuts\": num_cuts,\n            #     \"length_forced_cuts\": len(forcedcuts),\n            #     \"cut_features\": cuts_features\n            # }\n\n        return {\n            'cuts': sorted_cuts, # selected sorted cuts\n            'nselectedcuts': sel_cuts_num, # num of selected cuts\n            'result': SCIP_RESULT.SUCCESS\n        }\n\n    def _cutselselect_with_token(self, cuts, forcedcuts, root, maxnselectedcuts):\n        '''first method called in each iteration in the main solving loop. '''\n        # this method needs to be implemented by the user\n        logger.log(\"cut selection policy with token!!!\")\n        logger.log(f\"forcedcuts length: {len(forcedcuts)}\")\n        logger.log(f\"len cuts: {len(cuts)}\")\n        num_cuts = len(cuts)\n        # cur_lp_info = self._get_lp_info()\n        # for k in cur_lp_info.keys():\n        #     self.lp_info[k].append(cur_lp_info[k])\n        if num_cuts <= 1:\n            return {\n                'cuts': cuts, # selected sorted cuts\n                'nselectedcuts': 1, # num of selected cuts\n                'result': SCIP_RESULT.SUCCESS\n            }            \n        max_sel_cuts_num = len(cuts) + 1 \n        st_before_input = time.time()\n        cuts_features = advanced_cut_feature_generator(self.scip_model, cuts)\n        et_feature_extractor = time.time()\n        if self.mean_std is not None:\n            # normalize cut features\n            normalize_cut_features = self._normalize(cuts_features)\n            input_cuts = torch.from_numpy(normalize_cut_features).to(self.device)\n        else:\n            input_cuts = torch.from_numpy(cuts_features).to(self.device)\n        \n        input_cuts = input_cuts.reshape(input_cuts.shape[0], 1, input_cuts.shape[1])\n        st_end_input = time.time()\n        # 只做选择动作的功能，不做计算梯度的功能\n        with torch.no_grad():\n            _, input_idxs =  self.policy(input_cuts.float(), max_sel_cuts_num, self.decode_type) # (list of tensor, list of tensor)\n        st_end_inference = time.time()\n        print(f\"process input time: {st_end_input-st_before_input} s\")\n        print(f\"input feature extractor time: {et_feature_extractor-st_before_input} s\")\n        print(f\"input cpu data to gpu time: {st_end_input-et_feature_extractor} s\")\n        print(f\"pointer net inference time: {st_end_inference - st_end_input} s\")\n\n        idxes = [input.cpu().detach().item() for input in input_idxs]\n        sel_cuts_num = len(idxes)\n        if not self.data:\n            self.data = {\n                \"state\": cuts_features,\n                \"action\": idxes,\n                \"sel_cuts_num\": sel_cuts_num,\n            }\n        # select cuts \n        assert idxes[-1] == num_cuts\n        true_idxes = idxes[:-1] # remove the last index which is end token\n        assert len(set(true_idxes))==len(true_idxes) # 保证选择的idxes 没有重复的！\n        all_idxes = list(range(num_cuts))\n        not_sel_idxes = list(set(all_idxes).difference(true_idxes))\n        sorted_cuts = [cuts[idx] for idx in true_idxes]\n        not_sel_cuts = [cuts[n_idx] for n_idx in not_sel_idxes]\n        sorted_cuts.extend(not_sel_cuts)\n\n        return {\n            'cuts': sorted_cuts, # selected sorted cuts\n            'nselectedcuts': sel_cuts_num-1, # num of selected cuts\n            'result': SCIP_RESULT.SUCCESS\n        }\n\n    def _get_lp_info(self):\n        lp_info = {}\n        lp_info['lp_solution_value'] = self.scip_model.getLPObjVal()\n        cols = self.scip_model.getLPColsData()\n        col_solution_value = [col.getPrimsol() for col in cols if col.isIntegral()]\n        lp_info['lp_solution_integer_var_value'] = [val for val in col_solution_value if val != 0.]\n\n        return lp_info\n\n    def get_data(self):\n        return self.data\n\n    def get_lp_info(self):\n        return self.lp_info\n\n    # def get_cuts_info(self):\n    #     return self.cuts_info\n        \n    def free_problem(self):\n        self.scip_model.freeProb()\n\nclass HierarchyCutSelectAgent(CutSelectAgent):\n    def __init__(\n        self,\n        scip_model,\n        pointer_net,\n        cutsel_percent_policy,\n        value_net,\n        sel_cuts_percent,\n        device,\n        decode_type,\n        mean_std,\n        policy_type\n    ):\n        CutSelectAgent.__init__(\n            self,\n            scip_model,\n            pointer_net,\n            value_net,\n            sel_cuts_percent,\n            device,\n            decode_type,\n            mean_std,\n            policy_type\n        )\n        self.cutsel_percent_policy = cutsel_percent_policy\n        self.high_level_data = {}\n\n    def cutselselect(self, cuts, forcedcuts, root, maxnselectedcuts):\n        '''first method called in each iteration in the main solving loop. '''\n        # this method needs to be implemented by the user\n        logger.log(f\"forcedcuts length: {len(forcedcuts)}\")\n        logger.log(f\"len cuts: {len(cuts)}\")\n        num_cuts = len(cuts)\n        # cur_lp_info = self._get_lp_info()\n        # for k in cur_lp_info.keys():\n        #     self.lp_info[k].append(cur_lp_info[k])\n        if num_cuts <= 1:\n            return {\n                'cuts': cuts, # selected sorted cuts\n                'nselectedcuts': 1, # num of selected cuts\n                'result': SCIP_RESULT.SUCCESS\n            }\n        \n        st_before_input = time.time()\n\n        # compute states\n        cuts_features = advanced_cut_feature_generator(self.scip_model, cuts)\n        et_feature_extractor = time.time()\n        # normalize states\n        if self.mean_std is not None:\n            # normalize cut features\n            normalize_cut_features = self._normalize(cuts_features)\n            input_cuts = torch.from_numpy(normalize_cut_features).to(self.device)\n        else:\n            input_cuts = torch.from_numpy(cuts_features).to(self.device)\n        input_cuts = input_cuts.reshape(input_cuts.shape[0], 1, input_cuts.shape[1])\n\n        st_end_input = time.time()\n\n        # compute sel cuts percent\n        with torch.no_grad():\n            if self.decode_type == 'greedy':\n                deterministic = True\n            else:\n                deterministic = False\n            raw_sel_cuts_percent = self.cutsel_percent_policy.action(input_cuts.float(), deterministic=deterministic)\n        st_end_highlevel_policy_inference = time.time()\n\n        sel_cuts_percent = raw_sel_cuts_percent.item() * 0.5 + 0.5\n        sel_cuts_num = min(int(num_cuts * sel_cuts_percent), int(maxnselectedcuts))\n        sel_cuts_num = max(sel_cuts_num, 2)\n        # 只做选择动作的功能，不做计算梯度的功能\n        with torch.no_grad():\n            _, input_idxs =  self.policy(input_cuts.float(), sel_cuts_num, self.decode_type) # (list of tensor, list of tensor)\n        st_end_pointer_net_inference = time.time()\n\n        print(f\"process input time: {st_end_input-st_before_input} s\")\n        print(f\"input feature extractor time: {et_feature_extractor-st_before_input} s\")\n        print(f\"input cpu data to gpu time: {st_end_input-et_feature_extractor} s\")\n        print(f\"high level policy time: {st_end_highlevel_policy_inference-st_end_input} s\")\n        print(f\"pointer net inference time: {st_end_pointer_net_inference - st_end_highlevel_policy_inference} s\")\n\n        idxes = [input.cpu().detach().item() for input in input_idxs]\n        assert len(set(idxes))==len(idxes) # 保证选择的idxes 没有重复的！\n        all_idxes = list(range(num_cuts))\n        not_sel_idxes = list(set(all_idxes).difference(idxes))\n        sorted_cuts = [cuts[idx] for idx in idxes]\n        not_sel_cuts = [cuts[n_idx] for n_idx in not_sel_idxes]\n        sorted_cuts.extend(not_sel_cuts)\n        # debug\n        # sorted_cuts = cuts\n        # 只log 第一次cut 处的state 和 action\n        if not self.data:\n            self.data = {\n                \"state\": cuts_features,\n                \"action\": idxes,\n                \"sel_cuts_num\": sel_cuts_num,\n            }\n        if not self.high_level_data:\n            self.high_level_data = {\n                \"state\": cuts_features,\n                \"action\": raw_sel_cuts_percent.item()\n            }\n\n        return {\n            'cuts': sorted_cuts, # selected sorted cuts\n            'nselectedcuts': sel_cuts_num, # num of selected cuts\n            'result': SCIP_RESULT.SUCCESS\n        }\n\n    def get_high_level_data(self):\n        return self.high_level_data\n\n# ==========================================\n# File: parallel_reinforce_algorithm.py\n# Function/Context: generate_hierarchy_samples, evaluate_hierarchy, test_hierarchy\n# ==========================================\ndef generate_hierarchy_samples(return_queue,env,policy,cutsel_policy,value,epoch,samples_per_worker,sel_cuts_percent,device,train_decode_type,reward_type,seed,mean_std,policy_type,random_seed):\n    os.environ['CUDA_VISIBLE_DEVICES'] = device\n    device = 'cuda:0'\n    policy = policy.to(device)\n    cutsel_policy = cutsel_policy.to(device)\n    log_prefix = os.getpid()\n    _ = set_global_seed(seed%4096)\n    logger.log(f\"{log_prefix}: debug log random seed {seed%4096}\")\n    logger.log(f\"{log_prefix}: sampling data ...\")\n    env_step_infos = {\n        \"solving_time\": [],\n        \"ntotal_nodes\": [],\n        \"primal_dual_gap\": [],\n        \"primaldualintegral\": []\n    } # dict of list\n    training_datasets = {\n        \"state\": [],\n        \"action\": [],\n        \"sel_cuts_num\": [],\n        \"neg_reward\": []\n    } # list of numpy/list/int\n    training_high_level_datasets = {\n        \"state\": [],\n        \"action\": [],\n        \"neg_reward\": []\n    }\n    env.set_seed(seed)\n    for step in range(samples_per_worker):\n        logger.log(f\"{log_prefix}: training...  epoch: {epoch}...  steps: {step+1}\")\n        logger.log(f\"{log_prefix}: cuda memory: {torch.cuda.memory_allocated(0)/1024**3} GB\")\n        logger.log(f\"{log_prefix}: cuda cached: {torch.cuda.memory_cached(0)/1024**3} GB\")\n        env.reset()\n        # reset action agent\n        cutsel_agent = HierarchyCutSelectAgent(\n            env.m,\n            policy,\n            cutsel_policy,\n            value,\n            sel_cuts_percent,\n            device,\n            train_decode_type,\n            mean_std,\n            policy_type\n        )\n        env_step_info = env.step(cutsel_agent)\n        state_action_dict = cutsel_agent.get_data()\n        lp_info = cutsel_agent.get_lp_info()\n        high_level_state_action_dict = cutsel_agent.get_high_level_data()\n        if (not state_action_dict) or (not high_level_state_action_dict):\n            logger.log(f\"{log_prefix}: warning!!! current instance cuts len <= 1\")\n            continue\n        if reward_type == \"lp_solution_value\":\n            if len(lp_info[\"lp_solution_value\"]) < 2:\n                continue\n            else:\n                neg_reward = lp_info[\"lp_solution_value\"][0] - lp_info[\"lp_solution_value\"][1]\n        for key in env_step_info.keys():\n            assert key in env_step_infos.keys()\n            env_step_infos[key].append(env_step_info[key])\n        \n        for key in state_action_dict.keys():\n            training_datasets[key].append(state_action_dict[key])\n        for key in high_level_state_action_dict.keys():\n            training_high_level_datasets[key].append(high_level_state_action_dict[key])\n\n        if reward_type == 'lp_solution_value':\n            training_datasets['neg_reward'].append(neg_reward)\n            training_high_level_datasets['neg_reward'].append(neg_reward)\n        else:\n            training_datasets['neg_reward'].append(env_step_info[reward_type])\n            training_high_level_datasets['neg_reward'].append(env_step_info[reward_type])\n        cutsel_agent.free_problem()\n\n    # list dict numpy cuda tensor 都可以传，cpu tensor 传不了，带梯度信息的cuda tensor 传不了\n    return_queue.put((env_step_infos, training_datasets, training_high_level_datasets))\n\ndef evaluate_hierarchy(\n    return_queue,\n    env,\n    policy,\n    cutsel_percent_policy,\n    value,\n    epoch,\n    evaluate_samples_per_worker,\n    sel_cuts_percent,\n    device,\n    evaluate_decode_type,\n    seed,\n    mean_std,\n    policy_type,\n    random_seed\n):\n    os.environ['CUDA_VISIBLE_DEVICES'] = device\n    device = 'cuda:0'\n    policy = policy.to(device)\n    cutsel_percent_policy = cutsel_percent_policy.to(device)\n    _ = set_global_seed(random_seed)\n    log_prefix = os.getpid()\n    logger.log(f\"{log_prefix}: debug log random seed {random_seed}\")\n    logger.log(f\"{log_prefix}: evaluating...  epoch: {epoch}\")\n    neg_solving_time = np.zeros((evaluate_samples_per_worker, 1))\n    neg_total_nodes = np.zeros((evaluate_samples_per_worker, 1))\n    primaldualintegral = np.zeros((evaluate_samples_per_worker, 1))\n    primal_dual_gap = np.zeros((evaluate_samples_per_worker,1))\n    lp_solution_value = []\n    env.set_seed(seed)\n    for i in range(evaluate_samples_per_worker):\n        env.reset()\n        cutsel_agent = HierarchyCutSelectAgent(\n            env.m,\n            policy,\n            cutsel_percent_policy,\n            value,\n            sel_cuts_percent,\n            device,\n            evaluate_decode_type,\n            mean_std,\n            policy_type\n        )\n        env_step_info = env.step(cutsel_agent)\n        lp_info = cutsel_agent.get_lp_info()\n        neg_solving_time[i,:] = env_step_info['solving_time']\n        neg_total_nodes[i,:] = env_step_info['ntotal_nodes']\n        primaldualintegral[i,:] = env_step_info['primaldualintegral']\n        primal_dual_gap[i,:] = env_step_info['primal_dual_gap']\n        if len(lp_info['lp_solution_value']) >= 2:\n            lp_solution_value.append(lp_info['lp_solution_value'][0] - lp_info['lp_solution_value'][1])\n    return_queue.put(\n        (neg_solving_time, neg_total_nodes, primaldualintegral, lp_solution_value,primal_dual_gap)\n    )\n\ndef test_hierarchy(\n    return_queue,\n    instance_path,\n    instance_file_list,\n    policy,\n    cutsel_percent_policy,\n    sel_cuts_percent,\n    device,\n    test_decode_type,\n    seed,\n    mean_std,\n    policy_type,\n    scip_seed,\n    **env_kwargs\n):\n    os.environ['CUDA_VISIBLE_DEVICES'] = device\n    device = 'cuda:0'\n    policy = policy.to(device)\n    cutsel_percent_policy = cutsel_percent_policy.to(device)\n    _ = set_global_seed(seed)\n    print(f\"pid: {os.getpid()} debug log random seed {seed}\")\n    print(f\"pid: {os.getpid()}, instance_files: {instance_file_list}\")\n    neg_solving_time = np.zeros((len(instance_file_list), 1))\n    neg_total_nodes = np.zeros((len(instance_file_list), 1))\n    primaldualintegral = np.zeros((len(instance_file_list), 1))\n    primal_dual_gap = np.zeros((len(instance_file_list), 1))\n    sel_cuts_info = {\n        'sel_cuts_num': [],\n        'cuts_total_num': []\n    }\n    f_name_list = []\n    for i, f_name in enumerate(instance_file_list):\n        env_kwargs['single_instance_file'] = f_name\n        env = SCIPCutSelEnv(\n            instance_path,\n            scip_seed,\n            seed,\n            **env_kwargs\n        )\n        env.reset()\n        cutsel_agent = HierarchyCutSelectAgent(\n            env.m,\n            policy,\n            cutsel_percent_policy,\n            None,\n            sel_cuts_percent,\n            device,\n            test_decode_type,\n            mean_std,\n            policy_type\n        )\n        env_step_info = env.step(cutsel_agent)\n        state_action_dict = cutsel_agent.get_data()\n\n        neg_solving_time[i,:] = env_step_info['solving_time']\n        neg_total_nodes[i,:] = env_step_info['ntotal_nodes']\n        primaldualintegral[i,:] = env_step_info['primaldualintegral']\n        primal_dual_gap[i,:] = env_step_info['primal_dual_gap']\n        if not state_action_dict:\n            sel_cuts_info['sel_cuts_num'].append(1)\n            sel_cuts_info['cuts_total_num'].append(1)\n        else:\n            sel_cuts_info['sel_cuts_num'].append(state_action_dict['sel_cuts_num'])\n            sel_cuts_info['cuts_total_num'].append(len(state_action_dict['state']))\n        f_name_list.append(f_name)\n    return_queue.put(\n        (neg_solving_time, neg_total_nodes,primaldualintegral,primal_dual_gap,f_name_list,sel_cuts_info)\n    )\n\n# ==========================================\n# File: pointer_net.py\n# Function/Context: Decoder.forward\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.autograd as autograd\nfrom torch.autograd import Variable\nfrom torch.nn.parameter import Parameter\nimport torch.nn.functional as F\nfrom torch.distributions import Normal\nimport math\nimport numpy as np\n\nimport pyscipopt as scip\nfrom pyscipopt import SCIP_RESULT\n\n# from beam_search import Beam\nfrom utils import cut_feature_generator\nfrom logger import logger\n\nLOG_STD_MAX = 2\nLOG_STD_MIN = -20\n\nclass Encoder(nn.Module):\n    \"\"\"Maps a graph represented as an input sequence\n    to a hidden vector\"\"\"\n    def __init__(self, input_dim, hidden_dim, use_cuda):\n        super(Encoder, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.lstm = nn.LSTM(input_dim, hidden_dim) # default layers=1\n        self.use_cuda = use_cuda\n        self.enc_init_hx = nn.Parameter(torch.zeros(hidden_dim),requires_grad=False)\n        self.enc_init_cx = nn.Parameter(torch.zeros(hidden_dim),requires_grad=False)\n\n        self.enc_init_state = (self.enc_init_hx, self.enc_init_cx)\n\n    def forward(self, x, hidden):\n        output, hidden = self.lstm(x, hidden)\n        return output, hidden\n    \n    def init_hidden(self, hidden_dim):\n        \"\"\"Trainable initial hidden state\"\"\"\n        enc_init_hx = nn.Parameter(torch.zeros(hidden_dim),requires_grad=False)\n        # enc_init_hx = Variable(torch.zeros(hidden_dim), requires_grad=False)\n        # if self.use_cuda:\n        #     enc_init_hx = enc_init_hx.cuda()\n\n        # enc_init_hx.data.uniform_(-(1. / math.sqrt(hidden_dim)),\n        #        1. / math.sqrt(hidden_dim))\n\n        enc_init_cx = nn.Parameter(torch.zeros(hidden_dim),requires_grad=False)\n        # enc_init_cx = Variable(torch.zeros(hidden_dim), requires_grad=False)\n        # if self.use_cuda:\n        #     enc_init_cx = enc_init_cx.cuda()\n\n        #enc_init_cx = nn.Parameter(enc_init_cx)\n        # enc_init_cx.data.uniform_(-(1. / math.sqrt(hidden_dim)),\n        #        1. / math.sqrt(hidden_dim))\n        return (enc_init_hx, enc_init_cx)\n\nclass Attention(nn.Module):\n    \"\"\"A generic attention module for a decoder in seq2seq\"\"\"\n    def __init__(self, dim, use_tanh=False, C=10, use_cuda=True):\n        super(Attention, self).__init__()\n        self.use_tanh = use_tanh\n        self.project_query = nn.Linear(dim, dim)\n        self.project_ref = nn.Conv1d(dim, dim, 1, 1) # TODO: check 为何会有卷积\n        self.C = C  # tanh exploration\n        self.tanh = nn.Tanh()\n        \n        # v = torch.FloatTensor(dim)\n        # if use_cuda:\n        #     v = v.cuda()  \n        self.v = nn.Parameter(torch.FloatTensor(dim))\n        self.v.data.uniform_(-(1. / math.sqrt(dim)) , 1. / math.sqrt(dim))\n        \n    def forward(self, query, ref):\n        \"\"\"\n        Args: \n            query: is the hidden state of the decoder at the current\n                time step. batch x dim\n            ref: the set of hidden states from the encoder. \n                sourceL x batch x hidden_dim\n        \"\"\"\n        # ref is now [batch_size x hidden_dim x sourceL]\n        ref = ref.permute(1, 2, 0)\n        q = self.project_query(query).unsqueeze(2)  # batch x dim x 1\n        e = self.project_ref(ref)  # batch_size x hidden_dim x sourceL \n        # expand the query by sourceL\n        # batch x dim x sourceL\n        expanded_q = q.repeat(1, 1, e.size(2)) \n        # batch x 1 x hidden_dim\n        v_view = self.v.unsqueeze(0).expand(\n                expanded_q.size(0), len(self.v)).unsqueeze(1)\n        # [batch_size x 1 x hidden_dim] * [batch_size x hidden_dim x sourceL]\n        u = torch.bmm(v_view, self.tanh(expanded_q + e)).squeeze(1)\n        if self.use_tanh:\n            logits = self.C * self.tanh(u)\n        else:\n            logits = u  \n        return e, logits\n\n# TODO：保留beam search，我们的setting 用不了beam search，还得增加一个模式是sample 的模式 \nclass Decoder(nn.Module):\n    def __init__(self, \n            embedding_dim,\n            hidden_dim,\n            tanh_exploration,\n            use_tanh,\n            n_glimpses=1,\n            beam_size=0,\n            use_cuda=True):\n        super(Decoder, self).__init__()\n        \n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.n_glimpses = n_glimpses\n        self.beam_size = beam_size\n        self.use_cuda = use_cuda\n\n        self.input_weights = nn.Linear(embedding_dim, 4 * hidden_dim)\n        self.hidden_weights = nn.Linear(hidden_dim, 4 * hidden_dim)\n\n        self.pointer = Attention(hidden_dim, use_tanh=use_tanh, C=tanh_exploration, use_cuda=self.use_cuda)\n        self.glimpse = Attention(hidden_dim, use_tanh=False, use_cuda=self.use_cuda)\n        self.sm = nn.Softmax()\n\n    def apply_mask_to_logits(self, step, logits, mask, prev_idxs):    \n        if mask is None:\n            mask = torch.zeros(logits.size()).byte().to(self.pointer.v.device)\n            # if self.use_cuda:\n            #     mask = mask.cuda()\n    \n        maskk = mask.clone()\n        # print(f\"debug log maskk device: {maskk.device}\")\n\n        # to prevent them from being reselected. \n        # Or, allow re-selection and penalize in the objective function\n        if prev_idxs is not None:\n            # set most recently selected idx values to 1\n            maskk[[x for x in range(logits.size(0))],\n                    prev_idxs.data] = 1\n            logits[maskk] = -np.inf\n        return logits, maskk\n\n    def forward(self, decoder_input, embedded_inputs, hidden, context, max_length, decode_type): # TODO: max decode length 以参数传入forward 函数\n        \"\"\"\n        Args:\n            decoder_input: The initial input to the decoder\n                size is [batch_size x embedding_dim]. Trainable parameter.\n            embedded_inputs: [sourceL x batch_size x embedding_dim]\n            hidden: the prev hidden state, size is [batch_size x hidden_dim]. \n                Initially this is set to (enc_h[-1], enc_c[-1])\n            context: encoder outputs, [sourceL x batch_size x hidden_dim] \n        \"\"\"\n        def recurrence(x, hidden, logit_mask, prev_idxs, step):\n            \n            hx, cx = hidden  # batch_size x hidden_dim\n            \n            gates = self.input_weights(x) + self.hidden_weights(hx)\n            ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n\n            ingate = F.sigmoid(ingate)\n            forgetgate = F.sigmoid(forgetgate)\n            cellgate = F.tanh(cellgate)\n            outgate = F.sigmoid(outgate)\n\n            cy = (forgetgate * cx) + (ingate * cellgate)\n            hy = outgate * F.tanh(cy)  # batch_size x hidden_dim\n            \n            g_l = hy\n            for i in range(self.n_glimpses):\n                ref, logits = self.glimpse(g_l, context)\n                logits, logit_mask = self.apply_mask_to_logits(step, logits, logit_mask, prev_idxs)\n                # [batch_size x h_dim x sourceL] * [batch_size x sourceL x 1] = \n                # [batch_size x h_dim x 1]\n                g_l = torch.bmm(ref, self.sm(logits).unsqueeze(2)).squeeze(2) \n            _, logits = self.pointer(g_l, context) # logits 代表基于context vector 的概率分布\n            \n            logits, logit_mask = self.apply_mask_to_logits(step, logits, logit_mask, prev_idxs)\n            probs = self.sm(logits)\n            return hy, cy, probs, logit_mask\n    \n        batch_size = context.size(1)\n        outputs = []\n        selections = []\n        steps = range(max_length)  # or until terminating symbol ?\n        inps = []\n        idxs = None\n        mask = None\n       \n        if decode_type in [\"stochastic\", \"greedy\"]:\n            for i in steps:\n                hx, cx, probs, mask = recurrence(decoder_input, hidden, mask, idxs, i)\n                hidden = (hx, cx)\n                # select the next inputs for the decoder [batch_size x hidden_dim]\n                decoder_input, idxs = self.decode(\n                    probs,\n                    embedded_inputs,\n                    selections,\n                    decode_type) # 每一次decode 都是随机sample 一个输出\n                inps.append(decoder_input) \n                # use outs to point to next object\n                outputs.append(probs)\n                selections.append(idxs)\n            return (outputs, selections), hidden\n        \n        elif decode_type == \"beam_search\":\n            raise NotImplementedError\n            # Expand input tensors for beam search\n            # decoder_input = Variable(decoder_input.data.repeat(self.beam_size, 1))\n            # context = Variable(context.data.repeat(1, self.beam_size, 1))\n            # hidden = (Variable(hidden[0].data.repeat(self.beam_size, 1)),\n            #         Variable(hidden[1].data.repeat(self.beam_size, 1)))\n            \n            # beam = [\n            #         Beam(self.beam_size, max_length, cuda=self.use_cuda) \n            #         for k in range(batch_size)\n            # ]\n            \n            # for i in steps:\n            #     hx, cx, probs, mask = recurrence(decoder_input, hidden, mask, idxs, i)\n            #     hidden = (hx, cx)\n                \n            #     probs = probs.view(self.beam_size, batch_size, -1\n            #             ).transpose(0, 1).contiguous()\n                \n            #     n_best = 1\n            #     # select the next inputs for the decoder [batch_size x hidden_dim]\n            #     decoder_input, idxs, active = self.decode_beam(probs,\n            #             embedded_inputs, beam, batch_size, n_best, i)\n               \n            #     inps.append(decoder_input) \n            #     # use probs to point to next object\n            #     if self.beam_size > 1:\n            #         outputs.append(probs[:, 0,:])\n            #     else:\n            #         outputs.append(probs.squeeze(0))\n            #     # Check for indexing\n            #     selections.append(idxs)\n            #      # Should be done decoding\n            #     if len(active) == 0:\n            #         break\n            #     decoder_input = Variable(decoder_input.data.repeat(self.beam_size, 1))\n\n            # return (outputs, selections), hidden\n\n        else:\n            # TODO: 实现每轮输出最大概率对应的index\n            raise NotImplementedError\n\n    def decode(self, probs, embedded_inputs, selections, decode_type):\n        \"\"\"\n        Return the next input for the decoder by selecting the \n        input with sampling\n\n        Args: \n            probs: [batch_size x sourceL]\n            embedded_inputs: [sourceL x batch_size x embedding_dim]\n            selections: list of all of the previously selected indices during decoding\n       Returns:\n            Tensor of size [batch_size x sourceL] containing the embeddings\n            from the inputs corresponding to the [batch_size] indices\n            selected for this iteration of the decoding, as well as the \n            corresponding indicies\n        \"\"\"\n        batch_size = probs.size(0)\n        # idxs is [batch_size]\n        if decode_type == \"stochastic\":\n            idxs = probs.multinomial(num_samples=1).squeeze(1) # TODO: multinomial() 函数添加参数num_samples，应该也是torch 版本的问题\n        elif decode_type == \"greedy\":\n            max_probs, idxs = probs.max(1)\n        # due to race conditions, might need to resample here\n        # TODO: check，这里的mask 操作是O(n) 感觉不够快\n        # for old_idxs in selections:\n        #     # compare new idxs\n        #     # elementwise with the previous idxs. If any matches,\n        #     # then need to resample\n        #     if old_idxs.eq(idxs).data.any():\n        #         print(' [!] resampling due to race condition')\n        #         if decode_type == \"stochastic\":\n        #             idxs = probs.multinomial(num_samples=1).squeeze(1) # TODO: multinomial() 函数添加参数num_samples，应该也是torch 版本的问题\n        #         elif decode_type == \"greedy\":\n        #             max_probs, idxs = probs.max(1)\n        #         break\n        assert idxs not in set(selections)\n\n        sels = embedded_inputs[idxs.data, [i for i in range(batch_size)], :] \n        return sels, idxs\n\n# ==========================================\n# File: pointer_net_end_token.py\n# Function/Context: PointerNetworkEndToken\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.autograd as autograd\nfrom torch.autograd import Variable\nfrom torch.nn.parameter import Parameter\nimport torch.nn.functional as F\nfrom torch.distributions import Normal\nimport math\nimport numpy as np\n\nimport pyscipopt as scip\nfrom pyscipopt import SCIP_RESULT\n\nfrom beam_search import Beam\nfrom utils import cut_feature_generator\nfrom logger import logger\nfrom pointer_net import Encoder\n\nLOG_STD_MAX = 2\nLOG_STD_MIN = -20\n\nclass Attention(nn.Module):\n    \"\"\"A generic attention module for a decoder in seq2seq\"\"\"\n    def __init__(self, dim, use_tanh=False, C=10, use_cuda=True):\n        super(Attention, self).__init__()\n        self.use_tanh = use_tanh\n        self.project_query = nn.Linear(dim, dim)\n        self.project_ref = nn.Conv1d(dim, dim, 1, 1)\n        self.C = C  # tanh exploration\n        self.tanh = nn.Tanh()\n        \n        self.v = nn.Parameter(torch.FloatTensor(dim))\n        self.v.data.uniform_(-(1. / math.sqrt(dim)) , 1. / math.sqrt(dim))\n        \n    def forward(self, query, ref):\n        ref = ref.permute(1, 2, 0)\n        q = self.project_query(query).unsqueeze(2)\n        e = self.project_ref(ref)\n        expanded_q = q.repeat(1, 1, e.size(2))\n        v_view = self.v.unsqueeze(0).expand(\n                expanded_q.size(0), len(self.v)).unsqueeze(1)\n        u = torch.bmm(v_view, self.tanh(expanded_q + e)).squeeze(1)\n        if self.use_tanh:\n            logits = self.C * self.tanh(u)\n        else:\n            logits = u  \n        return e, logits\n\nclass DecoderEndToken(nn.Module):\n    def __init__(self, \n            embedding_dim,\n            hidden_dim,\n            tanh_exploration,\n            use_tanh,\n            n_glimpses=1,\n            beam_size=0,\n            use_cuda=True):\n        super(DecoderEndToken, self).__init__()\n        \n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.n_glimpses = n_glimpses\n        self.beam_size = beam_size\n        self.use_cuda = use_cuda\n\n        self.input_weights = nn.Linear(embedding_dim, 4 * hidden_dim)\n        self.hidden_weights = nn.Linear(hidden_dim, 4 * hidden_dim)\n\n        self.pointer = Attention(hidden_dim, use_tanh=use_tanh, C=tanh_exploration, use_cuda=self.use_cuda)\n        self.glimpse = Attention(hidden_dim, use_tanh=False, use_cuda=self.use_cuda)\n        self.sm = nn.Softmax()\n\n    def apply_mask_to_logits(self, step, logits, mask, prev_idxs):    \n        if mask is None:\n            mask = torch.zeros(logits.size()).byte().to(self.pointer.v.device)\n        maskk = mask.clone()\n        if prev_idxs is not None:\n            maskk[[x for x in range(logits.size(0))],\n                    prev_idxs.data] = 1\n            logits[maskk] = -np.inf\n        return logits, maskk\n\n    def forward(self, decoder_input, embedded_inputs, hidden, context, max_length, decode_type):\n        def recurrence(x, hidden, logit_mask, prev_idxs, step):\n            hx, cx = hidden\n            gates = self.input_weights(x) + self.hidden_weights(hx)\n            ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n            ingate = F.sigmoid(ingate)\n            forgetgate = F.sigmoid(forgetgate)\n            cellgate = F.tanh(cellgate)\n            outgate = F.sigmoid(outgate)\n            cy = (forgetgate * cx) + (ingate * cellgate)\n            hy = outgate * F.tanh(cy)\n            g_l = hy\n            for i in range(self.n_glimpses):\n                ref, logits = self.glimpse(g_l, context)\n                logits, logit_mask = self.apply_mask_to_logits(step, logits, logit_mask, prev_idxs)\n                g_l = torch.bmm(ref, self.sm(logits).unsqueeze(2)).squeeze(2)\n            _, logits = self.pointer(g_l, context)\n            logits, logit_mask = self.apply_mask_to_logits(step, logits, logit_mask, prev_idxs)\n            probs = self.sm(logits)\n            return hy, cy, probs, logit_mask\n    \n        batch_size = context.size(1)\n        outputs = []\n        selections = []\n        steps = range(max_length)\n        inps = []\n        idxs = None\n        mask = None\n       \n        if decode_type in [\"stochastic\", \"greedy\"]:\n            for i in steps:\n                hx, cx, probs, mask = recurrence(decoder_input, hidden, mask, idxs, i)\n                hidden = (hx, cx)\n                decoder_input, idxs = self.decode(\n                    probs,\n                    embedded_inputs,\n                    selections,\n                    decode_type)\n                inps.append(decoder_input)\n                outputs.append(probs)\n                selections.append(idxs)\n                if idxs.item() == (max_length - 1):\n                    break\n            return (outputs, selections), hidden\n        elif decode_type == \"beam_search\":\n            decoder_input = Variable(decoder_input.data.repeat(self.beam_size, 1))\n            context = Variable(context.data.repeat(1, self.beam_size, 1))\n            hidden = (Variable(hidden[0].data.repeat(self.beam_size, 1)),\n                    Variable(hidden[1].data.repeat(self.beam_size, 1)))\n            beam = [\n                    Beam(self.beam_size, max_length, cuda=self.use_cuda) \n                    for k in range(batch_size)\n            ]\n            for i in steps:\n                hx, cx, probs, mask = recurrence(decoder_input, hidden, mask, idxs, i)\n                hidden = (hx, cx)\n                probs = probs.view(self.beam_size, batch_size, -1\n                        ).transpose(0, 1).contiguous()\n                n_best = 1\n                decoder_input, idxs, active = self.decode_beam(probs,\n                        embedded_inputs, beam, batch_size, n_best, i)\n               \n                inps.append(decoder_input)\n                if self.beam_size > 1:\n                    outputs.append(probs[:, 0,:])\n                else:\n                    outputs.append(probs.squeeze(0))\n                selections.append(idxs)\n                if len(active) == 0:\n                    break\n                decoder_input = Variable(decoder_input.data.repeat(self.beam_size, 1))\n            return (outputs, selections), hidden\n        else:\n            raise NotImplementedError\n\n    def decode(self, probs, embedded_inputs, selections, decode_type):\n        batch_size = probs.size(0)\n        if decode_type == \"stochastic\":\n            idxs = probs.multinomial(num_samples=1).squeeze(1)\n        elif decode_type == \"greedy\":\n            max_probs, idxs = probs.max(1)\n        assert idxs not in set(selections)\n        sels = embedded_inputs[idxs.data, [i for i in range(batch_size)], :]\n        return sels, idxs\n\nclass PointerNetworkEndToken(nn.Module):\n    \"\"\"The pointer network, which is the core seq2seq model\"\"\"\n    def __init__(self, \n            embedding_dim,\n            hidden_dim,\n            n_glimpses,\n            tanh_exploration,\n            use_tanh,\n            beam_size,\n            use_cuda):\n        super(PointerNetworkEndToken, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.encoder = Encoder(\n                embedding_dim,\n                hidden_dim,\n                use_cuda)\n        self.decoder = DecoderEndToken(\n                embedding_dim,\n                hidden_dim,\n                tanh_exploration=tanh_exploration,\n                use_tanh=use_tanh,\n                n_glimpses=n_glimpses,\n                beam_size=beam_size,\n                use_cuda=use_cuda)\n        self.decoder_in_0 = nn.Parameter(torch.FloatTensor(embedding_dim))\n        self.decoder_in_0.data.uniform_(-(1. / math.sqrt(embedding_dim)),\n                1. / math.sqrt(embedding_dim))\n            \n    def forward(self, inputs, max_decode_len, decode_type):\n        batch_size = inputs.size(0)\n        input_length = inputs.size(1)\n        encoder_inputs = inputs.permute(1,0,2)\n        encoder_outputs, (hidden, context) = self.encoder(encoder_inputs)\n        decoder_input = self.decoder_in_0.unsqueeze(0).expand(batch_size, self.embedding_dim)\n        (outputs, selections), decoder_hidden = self.decoder(decoder_input,\n                                                              encoder_inputs,\n                                                              (hidden, context),\n                                                              encoder_outputs,\n                                                              max_decode_len,\n                                                              decode_type)\n        return outputs, selections",
  "description": "Combined Analysis:\n- [algorithms.py]: This file implements the core reinforcement learning training algorithm (REINFORCE with baseline) for the hierarchical sequence model (HEM) described in the paper. The ReinforceBaselineAlg class handles policy gradient updates, baseline computation (simple moving average or neural network critic), and training/evaluation loops. It directly corresponds to the RL training methodology used to learn cut selection policies, addressing the three subproblems (P1-P3) through the pointer network (policy) and optional value network (baseline). The code includes state normalization, reward scaling, gradient clipping, and learning rate decay—key components for stable RL training of the hierarchical model.\n- [cutsel_agent_parallel.py]: This file implements the hierarchical sequence model (HEM) described in the paper. The HierarchyCutSelectAgent class addresses all three cut selection problems: (P1) which cuts to prefer via pointer network selection, (P2) how many cuts to select via cutsel_percent_policy, and (P3) ordering via sequential pointer network decoding. The cutselselect method implements the complete hierarchical decision process: feature extraction → higher-level percentage prediction → lower-level ordered cut selection → return sorted cuts to SCIP solver.\n- [parallel_reinforce_algorithm.py]: This file implements the parallel training infrastructure for the hierarchical sequence model (HEM) described in the paper. The key functions generate_hierarchy_samples, evaluate_hierarchy, and test_hierarchy directly implement the hierarchical reinforcement learning algorithm where: 1) A higher-level policy (cutsel_policy) determines how many cuts to select (addressing P2), 2) A lower-level policy (policy) selects specific cuts and their ordering (addressing P1 and P3). The code orchestrates parallel data collection, evaluation, and testing using the HierarchyCutSelectAgent which encapsulates the two-level decision process. The reward signals (solving_time, lp_solution_value, etc.) align with the paper's objective of optimizing MILP solver performance.\n- [pointer_net.py]: This file implements the lower-level sequence model (pointer network) component of the hierarchical sequence model (HEM) described in the paper. The Decoder class uses an LSTM with attention mechanisms to sequentially select and order cuts from a candidate pool. The forward method handles different decoding strategies (stochastic/greedy) to generate cut sequences, directly addressing the paper's objectives of selecting which cuts to prefer (via attention scores) and determining their addition order. The model uses masked logits to prevent reselection and handles variable-length sequences via max_length parameter.\n- [pointer_net_end_token.py]: This file implements the lower-level sequence model (pointer network) from the hierarchical sequence model (HEM) described in the paper. The PointerNetworkEndToken uses an encoder-decoder architecture with attention to select an ordered subset of cuts from the candidate pool. The decoder includes an end token mechanism (max_length-1 index) to dynamically determine the number of cuts to select, addressing problems P2 (how many cuts) and P3 (ordering). The forward method processes input cut features, encodes them, then decodes to produce a sequence of selected cut indices until the end token is chosen. This directly corresponds to the paper's lower-level model that learns cut selection policies via reinforcement learning.",
  "dependencies": [
    "RunningMeanStd",
    "SCIPCutSelEnv",
    "torch.distributions",
    "copy",
    "Encoder",
    "Decoder.decode",
    "sqlalchemy",
    "torch.optim",
    "utilss.mean_std",
    "HierarchyCutSelectAgent",
    "cutsel_agent_parallel",
    "CutSelectAgent",
    "logger.logger",
    "pyscipopt",
    "torch.nn.functional",
    "logger",
    "osp",
    "utils.cut_feature_generator",
    "Decoder.apply_mask_to_logits",
    "Attention",
    "tqdm",
    "math",
    "json",
    "os",
    "utils.advanced_cut_feature_generator",
    "beam_search.Beam",
    "argparse",
    "set_global_seed",
    "create_stats_ordered_dict",
    "numpy",
    "setup_logger",
    "utils",
    "torch.nn",
    "torch.autograd",
    "lr_scheduler",
    "torch",
    "pointer_net.Encoder"
  ]
}