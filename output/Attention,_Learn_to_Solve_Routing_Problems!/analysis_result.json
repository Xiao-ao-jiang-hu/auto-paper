{
  "paper_id": "Attention,_Learn_to_Solve_Routing_Problems!",
  "title": "Attention, Learn to Solve Routing Problems!",
  "abstract": "The recently presented idea to learn heuristics for combinatorial optimization problems is promising as it can save costly development. However, to push this idea towards practical implementation, we need better models and better ways of training. We contribute in both directions: we propose a model based on attention layers with benefits over the Pointer Network and we show how to train this model using REINFORCE with a simple baseline based on a deterministic greedy rollout, which we find is more efficient than using a value function. We significantly improve over recent learned heuristics for the Travelling Salesman Problem (TSP), getting close to optimal results for problems up to 100 nodes. With the same hyperparameters, we learn strong heuristics for two variants of the Vehicle Routing Problem (VRP), the Orienteering Problem (OP) and (a stochastic variant of) the Prize Collecting TSP (PCTSP), outperforming a wide range of baselines and getting results close to highly optimized and specialized algorithms.",
  "problem_description_natural": "The paper addresses several routing problems in combinatorial optimization, primarily focusing on the Travelling Salesman Problem (TSP), where the goal is to find the shortest possible tour that visits each node exactly once and returns to the starting point. It also tackles two variants of the Vehicle Routing Problem (VRP), the Orienteering Problem (OP), and a stochastic variant of the Prize Collecting TSP (PCTSP). In these problems, the objective is to construct routes that optimize certain criteria—such as minimizing total distance or maximizing collected rewards—while respecting problem-specific constraints like vehicle capacity, time limits, or node visitation rules.",
  "problem_type": "combinatorial optimization",
  "datasets": [
    "Generated Euclidean TSP instances",
    "Nazari et al. VRP datasets",
    "Fischetti et al. OP prize distributions",
    "Balas PCTSP instances"
  ],
  "performance_metrics": [
    "Optimality Gap",
    "Tour Length",
    "Total Prize Collected",
    "Total Cost (tour length + penalties)"
  ],
  "lp_model": {
    "objective": "$\\min \\sum_{t=1}^{n-1} d(\\pi_t, \\pi_{t+1}) + d(\\pi_n, \\pi_1)$",
    "constraints": [
      "$\\pi_t \\in \\{1,\\ldots,n\\} \\quad \\forall t=1,\\ldots,n$",
      "$\\pi_t \\neq \\pi_{t'} \\quad \\forall t \\neq t'$"
    ],
    "variables": [
      "$\\pi_t$: index of node visited at position $t$ in the tour, for $t=1,\\ldots,n$"
    ]
  },
  "raw_latex_model": "$$\\min_{\\pi} \\sum_{t=1}^{n-1} d(\\pi_t, \\pi_{t+1}) + d(\\pi_n, \\pi_1) \\\\ \\text{subject to } \\pi_t \\in \\{1,\\ldots,n\\} \\forall t, \\\\ \\pi_t \\neq \\pi_{t'} \\forall t \\neq t'$$",
  "algorithm_description": "Attention-based encoder-decoder neural network model that defines a stochastic policy for generating solutions. Trained using REINFORCE policy gradient with a greedy rollout baseline to learn heuristics for combinatorial optimization problems."
}