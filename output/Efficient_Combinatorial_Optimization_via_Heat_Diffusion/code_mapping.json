{
  "file_path": "heo/opt.py, mvc.py, pbo.py, qubo.py, sat.py, tenaray_nn.py, variable_selection.py",
  "function_name": "heo, heo_mvc, eval, heo_train, heo_linear_reg",
  "code_snippet": "\n\n# ==========================================\n# File: heo/opt.py\n# Function/Context: heo\n# ==========================================\nimport numpy as np\nimport torch\nimport torch.optim as optim\n\n\nclass heo():\n    def __init__(self, dim, energy_func, opt_config, device='cuda:0'):\n        self.dim = dim\n        self.energy_func = energy_func\n        self.device = device\n        self.opt_config = opt_config\n\n    def phi(self, mu, sigma):\n        return torch.erf(mu / sigma / np.sqrt(2))\n    \n    def initialize(self):\n        self.theta = torch.ones(self.dim, device=self.device) * 0.5\n        self.theta.requires_grad = True\n        x = self.phi(self.theta - torch.rand(self.dim, device=self.device), 1)\n        self.energy = self.energy_func(x)\n    \n    def update(self, t, T):\n        sigma = (1 - t / T)\n        x = self.phi(self.theta - torch.rand(self.dim, device=self.device), sigma)\n        self.energy = self.energy_func(x)\n        self.energy.backward()\n        self.opt.step()\n        self.opt.zero_grad()\n        self.theta.data = torch.clamp(self.theta.data, 0, 1)\n\n    def solver(self, T, save_enegry=False):\n        self.initialize()\n        if self.opt_config['type'] == 'sgd':\n            self.opt=optim.SGD([self.theta], lr=self.opt_config['lr'], momentum=self.opt_config['momentum'])\n        elif self.opt_config['type'] == 'adam':\n            self.opt=optim.Adam([self.theta], lr=self.opt_config['lr'])\n        if save_enegry:\n            enegry_s = [self.energy.detach().cpu()]\n        for t in range(T):\n            self.update(t,T)\n            if save_enegry:\n                enegry_s.append(self.energy.detach().cpu())\n        if save_enegry:\n            return self.theta.detach().cpu(), enegry_s\n        else:\n            return self.theta.detach().cpu()\n\n# ==========================================\n# File: mvc.py\n# Function/Context: heo_mvc\n# ==========================================\nimport numpy as np\nimport torch\nimport time\nimport subprocess\nfrom collections import defaultdict\nimport re\nimport torch.optim as optim\n\nfrom heo.log import Logger\nimport os\n\nworkdir = 'mvc'\nos.makedirs(workdir,exist_ok=True)\nlogger = Logger(f'{workdir}/log.log','mvc')\n\nsize_pattern = re.compile(r'Final vertex cover size = (\\d+)')\ntime_pattern = re.compile(r'Time cost = (\\d+\\.\\d+)')\n\ndef refine(x):\n    for i,item in enumerate(x):\n        if item and x[G_dict[i]].all():\n            x[i] = False\n    return x\n\ndef phi(mu, sigma):\n    return torch.erf(mu / sigma)\n\ndef heo_mvc(dim, enegry_func, config):\n    p = torch.ones(dim, device='cuda:0') * 0.5\n    p.requires_grad = True\n    if config['type'] == 'sgd':\n        opt=optim.SGD([p], lr=config['lr'], momentum=config['momentum'])\n    elif config['type'] == 'adam':\n        opt=optim.Adam([p], lr=config['lr'])\n    for t in range(config['T']):\n        C = t / config['T'] * config['C_factor']\n        sigma = (1 - t / config['T']) * np.sqrt(2)\n        x = phi(p - torch.rand(dim, device='cuda:0'), sigma)\n        energy = enegry_func((x + 1) / 2, C)\n        energy.backward()\n        opt.step()\n        opt.zero_grad()\n        p.data = torch.clamp(p.data, 0, 1)\n    return p.detach().cpu()\n\ndef energy_func(x, C):\n    energy = C * (torch.sparse.mm(G, 1 - x.reshape(-1, 1)).reshape(-1) * (1 - x)).sum().reshape(-1) + x.sum()\n    return energy\n\n\nconfig = {'device':'cuda:0',\n          'type':'sgd',\n          'lr': 2.5,\n          'momentum':0.9999,\n          'times':10,\n          'C_factor':2.5,\n          'T':50}\n\nlogger.log(config)\n\nprob_path = r'example_data/tech-RL-caida.mtx'\ndim = 190914\n\nwith open(f'{prob_path}.mtx', 'r') as f:\n    prob_data = f.readlines()\nrows, cols = dim, dim\nu_s = []\nv_s = []\nG_dict = defaultdict(list)\nfor line in prob_data[2:]:\n    u, v = line.strip('\\n').split(' ')\n    u, v = int(u) - 1, int(v) - 1\n    u, v = min(u, v), max(u, v)\n    u_s.append(u)\n    v_s.append(v)\n    G_dict[u].append(v)\n    G_dict[v].append(u)\n\nindices = torch.LongTensor([u_s, v_s])\nvalues = torch.ones([len(v_s)]).float()\nG = torch.sparse.FloatTensor(indices, values, torch.Size([rows, cols])).cuda()\n\nresults = []\nruntimes = []\nfor i in range(config['times']):\n    start = time.time()\n    result = heo_mvc(dim, energy_func, config)\n    end = time.time()\n    x = (result > 0.5).int()\n    e1, e2 = energy_func(x.cuda().float(), C=0), energy_func(x.cuda().float(), C=100000)\n    if e2 < e1 + 1:\n        x = list(x.numpy())\n        with open(r'ver_cov', 'w', encoding='utf-8') as file:\n            file.writelines(['p ' + str(xx) + \"\\n\" for xx in x])\n        cpp_executable = \"./heo/vertex_cover_process/vertex\"\n        arg1 = f\"example_data/tech-RL-caida.mis\"\n        arg2 = f\"ver_cov\"\n        result_info = subprocess.run([cpp_executable, arg1, arg2], capture_output=True, text=True)\n        size_match = size_pattern.search(result_info.stdout)\n        time_match = time_pattern.search(result_info.stdout)\n        final_size = int(size_match.group(1))\n        time_cost = float(time_match.group(1))\n        results.append((final_size))\n        runtimes.append(end - start + time_cost)\nresults = np.array(results)\nruntimes = np.array(runtimes)\n\nlogger.log('{:.1f} ({:.1f}, {:.1f}), {})'.format(results.min(), results.mean(), results.std(),runtimes.mean()))\n\n# ==========================================\n# File: pbo.py\n# Function/Context: \n# ==========================================\nimport numpy as np\nfrom heo.opt import heo\nfrom heo.log import Logger\nimport torch\nimport os\n\nworkdir = 'pbo'\nos.makedirs(workdir,exist_ok=True)\nlogger = Logger(f'{workdir}/log.log','pbo')\n\ndim = 10000\ndim2 = 10000\n\ntorch.manual_seed(42)\n\na = 2 * torch.rand([dim]) - 1\nb = 2 * torch.rand([dim2]) - 1\nW = 2 * torch.rand([dim2, dim]) - 1\n\na = a.cuda()\nb = b.cuda()\nW = W.cuda()\n\nenergy_func = lambda x: (b * (torch.sigmoid(torch.matmul(W, x + a))).reshape(-1)).sum()\n\nconfig = {'device':'cuda:0',\n          'test_times':10,\n           'datapath': r'/example_data/qubo/rudy_all',\n           'unbounded':False,\n           'type':'sgd',\n           'lr':10,\n           'momentum':0.9999,\n           'T':5000\n           }\nlogger.log(config)\n\nsolver =  heo(dim=dim, energy_func=energy_func, opt_config=config)\np = solver.solver(T=config['T'])\nresult = 2 * (p > 0.5).float() - 1\nenergy = energy_func(result.cuda()).cpu().numpy()\n\nlogger.log('heo energy: {}'.format(energy))\n\n# ==========================================\n# File: qubo.py\n# Function/Context: eval\n# ==========================================\nimport numpy as np\nimport torch\nfrom heo.opt import heo\nfrom heo.log import Logger\nimport os\nfrom datetime import datetime\n\nfrom heo.other_methods import isingMac, Lqa\nimport pickle\n\nworkdir = 'qubo'\nos.makedirs(workdir,exist_ok=True)\nlogger = Logger(f'{workdir}/log.log','qubo')\n\n\ndef eval(method, couplings, config):\n    max_cut_s = []\n    if method in ['CIM','SIM-CIM']:\n        eigenvalues, _ = np.linalg.eig(couplings.cpu().numpy())\n        max_eigenvalue = np.real(max(eigenvalues))\n    for t in range(config['test_times']):\n        if method == 'lqa':\n            machine = Lqa(couplings, device='cuda:0')\n            machine.to(torch.device('cuda:0'))\n            energy = machine.minimise(step=1, N=config['T'], g=.1, f=0.1)\n        elif method in ['CIM','SIM-CIM']:\n            dt = 0.05 if method == 'CIM' else 1.\n            energy = isingMac(method, couplings, times=config['T'], dt=dt, max_eigenvalue=max_eigenvalue)\n        elif method in ['aSB','bSB','dSB']:\n            energy = isingMac(method, couplings, times=config['T'], dt=0.5, max_eigenvalue=None) \n        elif method in ['heo']:\n            def energy_func(x):\n                return torch.matmul(x.reshape(1, -1), torch.matmul(couplings, x.reshape(-1, 1))).reshape(-1)*0.5            \n            solver =  heo(dim=dim, energy_func=energy_func, opt_config=config)\n            p = solver.solver(T=config['T'])\n            result = 2 * (p > 0.5).float() - 1\n            energy = energy_func(result.cuda()).cpu().numpy()\n\n        max_cut_s.append(energy)\n    return max_cut_s\n\nconfig = {'device':'cuda:0',\n          'test_times':10,\n           'datapath': r'./example_data/rudy_all',\n           'unbounded':False,\n           'type':'sgd',\n           'lr':2,\n           'momentum':0,\n           'T':5000\n           }\n\nlogger.log(config)\n\nmethods = ['heo', 'lqa','CIM','SIM-CIM','aSB','bSB','dSB']\n\nprob_list = os.listdir(config['datapath'])\ninstance_num = len(prob_list)\n\nresult_dict = {}\nratio_s = 0\nratio_2_s = 0\nworst_12_s = 0\n\nfor method in methods:\n    result_dict[method] = np.zeros([instance_num,config['test_times']])\n\nfor inst_index in np.arange(instance_num): \n    logger.log(f'problem name: {prob_list[inst_index]}')\n    load_dir = r'{}/{}'.format(config['datapath'], prob_list[inst_index])\n    with open(load_dir) as f:\n        lines = f.readlines()\n\n    lines[0].strip('\\n').split(' ')\n\n    dim = int(lines[0].strip('\\n').split(' ')[0])\n    couplings = np.zeros([dim, dim])\n\n    for line in lines[1:]:\n        data = line.strip('\\n').split(' ')\n        if len(data) > 1:\n            u, v, w = data\n            u, v, w = int(u) - 1, int(v) - 1, int(w)\n            couplings[u, v] = w\n            couplings[v, u] = w\n\n    couplings = torch.from_numpy(couplings).float().cuda()\n\n    best_of_each_methods = []\n\n    for method in methods:\n        max_cut_s = eval(method, couplings, config)\n        result_dict[method][inst_index]=max_cut_s\n        logger.log(\"{} mean: {:.4f}, std: {:.4f}, min: {:.4f}\".format(method, np.mean(max_cut_s),\n                                                                            np.std(max_cut_s), np.min(max_cut_s)))\n        best = np.min(max_cut_s)\n        best_of_each_methods.append(best)\n    \n    best_of_each_methods = np.array(best_of_each_methods)\n\n    best_over_methods=np.min(best_of_each_methods)\n    ratio = (np.abs(best_of_each_methods-best_over_methods)/np.abs(best_over_methods))\n    ratio_s+=ratio\n    ratio_2_s+=ratio**2\n\n    worst_12 = []\n    for index, method in enumerate(methods):\n        if np.sum(best_of_each_methods>=best_of_each_methods[index])<=2:\n            worst_12.append(1)\n        else:\n            worst_12.append(0)\n    worst_12_s+=np.array(worst_12)\n\nworst_12_s.astype(np.float32)\nworst_12_s=worst_12_s/instance_num\nratio_s_mean =ratio_s/instance_num\nratio_2_s/=instance_num\nratio_s_std = np.sqrt(ratio_2_s-ratio_s_mean**2)\nlogger.log('-------------------------------------------------------------------------------')\nlogger.log(f'problem name: {prob_list[inst_index]}')\nfor index, method in enumerate(methods):\n    logger.log(\"{} ratio: {:.4f} ({:.4f}), worst 12 frequency {:.4f}\".format(\n        method, ratio_s_mean[index], ratio_s_std[index], worst_12_s[index]))\nlogger.log('-------------------------------------------------------------------------------')\n\nwith open(r'{}/result.pickle'.format(workdir), 'wb') as f:\n    pickle.dump(result_dict,f)\n\n# ==========================================\n# File: sat.py\n# Function/Context: \n# ==========================================\nimport numpy as np\nimport torch\nfrom heo.opt import heo\nfrom heo.log import Logger\nimport os\n\n\nworkdir = 'sat'\nos.makedirs(workdir,exist_ok=True)\nlogger = Logger(f'{workdir}/log.log','sat')\n\ndef SAT_original(x, index_list, factor_list):\n    return (((1 - x[index_list[:, 0]] * factor_list[:, 0]) * (1 - x[index_list[:, 1]] * factor_list[:, 1]) * (\n            1 - x[index_list[:, 2]] * factor_list[:, 2]) / 8)).sum()\n\n\ndef SAT_enegry(x, index_list, factor_list):\n    return (((1 - x[index_list[:, 0]] * factor_list[:, 0]) * (1 - x[index_list[:, 1]] * factor_list[:, 1]) * (\n            1 - x[index_list[:, 2]] * factor_list[:, 2]) / 8)**4).sum()\n\n\ndef formulate(prob):\n    with open(prob) as f:\n        data = f.readlines()\n    max_index = 0  \n    index_list = []\n    factor_list = []\n    for i, line in enumerate(data[8:-3]):\n        if i == 0:  \n            u, v, w = line.split(' ')[1:4]\n        else:\n            u, v, w = line.split(' ')[:3]\n        u, v, w = int(u), int(v), int(w)\n        max_index = np.max([np.abs(u), np.abs(v), np.abs(w), max_index])\n\n        u_sgn, v_sgn, w_sgn = np.sign(u), np.sign(v), np.sign(w)\n        u, v, w = np.abs(u) - 1, np.abs(v) - 1, np.abs(w) - 1\n        index_list.append([u, v, w])\n        factor_list.append(([u_sgn, v_sgn, w_sgn]))\n\n    index_list = np.array(index_list)\n    factor_list = torch.tensor(factor_list)  \n    return max_index, index_list, factor_list \n\n\nSAT_original_func = lambda x: SAT_original(x, index_list, factor_list)\nenergy_func = lambda x: SAT_enegry(x, index_list, factor_list)\n\n\nproblem_dicts = {\"250\":['uf250-1065/uf250-0',1065]}\ninst_s = np.arange(1, 11)\n\nvariable_dims = [\"250\"] \nconfig = {'device':'cuda:0',\n          'type':'sgd',\n          'lr':2,\n          'momentum':0.9999,'T':5000,'test_times':10,\n           'datapath': r'/home/mhy/SAT/'\n           }\n\n\nlogger.log(config)\n\n\nfor variable_dim in variable_dims:\n    logger.log(\"----------------------------variable_dim: {}--------------------------\".format(variable_dim))\n    enegry_s_s = []\n    prob_path = config['datapath']+problem_dicts[variable_dim][0]\n    clause_num = problem_dicts[variable_dim][1]\n    for inst in inst_s:\n        dim, index_list, factor_list = formulate(r'{}{}.cnf'.format(prob_path, inst))\n        factor_list = factor_list.to(config['device'])\n\n        solver =  heo(dim=dim, energy_func=energy_func, opt_config=config)\n\n        energy_s = []\n        for t in range(config['test_times']):\n            p = solver.solver(T=config['T'])\n            result = 2 * (p > 0.5) - 1\n            enegry = SAT_original_func(result.to(config['device'])).cpu().numpy()\n            energy_s.append(enegry)\n            print(enegry)\n        percent = 1 - np.array(energy_s) / index_list.shape[0]\n        logger.log(\"inst: {:d}, percent mean: {:.4f}, std: {:.4f}, sat all rate: {:.4f}\".format(\n            inst, np.mean(percent), np.std(percent), np.mean(percent==1)))\n        enegry_s_s.append(energy_s)\n    \n    enegry_s_s = np.array(enegry_s_s)\n    mean_sat = (1-enegry_s_s/clause_num).mean()\n    std_sat = (1-enegry_s_s/clause_num).std()\n    mean_all_sat = np.mean(enegry_s_s==0)\n    logger.log('satisfying rate: {:.4f} ({:.4f}), satisfying all rate: {}'.format(mean_sat,std_sat,mean_all_sat))\n\n    np.save(r'{}/result_{}'.format(workdir,variable_dim), enegry_s_s)\n\n# ==========================================\n# File: tenaray_nn.py\n# Function/Context: heo_train\n# ==========================================\nimport numpy as np\nimport torch\nimport torch.optim as optim\nimport os\nfrom heo.log import Logger\n\nworkdir = 'nn'\nos.makedirs(workdir,exist_ok=True)\nlogger = Logger(f'{workdir}/log.log','nn')\n\ndef phi(mu, sigma):\n    return torch.erf(mu / sigma)\n\ndef heo_train(dim, config, rep_energy=False):\n    p1 = torch.rand([dim2, dim], device='cuda:0')\n    p2 = torch.rand([dim2, dim], device='cuda:0')\n    p1.requires_grad = True\n    p2.requires_grad = True\n\n    opt = optim.SGD([p1,p2],lr=config['lr'],momentum=config['momentum'])\n\n    if rep_energy:\n        energy_s = []\n        uncern_s = []\n    for t in range(config['T']):\n        sigma = (1 - t / config['T']) * np.sqrt(2)\n        W = (phi(p1 - torch.rand([dim2, dim], device='cuda:0'), sigma) +\n             phi(p2 - torch.rand([dim2, dim], device='cuda:0'), sigma)) / 2\n        energy = ((new_work_ensemble(W, x) - y_real) ** 2).mean()\n        energy.backward()\n\n        opt.step()\n        opt.zero_grad()\n\n        p1.data = torch.clamp(p1.data, 0, 1)\n        p2.data = torch.clamp(p2.data, 0, 1)\n\n        if rep_energy:\n            energy_s.append(energy.detach().cpu().numpy())\n            uncern_s.append(((p1 * (1 - p1)).sum() + (p2 * (1 - p2)).sum()).detach().cpu().numpy())\n    else:\n        if rep_energy:\n            return p1.detach().cpu(), p2.detach().cpu(), energy_s, uncern_s\n        else:\n            return p1.detach().cpu(), p2.detach().cpu()\n\n\nconfig = {'test_times':10,'T':10000, 'lr':0.5,'momentum':0.99}\n\ndim = 100\ndim2 = 5\n\nrep_enegry = False\nbatch_size_s = [10, 20, 50, 100, 200, 500, 1000, 2000]\n\n\nacc_mean_s = []\nacc_std_s = []\nfor batch_size in batch_size_s:\n    acc_s = []\n    for _ in range(config['test_times']):\n        logger.log(f'batch size: {batch_size}')\n\n        W_real = torch.randint(-1, 2, [dim2, dim]).float()\n        W_real = W_real.cuda()\n\n        new_work_ensemble = lambda W, x: torch.relu(torch.matmul(W, x))\n        new_work_real_ensemble = lambda x: torch.relu(torch.matmul(W_real, x))\n\n        x = (torch.rand([dim, batch_size]).cuda() > 0.5).float() + (torch.rand([dim, batch_size]).cuda() > 0.5).float() - 1\n        y_real = new_work_real_ensemble(x)\n\n        correction = False\n        energy_s = []\n\n        p1, p2 = heo_train(dim, config, rep_energy=False)\n        result_W = (p1 > 0.5).float() + (p2 > 0.5).float() - 1\n        acc = ((result_W.cuda() == W_real).float()).mean().cpu().numpy()\n        logger.log(f'acc weight: {acc}')\n        acc_s.append(acc)\n    acc_mean_s.append(np.mean(acc_s))\n    acc_std_s.append(np.std(acc_s))\nnp.savez(f'{workdir}/three_value_acc_{dim}_{dim2}', acc_mean_s=np.array(acc_mean_s), acc_std_s=np.array(acc_std_s), batch_size_s=batch_size_s)\n\n# ==========================================\n# File: variable_selection.py\n# Function/Context: heo_linear_reg\n# ==========================================\nimport numpy as np\nimport torch\nimport torch.optim as optim\n\ndef phi(mu, sigma):\n    return torch.erf(mu / sigma)\n\ndef heo_linear_reg(dim, x_s, y_s, config):\n    theta = torch.ones(dim, device='cuda:0')\n    beta = torch.zeros(dim, device='cuda:0')\n    ratio_lr = config['steps']\n    theta.requires_grad = True\n    beta.requires_grad = True\n    opt1 = optim.SGD([theta],lr=config['lr'],momentum=config['momentum'])\n    opt2 = optim.SGD([beta],lr=config['lr']/ratio_lr,momentum=config['momentum'])\n\n    for t in range(config['steps']):\n        sigma = (1 - t / config['steps']) * np.sqrt(2)\n        x = (phi(theta - torch.rand(dim, device='cuda:0'), sigma) + 1) / 2\n        pred_y = ((x * beta).reshape(-1, 1) * x_s).sum(dim=0)\n        energy = ((y_s - pred_y) ** 2).mean()\n        energy.backward()\n        opt1.step()\n        opt2.step()\n        opt1.zero_grad()\n        opt2.zero_grad()\n\n        theta.data = torch.clamp(theta.data, 0, 1)\n    return theta.detach().cpu(), beta.detach().cpu()",
  "description": "Combined Analysis:\n- [heo/opt.py]: This file implements the core Heat Diffusion Optimization (HeO) algorithm from the paper. It transforms discrete combinatorial optimization into a continuous differentiable problem via Bernoulli relaxation (θ ∈ [0,1]^n). The key components are: 1) φ function (error function) implementing the heat diffusion transformation, 2) annealing schedule (σ = 1 - t/T) controlling diffusion strength, 3) stochastic gradient estimation via reparameterization (θ - rand), 4) gradient-based optimization with projection (clamp). The implementation directly corresponds to Algorithm 1 in the paper, where the smoothed objective u(τ,θ) is optimized via gradient descent on θ with decreasing diffusion parameter σ.\n- [mvc.py]: This file implements the Heat Diffusion Optimization (HeO) framework for the Minimum Vertex Cover (MVC) problem, aligning with the paper's core logic. The key components are:\n1. Continuous Relaxation: Binary variables are relaxed to continuous parameters p ∈ [0,1]^n (initialized at 0.5).\n2. Heat Diffusion Transformation: The phi function uses the error function (torch.erf) to transform the relaxed variables, with a time-decaying sigma parameter simulating heat diffusion.\n3. Gradient-Based Optimization: The heo_mvc function performs gradient descent (SGD/Adam) on the transformed energy function, with projection (clamping) to maintain p in [0,1].\n4. Energy Function: The MVC objective is formulated as a pseudo-Boolean function with a penalty term (C) for uncovered edges, consistent with the paper's handling of constraints via penalty methods.\n5. Algorithm Steps: The loop over T iterations anneals sigma and adjusts the penalty strength C, matching the heat diffusion process described in the paper.\nThe implementation directly corresponds to the HeO framework's use of heat diffusion to transform the objective while preserving optima, enabling efficient gradient-based search in the continuous space.\n- [pbo.py]: This file implements a pseudo-Boolean optimization problem and applies the Heat Diffusion Optimization (HeO) framework from the paper. The core logic is encapsulated in the imported 'heo' module, which performs gradient-based optimization on a continuous relaxation of binary variables using heat diffusion to transform the objective function. The code sets up an energy function, configures the HeO solver, and executes it to find a binary solution, aligning with the paper's methodology of preserving optima while enabling efficient information propagation.\n- [qubo.py]: This file implements a benchmarking framework for comparing Heat Diffusion Optimization (HeO) against other methods on QUBO/Max-Cut problems. The core HeO implementation is called via the 'heo' function from the heo.opt module. The file demonstrates the complete workflow: 1) Problem loading and preprocessing (Rudy format QUBO instances), 2) Configuration of HeO parameters (device, learning rate, iterations T=5000), 3) Energy function definition for the QUBO problem, 4) Execution of HeO solver with continuous relaxation (parameter p ∈ [0,1]^n), 5) Projection back to discrete solution (2*(p>0.5)-1), and 6) Performance evaluation against baseline methods. This matches the paper's methodology of transforming discrete optimization into continuous gradient-based optimization via heat diffusion.\n- [sat.py]: This file implements the Heat Diffusion Optimization (HeO) framework for solving 3-SAT problems. It directly maps to the paper's core logic: (1) Defines the original combinatorial objective (SAT_original) over binary variables (encoded as ±1), (2) Applies a heat-diffusion-inspired transformation (SAT_energy) by raising the clause satisfaction term to the 4th power, preserving optima while smoothing the landscape, (3) Uses the HeO solver (heo) to perform gradient-based optimization on the continuous relaxation (Bernoulli parameters θ), and (4) Projects continuous solutions back to discrete binary assignments. The code follows the paper's methodology of transforming the objective via heat diffusion, continuous relaxation, and gradient-based optimization with projection.\n- [tenaray_nn.py]: This file implements the core Heat Diffusion Optimization (HeO) framework for ternary-valued neural network weight optimization. Key matches include: 1) Continuous relaxation of discrete ternary variables {-1,0,1} via two Bernoulli parameters (p1, p2) ∈ [0,1]; 2) Heat diffusion transformation using the error function (torch.erf) as the heat kernel with decaying diffusion coefficient sigma; 3) Gradient-based optimization (SGD) on the transformed objective (MSE loss) with projection (clamping) to maintain feasibility; 4) Preservation of optima through the heat equation solution, enabling information propagation from distant solution regions. The implementation specifically solves a combinatorial optimization problem of recovering ternary weights from input-output data, aligning with the paper's methodology for pseudo-Boolean optimization with multi-valued variables.\n- [variable_selection.py]: This file implements the core Heat Diffusion Optimization (HeO) algorithm for variable selection in linear regression, aligning with the paper's methodology. The function `heo_linear_reg` transforms the discrete binary variable selection problem into a continuous optimization via Bernoulli relaxation (parameter `theta`). Heat diffusion is applied through the `phi` function (error function) with a time-dependent `sigma` that decreases linearly, smoothing the objective. Gradient descent (SGD with momentum) optimizes the transformed mean squared error, with projection via clamping to maintain feasibility. The algorithm handles combinatorial optimization by enabling gradient-based search over relaxed variables, using heat diffusion to propagate global information.",
  "dependencies": [
    "numpy",
    "heo.log.Logger",
    "collections.defaultdict",
    "re",
    "os",
    "pickle",
    "datetime",
    "heo.other_methods.isingMac",
    "heo.other_methods.Lqa",
    "torch.optim",
    "phi",
    "subprocess",
    "torch",
    "heo.opt.heo",
    "time"
  ]
}