{
  "file_path": "model_classes.py, so-ebm.py",
  "function_name": "SolveScheduling, main",
  "code_snippet": "\n\n# ==========================================\n# File: model_classes.py\n# Function/Context: SolveScheduling\n# ==========================================\nimport numpy as np\nimport scipy.stats as st\nimport operator\nfrom functools import reduce\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Function\nfrom torch.nn.parameter import Parameter\nimport torch.optim as optim\n\nfrom qpth.qp import QPFunction\nfrom constants import *\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n    def __init__(self, X, Y, hidden_layer_sizes):\n        super(Net, self).__init__()\n\n        # Linear -> BatchNorm -> ReLU -> Dropout layers\n        layer_sizes = [X.shape[1]] + hidden_layer_sizes\n        layers = reduce(operator.add, \n            [[nn.Linear(a,b), nn.BatchNorm1d(b), nn.ReLU(), nn.Dropout(p=0.2)] \n                for a,b in zip(layer_sizes[0:-1], layer_sizes[1:])])\n        layers += [nn.Linear(layer_sizes[-1], Y.shape[1]*2)]\n        self.net = nn.Sequential(*layers)\n                \n    def forward(self, x):\n        prediction = self.net(x)\n        mu = prediction[:,0:24]\n        sigma = prediction[:,24:]\n        sigma = F.softplus(sigma)+1e-6\n        \n        return mu, sigma\n            \n            \n\ndef GLinearApprox(gamma_under, gamma_over):\n    #Linear (gradient) approximation of G function at z\n    class GLinearApproxFn(Function):\n        @staticmethod    \n        def forward(ctx, z, mu, sig):\n            ctx.save_for_backward(z, mu, sig)\n            p = st.norm(mu.cpu().numpy(),sig.cpu().numpy())\n            res = torch.DoubleTensor((gamma_under + gamma_over) * p.cdf(\n                z.cpu().numpy()) - gamma_under)\n            if USE_GPU:\n                res = res.cuda()\n            return res\n        \n        @staticmethod\n        def backward(ctx, grad_output):\n            z, mu, sig = ctx.saved_tensors\n            p = st.norm(mu.cpu().numpy(),sig.cpu().numpy())\n            pz = torch.tensor(p.pdf(z.cpu().numpy()), dtype=torch.double, device=DEVICE)\n            \n            dz = (gamma_under + gamma_over) * pz\n            dmu = -dz\n            dsig = -(gamma_under + gamma_over)*(z-mu) / sig * pz\n            return grad_output * dz, grad_output * dmu, grad_output * dsig\n\n    return GLinearApproxFn.apply\n\ndef GQuadraticApprox(gamma_under, gamma_over):\n    \"\"\" Quadratic (gradient) approximation of G function at z\"\"\"\n    class GQuadraticApproxFn(Function):\n        @staticmethod\n        def forward(ctx, z, mu, sig):\n            ctx.save_for_backward(z, mu, sig)\n            p = st.norm(mu.cpu().numpy(),sig.cpu().numpy())\n            res = torch.DoubleTensor((gamma_under + gamma_over) * p.pdf(\n                z.cpu().numpy()))\n            if USE_GPU:\n                res = res.cuda()\n            return res\n        \n        @staticmethod\n        def backward(ctx, grad_output):\n            z, mu, sig = ctx.saved_tensors\n            p = st.norm(mu.cpu().numpy(),sig.cpu().numpy())\n            pz = torch.tensor(p.pdf(z.cpu().numpy()), dtype=torch.double, device=DEVICE)\n            \n            dz = -(gamma_under + gamma_over) * (z-mu) / (sig**2) * pz\n            dmu = -dz\n            dsig = (gamma_under + gamma_over) * ((z-mu)**2 - sig**2) / \\\n                (sig**3) * pz\n            \n            return grad_output * dz, grad_output * dmu, grad_output * dsig\n\n    return GQuadraticApproxFn.apply\n\n\nclass SolveSchedulingQP(nn.Module):\n    \"\"\" Solve a single SQP iteration of the scheduling problem\"\"\"\n    def __init__(self, params):\n        super(SolveSchedulingQP, self).__init__()\n        self.c_ramp = params[\"c_ramp\"]\n        self.n = params[\"n\"]\n        D = np.eye(self.n - 1, self.n) - np.eye(self.n - 1, self.n, 1)\n        self.G = torch.tensor(np.vstack([D,-D]), dtype=torch.double, device=DEVICE)\n        self.h = (self.c_ramp * torch.ones((self.n - 1) * 2, device=DEVICE)).double()\n        self.e = torch.DoubleTensor()\n        if USE_GPU:\n            self.e = self.e.cuda()\n        \n    def forward(self, z0, mu, dg, d2g):\n        nBatch, n = z0.size()\n        \n        Q = torch.cat([torch.diag(d2g[i] + 1).unsqueeze(0) \n            for i in range(nBatch)], 0).double()\n        p = (dg - d2g*z0 - mu).double()\n        G = self.G.unsqueeze(0).expand(nBatch, self.G.size(0), self.G.size(1))\n        h = self.h.unsqueeze(0).expand(nBatch, self.h.size(0))\n        \n        out = QPFunction(verbose=False)(Q, p, G, h, self.e, self.e)\n        return out\n\n\nclass SolveScheduling(nn.Module):\n    \"\"\" Solve the entire scheduling problem, using sequential quadratic \n        programming. \"\"\"\n    def __init__(self, params):\n        super(SolveScheduling, self).__init__()\n        self.params = params\n        self.c_ramp = params[\"c_ramp\"]\n        self.n = params[\"n\"]\n        \n        D = np.eye(self.n - 1, self.n) - np.eye(self.n - 1, self.n, 1)\n        self.G = torch.tensor(np.vstack([D,-D]), dtype=torch.double, device=DEVICE)\n        self.h = (self.c_ramp * torch.ones((self.n - 1) * 2, device=DEVICE)).double()\n        self.e = torch.DoubleTensor()\n        if USE_GPU:\n            self.e = self.e.cuda()\n        \n    def forward(self, mu, sig):\n        nBatch, n = mu.size()\n        \n        # Find the solution via sequential quadratic programming, \n        # not preserving gradients\n        z0 = mu.detach() \n        mu0 = mu.detach() \n        sig0 = sig.detach() \n        for i in range(20):\n            dg = GLinearApprox(self.params[\"gamma_under\"], \n                self.params[\"gamma_over\"])(z0, mu0, sig0)\n            d2g = GQuadraticApprox(self.params[\"gamma_under\"], \n                self.params[\"gamma_over\"])(z0, mu0, sig0)\n            z0_new = SolveSchedulingQP(self.params)(z0, mu0, dg, d2g)\n            solution_diff = (z0-z0_new).norm().item()\n            #print(\"+ SQP Iter: {}, Solution diff = {}\".format(i, solution_diff))\n            z0 = z0_new\n            if solution_diff < 1e-10:\n                break\n                  \n        # Now that we found the solution, compute the gradient-propagating \n        # version at the solution\n        dg = GLinearApprox(self.params[\"gamma_under\"], \n            self.params[\"gamma_over\"])(z0, mu, sig)\n        d2g = GQuadraticApprox(self.params[\"gamma_under\"], \n            self.params[\"gamma_over\"])(z0, mu, sig)\n        return SolveSchedulingQP(self.params)(z0, mu, dg, d2g)\n\n# ==========================================\n# File: so-ebm.py\n# Function/Context: main\n# ==========================================\nimport argparse\nimport os\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime as dt\nimport pytz\nfrom pandas.tseries.holiday import USFederalHolidayCalendar\nimport sys\ntry: import setGPU\nexcept ImportError: pass\nimport torch\nimport model_classes, nets\nfrom constants import *    \nimport cvxpy as cp\nimport torch.optim as optim\nimport math   \nimport time\nimport logging\nfrom datetime import datetime\n\n\ndef load_data_with_features(filename):\n    tz = pytz.timezone(\"America/New_York\")\n    df = pd.read_csv(filename, sep=\" \", header=None, usecols=[1,2,3], \n        names=[\"time\",\"load\",\"temp\"])\n    df[\"time\"] = df[\"time\"].apply(dt.fromtimestamp, tz=tz)\n    df[\"date\"] = df[\"time\"].apply(lambda x: x.date())\n    df[\"hour\"] = df[\"time\"].apply(lambda x: x.hour)\n    df.drop_duplicates(\"time\", inplace=True)\n\n    # Create one-day tables and interpolate missing entries\n    df_load = df.pivot(index=\"date\", columns=\"hour\", values=\"load\")\n    df_temp = df.pivot(index=\"date\", columns=\"hour\", values=\"temp\")\n    df_load = df_load.transpose().fillna(method=\"backfill\").transpose()\n    df_load = df_load.transpose().fillna(method=\"ffill\").transpose()\n    df_temp = df_temp.transpose().fillna(method=\"backfill\").transpose()\n    df_temp = df_temp.transpose().fillna(method=\"ffill\").transpose()\n\n    holidays = USFederalHolidayCalendar().holidays(\n        start='2008-01-01', end='2016-12-31').to_pydatetime()\n    holiday_dates = set([h.date() for h in holidays])\n\n    s = df_load.reset_index()[\"date\"]\n    data={\"weekend\": s.apply(lambda x: x.isoweekday() >= 6).values,\n          \"holiday\": s.apply(lambda x: x in holiday_dates).values,\n          \"dst\": s.apply(lambda x: tz.localize(\n            dt.combine(x, dt.min.time())).dst().seconds > 0).values,\n          \"cos_doy\": s.apply(lambda x: np.cos(\n            float(x.timetuple().tm_yday)/365*2*np.pi)).values,\n          \"sin_doy\": s.apply(lambda x: np.sin(\n            float(x.timetuple().tm_yday)/365*2*np.pi)).values}\n    df_feat = pd.DataFrame(data=data, index=df_load.index)\n\n    # Construct features and normalize (all but intercept)\n    X = np.hstack([df_load.iloc[:-1].values,        # past load\n                    df_temp.iloc[:-1].values,       # past temp\n                    df_temp.iloc[:-1].values**2,    # past temp^2\n                    df_temp.iloc[1:].values,        # future temp\n                    df_temp.iloc[1:].values**2,     # future temp^2\n                    df_temp.iloc[1:].values**3,     # future temp^3\n                    df_feat.iloc[1:].values,        \n                    np.ones((len(df_feat)-1, 1))]).astype(np.float64)\n    # X[:,:-1] = \\\n    #     (X[:,:-1] - np.mean(X[:,:-1], axis=0)) / np.std(X[:,:-1], axis=0)\n\n    Y = df_load.iloc[1:].values\n\n    return X, Y\n\n\ndef gaussian_pdf(mean, sig, z):\n    \n    var = sig**2\n    denom = (2*math.pi*var)**0.5\n    norm = torch.exp(-(z-mean)**2/(2*var))\n    pdf = norm/denom\n    return pdf\n\n\ndef gaussian_cdf(mean, sig, z):\n    x = (z - mean)/(sig*math.sqrt(2))\n    cdf = 0.5 * (1+torch.erf(x))    \n    return cdf\n\n\ndef task_loss_expectation(Y_sched, mean, sig, params):\n    \n    pdf = gaussian_pdf(mean, sig, Y_sched)\n    cdf = gaussian_cdf(mean, sig, Y_sched)\n    \n    loss = (params[\"gamma_under\"]+params[\"gamma_over\"])*((sig**2*pdf) + (Y_sched-mean)*cdf) \\\n            - params[\"gamma_under\"]*(Y_sched-mean) + 0.5*((Y_sched-mean)**2+sig**2)\n    \n    return loss\n\n\ndef task_loss(Y_sched, Y_actual, params):\n    return (params[\"gamma_under\"] * torch.clamp(Y_actual - Y_sched, min=0) + \n            params[\"gamma_over\"] * torch.clamp(Y_sched - Y_actual, min=0) + \n            0.5 * (Y_sched - Y_actual)**2)\n    \n    \n\ndef langevin_dynamics(model, Z, variables, params, steps=32, step_size=0.1, num_samples=1):\n    model.eval()\n    for p in model.parameters():\n        p.requires_grad = False\n    \n    noise = torch.randn(Z.shape, device=Z.device)\n    mean, sig = model(variables['X_train_'])\n    Z = Z.repeat(num_samples, 1)\n    Z.requires_grad = True\n    mean, sig = mean.repeat(num_samples, 1), sig.repeat(num_samples, 1)\n    for _ in range(steps):\n        \n        noise.normal_(0, 0.01)\n        Z.data.add_(noise.data)\n        out_Z = task_loss_expectation(Z, mean, sig, params).sum(1).mean()\n        out_Z.backward()\n        \n        Z.data.add_(-step_size * Z.grad.data)\n        Z.grad.detach_()\n        Z.grad.zero_()\n        \n        \n    for p in model.parameters():\n        p.requires_grad = True   \n    model.train()\n\n    return Z\n\n\ndef gauss_density_centered(x, std):\n    return torch.exp(-0.5*(x / std)**2) / (math.sqrt(2*math.pi)*std)\n\n\ndef gmm_density_centered(x, std):\n    \"\"\"\n    Assumes dim=-1 is the component dimension and dim=-2 is feature dimension. Rest are sample dimension.\n    \"\"\"\n    if x.dim() == std.dim() - 1:\n        x = x.unsqueeze(-1)\n    elif not (x.dim() == std.dim() and x.shape[-1] == 1):\n        raise ValueError('Last dimension must be the gmm stds.')\n    return gauss_density_centered(x, std).prod(-2).mean(-1)\n\n\ndef sample_gmm_centered(std, num_samples=1):\n    num_components = std.shape[-1]\n    num_dims = std.numel() // num_components\n\n    std = std.view(1, num_dims, num_components)\n\n    # Sample component ids\n    k = torch.randint(num_components, (num_samples,), dtype=torch.int64)\n    std_samp = std[0,:,k].t()\n\n    # Sample\n    x_centered = std_samp * torch.randn(num_samples, num_dims)\n    prob_dens = gmm_density_centered(x_centered, std)\n\n    return x_centered, prob_dens\n\n\n# Main training loop implementing SO-EBM algorithm\n# This implements the coupled training objective with MLE for optimal decisions\n# and KL-divergence regularization for the energy landscape\n# Using self-normalized importance sampling with Gaussian mixture proposal\n\n        # (shape: batch_size*num_samples, 24)     \n        E_gt = task_loss_expectation(variables['Z_train_'], mu_pred, sig_pred, params).sum(1)\n        negative_samples = langevin_dynamics(model, variables['Z_train_'], variables, params, args.steps, args.step_size, args.num_samples_mle)\n\n        E_model = task_loss_expectation(negative_samples, mu_pred, sig_pred, params).sum(1)\n        MLE = E_gt.mean(0) - E_model.mean(0)\n        \n        Z_samples_zero, q_Z_samples = sample_gmm_centered(stds, num_samples=args.num_samples_kld*24)\n        Z_samples_zero = Z_samples_zero.to(DEVICE) # (shape: (num_samples*24,1))\n        Z_samples_zero = Z_samples_zero.view(1, args.num_samples_kld, 24) #(shape: (1, num_samples, 24))\n        \n        q_Z_samples = q_Z_samples.view(1, args.num_samples_kld, 24)\n        \n        Z_s = variables['Z_train_'].view(-1,1, 24) #(shape: (batch_size,1, 24))\n        Z_samples = Z_s + Z_samples_zero # (shape: (batch_size, num_samples,24))\n        q_Z_samples = q_Z_samples*torch.ones(Z_samples.size())\n        q_Z_samples = q_Z_samples.to(DEVICE) # (shape:(batch_size, num_samples, 24))\n        \n        Y_s = variables['Y_train_']\n        #(shape: (batch_size, num_samples, 24))\n        Y_train_expand = Y_s[:,None,:].expand(variables['Y_train_'].shape[0], args.num_samples_kld, 24)\n        #(shape: (batch_size*num_samples,24))\n        p_Z_samples = task_loss(Z_samples.view(-1,24), Y_train_expand.reshape(-1,24), params)\n        #p_Z_samples = torch.exp(-p_Z_samples)\n        p_Z_samples = p_Z_samples.view(-1, args.num_samples_kld, 24)\n        \n        \n        mu_pred_expand = mu_pred[:,None,:].expand(mu_pred.shape[0], args.num_samples_kld, 24)\n        sig_pred_expand = sig_pred[:,None,:].expand(sig_pred.shape[0], args.num_samples_kld, 24)\n        \n        \n        scores_samples = task_loss_expectation(Z_samples.view(-1,24), mu_pred_expand.reshape(-1,24), sig_pred_expand.reshape(-1,24), params)\n        scores_samples = scores_samples.view(-1, args.num_samples_kld, 24)\n        \n        # weight_kld = p_Z_samples/q_Z_samples\n        # weight_kld = weight_kld/torch.sum(weight_kld, dim=1, keepdim=True)\n        \n        q_Z_samples = torch.prod(q_Z_samples, dim=2)",
  "description": "Combined Analysis:\n- [model_classes.py]: This file implements the core optimization logic for the stochastic energy scheduling problem described in the paper. Key components:\n1. **Net class**: Predicts Gaussian parameters (Œº, œÉ) for demand distribution from input features.\n2. **GLinearApprox/GQuadraticApprox**: Compute first/second derivatives of the expected cost function ùîº[G(z,y)] where G(z,y) = Œ≥_under[y-z]_+ + Œ≥_over[z-y]_+ + ¬Ω(z-y)¬≤, assuming y ~ N(Œº,œÉ¬≤). These implement the gradient/Hessian approximations needed for SQP.\n3. **SolveSchedulingQP**: Solves a single quadratic programming subproblem with ramp constraints |a_i - a_{i-1}| ‚â§ c_r.\n4. **SolveScheduling**: Implements sequential quadratic programming (SQP) to solve the full constrained optimization problem min_a ùîº[G(a,y)], using the above components. This directly corresponds to the paper's optimization model with ramp constraints.\n\nThe code enables gradient flow through the optimization layer (via QPFunction and custom autograd Functions), supporting end-to-end learning in two-stage and DFL baselines. However, it does not implement the SO-EBM's energy-based model or self-normalized importance sampling‚Äîthose are in separate files (so-ebm.py).\n- [so-ebm.py]: This file implements the core SO-EBM algorithm from the paper. Key components include: 1) The task loss expectation function that computes the expected cost under Gaussian uncertainty (matching the paper's objective), 2) Langevin dynamics for sampling from the energy-based model, 3) Gaussian mixture model proposal for importance sampling in KL-divergence regularization, 4) The coupled training objective combining MLE for optimal decisions and KL-divergence regularization. The implementation follows the paper's description of replacing the implicit optimization layer with an energy-based surrogate and using self-normalized importance sampling with a Gaussian mixture proposal.",
  "dependencies": [
    "setGPU",
    "sys",
    "torch.optim",
    "GQuadraticApprox",
    "nets",
    "pandas.tseries.holiday",
    "qpth.qp.QPFunction",
    "constants",
    "time",
    "logging",
    "torch.nn.functional",
    "cvxpy",
    "pytz",
    "math",
    "os",
    "datetime",
    "SolveSchedulingQP",
    "argparse",
    "model_classes",
    "GLinearApprox",
    "numpy",
    "pandas",
    "torch",
    "scipy.stats"
  ]
}