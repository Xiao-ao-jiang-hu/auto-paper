{
  "file_path": "eval_atsp/ASHPPModel.py, eval_atsp/ATSPModel.py, heatmap/cvrp/train.py, main.py, nets/attention_local.py, nets/partition_net.py, problems/cvrp.py, problems/pctsp.py, utils/functions.py",
  "function_name": "ASHPPModel, ATSPModel, train, eval_dataset, _eval_dataset, AttentionModel, Net, init, init, LCP_TSP, reconnect",
  "code_snippet": "\n\n# ==========================================\n# File: eval_atsp/ASHPPModel.py\n# Function/Context: ASHPPModel\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ATSPModel_LIB import AddAndInstanceNormalization, FeedForward, MixedScore_MultiHeadAttention\n\n\nclass ASHPPModel(nn.Module):\n\n    def __init__(self, **model_params):\n        super().__init__()\n        self.model_params = model_params\n\n        self.encoder = ATSP_Encoder(**model_params)\n        self.decoder = ATSP_Decoder(**model_params)\n\n        self.encoded_row = None\n        self.encoded_col = None\n        # shape: (batch, node, embedding)\n\n    def pre_forward(self, reset_state):\n\n        problems = reset_state.problems\n        # problems.shape: (batch, node, node)\n\n        batch_size = problems.size(0)\n        node_cnt = problems.size(1)\n        embedding_dim = self.model_params['embedding_dim']\n\n        row_emb = torch.zeros(size=(batch_size, node_cnt, embedding_dim))\n        # emb.shape: (batch, node, embedding)\n        col_emb = torch.zeros(size=(batch_size, node_cnt, embedding_dim))\n        # shape: (batch, node, embedding)\n\n        seed_cnt = self.model_params['one_hot_seed_cnt']\n        rand = torch.rand(batch_size, seed_cnt)\n        batch_rand_perm = rand.argsort(dim=1)\n        rand_idx = batch_rand_perm[:, :node_cnt]\n\n        b_idx = torch.arange(batch_size)[:, None].expand(batch_size, node_cnt)\n        n_idx = torch.arange(node_cnt)[None, :].expand(batch_size, node_cnt)\n        col_emb[b_idx, n_idx, rand_idx] = 1\n        # shape: (batch, node, embedding)\n\n        self.encoded_row, self.encoded_col = self.encoder(row_emb, col_emb, problems)\n        # encoded_nodes.shape: (batch, node, embedding)\n\n        self.decoder.set_kv(self.encoded_col)\n\n    def forward(self, state):\n\n        batch_size = state.BATCH_IDX.size(0)\n        pomo_size = state.BATCH_IDX.size(1)\n\n        if (state.current_node == 0).all():\n            encoded_last_row = _get_encoding(self.encoded_row, state.last_node)\n            self.decoder.set_q1(encoded_last_row)\n\n        encoded_current_row = _get_encoding(self.encoded_row, state.current_node)\n            \n        # shape: (batch, pomo, embedding)\n        all_job_probs = self.decoder(encoded_current_row, ninf_mask=state.ninf_mask)\n        # shape: (batch, pomo, job)\n\n        if self.training or self.model_params['eval_type'] == 'softmax':\n            while True:  # to fix pytorch.multinomial bug on selecting 0 probability elements\n                with torch.no_grad():\n                    selected = all_job_probs.reshape(batch_size * pomo_size, -1).multinomial(1) \\\n                        .squeeze(dim=1).reshape(batch_size, pomo_size)\n                    # shape: (batch, pomo)\n\n                prob = all_job_probs[state.BATCH_IDX, state.POMO_IDX, selected] \\\n                    .reshape(batch_size, pomo_size)\n                # shape: (batch, pomo)\n\n                if (prob != 0).all():\n                    break\n        else:\n            assert self.model_params['eval_type'] == 'greedy'\n            selected = all_job_probs.argmax(dim=2)\n            # shape: (batch, pomo)\n            prob = None\n\n        return selected, prob\n\n\ndef _get_encoding(encoded_nodes, node_index_to_pick):\n    # encoded_nodes.shape: (batch, problem, embedding)\n    # node_index_to_pick.shape: (batch, pomo)\n\n    batch_size = node_index_to_pick.size(0)\n    pomo_size = node_index_to_pick.size(1)\n    embedding_dim = encoded_nodes.size(2)\n\n    gathering_index = node_index_to_pick[:, :, None].expand(batch_size, pomo_size, embedding_dim)\n    # shape: (batch, pomo, embedding)\n\n    picked_nodes = encoded_nodes.gather(dim=1, index=gathering_index)\n    # shape: (batch, pomo, embedding)\n\n    return picked_nodes\n\n\n########################################\n# ENCODER\n########################################\nclass ATSP_Encoder(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n        encoder_layer_num = model_params['encoder_layer_num']\n        self.layers = nn.ModuleList([EncoderLayer(**model_params) for _ in range(encoder_layer_num)])\n\n    def forward(self, row_emb, col_emb, cost_mat):\n        # col_emb.shape: (batch, col_cnt, embedding)\n        # row_emb.shape: (batch, row_cnt, embedding)\n        # cost_mat.shape: (batch, row_cnt, col_cnt)\n\n        for layer in self.layers:\n            row_emb, col_emb = layer(row_emb, col_emb, cost_mat)\n\n        return row_emb, col_emb\n\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n        self.row_encoding_block = EncodingBlock(**model_params)\n        self.col_encoding_block = EncodingBlock(**model_params)\n\n    def forward(self, row_emb, col_emb, cost_mat):\n        # row_emb.shape: (batch, row_cnt, embedding)\n        # col_emb.shape: (batch, col_cnt, embedding)\n        # cost_mat.shape: (batch, row_cnt, col_cnt)\n        row_emb_out = self.row_encoding_block(row_emb, col_emb, cost_mat)\n        col_emb_out = self.col_encoding_block(col_emb, row_emb, cost_mat.transpose(1, 2))\n\n        return row_emb_out, col_emb_out\n\n\nclass EncodingBlock(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n        self.model_params = model_params\n        embedding_dim = self.model_params['embedding_dim']\n        head_num = self.model_params['head_num']\n        qkv_dim = self.model_params['qkv_dim']\n\n        self.Wq = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.Wk = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.Wv = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.mixed_score_MHA = MixedScore_MultiHeadAttention(**model_params)\n        self.multi_head_combine = nn.Linear(head_num * qkv_dim, embedding_dim)\n\n        self.add_n_normalization_1 = AddAndInstanceNormalization(**model_params)\n        self.feed_forward = FeedForward(**model_params)\n        self.add_n_normalization_2 = AddAndInstanceNormalization(**model_params)\n\n    def forward(self, row_emb, col_emb, cost_mat):\n        # NOTE: row and col can be exchanged, if cost_mat.transpose(1,2) is used\n        # input1.shape: (batch, row_cnt, embedding)\n        # input2.shape: (batch, col_cnt, embedding)\n        # cost_mat.shape: (batch, row_cnt, col_cnt)\n        head_num = self.model_params['head_num']\n\n        q = reshape_by_heads(self.Wq(row_emb), head_num=head_num)\n        # q shape: (batch, head_num, row_cnt, qkv_dim)\n        k = reshape_by_heads(self.Wk(col_emb), head_num=head_num)\n        v = reshape_by_heads(self.Wv(col_emb), head_num=head_num)\n        # kv shape: (batch, head_num, col_cnt, qkv_dim)\n\n        out_concat = self.mixed_score_MHA(q, k, v, cost_mat)\n        # shape: (batch, row_cnt, head_num*qkv_dim)\n\n        multi_head_out = self.multi_head_combine(out_concat)\n        # shape: (batch, row_cnt, embedding)\n\n        out1 = self.add_n_normalization_1(row_emb, multi_head_out)\n        out2 = self.feed_forward(out1)\n        out3 = self.add_n_normalization_2(out1, out2)\n\n        return out3\n        # shape: (batch, row_cnt, embedding)\n\n\n########################################\n# Decoder\n########################################\n\nclass ATSP_Decoder(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n        self.model_params = model_params\n        embedding_dim = self.model_params['embedding_dim']\n        head_num = self.model_params['head_num']\n        qkv_dim = self.model_params['qkv_dim']\n\n        self.Wq_0 = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.Wq_1 = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.Wk = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.Wv = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n\n        self.multi_head_combine = nn.Linear(head_num * qkv_dim, embedding_dim)\n\n        self.k = None  # saved key, for multi-head attention\n        self.v = None  # saved value, for multi-head_attention\n        self.single_head_key = None  # saved key, for single-head attention\n        self.q1 = None  # saved q1, for multi-head attention\n\n    def set_kv(self, encoded_jobs):\n        # encoded_jobs.shape: (batch, job, embedding)\n        head_num = self.model_params['head_num']\n\n        self.k = reshape_by_heads(self.Wk(encoded_jobs), head_num=head_num)\n        self.v = reshape_by_heads(self.Wv(encoded_jobs), head_num=head_num)\n        # shape: (batch, head_num, job, qkv_dim)\n        self.single_head_key = encoded_jobs.transpose(1, 2)\n        # shape: (batch, embedding, job)\n\n    def set_q1(self, encoded_q1):\n        # encoded_q.shape: (batch, n, embedding)  # n can be 1 or pomo\n        head_num = self.model_params['head_num']\n\n        self.q1 = reshape_by_heads(self.Wq_1(encoded_q1), head_num=head_num)\n        # shape: (batch, head_num, n, qkv_dim)\n\n    def forward(self, encoded_q0, ninf_mask):\n        # encoded_q4.shape: (batch, pomo, embedding)\n        # ninf_mask.shape: (batch, pomo, job)\n\n        head_num = self.model_params['head_num']\n\n        #  Multi-Head Attention\n        #######################################################\n        q0 = reshape_by_heads(self.Wq_0(encoded_q0), head_num=head_num)\n        # shape: (batch, head_num, pomo, qkv_dim)\n\n        q = self.q1 + q0\n        # shape: (batch, head_num, pomo, qkv_dim)\n\n        out_concat = self._multi_head_attention(q, self.k, self.v, rank3_ninf_mask=ninf_mask)\n        # shape: (batch, pomo, head_num*qkv_dim)\n\n        mh_atten_out = self.multi_head_combine(out_concat)\n        # shape: (batch, pomo, embedding)\n\n        #  Single-Head Attention, for probability calculation\n        #######################################################\n        score = torch.matmul(mh_atten_out, self.single_head_key)\n        # shape: (batch, pomo, job)\n\n        sqrt_embedding_dim = self.model_params['sqrt_embedding_dim']\n        logit_clipping = self.model_params['logit_clipping']\n\n        score_scaled = score / sqrt_embedding_dim\n        # shape: (batch, pomo, job)\n\n        score_clipped = logit_clipping * torch.tanh(score_scaled)\n\n        score_masked = score_clipped + ninf_mask\n\n        probs = F.softmax(score_masked, dim=2)\n        # shape: (batch, pomo, job)\n\n        return probs\n\n    def _multi_head_attention(self, q, k, v, rank2_ninf_mask=None, rank3_ninf_mask=None):\n        # q shape: (batch, head_num, n, key_dim)   : n can be either 1 or pomo\n        # k,v shape: (batch, head_num, node, key_dim)\n        # rank2_ninf_mask.shape: (batch, node)\n        # rank3_ninf_mask.shape: (batch, group, node)\n\n        batch_s = q.size(0)\n        n = q.size(2)\n        node_cnt = k.size(2)\n\n        head_num = self.model_params['head_num']\n        qkv_dim = self.model_params['qkv_dim']\n        sqrt_qkv_dim = self.model_params['sqrt_qkv_dim']\n\n        score = torch.matmul(q, k.transpose(2, 3))\n        # shape: (batch, head_num, n, node)\n\n        score_scaled = score / sqrt_qkv_dim\n        if rank2_ninf_mask is not None:\n            score_scaled = score_scaled + rank2_ninf_mask[:, None, None, :].expand(batch_s, head_num, n, node_cnt)\n        if rank3_ninf_mask is not None:\n            score_scaled = score_scaled + rank3_ninf_mask[:, None, :, :].expand(batch_s, head_num, n, node_cnt)\n\n        weights = nn.Softmax(dim=3)(score_scaled)\n        # shape: (batch, head_num, n, node)\n\n        out = torch.matmul(weights, v)\n        # shape: (batch, head_num, n, key_dim)\n\n        out_transposed = out.transpose(1, 2)\n        # shape: (batch, n, head_num, key_dim)\n\n        out_concat = out_transposed.reshape(batch_s, n, head_num * qkv_dim)\n        # shape: (batch, n, head_num*key_dim)\n\n        return out_concat\n\n\n########################################\n# NN SUB FUNCTIONS\n########################################\n\ndef reshape_by_heads(qkv, head_num):\n    # q.shape: (batch, n, head_num*key_dim)   : n can be either 1 or PROBLEM_SIZE\n\n    batch_s = qkv.size(0)\n    n = qkv.size(1)\n\n    q_reshaped = qkv.reshape(batch_s, n, head_num, -1)\n    # shape: (batch, n, head_num, key_dim)\n\n    q_transposed = q_reshaped.transpose(1, 2)\n    # shape: (batch, head_num, n, key_dim)\n\n    return q_transposed\n\n# ==========================================\n# File: eval_atsp/ATSPModel.py\n# Function/Context: ATSPModel\n# ==========================================\nSee above complete implementation\n\n# ==========================================\n# File: heatmap/cvrp/train.py\n# Function/Context: train\n# ==========================================\nimport sys\nsys.path.insert(0, './')\nimport torch\nimport time\nimport argparse\nfrom tqdm import tqdm\nfrom heatmap.cvrp.inst import gen_inst, gen_pyg_data, trans_tsp\nfrom heatmap.cvrp.eval import eval\nfrom heatmap.cvrp.sampler import Sampler\nfrom nets.partition_net import Net\nfrom utils import load_model\n\nEPS = 1e-10\nDEVICE = 'cuda:0'\nLR = 3e-4\nK_SPARSE = {\n    1000: 100,\n    2000: 200,\n}\n\ndef infer_heatmap(model, pyg_data):\n    heatmap = model(pyg_data)\n    heatmap = heatmap / (heatmap.min()+1e-5)\n    heatmap = model.reshape(pyg_data, heatmap) + 1e-5\n    return heatmap\n    \ndef train_batch(model, optimizer, n, bs, opts):\n    model.train()\n    loss_lst = []\n    for _ in range(opts.batch_size):\n        coors, demand, capacity = gen_inst(n, DEVICE)\n        pyg_data = gen_pyg_data(coors, demand, capacity, K_SPARSE[n])\n        heatmap = infer_heatmap(model, pyg_data)\n        sampler = Sampler(demand, heatmap, capacity, bs, DEVICE)\n        routes, log_probs = sampler.gen_subsets(require_prob=True)\n        tsp_insts, n_tsps_per_route = trans_tsp(coors, routes)\n        objs = eval(tsp_insts, n_tsps_per_route, opts)\n        baseline = objs.mean()\n        log_probs = log_probs.to(DEVICE)\n        reinforce_loss = torch.sum((objs-baseline) * log_probs.sum(dim=1)) / bs\n        loss_lst.append(reinforce_loss)\n    \n    loss = sum(loss_lst) / opts.batch_size\n    optimizer.zero_grad()\n    loss.backward()\n    if not opts.no_clip:\n        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=opts.max_norm, norm_type=2)\n    optimizer.step()\n    \ndef infer_instance(model, inst, opts):\n    model.eval()\n    coors, demand, capacity = inst\n    n = demand.size(0)-1\n    pyg_data = gen_pyg_data(coors, demand, capacity, K_SPARSE[n])\n    heatmap = infer_heatmap(model, pyg_data)\n    sampler = Sampler(demand, heatmap, capacity, 1, DEVICE)\n    routes = sampler.gen_subsets(require_prob=False, greedy_mode=True)\n    tsp_insts, n_tsps_per_route = trans_tsp(coors, routes)\n    obj = eval(tsp_insts, n_tsps_per_route, opts).min()\n    return obj\n\ndef train_epoch(n, bs, steps_per_epoch, net, optimizer, scheduler, opts):\n    for _ in tqdm(range(steps_per_epoch)):\n        train_batch(net, optimizer, n, bs, opts)\n    scheduler.step()\n\n@torch.no_grad()\ndef validation(n, net, opts):\n    sum_obj = 0\n    for _ in range(opts.val_size):\n        inst = gen_inst(n, DEVICE)\n        obj = infer_instance(net, inst, opts)\n        sum_obj += obj\n    avg_obj = sum_obj / opts.val_size\n    return avg_obj\n\ndef train(n, bs, steps_per_epoch, n_epochs, opts):\n    revisers = []\n    for reviser_size in opts.revision_lens:\n        reviser_path = f'pretrained/Reviser-stage2/reviser_{reviser_size}/epoch-299.pt'\n        reviser, _ = load_model(reviser_path, is_local=True)\n        revisers.append(reviser)\n    for reviser in revisers:\n        reviser.to(DEVICE)\n        reviser.eval()\n        reviser.set_decode_type(opts.decode_strategy)    \n    opts.revisers = revisers\n    \n    net = Net(opts.units, 3, K_SPARSE[n], 2, depth=opts.depth).to(DEVICE)\n    optimizer = torch.optim.AdamW(net.parameters(), lr=LR)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=n_epochs)\n    \n    if opts.checkpoint_path == '':\n        starting_epoch = 1\n    else:\n        checkpoint = torch.load(opts.checkpoint_path)\n        net.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        starting_epoch = checkpoint['epoch'] + 1\n        \n    sum_time = 0\n    best_avg_obj = validation(n, net, opts)\n    print('epoch 0', best_avg_obj.item())\n    for epoch in range(starting_epoch, n_epochs + 1):\n        start = time.time()\n        train_epoch(n, bs, steps_per_epoch, net, optimizer, scheduler, opts)\n        sum_time += time.time() - start\n        avg_obj = validation(n, net, opts)\n        print(f'epoch {epoch}: ', avg_obj.item())\n        if best_avg_obj > avg_obj:\n            best_avg_obj = avg_obj\n            print(f'Save checkpoint-{epoch}.')\n            checkpoint = {\n                'epoch': epoch,\n                'model_state_dict': net.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict(),\n            }\n            torch.save(checkpoint, f'./pretrained/Partitioner/cvrp/cvrp-{n}-{epoch}-cos.pt')\n    print('total training duration:', sum_time)\n    \nif __name__ == '__main__':\n    import pprint as pp\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--problem_size', type=int, default=400)\n    parser.add_argument('--revision_lens', nargs='+', default=[20] ,type=int,\n                        help='The sizes of revisers')\n    parser.add_argument('--revision_iters', nargs='+', default=[5], type=int,\n                        help='Revision iterations (I_n)')\n    parser.add_argument('--decode_strategy', type=str, default='greedy', help='decode strategy of the model')\n    parser.add_argument('--width', type=int, default=10)\n    parser.add_argument('--no_aug', action='store_true', help='Disable instance augmentation')\n    parser.add_argument('--seed', type=int, default=1, help='Random seed')\n    parser.add_argument('--val_size', type=int, default=100)\n    parser.add_argument('--n_epochs', type=int, default=20)\n    parser.add_argument('--steps_per_epoch', type=int, default=256)\n    parser.add_argument('--checkpoint_path', type=str, default='')\n    parser.add_argument('--max_norm', type=float, default=1)\n    parser.add_argument('--units', type=int, default=48)\n    parser.add_argument('--no_clip', action='store_true')\n    parser.add_argument('--batch_size', type=int, default=10)\n    parser.add_argument('--depth', type=int, default=12)\n    opts = parser.parse_args()\n    opts.no_aug = True\n    opts.no_prune = False\n    opts.problem_type = 'tsp'\n    \n    torch.manual_seed(opts.seed)\n    pp.pprint(vars(opts))\n    train(opts.problem_size, opts.width, opts.steps_per_epoch, opts.n_epochs, opts)\n\n# ==========================================\n# File: main.py\n# Function/Context: eval_dataset, _eval_dataset\n# ==========================================\nimport math\nimport torch\nimport argparse\nimport warnings\nimport numpy as np\nfrom tqdm import tqdm\nfrom utils import load_model\nfrom torch.utils.data import DataLoader\nimport time\nfrom utils.functions import reconnect\nfrom utils.functions import load_problem\nimport pprint as pp\nfrom utils.insertion import random_insertion_parallel\nfrom heatmap.cvrp.infer import load_partitioner\nfrom heatmap.cvrp.inst import sum_cost\n\ndef eval_dataset(dataset_path, opts):\n    pp.pprint(vars(opts))\n    \n    revisers = []\n    revision_lens = opts.revision_lens\n\n    for reviser_size in revision_lens:\n        reviser_path = f'pretrained/Reviser-stage2/reviser_{reviser_size}/epoch-299.pt'\n        reviser, _ = load_model(reviser_path, is_local=True)\n        revisers.append(reviser)\n        \n    for reviser in revisers:\n        reviser.to(opts.device)\n        reviser.eval()\n        reviser.set_decode_type(opts.decode_strategy)\n        \n    results, duration = _eval_dataset(dataset_path, opts, opts.device, revisers)\n\n    costs, costs_revised, costs_revised_with_penalty, tours = zip(*results)\n    costs = torch.tensor(costs)\n    if opts.problem_type in ['cvrp', 'cvrplib']:\n        costs_revised = torch.stack(costs_revised)\n    else:\n        costs_revised = torch.cat(costs_revised, dim=0)\n    \n    if opts.problem_type == 'pctsp':\n        costs_revised_with_penalty = torch.cat(costs_revised_with_penalty, dim=0)\n\n    print(\"Average cost: {} +- {}\".format(costs.mean(), (2 * torch.std(costs) / math.sqrt(len(costs))).item()))\n    print(\"Average cost_revised: {} +- {}\".format(costs_revised.mean().item(), \n                            (2 * torch.std(costs_revised) / math.sqrt(len(costs_revised))).item()))\n    if opts.problem_type == 'pctsp':\n        print(\"Average cost_revised with penalty: {} +- {}\".format(costs_revised_with_penalty.mean().item(), \n                            (2 * torch.std(costs_revised_with_penalty) / math.sqrt(len(costs_revised_with_penalty))).item()))\n    print(\"Total duration: {}\".format(duration))\n    \n    if opts.problem_type != 'cvrp':\n        tours = torch.cat(tours, dim=0)\n    return tours\n\ndef _eval_dataset(dataset_path, opts, device, revisers):\n    start = time.time()\n    if opts.problem_type == 'tsp':\n        dataset = revisers[0].problem.make_dataset(filename=dataset_path, num_samples=opts.val_size, offset=0)\n        if opts.problem_size <= 100:\n            if opts.width >= 4:\n                opts.width //= 4\n                opts.tsp_aug = True\n            else:\n                opts.tsp_aug = False\n        orders = [torch.randperm(opts.problem_size) for i in range(opts.width)]\n        pi_all = [random_insertion_parallel(dataset, order) for order in orders] # instance: (p_size, 2)\n        pi_all = torch.tensor(np.array(pi_all).astype(np.int64)).reshape(len(orders), opts.val_size, opts.problem_size) # width, val_size, p_size\n    elif opts.problem_type == 'pctsp': # dataset (n_cons*val_size, p_size, 2), pi_all (width, n_cons*val_size, p_size), penalty (n_cons, val_size)\n        from problems.pctsp import init\n        opts.eval_batch_size = opts.eval_batch_size * opts.n_subset\n        dataset, penalty= init(dataset_path, opts) # (n_val*n_subset, max_seq_len, 2), (val_size*n_subset, )\n        dataset = dataset.cpu()\n        max_seq_len = dataset.size(1)\n        order = torch.arange(max_seq_len) # width=1 by default for pctsp\n        pi_all = random_insertion_parallel(dataset, order) # (n_val*n_subset, max_seq_len)\n        pi_all = torch.tensor(pi_all.astype(np.int64)).unsqueeze(0)  # (1, n_val*n_subset, max_seq_len)\n        assert pi_all.shape == (1, opts.val_size*opts.n_subset, max_seq_len)\n    elif opts.problem_type == 'cvrp':\n        from problems.cvrp import init  \n        dataset, n_tsps_per_route_lst = init(dataset_path, opts)\n        opts.eval_batch_size = 1\n    elif opts.problem_type == 'cvrplib':\n        from problems.cvrp import init  \n        ckpt_path = \"./pretrained/Partitioner/cvrp/cvrp-2000-cvrplib.pt\" if opts.ckpt_path == '' else opts.ckpt_path   \n        partitioner = load_partitioner(2000, opts.device, ckpt_path, 300, 6)\n        dataset, n_tsps_per_route_lst = init(dataset_path, opts, partitioner)\n        opts.eval_batch_size = 1\n        \n    dataloader = DataLoader(dataset, batch_size=opts.eval_batch_size)\n    \n\n    problem = load_problem('tsp')\n    get_cost_func = lambda input, pi: problem.get_costs(input, pi, return_local=True)\n    \n    results = []\n    for batch_id, batch in tqdm(enumerate(dataloader), disable=opts.no_progress_bar):\n        # tsp batch shape: (bs, problem size, 2)\n        avg_cost = 0\n        with torch.no_grad():\n            if opts.problem_type in ['tsp', 'pctsp']:\n                p_size = batch.size(1)\n                batch = batch.repeat(opts.width, 1, 1) # (1,1,1) for pctsp\n                pi_batch = pi_all[:, batch_id*opts.eval_batch_size: (batch_id+1)*opts.eval_batch_size, :].reshape(-1, p_size)\n                seed = batch.gather(1, pi_batch.unsqueeze(-1).repeat(1,1,2))\n            elif opts.problem_type in ['cvrp', 'cvrplib']:\n                batch = batch.squeeze() # (n_subTSPs_for_width_routes, max_seq_len, 2)\n                n_subTSPs, max_seq_len, _ = batch.shape\n                n_tsps_per_route = n_tsps_per_route_lst[batch_id]\n                assert sum(n_tsps_per_route) == n_subTSPs\n                opts.eval_batch_size = n_subTSPs\n                order = torch.arange(max_seq_len)\n                pi_batch = random_insertion_parallel(batch, order)\n                pi_batch = torch.tensor(pi_batch.astype(np.int64))\n                assert pi_batch.shape == (n_subTSPs, max_seq_len)\n                seed = batch.gather(1, pi_batch.unsqueeze(-1).repeat(1,1,2))\n                assert seed.shape == (n_subTSPs, max_seq_len, 2)\n            else:\n                raise NotImplementedError\n                \n            seed = seed.to(device)\n            cost_ori = (seed[:, 1:] - seed[:, :-1]).norm(p=2, dim=2).sum(1) + (seed[:, 0] - seed[:, -1]).norm(p=2, dim=1)\n            if opts.problem_type in ['tsp', 'pctsp']:\n                cost_ori, _ = cost_ori.reshape(-1, opts.eval_batch_size).min(0) # width, bs\n                avg_cost = cost_ori.mean().item()\n            elif opts.problem_type in ['cvrp', 'cvrplib']:\n                avg_cost = sum_cost(cost_ori, n_tsps_per_route).min()\n            else:\n                raise NotImplementedError\n\n            if opts.problem_size <= 100 and opts.problem_type=='tsp' and opts.tsp_aug:\n                seed2 = torch.cat((1 - seed[:, :, [0]], seed[:, :, [1]]), dim=2)\n                seed3 = torch.cat((seed[:, :, [0]], 1 - seed[:, :, [1]]), dim=2)\n                seed4 = torch.cat((1 - seed[:, :, [0]], 1 - seed[:, :, [1]]), dim=2)\n                seed = torch.cat((seed, seed2, seed3, seed4), dim=0)\n                \n            tours, costs_revised = reconnect( \n                                        get_cost_func=get_cost_func,\n                                        batch=seed,\n                                        opts=opts,\n                                        revisers=revisers,\n                                        )\n\n        if opts.problem_type == 'pctsp':\n            costs_revised_with_penalty, costs_revised_minidx = (costs_revised.reshape(-1, opts.n_subset)+ \\\n                penalty[batch_id*opts.eval_batch_size: (batch_id+1)*opts.eval_batch_size].reshape(-1, opts.n_subset)).min(1)\n            costs_revised, _ = costs_revised.reshape(-1, opts.n_subset).min(1)\n            tours = tours.reshape(-1, opts.n_subset, max_seq_len, 2)[torch.arange(opts.eval_batch_size//opts.n_subset), costs_revised_minidx, :, :]\n            assert costs_revised.size(0) == costs_revised_with_penalty.size(0) == tours.size(0) == opts.eval_batch_size//opts.n_subset\n        elif opts.problem_type in ['cvrp', 'cvrplib']:\n            assert costs_revised.shape == (n_subTSPs,)\n            costs_revised, best_partition_idx = sum_cost(costs_revised, n_tsps_per_route).min(dim=0)\n            subtour_start = sum(n_tsps_per_route[:best_partition_idx])\n            tours = tours[subtour_start: subtour_start+n_tsps_per_route[best_partition_idx]]\n            assert tours.shape == (n_tsps_per_route[best_partition_idx], max_seq_len, 2)\n            tours = tours.reshape(-1, 2)\n        \n        if opts.problem_type == 'pctsp':\n            results.append((avg_cost, costs_revised, costs_revised_with_penalty, tours))\n        elif opts.problem_type in ['tsp', 'cvrp', 'cvrplib']:\n            results.append((avg_cost, costs_revised, None, tours))\n        else:\n            raise NotImplementedError\n        \n\n    duration = time.time() - start\n\n    return results, duration\n\n# ==========================================\n# File: nets/attention_local.py\n# Function/Context: AttentionModel\n# ==========================================\nimport torch\nfrom torch import nn\nfrom torch.utils.checkpoint import checkpoint\nimport math\nfrom typing import NamedTuple\nfrom utils.tensor_functions import compute_in_batches\n\nfrom nets.graph_encoder import GraphAttentionEncoder\n\nfrom torch.nn import DataParallel\nfrom utils.beam_search import CachedLookup\nfrom utils.functions import sample_many\n\ndef set_decode_type(model, decode_type):\n    if isinstance(model, DataParallel):\n        model = model.module\n    model.set_decode_type(decode_type)\n\n\nclass AttentionModelFixed(NamedTuple):\n    \"\"\"\n    Context for AttentionModel decoder that is fixed during decoding so can be precomputed/cached\n    This class allows for efficient indexing of multiple Tensors at once\n    \"\"\"\n    node_embeddings: torch.Tensor\n    context_node_projected: torch.Tensor\n    glimpse_key: torch.Tensor\n    glimpse_val: torch.Tensor\n    logit_key: torch.Tensor\n\n    def __getitem__(self, key):\n        assert torch.is_tensor(key) or isinstance(key, slice)\n        return AttentionModelFixed(\n            node_embeddings=self.node_embeddings[key],\n            context_node_projected=self.context_node_projected[key],\n            glimpse_key=self.glimpse_key[:, key],  # dim 0 are the heads\n            glimpse_val=self.glimpse_val[:, key],  # dim 0 are the heads\n            logit_key=self.logit_key[key]\n            )\n\n\nclass AttentionModel(nn.Module):\n\n    def __init__(self,\n                 embedding_dim,\n                 hidden_dim,\n                 problem,\n                 n_encode_layers=2,\n                 tanh_clipping=10.,\n                 mask_inner=True,\n                 mask_logits=True,\n                 normalization='batch',\n                 n_heads=8,\n                 checkpoint_encoder=False,\n                 shrink_size=None,\n                 ):\n        super(AttentionModel, self).__init__()\n\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.n_encode_layers = n_encode_layers\n        self.decode_type = None\n        self.temp = 1.0\n        self.allow_partial = problem.NAME == 'sdvrp'\n        self.is_vrp = problem.NAME == 'cvrp' or problem.NAME == 'sdvrp'\n        self.is_orienteering = problem.NAME == 'op'\n        self.is_pctsp = problem.NAME == 'pctsp'\n\n        self.tanh_clipping = tanh_clipping\n\n        self.mask_inner = mask_inner\n        self.mask_logits = mask_logits\n        self.problem = problem\n        self.n_heads = n_heads\n        self.checkpoint_encoder = checkpoint_encoder\n        self.shrink_size = shrink_size\n\n        # Problem specific context parameters (placeholder and step context dimension)\n        if self.is_vrp or self.is_orienteering or self.is_pctsp:\n            # Embedding of last node + remaining_capacity / remaining length / remaining prize to collect\n            step_context_dim = embedding_dim + 1\n\n            if self.is_pctsp:\n                node_dim = 4  # x, y, expected_prize, penalty\n            else:\n                node_dim = 3  # x, y, demand / prize\n\n            # Special embedding projection for depot node\n            self.init_embed_depot = nn.Linear(2, embedding_dim)\n            \n            if self.is_vrp and self.allow_partial:  # Need to include the demand if split delivery allowed\n                self.project_node_step = nn.Linear(1, 3 * embedding_dim, bias=False)\n        else:  # TSP\n            step_context_dim = 2 * embedding_dim  # Embedding of first and last node\n            node_dim = 2  # x, y\n            \n            # Learned input symbols for first action\n            self.W_placeholder = nn.Parameter(torch.Tensor(2 * embedding_dim))\n            self.W_placeholder.data.uniform_(-1, 1)  # Placeholder should be in range of activations\n\n        self.init_embed = nn.Linear(node_dim, embedding_dim)\n\n        self.embedder = GraphAttentionEncoder(\n            n_heads=n_heads,\n            embed_dim=embedding_dim,\n            n_layers=self.n_encode_layers,\n            normalization=normalization\n        )\n\n        # For each node we compute (glimpse key, glimpse value, logit key) so 3 * embedding_dim\n        self.project_node_embeddings = nn.Linear(embedding_dim, 3 * embedding_dim, bias=False)\n        self.project_fixed_context = nn.Linear(embedding_dim, embedding_dim, bias=False)\n        self.project_step_context = nn.Linear(step_context_dim, embedding_dim, bias=False)\n        assert embedding_dim % n_heads == 0\n        # Note n_heads * val_dim == embedding_dim so input to project_out is embedding_dim\n        self.project_out = nn.Linear(embedding_dim, embedding_dim, bias=False)\n\n    def set_decode_type(self, decode_type, temp=None):\n        self.decode_type = decode_type\n        if temp is not None:  # Do not change temperature if not provided\n            self.temp = temp\n\n    def forward(self, input, return_pi=False, return_embedding=False, embeddings=None):\n        \"\"\"\n        :param input: (batch_size, graph_size, node_dim) input node features or dictionary with multiple tensors\n        :param return_pi: whether to return the output sequences, this is optional as it is not compatible with\n        using DataParallel as the results may be of different lengths on different GPUs\n        :return:\n        \"\"\"\n        # return_pi is true means during inference\n        if return_pi:\n            assert not self.training\n        \n        if self.checkpoint_encoder and self.training:  # Only checkpoint if we need gradients\n            embeddings, _ = checkpoint(self.embedder, self._init_embed(input))\n        \n        elif embeddings is None:\n            embeddings, _ = self.embedder(self._init_embed(input))\n\n        # embeddings shape: (batch size (e.g. width x decomposed pieces), problem size, embedding size)\n\n        # _log_p, pi, entropies = self._inner(input, embeddings)\n        _log_p, pi, _log_p2, pi2 = self._inner(input, embeddings)\n\n        cost, mask = self.problem.get_costs(input, pi)\n        cost2, mask2 = self.problem.get_costs(input, pi2)\n        if return_pi:\n            if return_embedding:\n                return cost, pi, cost2, torch.flip(pi2, dims=(-1,)), embeddings\n            return cost, pi, cost2, torch.flip(pi2, dims=(-1,))\n            \n        ll = self._calc_log_likelihood(_log_p, pi, mask)\n        ll2 = self._calc_log_likelihood(_log_p2, pi2, mask2)\n    \n        return cost, ll, cost2, ll2\n\n        \n    def beam_search(self, *args, **kwargs):\n        return self.problem.beam_search(*args, **kwargs, model=self)\n\n    def precompute_fixed(self, input):\n        embeddings, _ = self.embedder(self._init_embed(input))\n        # Use a CachedLookup such that if we repeatedly index this object with the same index we only need to do\n        # the lookup once... this is the case if all elements in the batch have maximum batch size\n        return CachedLookup(self._precompute(embeddings))\n\n    def propose_expansions(self, beam, fixed, expand_size=None, normalize=False, max_calc_batch_size=4096):\n        raise NotImplementedError\n        # First dim = batch_size * cur_beam_size\n        log_p_topk, ind_topk = compute_in_batches(\n            lambda b: self._get_log_p_topk(fixed[b.ids], b.state, k=expand_size, normalize=normalize),\n            max_calc_batch_size, beam, n=beam.size()\n        )\n\n        assert log_p_topk.size(1) == 1, \"Can only have single step\"\n        # This will broadcast, calculate log_p (score) of expansions\n        score_expand = beam.score[:, None] + log_p_topk[:, 0, :]\n\n        # We flatten the action as we need to filter and this cannot be done in 2d\n        flat_action = ind_topk.view(-1)\n        flat_score = score_expand.view(-1)\n        flat_feas = flat_score > -1e10  # != -math.inf triggers\n\n        # Parent is row idx of ind_topk, can be found by enumerating elements and dividing by number of columns\n        flat_parent = torch.arange(flat_action.size(-1), out=flat_action.new()) / ind_topk.size(-1)\n\n        # Filter infeasible\n        feas_ind_2d = torch.nonzero(flat_feas)\n\n        if len(feas_ind_2d) == 0:\n            # Too bad, no feasible expansions at all :(\n            return None, None, None\n\n        feas_ind = feas_ind_2d[:, 0]\n\n        return flat_parent[feas_ind], flat_action[feas_ind], flat_score[feas_ind]\n\n    def _calc_log_likelihood(self, _log_p, a, mask):\n\n        # Get log_p corresponding to selected actions\n        log_p = _log_p.gather(2, a.unsqueeze(-1)).squeeze(-1)\n\n        # Optional: mask out actions irrelevant to objective so they do not get reinforced\n        if mask is not None:\n            log_p[mask] = 0\n\n        assert (log_p > -1000).data.all(), \"Logprobs should not be -inf, check sampling procedure!\"\n\n        # Calculate log_likelihood\n        return log_p.sum(1)\n\n    def _init_embed(self, input):\n\n        if self.is_vrp or self.is_orienteering or self.is_pctsp:\n            if self.is_vrp:\n                features = ('demand', )\n            elif self.is_orienteering:\n                features = ('prize', )\n            else:\n                assert self.is_pctsp\n                features = ('deterministic_prize', 'penalty')\n            return torch.cat(\n                (\n                    self.init_embed_depot(input['depot'])[:, None, :],\n                    self.init_embed(torch.cat((\n                        input['loc'],\n                        *(input[feat][:, :, None] for feat in features)\n                    ), -1))\n                ),\n                1\n            )\n        # TSP\n        return self.init_embed(input)\n\n    def _inner(self, input, embeddings):\n\n        outputs = []\n        sequences = []\n\n        outputs2 = []\n        sequences2 = []\n\n        state = self.problem.make_state(input)\n        state2 = self.problem.make_state(input)\n\n        # Compute keys, values for the glimpse and keys for the logits once as they can be reused in every step\n        fixed = self._precompute(embeddings)\n\n        batch_size = state.ids.size(0)\n\n        # Perform decoding steps\n        i = 0\n\n        while not (self.shrink_size is None and state.all_finished()):\n\n            if self.shrink_size is not None:\n                raise NotImplementedError\n                unfinished = torch.nonzero(state.get_finished() == 0)\n                if len(unfinished) == 0:\n                    break\n                unfinished = unfinished[:, 0]\n                # Check if we can shrink by at least shrink_size and if this leaves at least 16\n                # (otherwise batch norm will not work well and it is inefficient anyway)\n                if 16 <= len(unfinished) <= state.ids.size(0) - self.shrink_size:\n                    # Filter states\n                    state = state[unfinished] # TODO double state\n                    fixed = fixed[unfinished]\n\n            # log_p, mask,A = self._get_log_p(fixed, state,i=i)\n            log_p, mask = self._get_log_p(fixed, state, i=i)\n            log_p2, mask2 = self._get_log_p(fixed, state2, i=i, reverse=True)\n\n            # Select the indices of the next nodes in the sequences, result (batch_size) long\n            selected = self._select_node(log_p.exp()[:, 0, :], mask[:, 0, :])  # Squeeze out steps dimension\n            selected2 = self._select_node(log_p2.exp()[:, 0, :], mask2[:, 0, :])\n            # print('selected1', selected)\n            # print('selected2', selected2)\n            state = state.update(selected)\n\n            state2 = state2.update(selected2)\n\n            # Now make log_p, selected desired output size by 'unshrinking'\n            if self.shrink_size is not None and state.ids.size(0) < batch_size:\n                raise NotImplementedError\n                log_p_, selected_ = log_p, selected\n                log_p = log_p_.new_zeros(batch_size, *log_p_.size()[1:])\n                selected = selected_.new_zeros(batch_size)\n\n                log_p[state.ids[:, 0]] = log_p_\n                selected[state.ids[:, 0]] = selected_\n\n            # Collect output of step\n            outputs.append(log_p[:, 0, :])\n            sequences.append(selected)\n\n            outputs2.append(log_p2[:, 0, :])\n            sequences2.append(selected2)\n\n            i += 1\n\n        return torch.stack(outputs, 1), torch.stack(sequences, 1), \\\n                torch.stack(outputs2, 1), torch.stack(sequences2, 1)\n\n\n    def sample_many(self, input, batch_rep=1, iter_rep=1,model_local=None):\n        \"\"\"\n        :param input: (batch_size, graph_size, node_dim) input node features\n        :return:\n        \"\"\"\n        # Bit ugly but we need to pass the embeddings as well.\n        # Making a tuple will not work with the problem.get_cost function\n        raise NotImplementedError\n        return sample_many(\n            lambda input: self._inner(*input),  # Need to unpack tuple into arguments\n            lambda input, pi: self.problem.get_costs(input[0], pi,return_two=True),  # Don't need embeddings as input to get_costs\n            (input, self.embedder(self._init_embed(input))[0]),  # Pack input with embeddings (additional input)\n            batch_rep, iter_rep,model_local\n        )\n\n    def _select_node(self, probs, mask):\n\n        assert (probs == probs).all(), \"Probs should not contain any nans\"\n\n        if self.decode_type == \"greedy\":\n      \n            \n            ---\n\n# ==========================================\n# File: nets/partition_net.py\n# Function/Context: Net\n# ==========================================\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom copy import deepcopy\nimport torch_geometric.nn as gnn\n\n# GNN for edge embeddings\nclass EmbNet(nn.Module):\n    def __init__(self, depth=12, feats=2, edge_feats=1, units=48, act_fn='silu', agg_fn='mean'): # TODO feats=1\n        super().__init__()\n        self.depth = depth\n        self.feats = feats\n        self.units = units\n        self.act_fn = getattr(F, act_fn)\n        self.agg_fn = getattr(gnn, f'global_{agg_fn}_pool')\n        self.v_lin0 = nn.Linear(self.feats, self.units)\n        self.v_lins1 = nn.ModuleList([nn.Linear(self.units, self.units) for i in range(self.depth)])\n        self.v_lins2 = nn.ModuleList([nn.Linear(self.units, self.units) for i in range(self.depth)])\n        self.v_lins3 = nn.ModuleList([nn.Linear(self.units, self.units) for i in range(self.depth)])\n        self.v_lins4 = nn.ModuleList([nn.Linear(self.units, self.units) for i in range(self.depth)])\n        self.v_bns = nn.ModuleList([gnn.BatchNorm(self.units) for i in range(self.depth)])\n        self.e_lin0 = nn.Linear(edge_feats, self.units)\n        self.e_lins0 = nn.ModuleList([nn.Linear(self.units, self.units) for i in range(self.depth)])\n        self.e_bns = nn.ModuleList([gnn.BatchNorm(self.units) for i in range(self.depth)])\n    def reset_parameters(self):\n        raise NotImplementedError\n    def forward(self, x, edge_index, edge_attr):\n        x = x\n        w = edge_attr\n        x = self.v_lin0(x)\n        x = self.act_fn(x)\n        w = self.e_lin0(w)\n        w = self.act_fn(w)\n        for i in range(self.depth):\n            x0 = x\n            x1 = self.v_lins1[i](x0)\n            x2 = self.v_lins2[i](x0)\n            x3 = self.v_lins3[i](x0)\n            x4 = self.v_lins4[i](x0)\n            w0 = w\n            w1 = self.e_lins0[i](w0)\n            w2 = torch.sigmoid(w0)\n            x = x0 + self.act_fn(self.v_bns[i](x1 + self.agg_fn(w2 * x2[edge_index[1]], edge_index[0])))\n            w = w0 + self.act_fn(self.e_bns[i](w1 + x3[edge_index[0]] + x4[edge_index[1]]))\n        return w\n\n# general class for MLP\nclass MLP(nn.Module):\n    @property\n    def device(self):\n        return self._dummy.device\n    def __init__(self, units_list, act_fn):\n        super().__init__()\n        self._dummy = nn.Parameter(torch.empty(0), requires_grad = False)\n        self.units_list = units_list\n        self.depth = len(self.units_list) - 1\n        self.act_fn = getattr(F, act_fn)\n        self.lins = nn.ModuleList([nn.Linear(self.units_list[i], self.units_list[i + 1]) for i in range(self.depth)])\n    def forward(self, x, k_sparse):\n        for i in range(self.depth):\n            x = self.lins[i](x)\n            if i < self.depth - 1:\n                x = self.act_fn(x)\n            else:\n                x = x.reshape(-1, k_sparse)\n                x = torch.softmax(x, dim=1)\n                x = x.flatten()\n        return x\n\n# MLP for predicting parameterization theta\nclass ParNet(MLP):\n    def __init__(self, k_sparse, depth=3, units=48, preds=1, act_fn='silu'):\n        self.units = units\n        self.preds = preds\n        self.k_sparse = k_sparse\n        super().__init__([self.units] * depth + [self.preds], act_fn)\n    def forward(self, x):\n        return super().forward(x, self.k_sparse).squeeze(dim = -1)\n    \n\nclass Net(nn.Module):\n    def __init__(self, units, feats, k_sparse, edge_feats=1, depth=12):\n        super().__init__()\n        self.emb_net = EmbNet(depth=depth, units=units, feats=feats, edge_feats=edge_feats)\n        self.par_net_heu = ParNet(units=units, k_sparse=k_sparse)\n        \n    def forward(self, pyg):\n        '''\n        Args:\n            pyg: torch_geometric.data.Data instance with x, edge_index, and edge attr\n        Returns:\n            heu: heuristic vector [n_nodes * k_sparsification,]\n        '''\n        x, edge_index, edge_attr = pyg.x, pyg.edge_index, pyg.edge_attr\n        emb = self.emb_net(x, edge_index, edge_attr)\n        heu = self.par_net_heu(emb)\n        return heu\n            \n    @staticmethod\n    def reshape(pyg, vector):\n        '''Turn heu vector into matrix with zero padding \n        '''\n        n_nodes = pyg.x.shape[0]\n        device = pyg.x.device\n        matrix = torch.zeros(size=(n_nodes, n_nodes), device=device)\n        matrix[pyg.edge_index[0], pyg.edge_index[1]] = vector\n        try:\n            assert (matrix.sum(dim=1) >= 0.99).all()\n        except:\n            torch.save(matrix, './error_reshape.pt')\n        return matrix\n\n# ==========================================\n# File: problems/cvrp.py\n# Function/Context: init\n# ==========================================\nimport sys\nsys.path.insert(0, './')\nimport pickle\nimport torch\nimport numpy as np\nfrom heatmap.cvrp.infer import infer, load_partitioner\nfrom heatmap.cvrp.sampler import Sampler\nfrom heatmap.cvrp.inst import trans_tsp\n  \ndef load_dataset(path):\n    with open(path, 'rb') as f:\n        data = pickle.load(f)\n    return data\n    \ndef concat_list(depot_coor, coors, demand, opts):\n    coor =  torch.cat([torch.tensor(depot_coor, device=opts.device).unsqueeze(0), \n                    torch.tensor(coors, device=opts.device)], dim=0) # 1+p_size, 2 \n    demand = torch.cat([torch.zeros((1), device=opts.device), \n                            torch.tensor(demand, device=opts.device)]) # 1+p_size\n    return coor, demand\n\ndef add_padding(pi_all, max_seq_len, opts):\n    ret = []\n    for subset_pi in pi_all:\n        assert subset_pi.size(0) == opts.n_subset\n        diff = max_seq_len - subset_pi.size(-1)\n        subset_pi = torch.cat([subset_pi, torch.zeros((opts.n_subset, diff), dtype=torch.int64 ,device=subset_pi.device)], dim=1)\n        ret.append(subset_pi)\n    ret = torch.cat(ret, dim=0) # n_val*n_subset, max_len\n    assert ret.shape == (opts.val_size * opts.n_subset, max_seq_len)\n    return ret\n        \ndef init(path, opts, partitioner=None): # for cvrplib, partioner is not None, and depth is set to 6, k_sparse to 300 \n    is_cvrplib = True if partitioner is not None else False\n    k_sparse = 300 if is_cvrplib else None\n    \n    data = load_dataset(path)\n    greedy_mode = True if opts.n_partition == 1 else False \n    partitioner = load_partitioner(opts.problem_size, opts.device, opts.ckpt_path) if partitioner is None else partitioner\n    dataset = []\n    n_tsps_per_route_lst = []\n    for inst_id, inst in enumerate(data[:opts.val_size]):\n        depot_coor, coors, demand, capacity = inst\n        coors, demand = concat_list(depot_coor, coors, demand, opts)\n        heatmap = infer(partitioner, coors, demand, capacity, k_sparse, is_cvrplib)\n        sampler = Sampler(demand, heatmap, capacity, opts.n_partition, 'cpu')\n        routes = sampler.gen_subsets(require_prob=False, greedy_mode=greedy_mode) # n_partition, max_len\n        assert routes.size(0) == opts.n_partition\n        tsp_insts, n_tsps_per_route = trans_tsp(coors.cpu(), routes)\n        assert tsp_insts.size(0) == sum(n_tsps_per_route)\n        dataset.append(tsp_insts)\n        n_tsps_per_route_lst.append(n_tsps_per_route)\n    return dataset, n_tsps_per_route_lst\n\n# ==========================================\n# File: problems/pctsp.py\n# Function/Context: init\n# ==========================================\nimport sys\nsys.path.insert(0, './')\nimport pickle\nimport torch\nfrom heatmap.pctsp.infer import infer, load_partitioner\nfrom heatmap.pctsp.sampler import Sampler\n\ndef load_dataset(path='./data/pctsp/pctsp500_test_seed1234.pkl'):\n    with open(path, 'rb') as f:\n        data = pickle.load(f)\n    return data\n    \ndef concat_list(depot_coor, coors, penalty, prize, opts):\n    coor =  torch.cat([torch.tensor(depot_coor, device=opts.device).unsqueeze(0), \n                    torch.tensor(coors, device=opts.device)], dim=0) # 1+p_size, 2 \n    penalty = torch.cat([torch.zeros((1), device=opts.device), \n                            torch.tensor(penalty, device=opts.device)]) # 1+p_size\n    prize = torch.cat([torch.zeros((1), device=opts.device), \n                            torch.tensor(prize, device=opts.device)]) # 1+p_size\n    return coor, penalty, prize\n\ndef add_padding(pi_all, max_seq_len, opts):\n    ret = []\n    for subset_pi in pi_all:\n        assert subset_pi.size(0) == opts.n_subset\n        diff = max_seq_len - subset_pi.size(-1)\n        subset_pi = torch.cat([subset_pi, torch.zeros((opts.n_subset, diff), dtype=torch.int64 ,device=subset_pi.device)], dim=1)\n        ret.append(subset_pi)\n    ret = torch.cat(ret, dim=0) # n_val*n_subset, max_len\n    assert ret.shape == (opts.val_size * opts.n_subset, max_seq_len)\n    return ret\n        \ndef init(path, opts):\n    data = load_dataset(path)\n    partitioner = load_partitioner(opts.problem_size, opts.device)\n    dataset = []\n    pi_all = []\n    penalty_all = []\n    max_seq_len = 0\n    greedy_mode = (opts.n_subset == 1)\n    for inst_id, inst in enumerate(data[:opts.val_size]):\n        depot_coor, coors, penalty, prize, _ = inst\n        coors, penalty, prize = concat_list(depot_coor, coors, penalty, prize, opts)\n        dataset.append(coors)\n        heatmap = infer(partitioner, prize, penalty, coors)\n        sampler = Sampler(prize, heatmap, opts.n_subset, opts.device)\n        subset = sampler.gen_subsets(require_prob=False, greedy_mode=greedy_mode) # n_subset, max_len\n        assert subset.size(0) == opts.n_subset\n        if subset.size(1) > max_seq_len:\n            max_seq_len = subset.size(1)\n        penalty = sampler.gen_penalty(subset, penalty) # n_subset\n        pi_all.append(subset)  # pi_all: list, (val_size, n_subset, max_len)\n        penalty_all.append(penalty) # list, (val_size, n_subset)\n    pi_all = add_padding(pi_all, max_seq_len, opts) # n_val*n_subset, max_len\n    \n    # transform into TSPs\n    dataset = torch.stack(dataset) # val_size, p+1, 2\n    dataset = torch.repeat_interleave(dataset, opts.n_subset, 0) # n_val*n_subset, p+1, 2\n    seed = dataset.gather(1, pi_all.unsqueeze(-1).repeat(1, 1, 2)) # (n_val*n_subset, max_seq_len, 2)\n    penalty = torch.cat(penalty_all, dim=0)\n    return seed, penalty # (n_val*n_subset, max_seq_len, 2), (val_size*n_subset, )\n\n# ==========================================\n# File: utils/functions.py\n# Function/Context: LCP_TSP, reconnect\n# ==========================================\nimport warnings\nimport torch\nimport numpy as np\nimport os\nimport json\nfrom tqdm import tqdm\nfrom multiprocessing.dummy import Pool as ThreadPool\nfrom multiprocessing import Pool\nimport torch.nn.functional as F\nimport math\nimport time\n\ndef decomposition(seeds, coordinate_dim, revision_len, offset, shift_len = 1):\n    # change decomposition point\n    seeds = torch.cat([seeds[:, shift_len:],seeds[:, :shift_len]], 1)\n\n    if offset!=0:\n        decomposed_seeds = seeds[:, :-offset]\n        offset_seeds = seeds[:,-offset:]\n    else:\n        decomposed_seeds = seeds\n        offset_seeds = None\n    # decompose original seeds\n    decomposed_seeds = decomposed_seeds.reshape(-1, revision_len, coordinate_dim)\n    return decomposed_seeds, offset_seeds\n\ndef coordinate_transformation(x):\n    input = x.clone()\n    max_x, indices_max_x = input[:,:,0].max(dim=1)\n    max_y, indices_max_y = input[:,:,1].max(dim=1)\n    min_x, indices_min_x = input[:,:,0].min(dim=1)\n    min_y, indices_min_y = input[:,:,1].min(dim=1)\n    # shapes: (batch_size, ); (batch_size, )\n    \n    diff_x = max_x - min_x\n    diff_y = max_y - min_y\n    xy_exchanged = diff_y > diff_x\n\n    # shift to zero\n    input[:, :, 0] -= (min_x).unsqueeze(-1)\n    input[:, :, 1] -= (min_y).unsqueeze(-1)\n\n    # exchange coordinates for those diff_y > diff_x\n    input[xy_exchanged, :, 0], input[xy_exchanged, :, 1] =  input[xy_exchanged, :, 1], input[xy_exchanged, :, 0]\n    \n    # scale to (0, 1)\n    scale_degree = torch.max(diff_x, diff_y)\n    scale_degree = scale_degree.view(input.shape[0], 1, 1)\n    input /= scale_degree + 1e-10\n    return input\n\ndef revision(opts, revision_cost_func, reviser, decomposed_seeds, original_subtour, iter=None, embeddings=None):\n\n    # tour length of segment TSPs\n    reviser_size = original_subtour.shape[0]\n    init_cost = revision_cost_func(decomposed_seeds, original_subtour)\n    \n    # coordinate transformation\n    transformed_seeds = coordinate_transformation(decomposed_seeds)\n    # augmentation\n    if not opts.no_aug:\n        seed2 = torch.cat((1 - transformed_seeds[:, :, [0]], transformed_seeds[:, :, [1]]), dim=2)\n        seed3 = torch.cat((transformed_seeds[:, :, [0]], 1 - transformed_seeds[:, :, [1]]), dim=2)\n        seed4 = torch.cat((1 - transformed_seeds[:, :, [0]], 1 - transformed_seeds[:, :, [1]]), dim=2)\n        augmented_seeds = torch.cat((transformed_seeds, seed2, seed3, seed4), dim=0)\n    else:\n        augmented_seeds = transformed_seeds\n\n    if iter is None:\n        cost_revised1, sub_tour1, cost_revised2, sub_tour2 = reviser(augmented_seeds, return_pi=True)\n    elif iter == 0:\n        cost_revised1, sub_tour1, cost_revised2, sub_tour2, embeddings = reviser(augmented_seeds, return_pi=True, return_embedding=True)\n    else:\n        cost_revised1, sub_tour1, cost_revised2, sub_tour2 = reviser(augmented_seeds, return_pi=True, embeddings=embeddings)\n    \n    if not opts.no_aug:\n        _, better_tour_idx = torch.cat([cost_revised1, cost_revised2], dim=0).reshape(8,-1).min(dim=0)\n        sub_tour = torch.cat([sub_tour1, sub_tour2], dim=0).reshape(8,-1, reviser_size)[better_tour_idx, torch.arange(sub_tour1.shape[0]//4), :]\n    else:\n        _, better_tour_idx = torch.stack((cost_revised1, cost_revised2)).min(dim=0)\n        sub_tour = torch.stack((sub_tour1, sub_tour2))[better_tour_idx, torch.arange(sub_tour1.shape[0])]\n\n    cost_revised, _ = reviser.problem.get_costs(decomposed_seeds, sub_tour)\n    reduced_cost = init_cost - cost_revised\n    \n    sub_tour[reduced_cost < 0] = original_subtour\n    decomposed_seeds = decomposed_seeds.gather(1, sub_tour.unsqueeze(-1).expand_as(decomposed_seeds))\n    \n    if embeddings is not None:\n        if not opts.no_aug:\n            embeddings = embeddings.gather(1, sub_tour.repeat(4, 1).unsqueeze(-1).expand_as(embeddings))\n        else:\n            embeddings = embeddings.gather(1, sub_tour.unsqueeze(-1).expand_as(embeddings))\n\n    return decomposed_seeds, embeddings\n\ndef LCP_TSP(\n    seeds,\n    cost_func,\n    reviser,\n    revision_len,\n    revision_iter,\n    opts,\n    shift_len\n    ):\n    \n    batch_size, num_nodes, coordinate_dim = seeds.shape\n    offset = num_nodes % revision_len\n    embeddings = None # used only in case problem_size == revision_len for efficiency\n    for i in range(revision_iter):\n\n        decomposed_seeds, offset_seed = decomposition(seeds, \n                                        coordinate_dim,\n                                        revision_len,\n                                        offset,\n                                        shift_len\n                                        )\n\n        original_subtour = torch.arange(0, revision_len, dtype=torch.long).to(decomposed_seeds.device)\n\n        if revision_len == num_nodes:\n            decomposed_seeds_revised, embeddings = revision(opts, cost_func, reviser, decomposed_seeds, original_subtour, iter=i, embeddings=embeddings)\n            embeddings = torch.cat([embeddings[:, shift_len:],embeddings[:, :shift_len]], 1) # roll the embeddings\n        else:\n            decomposed_seeds_revised, _ = revision(opts, cost_func, reviser, decomposed_seeds, original_subtour)\n\n        seeds = decomposed_seeds_revised.reshape(batch_size, -1, coordinate_dim) \n        if offset_seed is not None:\n            seeds = torch.cat([seeds,offset_seed], dim=1)\n    return seeds\n\n\ndef reconnect( \n        get_cost_func,\n        batch,\n        opts, \n        revisers,\n    ):\n    seed = batch\n    problem_size = seed.size(1) \n    if len(revisers) == 0:\n        cost_revised = (seed[:, 1:] - seed[:, :-1]).norm(p=2, dim=2).sum(1) + (seed[:, 0] - seed[:, -1]).norm(p=2, dim=1)\n    \n    for revision_id in range(len(revisers)):\n        assert opts.revision_lens[revision_id] <= seed.size(1)\n        start_time = time.time()\n        shift_len = max(opts.revision_lens[revision_id]//opts.revision_iters[revision_id], 1)\n        seed = LCP_TSP(\n            seed, \n            get_cost_func,\n            revisers[revision_id],\n            opts.revision_lens[revision_id],\n            opts.revision_iters[revision_id],\n            opts=opts,\n            shift_len=shift_len\n            )\n        cost_revised = (seed[:, 1:] - seed[:, :-1]).norm(p=2, dim=2).sum(1) + (seed[:, 0] - seed[:, -1]).norm(p=2, dim=1)      \n        duration = time.time() - start_time\n        \n        if revision_id == 0 and not opts.no_prune: # eliminate the underperforming ones after the first round of revisions\n            cost_revised, cost_revised_minidx = cost_revised.reshape(-1, opts.eval_batch_size).min(0) # width, bs\n            seed = seed.reshape(-1, opts.eval_batch_size, seed.shape[-2], 2)[cost_revised_minidx, torch.arange(opts.eval_batch_size)]\n    if opts.no_prune:\n            cost_revised, cost_revised_minidx = cost_revised.reshape(-1, opts.eval_batch_size).min(0)\n            seed = seed.reshape(-1, opts.eval_batch_size, seed.shape[-2], 2)[cost_revised_minidx, torch.arange(opts.eval_batch_size)]\n    assert cost_revised.shape == (opts.eval_batch_size,)\n    assert seed.shape == (opts.eval_batch_size, problem_size, 2)\n        \n    return seed, cost_revised\n",
  "description": "Combined Analysis:\n- [eval_atsp/ASHPPModel.py]: This file implements the autoregressive neural heuristic for local construction in the GLOP framework, specifically for solving Asymmetric Shortest Hamiltonian Path Problems (ASHPPs) which are subproblems of ATSP. The ASHPPModel class uses an encoder-decoder architecture with attention mechanisms to construct solutions step-by-step, aligning with the paper's 'local construction' phase. The model processes asymmetric cost matrices, uses mixed-score multi-head attention that incorporates cost information, and generates node selection probabilities through an autoregressive decoding process.\n- [eval_atsp/ATSPModel.py]: This file implements the autoregressive neural construction component for ATSP using an encoder-decoder attention architecture. It handles asymmetric costs through separate row/column embeddings and uses mixed-score attention that incorporates the cost matrix. The model generates solutions step-by-step through probabilistic node selection, which aligns with the 'local construction' part of GLOP's hierarchical framework. However, it does not implement the global partitioning or the complete GLOP algorithm described in the paper.\n- [heatmap/cvrp/train.py]: This file implements the training pipeline for the global partition component of GLOP specifically for CVRP. It trains a neural network (Net) to generate heatmaps that guide the partitioning of customers into vehicle routes. The training uses REINFORCE with a baseline, where routes are sampled from the heatmap, converted to TSP instances, and evaluated using pre-trained revisers (local construction models). The key algorithm steps include: 1) generating CVRP instances, 2) computing heatmaps via a GNN, 3) sampling routes with capacity constraints, 4) transforming routes into TSP subproblems, 5) evaluating TSPs with autoregressive revisers, and 6) optimizing the partitioner via policy gradient. This aligns with GLOP's hierarchical approach: non-autoregressive partitioning followed by autoregressive construction.\n- [main.py]: This file implements the core evaluation pipeline for GLOP's hierarchical framework. It demonstrates the key algorithm steps: 1) Global partition (for CVRP/CVRPLIB via partitioner models), 2) Local construction via reviser models (autoregressive neural heuristics), and 3) Solution improvement through iterative reconnection. The code handles multiple routing problems (TSP, PCTSP, CVRP, CVRPLIB) with problem-specific initializations and cost calculations. The mathematical optimization objective (minimizing total distance/cost) is computed via cost functions, while constraints are implicitly enforced through the neural construction process and problem-specific handling (e.g., capacity constraints in CVRP via partitioning).\n- [nets/attention_local.py]: This file implements the core autoregressive neural heuristic for local construction in GLOP's hierarchical framework. The AttentionModel class is a transformer-based architecture that sequentially constructs solutions for routing problems (TSP, ATSP, CVRP, PCTSP). Key aspects:\n1. Encoder-decoder architecture with GraphAttentionEncoder for node embeddings\n2. Autoregressive decoding via the _inner() method that iteratively selects nodes\n3. Supports both forward and reverse tour construction (pi and pi2) for bidirectional decoding\n4. Problem-specific adaptations through state management and masking\n5. Implements greedy and sampling-based decoding strategies\n\nThis directly corresponds to the 'local construction' phase in GLOP where Shortest Hamiltonian Path Problems (SHPPs) are solved autoregressively after global partitioning.\n- [nets/partition_net.py]: This file implements the neural network architecture for the global partition component of GLOP. The Net class combines an EmbNet (graph neural network for edge embeddings) with a ParNet (MLP for predicting heuristic values) to produce a heuristic vector over edges. This corresponds to the non-autoregressive partitioning policy described in the paper, which learns to sparsify the graph by assigning probabilities to edges, effectively partitioning the large routing problem into smaller subproblems (TSPs). The reshape method converts the heuristic vector into a sparse adjacency matrix, aligning with the mathematical model's edge selection variables x_ij.\n- [problems/cvrp.py]: This file implements the global partition step of the GLOP algorithm for the Capacitated Vehicle Routing Problem (CVRP). It aligns with the paper's core logic by dividing large CVRP instances into smaller Travelling Salesman Problems (TSPs) through non-autoregressive neural partitioning. Specifically, the 'init' function loads CVRP data, uses a learned partitioner to infer heatmaps for customer assignments, samples routes (subsets) based on capacity constraints, and transforms these routes into TSP instances. This corresponds to the hierarchical decomposition where global partition policies break down the problem into manageable sub-problems (TSPs), which are then solved by local construction policies in subsequent steps. The code does not directly solve the mathematical optimization model but enables scalability by preprocessing instances for neural construction solvers.\n- [problems/pctsp.py]: This file implements the global partition phase of GLOP for PCTSP. It loads problem instances, uses a neural partitioner (non-autoregressive) to generate heatmaps, samples subsets of nodes (partitions) via a sampler, and transforms each subset into a TSP instance for subsequent local construction. The code aligns with GLOP's hierarchical approach: partitioning large PCTSP into smaller TSPs (global partition) before solving each as SHPP (local construction, not shown here). Key steps include: 1) data preparation with depot, coordinates, penalties, and prizes; 2) heatmap inference via a pre-trained partitioner; 3) subset sampling (partition generation); 4) penalty calculation per subset; 5) padding and batching of subsets; 6) transformation of subsets into TSP coordinate sequences. This directly corresponds to the paper's core algorithm of using non-autoregressive models for coarse-grained problem partitioning.\n- [utils/functions.py]: This file implements the core LCP-TSP (Local Construction and Partition for TSP) algorithm, which is a key component of GLOP's hierarchical framework. The code performs iterative decomposition of large TSP tours into smaller subtours, applies neural revisers (autoregressive models) to reconstruct each subtour, and then recomposes the improved subtours back into a full tour. This aligns with the paper's algorithm steps of partitioning large routing problems into TSPs and then TSPs into SHPPs for local construction. The functions handle coordinate transformations, data augmentation, and multi-level revision with pruning, directly supporting the optimization objective of minimizing total tour distance.",
  "dependencies": [
    "heatmap.cvrp.inst.gen_pyg_data",
    "ATSPModel_LIB.AddAndInstanceNormalization",
    "typing.NamedTuple",
    "problems.cvrp.init",
    "LCP_TSP",
    "utils.functions.sample_many",
    "ParNet",
    "EmbNet",
    "pickle",
    "heatmap.cvrp.sampler.Sampler",
    "sys",
    "load_dataset",
    "multiprocessing",
    "ATSPModel_LIB.FeedForward",
    "heatmap.cvrp.eval.eval",
    "torch.utils.data.DataLoader",
    "time",
    "torch.utils.checkpoint",
    "copy.deepcopy",
    "torch_geometric.nn",
    "heatmap.pctsp.infer.load_partitioner",
    "utils.load_model",
    "problems.pctsp.init",
    "heatmap.cvrp.infer.infer",
    "torch.nn.functional",
    "utils.beam_search.CachedLookup",
    "heatmap.pctsp.infer.infer",
    "utils.functions.load_problem",
    "concat_list",
    "coordinate_transformation",
    "heatmap.cvrp.inst.gen_inst",
    "utils.tensor_functions.compute_in_batches",
    "tqdm",
    "math",
    "heatmap.cvrp.inst.sum_cost",
    "utils.insertion.random_insertion_parallel",
    "AddAndInstanceNormalization",
    "FeedForward",
    "add_padding",
    "heatmap.cvrp.infer.load_partitioner",
    "json",
    "revision",
    "os",
    "decomposition",
    "utils.functions.reconnect",
    "reconnect",
    "argparse",
    "_get_encoding",
    "MixedScore_MultiHeadAttention",
    "heatmap.pctsp.sampler.Sampler",
    "nets.partition_net.Net",
    "numpy",
    "torch.nn.DataParallel",
    "reshape_by_heads",
    "heatmap.cvrp.inst.trans_tsp",
    "ATSPModel_LIB",
    "ATSPModel_LIB.MixedScore_MultiHeadAttention",
    "torch.nn",
    "MLP",
    "torch",
    "nets.graph_encoder.GraphAttentionEncoder"
  ]
}