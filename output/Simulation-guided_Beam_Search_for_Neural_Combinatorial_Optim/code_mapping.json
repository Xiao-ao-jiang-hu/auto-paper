{
  "file_path": "CVRP/2_SGBS/CVRPModel.py, CVRP/3_SGBS+EAS/E_CVRPModel.py, FFSP/2_SGBS/FFSPEnv.py, FFSP/3_SGBS+EAS/E_FFSPModel.py, TSP/3_SGBS+EAS/E_TSPModel.py",
  "function_name": "CVRPModel, E_CVRPModel, EAS_CVRP_Decoder, FFSPEnv, E_FFSPModel, E_OneStageModel, E_FFSP_Decoder, E_TSPModel, E_TSP_Decoder",
  "code_snippet": "\n\n# ==========================================\n# File: CVRP/2_SGBS/CVRPModel.py\n# Function/Context: CVRPModel\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass CVRPModel(nn.Module):\n\n    def __init__(self, **model_params):\n        super().__init__()\n        self.model_params = model_params\n\n        self.encoder = CVRP_Encoder(**model_params)\n        self.decoder = CVRP_Decoder(**model_params)\n        self.encoded_nodes = None\n        # shape: (batch, problem+1, EMBEDDING_DIM)\n\n    def pre_forward(self, reset_state):\n        depot_xy = reset_state.depot_xy\n        # shape: (batch, 1, 2)\n        node_xy = reset_state.node_xy\n        # shape: (batch, problem, 2)\n        node_demand = reset_state.node_demand\n        # shape: (batch, problem)\n        node_xy_demand = torch.cat((node_xy, node_demand[:, :, None]), dim=2)\n        # shape: (batch, problem, 3)\n\n        self.encoded_nodes = self.encoder(depot_xy, node_xy_demand)\n        # shape: (batch, problem+1, embedding)\n        self.decoder.set_kv(self.encoded_nodes)\n\n\n    def get_expand_prob(self, state):\n\n        encoded_last_node = _get_encoding(self.encoded_nodes, state.current_node)\n        # shape: (batch, beam_width, embedding)\n        probs = self.decoder(encoded_last_node, state.load, ninf_mask=state.ninf_mask)\n        # shape: (batch, beam_width, problem+1)\n\n        return probs\n\n        \n    def forward(self, state, eval_type='greedy'):\n        batch_size = state.BATCH_IDX.size(0)\n        pomo_size = state.BATCH_IDX.size(1)\n\n        if state.selected_count == 0:  # First Move, depot\n            selected = torch.zeros(size=(batch_size, pomo_size), dtype=torch.long)\n            prob = torch.ones(size=(batch_size, pomo_size))\n\n            # # Use Averaged encoded nodes for decoder input_1\n            # encoded_nodes_mean = self.encoded_nodes.mean(dim=1, keepdim=True)\n            # # shape: (batch, 1, embedding)\n            # self.decoder.set_q1(encoded_nodes_mean)\n\n            # # Use encoded_depot for decoder input_2\n            # encoded_first_node = self.encoded_nodes[:, [0], :]\n            # # shape: (batch, 1, embedding)\n            # self.decoder.set_q2(encoded_first_node)\n\n        elif state.selected_count == 1:  # Second Move, POMO\n            selected = torch.arange(start=1, end=pomo_size+1)[None, :].expand(batch_size, pomo_size)\n            prob = torch.ones(size=(batch_size, pomo_size))\n\n        else:\n            encoded_last_node = _get_encoding(self.encoded_nodes, state.current_node)\n            # shape: (batch, pomo, embedding)\n            probs = self.decoder(encoded_last_node, state.load, ninf_mask=state.ninf_mask)\n            # shape: (batch, pomo, problem+1)\n\n            if self.training or eval_type == 'softmax':\n                categorical = torch.distributions.categorical.Categorical(probs=probs)\n                selected = categorical.sample()\n                # shape: (batch, pomo)\n                prob = probs[state.BATCH_IDX, state.POMO_IDX, selected]\n                # shape: (batch, pomo)\n\n            else:\n                selected = probs.argmax(dim=2)\n                # shape: (batch, pomo)\n                prob = None  # value not needed. Can be anything.\n\n        return selected, prob\n\n\ndef _get_encoding(encoded_nodes, node_index_to_pick):\n    # encoded_nodes.shape: (batch, problem, embedding)\n    # node_index_to_pick.shape: (batch, pomo)\n\n    batch_size = node_index_to_pick.size(0)\n    pomo_size = node_index_to_pick.size(1)\n    embedding_dim = encoded_nodes.size(2)\n\n    gathering_index = node_index_to_pick[:, :, None].expand(batch_size, pomo_size, embedding_dim)\n    # shape: (batch, pomo, embedding)\n\n    picked_nodes = encoded_nodes.gather(dim=1, index=gathering_index)\n    # shape: (batch, pomo, embedding)\n\n    return picked_nodes\n\n\n########################################\n# ENCODER\n########################################\n\nclass CVRP_Encoder(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n        self.model_params = model_params\n        embedding_dim = self.model_params['embedding_dim']\n        encoder_layer_num = self.model_params['encoder_layer_num']\n\n        self.embedding_depot = nn.Linear(2, embedding_dim)\n        self.embedding_node = nn.Linear(3, embedding_dim)\n        self.layers = nn.ModuleList([EncoderLayer(**model_params) for _ in range(encoder_layer_num)])\n\n    def forward(self, depot_xy, node_xy_demand):\n        # depot_xy.shape: (batch, 1, 2)\n        # node_xy_demand.shape: (batch, problem, 3)\n\n        embedded_depot = self.embedding_depot(depot_xy)\n        # shape: (batch, 1, embedding)\n        embedded_node = self.embedding_node(node_xy_demand)\n        # shape: (batch, problem, embedding)\n\n        out = torch.cat((embedded_depot, embedded_node), dim=1)\n        # shape: (batch, problem+1, embedding)\n\n        for layer in self.layers:\n            out = layer(out)\n\n        return out\n        # shape: (batch, problem+1, embedding)\n\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n        self.model_params = model_params\n        embedding_dim = self.model_params['embedding_dim']\n        head_num = self.model_params['head_num']\n        qkv_dim = self.model_params['qkv_dim']\n\n        self.Wq = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.Wk = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.Wv = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.multi_head_combine = nn.Linear(head_num * qkv_dim, embedding_dim)\n\n        self.add_n_normalization_1 = AddAndInstanceNormalization(**model_params)\n        self.feed_forward = FeedForward(**model_params)\n        self.add_n_normalization_2 = AddAndInstanceNormalization(**model_params)\n\n    def forward(self, input1):\n        # input1.shape: (batch, problem+1, embedding)\n        head_num = self.model_params['head_num']\n\n        q = reshape_by_heads(self.Wq(input1), head_num=head_num)\n        k = reshape_by_heads(self.Wk(input1), head_num=head_num)\n        v = reshape_by_heads(self.Wv(input1), head_num=head_num)\n        # qkv shape: (batch, head_num, problem, qkv_dim)\n\n        out_concat = multi_head_attention(q, k, v)\n        # shape: (batch, problem, head_num*qkv_dim)\n\n        multi_head_out = self.multi_head_combine(out_concat)\n        # shape: (batch, problem, embedding)\n\n        out1 = self.add_n_normalization_1(input1, multi_head_out)\n        out2 = self.feed_forward(out1)\n        out3 = self.add_n_normalization_2(out1, out2)\n\n        return out3\n        # shape: (batch, problem, embedding)\n\n\n########################################\n# DECODER\n########################################\n\nclass CVRP_Decoder(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n        self.model_params = model_params\n        embedding_dim = self.model_params['embedding_dim']\n        head_num = self.model_params['head_num']\n        qkv_dim = self.model_params['qkv_dim']\n\n        # self.Wq_1 = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        # self.Wq_2 = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.Wq_last = nn.Linear(embedding_dim+1, head_num * qkv_dim, bias=False)\n        self.Wk = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n        self.Wv = nn.Linear(embedding_dim, head_num * qkv_dim, bias=False)\n\n        self.multi_head_combine = nn.Linear(head_num * qkv_dim, embedding_dim)\n\n        self.k = None  # saved key, for multi-head attention\n        self.v = None  # saved value, for multi-head_attention\n        self.single_head_key = None  # saved, for single-head attention\n        # self.q1 = None  # saved q1, for multi-head attention\n        # self.q2 = None  # saved q2, for multi-head attention\n\n    def set_kv(self, encoded_nodes):\n        # encoded_nodes.shape: (batch, problem+1, embedding)\n        head_num = self.model_params['head_num']\n\n        self.k = reshape_by_heads(self.Wk(encoded_nodes), head_num=head_num)\n        self.v = reshape_by_heads(self.Wv(encoded_nodes), head_num=head_num)\n        # shape: (batch, head_num, problem+1, qkv_dim)\n        self.single_head_key = encoded_nodes.transpose(1, 2)\n        # shape: (batch, embedding, problem+1)\n\n    def set_q1(self, encoded_q1):\n        # encoded_q.shape: (batch, n, embedding)  # n can be 1 or pomo\n        head_num = self.model_params['head_num']\n        self.q1 = reshape_by_heads(self.Wq_1(encoded_q1), head_num=head_num)\n        # shape: (batch, head_num, n, qkv_dim)\n\n    def set_q2(self, encoded_q2):\n        # encoded_q.shape: (batch, n, embedding)  # n can be 1 or pomo\n        head_num = self.model_params['head_num']\n        self.q2 = reshape_by_heads(self.Wq_2(encoded_q2), head_num=head_num)\n        # shape: (batch, head_num, n, qkv_dim)\n\n    def forward(self, encoded_last_node, load, ninf_mask):\n        # encoded_last_node.shape: (batch, pomo, embedding)\n        # load.shape: (batch, pomo)\n        # ninf_mask.shape: (batch, pomo, problem)\n\n        head_num = self.model_params['head_num']\n\n        #  Multi-Head Attention\n        #######################################################\n        input_cat = torch.cat((encoded_last_node, load[:, :, None]), dim=2)\n        # shape = (batch, group, EMBEDDING_DIM+1)\n\n        q_last = reshape_by_heads(self.Wq_last(input_cat), head_num=head_num)\n        # shape: (batch, head_num, pomo, qkv_dim)\n\n        # q = self.q1 + self.q2 + q_last\n        # # shape: (batch, head_num, pomo, qkv_dim)\n        q = q_last\n        # shape: (batch, head_num, pomo, qkv_dim)\n\n        out_concat = multi_head_attention(q, self.k, self.v, rank3_ninf_mask=ninf_mask)\n        # shape: (batch, pomo, head_num*qkv_dim)\n\n        mh_atten_out = self.multi_head_combine(out_concat)\n        # shape: (batch, pomo, embedding)\n\n        #  Single-Head Attention, for probability calculation\n        #######################################################\n        score = torch.matmul(mh_atten_out, self.single_head_key)\n        # shape: (batch, pomo, problem)\n\n        sqrt_embedding_dim = self.model_params['sqrt_embedding_dim']\n        logit_clipping = self.model_params['logit_clipping']\n\n        score_scaled = score / sqrt_embedding_dim\n        # shape: (batch, pomo, problem)\n\n        score_clipped = logit_clipping * torch.tanh(score_scaled)\n\n        score_masked = score_clipped + ninf_mask\n\n        probs = F.softmax(score_masked, dim=2)\n        # shape: (batch, pomo, problem)\n\n        return probs\n\n\n########################################\n# NN SUB CLASS / FUNCTIONS\n########################################\n\ndef reshape_by_heads(qkv, head_num):\n    # q.shape: (batch, n, head_num*key_dim)   : n can be either 1 or PROBLEM_SIZE\n\n    batch_s = qkv.size(0)\n    n = qkv.size(1)\n\n    q_reshaped = qkv.reshape(batch_s, n, head_num, -1)\n    # shape: (batch, n, head_num, key_dim)\n\n    q_transposed = q_reshaped.transpose(1, 2)\n    # shape: (batch, head_num, n, key_dim)\n\n    return q_transposed\n\n\ndef multi_head_attention(q, k, v, rank2_ninf_mask=None, rank3_ninf_mask=None):\n    # q shape: (batch, head_num, n, key_dim)   : n can be either 1 or PROBLEM_SIZE\n    # k,v shape: (batch, head_num, problem, key_dim)\n    # rank2_ninf_mask.shape: (batch, problem)\n    # rank3_ninf_mask.shape: (batch, group, problem)\n\n    batch_s = q.size(0)\n    head_num = q.size(1)\n    n = q.size(2)\n    key_dim = q.size(3)\n\n    input_s = k.size(2)\n\n    score = torch.matmul(q, k.transpose(2, 3))\n    # shape: (batch, head_num, n, problem)\n\n    score_scaled = score / torch.sqrt(torch.tensor(key_dim, dtype=torch.float))\n    if rank2_ninf_mask is not None:\n        score_scaled = score_scaled + rank2_ninf_mask[:, None, None, :].expand(batch_s, head_num, n, input_s)\n    if rank3_ninf_mask is not None:\n        score_scaled = score_scaled + rank3_ninf_mask[:, None, :, :].expand(batch_s, head_num, n, input_s)\n\n    weights = nn.Softmax(dim=3)(score_scaled)\n    # shape: (batch, head_num, n, problem)\n\n    out = torch.matmul(weights, v)\n    # shape: (batch, head_num, n, key_dim)\n\n    out_transposed = out.transpose(1, 2)\n    # shape: (batch, n, head_num, key_dim)\n\n    out_concat = out_transposed.reshape(batch_s, n, head_num * key_dim)\n    # shape: (batch, n, head_num*key_dim)\n\n    return out_concat\n\n\nclass AddAndInstanceNormalization(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n        embedding_dim = model_params['embedding_dim']\n        self.norm = nn.InstanceNorm1d(embedding_dim, affine=True, track_running_stats=False)\n\n    def forward(self, input1, input2):\n        # input.shape: (batch, problem, embedding)\n\n        added = input1 + input2\n        # shape: (batch, problem, embedding)\n\n        transposed = added.transpose(1, 2)\n        # shape: (batch, embedding, problem)\n\n        normalized = self.norm(transposed)\n        # shape: (batch, embedding, problem)\n\n        back_trans = normalized.transpose(1, 2)\n        # shape: (batch, problem, embedding)\n\n        return back_trans\n\n\nclass AddAndBatchNormalization(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n        embedding_dim = model_params['embedding_dim']\n        self.norm_by_EMB = nn.BatchNorm1d(embedding_dim, affine=True)\n        # 'Funny' Batch_Norm, as it will normalized by EMB dim\n\n    def forward(self, input1, input2):\n        # input.shape: (batch, problem, embedding)\n\n        batch_s = input1.size(0)\n        problem_s = input1.size(1)\n        embedding_dim = input1.size(2)\n\n        added = input1 + input2\n        normalized = self.norm_by_EMB(added.reshape(batch_s * problem_s, embedding_dim))\n        back_trans = normalized.reshape(batch_s, problem_s, embedding_dim)\n\n        return back_trans\n\nclass FeedForward(nn.Module):\n    def __init__(self, **model_params):\n        super().__init__()\n        embedding_dim = model_params['embedding_dim']\n        ff_hidden_dim = model_params['ff_hidden_dim']\n\n        self.W1 = nn.Linear(embedding_dim, ff_hidden_dim)\n        self.W2 = nn.Linear(ff_hidden_dim, embedding_dim)\n\n    def forward(self, input1):\n        # input.shape: (batch, problem, embedding)\n\n        return self.W2(F.relu(self.W1(input1)))\n\n# ==========================================\n# File: CVRP/3_SGBS+EAS/E_CVRPModel.py\n# Function/Context: E_CVRPModel, EAS_CVRP_Decoder\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom CVRPModel import CVRPModel, _get_encoding, CVRP_Encoder, CVRP_Decoder, reshape_by_heads, multi_head_attention\n\nclass E_CVRPModel(CVRPModel):\n\n    def __init__(self, **model_params):\n        nn.Module.__init__(self)\n        self.model_params = model_params\n\n        self.encoder = CVRP_Encoder(**model_params)\n        self.decoder = EAS_CVRP_Decoder(**model_params)\n        self.encoded_nodes = None\n        # shape: (batch, problem, embedding)\n\n    def get_expand_prob(self, state):\n\n        encoded_last_node = _get_encoding(self.encoded_nodes, state.current_node)\n        # shape: (batch, beam_width, embedding)\n        probs = self.decoder(encoded_last_node, state.load, ninf_mask=state.ninf_mask)\n        # shape: (batch, beam_width, problem+1)\n\n        return probs\n\n    def forward_w_incumbent(self, state, best_action):\n        # best_action.shape = (batch,)\n\n        batch_size = state.BATCH_IDX.size(0)\n        pomo_size_p1 = state.BATCH_IDX.size(1)\n        pomo_size = pomo_size_p1-1\n\n        if state.selected_count == 0:  # First Move, depot\n            selected = torch.zeros(size=(batch_size, pomo_size_p1), dtype=torch.long)\n            prob = torch.ones(size=(batch_size, pomo_size_p1))\n\n        elif state.selected_count == 1:  # Second Move, POMO\n            selected = torch.arange(start=1, end=pomo_size+1)[None, :].expand(batch_size, pomo_size)\n            selected = torch.cat((selected, best_action[:, None]), dim=1)\n            # shape: (batch, pomo+1)\n            prob = torch.ones(size=(batch_size, pomo_size_p1))\n\n        else:\n            encoded_last_node = _get_encoding(self.encoded_nodes, state.current_node)\n            # shape: (batch, pomo+1, embedding)\n            probs = self.decoder(encoded_last_node, state.load, ninf_mask=state.ninf_mask)\n            # shape: (batch, pomo+1, problem+1)\n\n            if self.training or self.model_params['eval_type'] == 'softmax':\n                while True:  # to fix pytorch.multinomial bug on selecting 0 probability elements\n                    with torch.no_grad():\n                        selected = probs.reshape(batch_size * pomo_size_p1, -1).multinomial(1) \\\n                            .squeeze(dim=1).reshape(batch_size, pomo_size_p1)\n                    # shape: (batch, pomo+1)\n                    selected[:, -1] = best_action\n                    prob = probs[state.BATCH_IDX, state.POMO_IDX, selected].reshape(batch_size, pomo_size_p1)\n                    # shape: (batch, pomo+1)\n                    if (prob != 0).all():\n                        break\n\n            else:\n                selected = probs.argmax(dim=2)\n                # shape: (batch, pomo+1)\n                selected[:, -1] = best_action\n                prob = None  # value not needed. Can be anything.\n\n        return selected, prob\n\n\n    def forward_w_incumbent_probs(self, state, best_action):\n        # best_action.shape = (batch,)\n\n        batch_size = state.BATCH_IDX.size(0)\n        pomo_size_p1 = state.BATCH_IDX.size(1)\n        pomo_size = pomo_size_p1-1\n\n        probs = None\n        \n        if state.selected_count == 0:  # First Move, depot\n            selected = torch.zeros(size=(batch_size, pomo_size_p1), dtype=torch.long)\n            prob = torch.ones(size=(batch_size, pomo_size_p1))\n\n        elif state.selected_count == 1:  # Second Move, POMO\n            selected = torch.arange(start=1, end=pomo_size+1)[None, :].expand(batch_size, pomo_size)\n            selected = torch.cat((selected, best_action[:, None]), dim=1)\n            # shape: (batch, pomo+1)\n            prob = torch.ones(size=(batch_size, pomo_size_p1))\n\n        else:\n            encoded_last_node = _get_encoding(self.encoded_nodes, state.current_node)\n            # shape: (batch, pomo+1, embedding)\n            probs = self.decoder(encoded_last_node, state.load, ninf_mask=state.ninf_mask)\n            # shape: (batch, pomo+1, problem+1)\n\n            if self.training or self.model_params['eval_type'] == 'softmax':\n                while True:  # to fix pytorch.multinomial bug on selecting 0 probability elements\n                    with torch.no_grad():\n                        selected = probs.reshape(batch_size * pomo_size_p1, -1).multinomial(1) \\\n                            .squeeze(dim=1).reshape(batch_size, pomo_size_p1)\n                    # shape: (batch, pomo+1)\n                    selected[:, -1] = best_action\n                    prob = probs[state.BATCH_IDX, state.POMO_IDX, selected].reshape(batch_size, pomo_size_p1)\n                    # shape: (batch, pomo+1)\n                    if (prob != 0).all():\n                        break\n\n            else:\n                selected = probs.argmax(dim=2)\n                # shape: (batch, pomo+1)\n                selected[:, -1] = best_action\n                prob = None  # value not needed. Can be anything.\n\n        return selected, prob, probs\n\n\nclass EAS_CVRP_Decoder(CVRP_Decoder):\n\n    def __init__(self, **model_params):\n        super().__init__(**model_params)\n\n        self.enable_EAS = None  # bool\n\n        self.eas_W1 = None\n        # shape: (batch, embedding, embedding)\n        self.eas_b1 = None\n        # shape: (batch, embedding)\n        self.eas_W2 = None\n        # shape: (batch, embedding, embedding)\n        self.eas_b2 = None\n        # shape: (batch, embedding)\n        \n    def init_eas_layers_random(self, batch_size):\n        emb_dim = self.model_params['embedding_dim']  # 128\n        init_lim = (1/emb_dim)**(1/2)\n        \n        weight1 = torch.torch.distributions.Uniform(low=-init_lim, high=init_lim).sample((batch_size, emb_dim, emb_dim))\n        bias1 = torch.torch.distributions.Uniform(low=-init_lim, high=init_lim).sample((batch_size, emb_dim))\n        self.eas_W1 = torch.nn.Parameter(weight1)\n        self.eas_b1 = torch.nn.Parameter(bias1)\n        self.eas_W2 = torch.nn.Parameter(torch.zeros(size=(batch_size, emb_dim, emb_dim)))\n        self.eas_b2 = torch.nn.Parameter(torch.zeros(size=(batch_size, emb_dim)))\n\n    def init_eas_layers_manual(self, W1, b1, W2, b2):\n        self.eas_W1 = torch.nn.Parameter(W1)\n        self.eas_b1 = torch.nn.Parameter(b1)\n        self.eas_W2 = torch.nn.Parameter(W2)\n        self.eas_b2 = torch.nn.Parameter(b2)\n\n    def eas_parameters(self):\n        return [self.eas_W1, self.eas_b1, self.eas_W2, self.eas_b2]\n        \n    def forward(self, encoded_last_node, load, ninf_mask):\n        # encoded_last_node.shape: (batch, pomo, embedding)\n        # load.shape: (batch, pomo)\n        # ninf_mask.shape: (batch, pomo, problem)\n\n        head_num = self.model_params['head_num']\n\n        #  Multi-Head Attention\n        #######################################################\n        input_cat = torch.cat((encoded_last_node, load[:, :, None]), dim=2)\n        # shape = (batch, group, EMBEDDING_DIM+1)\n\n        q_last = reshape_by_heads(self.Wq_last(input_cat), head_num=head_num)\n        # shape: (batch, head_num, pomo, qkv_dim)\n\n        out_concat = multi_head_attention(q_last, self.k, self.v, rank3_ninf_mask=ninf_mask)\n        # shape: (batch, pomo, head_num*qkv_dim)\n\n        mh_atten_out = self.multi_head_combine(out_concat)\n        # shape: (batch, pomo, embedding)\n\n        # EAS Layer Insert\n        #######################################################\n        if self.enable_EAS:\n            ms1 = torch.matmul(mh_atten_out, self.eas_W1)\n            # shape: (batch, pomo, embedding)\n\n            ms1 = ms1 + self.eas_b1[:, None, :]\n            # shape: (batch, pomo, embedding)\n\n            ms1_activated = F.relu(ms1)\n            # shape: (batch, pomo, embedding)\n\n            ms2 = torch.matmul(ms1_activated, self.eas_W2)\n            # shape: (batch, pomo, embedding)\n\n            ms2 = ms2 + self.eas_b2[:, None, :]\n            # shape: (batch, pomo, embedding)\n\n            mh_atten_out = mh_atten_out + ms2\n            # shape: (batch, pomo, embedding)\n\n        #  Single-Head Attention, for probability calculation\n        #######################################################\n        score = torch.matmul(mh_atten_out, self.single_head_key)\n        # shape: (batch, pomo, problem)\n\n        sqrt_embedding_dim = self.model_params['sqrt_embedding_dim']\n        logit_clipping = self.model_params['logit_clipping']\n\n        score_scaled = score / sqrt_embedding_dim\n        # shape: (batch, pomo, problem)\n\n        score_clipped = logit_clipping * torch.tanh(score_scaled)\n\n        score_masked = score_clipped + ninf_mask\n\n        probs = F.softmax(score_masked, dim=2)\n        # shape: (batch, pomo, problem)\n\n        return probs\n\n# ==========================================\n# File: FFSP/2_SGBS/FFSPEnv.py\n# Function/Context: FFSPEnv\n# ==========================================\nfrom dataclasses import dataclass\nimport torch\nimport itertools  # for permutation list\n\nfrom FFSProblemDef import get_random_problems\n\n# For Gantt Chart\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n\n@dataclass\nclass Reset_State:\n    problems_list: list\n    # len(problems_list) = stage_cnt\n    # problems_list[current_stage].shape: (batch, job, machine_cnt_list[current_stage])\n    # float type\n\n\n@dataclass\nclass Step_State:\n    BATCH_IDX: torch.Tensor\n    POMO_IDX: torch.Tensor\n    # shape: (batch, pomo)\n    #--------------------------------------\n    step_cnt: int = 0\n    stage_idx: torch.Tensor = None\n    # shape: (batch, pomo)\n    stage_machine_idx: torch.Tensor = None\n    # shape: (batch, pomo)\n    job_ninf_mask: torch.Tensor = None\n    # shape: (batch, pomo, job+1)\n    finished: torch.Tensor = None\n    # shape: (batch, pomo)\n\n\nclass FFSPEnv:\n    def __init__(self, **env_params):\n\n        # Const @INIT\n        ####################################\n        self.env_params = env_params\n        self.stage_cnt = env_params['stage_cnt']\n        self.machine_cnt_list = env_params['machine_cnt_list']\n        self.total_machine_cnt = sum(self.machine_cnt_list)\n        self.job_cnt = env_params['job_cnt']\n        self.process_time_params = env_params['process_time_params']\n        self.pomo_size = env_params['pomo_size']\n        self.sm_indexer = _Stage_N_Machine_Index_Converter(self)\n\n        # Const @Load_Problem\n        ####################################\n        self.batch_size = None\n        self.BATCH_IDX = None\n        self.POMO_IDX = None\n        # IDX.shape: (batch, pomo)\n        self.problems_list = None\n        # len(problems_list) = stage_cnt\n        # problems_list[current_stage].shape: (batch, job, machine_cnt_list[current_stage])\n        self.job_durations = None\n        # shape: (batch, job+1, total_machine)\n        # last job means NO_JOB ==> duration = 0\n\n        # Dynamic\n        ####################################\n        self.time_idx = None\n        # shape: (batch, pomo)\n        self.sub_time_idx = None  # 0 ~ total_machine_cnt-1\n        # shape: (batch, pomo)\n        self.machine_idx = None  # must update according to sub_time_idx\n        # shape: (batch, pomo)\n\n        self.schedule = None\n        # shape: (batch, pomo, machine, job+1)\n        # records start time of each job at each machine\n        self.machine_wait_step = None\n        # shape: (batch, pomo, machine)\n        # How many time steps each machine needs to run, before it become available for a new job\n        self.job_location = None\n        # shape: (batch, pomo, job+1)\n        # index of stage each job can be processed at. if stage_cnt, it means the job is finished (when job_wait_step=0)\n        self.job_wait_step = None\n        # shape: (batch, pomo, job+1)\n        # how many time steps job needs to wait, before it is completed and ready to start at job_location\n        self.finished = None  # is scheduling done?\n        # shape: (batch, pomo)\n\n        # STEP-State\n        ####################################\n        self.step_state = None\n\n    def load_problems(self, batch_size):\n        self.batch_size = batch_size\n        self.BATCH_IDX = torch.arange(self.batch_size)[:, None].expand(self.batch_size, self.pomo_size)\n        self.POMO_IDX = torch.arange(self.pomo_size)[None, :].expand(self.batch_size, self.pomo_size)\n\n        problems_INT_list = get_random_problems(batch_size, self.stage_cnt, self.machine_cnt_list,\n                                                self.job_cnt, self.process_time_params)\n\n        problems_list = []\n        for stage_num in range(self.stage_cnt):\n            stage_problems_INT = problems_INT_list[stage_num]\n            stage_problems = stage_problems_INT.clone().type(torch.float)\n            problems_list.append(stage_problems)\n        self.problems_list = problems_list\n\n        self.job_durations = torch.empty(size=(self.batch_size, self.job_cnt+1, self.total_machine_cnt),\n                                         dtype=torch.long)\n        # shape: (batch, job+1, total_machine)\n        self.job_durations[:, :self.job_cnt, :] = torch.cat(problems_INT_list, dim=2)\n        self.job_durations[:, self.job_cnt, :] = 0\n\n    def load_problems_manual(self, problems_INT_list):\n        # problems_INT_list[current_stage].shape: (batch, job, machine_cnt_list[current_stage])\n\n        self.batch_size = problems_INT_list[0].size(0)\n        self.BATCH_IDX = torch.arange(self.batch_size)[:, None].expand(self.batch_size, self.pomo_size)\n        self.POMO_IDX = torch.arange(self.pomo_size)[None, :].expand(self.batch_size, self.pomo_size)\n\n        problems_list = []\n        for stage_num in range(self.stage_cnt):\n            stage_problems_INT = problems_INT_list[stage_num]\n            stage_problems = stage_problems_INT.clone().type(torch.float)\n            problems_list.append(stage_problems)\n        self.problems_list = problems_list\n\n        self.job_durations = torch.empty(size=(self.batch_size, self.job_cnt+1, self.total_machine_cnt),\n                                         dtype=torch.long)\n        # shape: (batch, job+1, total_machine)\n        self.job_durations[:, :self.job_cnt, :] = torch.cat(problems_INT_list, dim=2)\n        self.job_durations[:, self.job_cnt, :] = 0\n\n    def reset(self):\n        self.time_idx = torch.zeros(size=(self.batch_size, self.pomo_size), dtype=torch.long)\n        # shape: (batch, pomo)\n        self.sub_time_idx = torch.zeros(size=(self.batch_size, self.pomo_size), dtype=torch.long)\n        # shape: (batch, pomo)\n        self.machine_idx = self.sm_indexer.get_machine_index(self.POMO_IDX, self.sub_time_idx)\n        # shape: (batch, pomo)\n\n        self.schedule = torch.full(size=(self.batch_size, self.pomo_size, self.total_machine_cnt, self.job_cnt+1),\n                                   dtype=torch.long, fill_value=-999999)\n        # shape: (batch, pomo, machine, job+1)\n        self.machine_wait_step = torch.zeros(size=(self.batch_size, self.pomo_size, self.total_machine_cnt),\n                                             dtype=torch.long)\n        # shape: (batch, pomo, machine)\n        self.job_location = torch.zeros(size=(self.batch_size, self.pomo_size, self.job_cnt+1), dtype=torch.long)\n        # shape: (batch, pomo, job+1)\n        self.job_wait_step = torch.zeros(size=(self.batch_size, self.pomo_size, self.job_cnt+1), dtype=torch.long)\n        # shape: (batch, pomo, job+1)\n        self.finished = torch.full(size=(self.batch_size, self.pomo_size), dtype=torch.bool, fill_value=False)\n        # shape: (batch, pomo)\n\n        self.step_state = Step_State(BATCH_IDX=self.BATCH_IDX, POMO_IDX=self.POMO_IDX)\n\n        reward = None\n        done = None\n        return Reset_State(self.problems_list), reward, done\n\n    def pre_step(self):\n        self._update_step_state()\n        self.step_state.step_cnt = 0\n        reward = None\n        done = False\n        return self.step_state, reward, done\n\n    def step(self, job_idx):\n        # job_idx.shape: (batch, pomo)\n\n        self.schedule[self.BATCH_IDX, self.POMO_IDX, self.machine_idx, job_idx] = self.time_idx\n\n        job_length = self.job_durations[self.BATCH_IDX, job_idx, self.machine_idx]\n        # shape: (batch, pomo)\n        self.machine_wait_step[self.BATCH_IDX, self.POMO_IDX, self.machine_idx] = job_length\n        # shape: (batch, pomo, machine)\n        self.job_location[self.BATCH_IDX, self.POMO_IDX, job_idx] += 1\n        # shape: (batch, pomo, job+1)\n        self.job_wait_step[self.BATCH_IDX, self.POMO_IDX, job_idx] = job_length\n        # shape: (batch, pomo, job+1)\n        self.finished = (self.job_location[:, :, :self.job_cnt] == self.stage_cnt).all(dim=2)\n        # shape: (batch, pomo)\n\n        ####################################\n        done = self.finished.all()\n\n        if done:\n            pass  # do nothing. do not update step_state, because it won't be used anyway\n        else:\n            self._move_to_next_machine()\n            self._update_step_state()\n\n        if done:\n            reward = -self._get_makespan()  # Note the MINUS Sign ==> We want to MAXIMIZE reward\n            # shape: (batch, pomo)\n        else:\n            reward = None\n\n        return self.step_state, reward, done\n\n    def _move_to_next_machine(self):\n\n        b_idx = torch.flatten(self.BATCH_IDX)\n        # shape: (batch*pomo,) == (not_ready_cnt,)\n        p_idx = torch.flatten(self.POMO_IDX)\n        # shape: (batch*pomo,) == (not_ready_cnt,)\n        ready = torch.flatten(self.finished)\n        # shape: (batch*pomo,) == (not_ready_cnt,)\n\n        b_idx = b_idx[~ready]\n        # shape: ( (NEW) not_ready_cnt,)\n        p_idx = p_idx[~ready]\n        # shape: ( (NEW) not_ready_cnt,)\n\n        while ~ready.all():\n            new_sub_time_idx = self.sub_time_idx[b_idx, p_idx] + 1\n            # shape: (not_ready_cnt,)\n            step_time_required = new_sub_time_idx == self.total_machine_cnt\n            # shape: (not_ready_cnt,)\n            self.time_idx[b_idx, p_idx] += step_time_required.long()\n            new_sub_time_idx[step_time_required] = 0\n            self.sub_time_idx[b_idx, p_idx] = new_sub_time_idx\n            new_machine_idx = self.sm_indexer.get_machine_index(p_idx, new_sub_time_idx)\n            self.machine_idx[b_idx, p_idx] = new_machine_idx\n\n            machine_wait_steps = self.machine_wait_step[b_idx, p_idx, :]\n            # shape: (not_ready_cnt, machine)\n            machine_wait_steps[step_time_required, :] -= 1\n            machine_wait_steps[machine_wait_steps < 0] = 0\n            self.machine_wait_step[b_idx, p_idx, :] = machine_wait_steps\n\n            job_wait_steps = self.job_wait_step[b_idx, p_idx, :]\n            # shape: (not_ready_cnt, job+1)\n            job_wait_steps[step_time_required, :] -= 1\n            job_wait_steps[job_wait_steps < 0] = 0\n            self.job_wait_step[b_idx, p_idx, :] = job_wait_steps\n\n            machine_ready = self.machine_wait_step[b_idx, p_idx, new_machine_idx] == 0\n            # shape: (not_ready_cnt,)\n\n            new_stage_idx = self.sm_indexer.get_stage_index(new_sub_time_idx)\n            # shape: (not_ready_cnt,)\n            job_ready_1 = (self.job_location[b_idx, p_idx, :self.job_cnt] == new_stage_idx[:, None])\n            # shape: (not_ready_cnt, job)\n            job_ready_2 = (self.job_wait_step[b_idx, p_idx, :self.job_cnt] == 0)\n            # shape: (not_ready_cnt, job)\n            job_ready = (job_ready_1 & job_ready_2).any(dim=1)\n            # shape: (not_ready_cnt,)\n\n            ready = machine_ready & job_ready\n            # shape: (not_ready_cnt,)\n\n            b_idx = b_idx[~ready]\n            # shape: ( (NEW) not_ready_cnt,)\n            p_idx = p_idx[~ready]\n            # shape: ( (NEW) not_ready_cnt,)\n\n    def _update_step_state(self):\n        self.step_state.step_cnt += 1\n\n        self.step_state.stage_idx = self.sm_indexer.get_stage_index(self.sub_time_idx)\n        # shape: (batch, pomo)\n        self.step_state.stage_machine_idx = self.sm_indexer.get_stage_machine_index(self.POMO_IDX, self.sub_time_idx)\n        # shape: (batch, pomo)\n\n        job_loc = self.job_location[:, :, :self.job_cnt]\n        # shape: (batch, pomo, job)\n        job_wait_t = self.job_wait_step[:, :, :self.job_cnt]\n        # shape: (batch, pomo, job)\n\n        job_in_stage = job_loc == self.step_state.stage_idx[:, :, None]\n        # shape: (batch, pomo, job)\n        job_not_waiting = (job_wait_t == 0)\n        # shape: (batch, pomo, job)\n        job_available = job_in_stage & job_not_waiting\n        # shape: (batch, pomo, job)\n\n        job_in_previous_stages = (job_loc < self.step_state.stage_idx[:, :, None]).any(dim=2)\n        # shape: (batch, pomo)\n        job_waiting_in_stage = (job_in_stage & (job_wait_t > 0)).any(dim=2)\n        # shape: (batch, pomo)\n        wait_allowed = job_in_previous_stages + job_waiting_in_stage + self.finished\n        # shape: (batch, pomo)\n\n        self.step_state.job_ninf_mask = torch.full(size=(self.batch_size, self.pomo_size, self.job_cnt+1),\n                                                   fill_value=float('-inf'))\n        # shape: (batch, pomo, job+1)\n        job_enable = torch.cat((job_available, wait_allowed[:, :, None]), dim=2)\n        # shape: (batch, pomo, job+1)\n        self.step_state.job_ninf_mask[job_enable] = 0\n        # shape: (batch, pomo, job+1)\n\n        self.step_state.finished = self.finished\n        # shape: (batch, pomo)\n\n    def _get_makespan(self):\n\n        job_durations_perm = self.job_durations.permute(0, 2, 1)\n        # shape: (batch, machine, job+1)\n        end_schedule = self.schedule + job_durations_perm[:, None, :, :]\n        # shape: (batch, pomo, machine, job+1)\n\n        end_time_max, _ = end_schedule[:, :, :, :self.job_cnt].max(dim=3)\n        # shape: (batch, pomo, machine)\n        end_time_max, _ = end_time_max.max(dim=2)\n        # shape: (batch, pomo)\n\n        return end_time_max\n\n    def draw_Gantt_Chart(self, batch_i, pomo_i):\n\n        job_durations = self.job_durations[batch_i, :, :]\n        # shape: (job, machine)\n        schedule = self.schedule[batch_i, pomo_i, :, :]\n        # shape: (machine, job)\n\n        total_machine_cnt = self.total_machine_cnt\n        makespan = self._get_makespan()[batch_i, pomo_i].item()\n\n        # Create figure and axes\n        fig,ax = plt.subplots(figsize=(makespan/3, 5))\n        cmap = self._get_cmap(self.job_cnt)\n\n        plt.xlim(0, makespan)\n        plt.ylim(0, total_machine_cnt)\n        ax.invert_yaxis()\n\n        plt.plot([0, makespan], [4, 4], 'black')\n        plt.plot([0, makespan], [8, 8], 'black')\n\n        for machine_idx in range(total_machine_cnt):\n\n            duration = job_durations[:, machine_idx]\n            # shape: (job)\n            machine_schedule = schedule[machine_idx, :]\n            # shape: (job)\n\n            for job_idx in range(self.job_cnt):\n\n                job_le\n\n# ==========================================\n# File: FFSP/3_SGBS+EAS/E_FFSPModel.py\n# Function/Context: E_FFSPModel, E_OneStageModel, E_FFSP_Decoder\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom FFSPModel import FFSPModel, FFSP_Encoder, FFSP_Decoder, OneStageModel, reshape_by_heads\n\n\nclass E_FFSPModel(FFSPModel):\n\n    def __init__(self, **model_params):\n        nn.Module.__init__(self)\n        self.model_params = model_params\n\n        stage_cnt = self.model_params['stage_cnt']\n        self.stage_models = nn.ModuleList([E_OneStageModel(stage_idx, **model_params) for stage_idx in range(stage_cnt)])\n\n    def pre_forward_w_saved_encodings(self, encoded_row_list, encoded_col_list):\n        stage_cnt = self.model_params['stage_cnt']\n        for stage_idx in range(stage_cnt):\n            model = self.stage_models[stage_idx]\n            encoded_row = encoded_row_list[stage_idx]\n            encoded_col = encoded_col_list[stage_idx]\n            model.pre_forward_w_saved_encodings(encoded_row, encoded_col)\n\n    def enable_EAS(self, bool):\n        stage_cnt = self.model_params['stage_cnt']\n        for stage_idx in range(stage_cnt):\n            model = self.stage_models[stage_idx]\n            model.decoder.enable_EAS = bool\n\n    def init_eas_layers_manual(self, W1_list, b1_list, W2_list, b2_list):\n        stage_cnt = self.model_params['stage_cnt']\n        for stage_idx in range(stage_cnt):\n            model = self.stage_models[stage_idx]\n            W1 = W1_list[stage_idx]\n            b1 = b1_list[stage_idx]\n            W2 = W2_list[stage_idx]\n            b2 = b2_list[stage_idx]\n            model.decoder.init_eas_layers_manual(W1, b1, W2, b2)\n\n    def eas_parameters(self):\n        stage_cnt = self.model_params['stage_cnt']\n        params = []\n        for stage_idx in range(stage_cnt):\n            model = self.stage_models[stage_idx]\n            params += model.decoder.eas_parameters()\n        return params\n\n    def forward_w_incumbent(self, state, best_action):\n        batch_size = state.BATCH_IDX.size(0)\n        pomo_size = state.BATCH_IDX.size(1)\n\n        stage_cnt = self.model_params['stage_cnt']\n        action_stack = torch.empty(size=(batch_size, pomo_size, stage_cnt), dtype=torch.long)\n        prob_stack = torch.empty(size=(batch_size, pomo_size, stage_cnt))\n\n        for stage_idx in range(stage_cnt):\n            model = self.stage_models[stage_idx]\n            action, prob = model.forward_w_incumbent(state, best_action)\n\n            action_stack[:, :, stage_idx] = action\n            prob_stack[:, :, stage_idx] = prob\n\n        gathering_index = state.stage_idx[:, :, None]\n        # shape: (batch, pomo, 1)\n        action = action_stack.gather(dim=2, index=gathering_index).squeeze(dim=2)\n        prob = prob_stack.gather(dim=2, index=gathering_index).squeeze(dim=2)\n        # shape: (batch, pomo)\n\n        return action, prob\n\n    def get_expand_prob(self, state):\n        batch_size = state.BATCH_IDX.size(0)\n        pomo_size = state.BATCH_IDX.size(1)\n        job_cnt_p1 = state.job_ninf_mask.size(2)\n\n        stage_cnt = self.model_params['stage_cnt']\n        probs_stack = torch.empty(size=(batch_size, pomo_size, job_cnt_p1, stage_cnt))\n\n        for stage_idx in range(stage_cnt):\n            model = self.stage_models[stage_idx]\n            probs = model.get_expand_prob(state)\n\n            probs_stack[:, :, :, stage_idx] = probs\n\n        gathering_index = state.stage_idx[:, :, None, None].expand(batch_size, pomo_size, job_cnt_p1, 1)\n        # shape: (batch, pomo, job_cnt+1, 1)\n\n        probs = probs_stack.gather(dim=3, index=gathering_index).squeeze(dim=3)\n        # shape: (batch, pomo, job_cnt+1)\n\n        return probs\n\n\nclass E_OneStageModel(OneStageModel):\n\n    def __init__(self, stage_idx, **model_params):\n        nn.Module.__init__(self)\n        self.model_params = model_params\n\n        self.encoder = FFSP_Encoder(**model_params)\n        self.decoder = E_FFSP_Decoder(**model_params)\n\n        self.encoded_col = None\n        # shape: (batch, machine_cnt, embedding)\n        self.encoded_row = None\n        # shape: (batch, job_cnt, embedding)\n\n    def pre_forward_w_saved_encodings(self, encoded_row, encoded_col):\n\n        self.encoded_row = encoded_row\n        self.encoded_col = encoded_col\n        # encoded_row.shape: (batch, job_cnt, embedding)\n        # encoded_col.shape: (batch, machine_cnt, embedding)\n\n        self.decoder.set_kv(self.encoded_row)\n\n    def forward_w_incumbent(self, state, best_action):\n        # best_action.shape = (batch,)\n\n        batch_size = state.BATCH_IDX.size(0)\n        pomo_size_p1 = state.BATCH_IDX.size(1)\n\n        encoded_current_machine = self._get_encoding(self.encoded_col, state.stage_machine_idx)\n        # shape: (batch, pomo+1, embedding)\n        all_job_probs = self.decoder(encoded_current_machine,\n                                     ninf_mask=state.job_ninf_mask)\n        # shape: (batch, pomo+1, job+1)\n\n        if self.training or self.model_params['eval_type'] == 'softmax':\n            while True:  # to fix pytorch.multinomial bug on selecting 0 probability elements\n                job_selected = all_job_probs.reshape(batch_size * pomo_size_p1, -1).multinomial(1) \\\n                    .squeeze(dim=1).reshape(batch_size, pomo_size_p1)\n                # shape: (batch, pomo+1)\n                job_selected[:, -1] = best_action\n                job_prob = all_job_probs[state.BATCH_IDX, state.POMO_IDX, job_selected] \\\n                    .reshape(batch_size, pomo_size_p1)\n                # shape: (batch, pomo+1)\n                job_prob[state.finished] = 1  # do not backprob finished episodes\n\n                if (job_prob[:, :-1] != 0).all():\n                    break\n        else:\n            job_selected = all_job_probs.argmax(dim=2)\n            # shape: (batch, pomo+1)\n            job_selected[:, -1] = best_action\n            job_prob = None  # any number is okay\n\n        return job_selected, job_prob\n\n    def get_expand_prob(self, state):\n        encoded_current_machine = self._get_encoding(self.encoded_col, state.stage_machine_idx)\n        # shape: (batch, pomo+1, embedding)\n        all_job_probs = self.decoder(encoded_current_machine,\n                                     ninf_mask=state.job_ninf_mask)\n        # shape: (batch, pomo, job+1)\n\n        return all_job_probs\n\nclass E_FFSP_Decoder(FFSP_Decoder):\n\n    def __init__(self, **model_params):\n        super().__init__(**model_params)\n\n        self.enable_EAS = None  # bool\n\n        self.eas_W1 = None\n        # shape: (batch, embedding, embedding)\n        self.eas_b1 = None\n        # shape: (batch, embedding)\n        self.eas_W2 = None\n        # shape: (batch, embedding, embedding)\n        self.eas_b2 = None\n        # shape: (batch, embedding)\n\n    def init_eas_layers_manual(self, W1, b1, W2, b2):\n        self.eas_W1 = torch.nn.Parameter(W1)\n        self.eas_b1 = torch.nn.Parameter(b1)\n        self.eas_W2 = torch.nn.Parameter(W2)\n        self.eas_b2 = torch.nn.Parameter(b2)\n\n    def eas_parameters(self):\n        return [self.eas_W1, self.eas_b1, self.eas_W2, self.eas_b2]\n\n    def forward(self, encoded_machine, ninf_mask):\n        # encoded_machine.shape: (batch, pomo, embedding)\n        # ninf_mask.shape: (batch, pomo, job_cnt+1)\n\n        head_num = self.model_params['head_num']\n\n        #  Multi-Head Attention\n        #######################################################\n        q = reshape_by_heads(self.Wq_3(encoded_machine), head_num=head_num)\n        # shape: (batch, head_num, pomo, qkv_dim)\n\n        out_concat = self._multi_head_attention_for_decoder(q, self.k, self.v,\n                                                            rank3_ninf_mask=ninf_mask)\n        # shape: (batch, pomo, head_num*qkv_dim)\n\n        mh_atten_out = self.multi_head_combine(out_concat)\n        # shape: (batch, pomo, embedding)\n\n        # EAS Layer Insert\n        #######################################################\n        if self.enable_EAS:\n            ms1 = torch.matmul(mh_atten_out, self.eas_W1)\n            # shape: (batch, pomo, embedding)\n\n            ms1 = ms1 + self.eas_b1[:, None, :]\n            # shape: (batch, pomo, embedding)\n\n            ms1_activated = F.relu(ms1)\n            # shape: (batch, pomo, embedding)\n\n            ms2 = torch.matmul(ms1_activated, self.eas_W2)\n            # shape: (batch, pomo, embedding)\n\n            ms2 = ms2 + self.eas_b2[:, None, :]\n            # shape: (batch, pomo, embedding)\n\n            mh_atten_out = mh_atten_out + ms2\n            # shape: (batch, pomo, embedding)\n\n        #  Single-Head Attention, for probability calculation\n        #######################################################\n        score = torch.matmul(mh_atten_out, self.single_head_key)\n        # shape: (batch, pomo, job_cnt+1)\n\n        sqrt_embedding_dim = self.model_params['sqrt_embedding_dim']\n        logit_clipping = self.model_params['logit_clipping']\n\n        score_scaled = score / sqrt_embedding_dim\n        # shape: (batch, pomo, job_cnt+1)\n\n        score_clipped = logit_clipping * torch.tanh(score_scaled)\n\n        score_masked = score_clipped + ninf_mask\n\n        probs = F.softmax(score_masked, dim=2)\n        # shape: (batch, pomo, job_cnt+1)\n\n        return probs\n\n# ==========================================\n# File: TSP/3_SGBS+EAS/E_TSPModel.py\n# Function/Context: E_TSPModel, E_TSP_Decoder\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom TSPModel import TSPModel, _get_encoding, TSP_Encoder, TSP_Decoder, reshape_by_heads, multi_head_attention\n\nclass E_TSPModel(TSPModel):\n\n    def __init__(self, **model_params):\n        nn.Module.__init__(self)\n        self.model_params = model_params\n\n        self.encoder = TSP_Encoder(**model_params)\n        self.decoder = E_TSP_Decoder(**model_params)\n        self.encoded_nodes = None\n        # shape: (batch, problem, embedding)\n\n    def pre_forward(self, reset_state):  # now includes decoder.set_q1\n        self.encoded_nodes = self.encoder(reset_state.problems)\n        # shape: (batch, problem, EMBEDDING_DIM)\n        self.decoder.set_kv(self.encoded_nodes)\n\n        batch_size = reset_state.problems.size(0)\n        problem_size = reset_state.problems.size(1)\n        all_nodes = torch.arange(problem_size)[None, :].expand(batch_size, problem_size)\n        encoded_first_node = _get_encoding(self.encoded_nodes, all_nodes)\n        # shape: (batch, pomo, embedding)\n        self.decoder.set_q1(encoded_first_node)\n\n    def get_expand_prob(self, state):\n\n        encoded_last_node = _get_encoding(self.encoded_nodes, state.current_node)\n        # shape: (batch, beam_width, embedding)\n        probs = self.decoder(encoded_last_node, ninf_mask=state.ninf_mask, first_node=state.first_node)\n        # shape: (batch, beam_width, problem)\n\n        return probs\n\n    def forward(self, state):\n        batch_size = state.BATCH_IDX.size(0)\n        pomo_size = state.BATCH_IDX.size(1)\n\n        if state.current_node is None:\n            selected = torch.arange(pomo_size)[None, :].expand(batch_size, pomo_size)\n            prob = torch.ones(size=(batch_size, pomo_size))\n\n        else:\n            encoded_last_node = _get_encoding(self.encoded_nodes, state.current_node)\n            # shape: (batch, pomo, embedding)\n            probs = self.decoder(encoded_last_node, ninf_mask=state.ninf_mask, first_node=state.first_node)\n            # shape: (batch, pomo, problem)\n\n            if self.training or self.model_params['eval_type'] == 'softmax':\n                selected = probs.reshape(batch_size * pomo_size, -1).multinomial(1) \\\n                    .squeeze(dim=1).reshape(batch_size, pomo_size)\n                # shape: (batch, pomo)\n\n                prob = probs[state.BATCH_IDX, state.POMO_IDX, selected] \\\n                    .reshape(batch_size, pomo_size)\n                # shape: (batch, pomo)\n\n            else:\n                selected = probs.argmax(dim=2)\n                # shape: (batch, pomo)\n                prob = None\n\n        return selected, prob\n\n    def forward_w_incumbent(self, state, best_action):\n        # best_action.shape = (batch,)\n        # first_action.shape = (batch,)\n\n        batch_size = state.BATCH_IDX.size(0)\n        pomo_size_p1 = state.BATCH_IDX.size(1)\n        pomo_size = pomo_size_p1-1\n\n        if state.current_node is None:  # First Move, POMO\n            pomo_select = torch.arange(pomo_size)[None, :].expand(batch_size, pomo_size)\n            # shape: (batch, pomo)\n            selected = torch.cat((pomo_select, best_action[:, None]), dim=1)\n            # shape: (batch, pomo+1)\n            prob = torch.ones(size=(batch_size, pomo_size_p1))\n\n        else:\n            encoded_last_node = _get_encoding(self.encoded_nodes, state.current_node)\n            # shape: (batch, pomo+1, embedding)\n            probs = self.decoder(encoded_last_node, ninf_mask=state.ninf_mask, first_node=state.first_node)\n            # shape: (batch, pomo+1, problem+1)\n\n            if self.training or self.model_params['eval_type'] == 'softmax':\n                while True:  # to fix pytorch.multinomial bug on selecting 0 probability elements\n                    with torch.no_grad():\n                        selected = probs.reshape(batch_size * pomo_size_p1, -1).multinomial(1) \\\n                            .squeeze(dim=1).reshape(batch_size, pomo_size_p1)\n                    # shape: (batch, pomo+1)\n                    selected[:, -1] = best_action\n                    prob = probs[state.BATCH_IDX, state.POMO_IDX, selected].reshape(batch_size, pomo_size_p1)\n                    # shape: (batch, pomo+1)\n                    if (prob != 0).all():\n                        break\n\n            else:\n                selected = probs.argmax(dim=2)\n                # shape: (batch, pomo+1)\n                selected[:, -1] = best_action\n                prob = None  # value not needed. Can be anything.\n\n        return selected, prob\n\n\nclass E_TSP_Decoder(TSP_Decoder):\n\n    def __init__(self, **model_params):\n        super().__init__(**model_params)\n\n        self.enable_EAS = None  # bool\n\n        self.eas_W1 = None\n        # shape: (batch, embedding, embedding)\n        self.eas_b1 = None\n        # shape: (batch, embedding)\n        self.eas_W2 = None\n        # shape: (batch, embedding, embedding)\n        self.eas_b2 = None\n        # shape: (batch, embedding)\n        \n    def init_eas_layers_random(self, batch_size):\n        emb_dim = self.model_params['embedding_dim']  # 128\n        init_lim = (1/emb_dim)**(1/2)\n        \n        weight1 = torch.torch.distributions.Uniform(low=-init_lim, high=init_lim).sample((batch_size, emb_dim, emb_dim))\n        bias1 = torch.torch.distributions.Uniform(low=-init_lim, high=init_lim).sample((batch_size, emb_dim))\n        self.eas_W1 = torch.nn.Parameter(weight1)\n        self.eas_b1 = torch.nn.Parameter(bias1)\n        self.eas_W2 = torch.nn.Parameter(torch.zeros(size=(batch_size, emb_dim, emb_dim)))\n        self.eas_b2 = torch.nn.Parameter(torch.zeros(size=(batch_size, emb_dim)))\n\n    def init_eas_layers_manual(self, W1, b1, W2, b2):\n        self.eas_W1 = torch.nn.Parameter(W1)\n        self.eas_b1 = torch.nn.Parameter(b1)\n        self.eas_W2 = torch.nn.Parameter(W2)\n        self.eas_b2 = torch.nn.Parameter(b2)\n\n    def eas_parameters(self):\n        return [self.eas_W1, self.eas_b1, self.eas_W2, self.eas_b2]\n        \n    def forward(self, encoded_last_node, ninf_mask, first_node=None):\n        # encoded_last_node.shape: (batch, pomo, embedding)\n        # load.shape: (batch, pomo)\n        # ninf_mask.shape: (batch, pomo, problem)\n        # first_node.shape: (batch, modified_pomo)  # use first_node=None when pomo = {1, 2, ..., problem}\n\n        head_num = self.model_params['head_num']\n\n        #  Multi-Head Attention\n        #######################################################\n        q_last = reshape_by_heads(self.Wq_last(encoded_last_node), head_num=head_num)\n        # shape: (batch, head_num, pomo, qkv_dim)\n\n        if first_node is None:\n            q_first = self.q_first\n            # shape: (batch, head_num, pomo, qkv_dim)\n        else:\n            qkv_dim = self.model_params['qkv_dim']\n            gathering_index = first_node[:, None, :, None].expand(-1, head_num, -1, qkv_dim)\n            q_first = self.q_first.gather(dim=2, index=gathering_index)\n            # shape: (batch, head_num, mod_pomo, qkv_dim)\n\n        q = q_first + q_last\n\n        out_concat = multi_head_attention(q, self.k, self.v, rank3_ninf_mask=ninf_mask)\n        # shape: (batch, pomo, head_num*qkv_dim)\n\n        mh_atten_out = self.multi_head_combine(out_concat)\n        # shape: (batch, pomo, embedding)\n\n        # EAS Layer Insert\n        #######################################################\n        if self.enable_EAS:\n            ms1 = torch.matmul(mh_atten_out, self.eas_W1)\n            # shape: (batch, pomo, embedding)\n\n            ms1 = ms1 + self.eas_b1[:, None, :]\n            # shape: (batch, pomo, embedding)\n\n            ms1_activated = F.relu(ms1)\n            # shape: (batch, pomo, embedding)\n\n            ms2 = torch.matmul(ms1_activated, self.eas_W2)\n            # shape: (batch, pomo, embedding)\n\n            ms2 = ms2 + self.eas_b2[:, None, :]\n            # shape: (batch, pomo, embedding)\n\n            mh_atten_out = mh_atten_out + ms2\n            # shape: (batch, pomo, embedding)\n\n        #  Single-Head Attention, for probability calculation\n        #######################################################\n        score = torch.matmul(mh_atten_out, self.single_head_key)\n        # shape: (batch, pomo, problem)\n\n        sqrt_embedding_dim = self.model_params['sqrt_embedding_dim']\n        logit_clipping = self.model_params['logit_clipping']\n\n        score_scaled = score / sqrt_embedding_dim\n        # shape: (batch, pomo, problem)\n\n        score_clipped = logit_clipping * torch.tanh(score_scaled)\n\n        score_masked = score_clipped + ninf_mask\n\n        probs = F.softmax(score_masked, dim=2)\n        # shape: (batch, pomo, problem)\n\n        return probs",
  "description": "Combined Analysis:\n- [CVRP/2_SGBS/CVRPModel.py]: This file implements the core neural network architecture for the CVRP model used in Simulation-Guided Beam Search. The CVRPModel class provides the policy network that outputs probability distributions over actions (next nodes to visit). Key components: 1) Encoder-decoder architecture with attention mechanisms that process depot/customer coordinates and demands. 2) The get_expand_prob() method computes action probabilities for beam search expansion. 3) The forward() method supports different inference modes (greedy, sampling) as mentioned in the paper. 4) The decoder incorporates vehicle load information and uses ninf_mask to enforce constraints (infeasible actions get - probability). This directly implements the neural policy (a|s) used in the paper's Equation 1 and Algorithm 1 steps.\n- [CVRP/3_SGBS+EAS/E_CVRPModel.py]: This file implements the core neural network architecture for the SGBS+EAS method for CVRP. Specifically:\n1. E_CVRPModel extends the base CVRPModel with methods that support incumbent-based search (forward_w_incumbent) and probability expansion (get_expand_prob), which are essential for simulation-guided beam search.\n2. EAS_CVRP_Decoder implements the Efficient Active Search (EAS) component by adding learnable parameters (eas_W1, eas_b1, eas_W2, eas_b2) that are fine-tuned during inference to adapt the policy to specific problem instances.\n3. The mathematical model corresponds to the neural policy that outputs action probabilities $\\pi_\\theta(a_i|s_i)$, where the decoder computes attention scores and applies EAS layers before softmax normalization.\n4. The code directly supports the paper's algorithm steps by providing the policy network used in SGBS and the parameter adaptation mechanism for EAS.\n- [FFSP/2_SGBS/FFSPEnv.py]: This file implements the Flexible Flow Shop Problem (FFSP) environment that serves as the optimization model for the SGBS algorithm. It directly implements the mathematical model's objective (maximizing reward = minimizing makespan) and constraints through state management and action masking. The environment maintains job processing times, machine availability, stage progression, and enforces FFSP constraints via job_ninf_mask that prevents invalid job assignments. The step function implements the transition dynamics, and _get_makespan computes the objective value. This provides the foundational optimization model upon which the SGBS search algorithm operates.\n- [FFSP/3_SGBS+EAS/E_FFSPModel.py]: This file implements the core neural network architecture for the SGBS+EAS method applied to FFSP. It extends the base FFSP model with Efficient Active Search (EAS) layers that enable test-time fine-tuning. The key components are: 1) E_FFSPModel - main model wrapper with EAS parameter management, 2) E_OneStageModel - stage-specific model with incumbent-aware forward pass, 3) E_FFSP_Decoder - decoder with EAS layers inserted after multi-head attention. The code implements the policy network (a|s) used in the paper's algorithm, with EAS layers (W1,b1,W2,b2) that are fine-tuned during active search to guide exploration toward promising regions. The forward_w_incumbent method supports the simulation-guided search by incorporating best-known actions during rollouts.\n- [TSP/3_SGBS+EAS/E_TSPModel.py]: This file implements the core neural network architecture for the SGBS+EAS method. Specifically, it defines the E_TSPModel and E_TSP_Decoder classes that incorporate Efficient Active Search (EAS) layers into the decoder. The EAS layers (W1, b1, W2, b2) are instance-specific parameters that can be fine-tuned during test time to adapt the model to individual problem instances. The forward methods implement the policy network that outputs action probabilities, which is the core component used in the SGBS algorithm for node expansion. The forward_w_incumbent method handles the special case of including an incumbent solution in the search beam, which is crucial for the EAS component of the algorithm.",
  "dependencies": [
    "FFSPModel",
    "OneStageModel",
    "FFSP_Encoder",
    "matplotlib.pyplot",
    "torch.distributions.categorical.Categorical",
    "CVRP_Encoder",
    "torch.nn.functional",
    "matplotlib.colors.ListedColormap",
    "FFSP_Decoder",
    "FFSProblemDef.get_random_problems",
    "TSP_Encoder",
    "TSPModel",
    "CVRP_Decoder",
    "TSP_Decoder",
    "multi_head_attention",
    "_get_encoding",
    "torch",
    "dataclasses",
    "reshape_by_heads",
    "CVRPModel",
    "torch.nn",
    "matplotlib.patches",
    "itertools"
  ]
}