{
  "file_path": "models/CIE/model.py, src/evaluation_metric.py, src/gconv.py, src/lap_solvers/hungarian.py, src/lap_solvers/sinkhorn.py, src/loss_func.py",
  "function_name": "Net, objective_score, ChannelIndependentConv, Siamese_ChannelIndependentConv, hungarian, Sinkhorn.forward_log, PermutationLossHung",
  "code_snippet": "\n\n# ==========================================\n# File: models/CIE/model.py\n# Function/Context: Net\n# ==========================================\nimport torch\nimport torch.nn as nn\n\nfrom src.lap_solvers.sinkhorn import Sinkhorn\nfrom src.feature_align import feature_align\nfrom src.gconv import Siamese_ChannelIndependentConv #, Siamese_GconvEdgeDPP, Siamese_GconvEdgeOri\nfrom models.PCA.affinity_layer import Affinity\nfrom src.lap_solvers.hungarian import hungarian\n\nfrom src.utils.config import cfg\n\nfrom src.backbone import *\nCNN = eval(cfg.BACKBONE)\n\n\nclass Net(CNN):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.sinkhorn = Sinkhorn(max_iter=cfg.CIE.SK_ITER_NUM, epsilon=cfg.CIE.SK_EPSILON, tau=cfg.CIE.SK_TAU)\n        self.l2norm = nn.LocalResponseNorm(cfg.CIE.FEATURE_CHANNEL * 2, alpha=cfg.CIE.FEATURE_CHANNEL * 2, beta=0.5, k=0)\n        self.gnn_layer = cfg.CIE.GNN_LAYER # numbur of GNN layers\n        for i in range(self.gnn_layer):\n            if i == 0:\n                gnn_layer = Siamese_ChannelIndependentConv(cfg.CIE.FEATURE_CHANNEL * 2, cfg.CIE.GNN_FEAT, 1)\n            else:\n                gnn_layer = Siamese_ChannelIndependentConv(cfg.CIE.GNN_FEAT, cfg.CIE.GNN_FEAT, cfg.CIE.GNN_FEAT)\n            self.add_module('gnn_layer_{}'.format(i), gnn_layer)\n            self.add_module('affinity_{}'.format(i), Affinity(cfg.CIE.GNN_FEAT))\n            if i == self.gnn_layer - 2:  # only second last layer will have cross-graph module\n                self.add_module('cross_graph_{}'.format(i), nn.Linear(cfg.CIE.GNN_FEAT * 2, cfg.CIE.GNN_FEAT))\n                self.add_module('cross_graph_edge_{}'.format(i), nn.Linear(cfg.CIE.GNN_FEAT * 2, cfg.CIE.GNN_FEAT))\n        self.rescale = cfg.PROBLEM.RESCALE\n\n    def forward(self, data_dict, **kwargs):\n        if 'images' in data_dict:\n            # real image data\n            src, tgt = data_dict['images']\n            P_src, P_tgt = data_dict['Ps']\n            ns_src, ns_tgt = data_dict['ns']\n            G_src, G_tgt = data_dict['Gs']\n            H_src, H_tgt = data_dict['Hs']\n            # extract feature\n            src_node = self.node_layers(src)\n            src_edge = self.edge_layers(src_node)\n            tgt_node = self.node_layers(tgt)\n            tgt_edge = self.edge_layers(tgt_node)\n\n            # feature normalization\n            src_node = self.l2norm(src_node)\n            src_edge = self.l2norm(src_edge)\n            tgt_node = self.l2norm(tgt_node)\n            tgt_edge = self.l2norm(tgt_edge)\n\n            # arrange features\n            U_src = feature_align(src_node, P_src, ns_src, self.rescale)\n            F_src = feature_align(src_edge, P_src, ns_src, self.rescale)\n            U_tgt = feature_align(tgt_node, P_tgt, ns_tgt, self.rescale)\n            F_tgt = feature_align(tgt_edge, P_tgt, ns_tgt, self.rescale)\n        elif 'features' in data_dict:\n            # synthetic data\n            src, tgt = data_dict['features']\n            ns_src, ns_tgt = data_dict['ns']\n            G_src, G_tgt = data_dict['Gs']\n            H_src, H_tgt = data_dict['Hs']\n\n            U_src = src[:, :src.shape[1] // 2, :]\n            F_src = src[:, src.shape[1] // 2:, :]\n            U_tgt = tgt[:, :tgt.shape[1] // 2, :]\n            F_tgt = tgt[:, tgt.shape[1] // 2:, :]\n        else:\n            raise ValueError('Unknown data type for this model.')\n\n        P_src_dis = (P_src.unsqueeze(1) - P_src.unsqueeze(2))\n        P_src_dis = torch.norm(P_src_dis, p=2, dim=3).detach()\n        P_tgt_dis = (P_tgt.unsqueeze(1) - P_tgt.unsqueeze(2))\n        P_tgt_dis = torch.norm(P_tgt_dis, p=2, dim=3).detach()\n\n        Q_src = torch.exp(-P_src_dis / self.rescale[0])\n        Q_tgt = torch.exp(-P_tgt_dis / self.rescale[0])\n\n        emb_edge1 = Q_src.unsqueeze(-1)\n        emb_edge2 = Q_tgt.unsqueeze(-1)\n\n        # adjacency matrices\n        A_src = torch.bmm(G_src, H_src.transpose(1, 2))\n        A_tgt = torch.bmm(G_tgt, H_tgt.transpose(1, 2))\n\n        # U_src, F_src are features at different scales\n        emb1, emb2 = torch.cat((U_src, F_src), dim=1).transpose(1, 2), torch.cat((U_tgt, F_tgt), dim=1).transpose(1, 2)\n        ss = []\n\n        for i in range(self.gnn_layer):\n            gnn_layer = getattr(self, 'gnn_layer_{}'.format(i))\n\n            # during forward process, the network structure will not change\n            emb1, emb2, emb_edge1, emb_edge2 = gnn_layer([A_src, emb1, emb_edge1], [A_tgt, emb2, emb_edge2])\n\n            affinity = getattr(self, 'affinity_{}'.format(i))\n            s = affinity(emb1, emb2) # xAx^T\n\n            s = self.sinkhorn(s, ns_src, ns_tgt)\n            ss.append(s)\n\n            if i == self.gnn_layer - 2:\n                cross_graph = getattr(self, 'cross_graph_{}'.format(i))\n                new_emb1 = cross_graph(torch.cat((emb1, torch.bmm(s, emb2)), dim=-1))\n                new_emb2 = cross_graph(torch.cat((emb2, torch.bmm(s.transpose(1, 2), emb1)), dim=-1))\n                emb1 = new_emb1\n                emb2 = new_emb2\n\n                # edge cross embedding\n                '''\n                cross_graph_edge = getattr(self, 'cross_graph_edge_{}'.format(i))\n                emb_edge1 = emb_edge1.permute(0, 3, 1, 2)\n                emb_edge2 = emb_edge2.permute(0, 3, 1, 2)\n                s = s.unsqueeze(1)\n                new_emb_edge1 = cross_graph_edge(torch.cat((emb_edge1, torch.matmul(torch.matmul(s, emb_edge2), s.transpose(2, 3))), dim=1).permute(0, 2, 3, 1))\n                new_emb_edge2 = cross_graph_edge(torch.cat((emb_edge2, torch.matmul(torch.matmul(s.transpose(2, 3), emb_edge1), s)), dim=1).permute(0, 2, 3, 1))\n                emb_edge1 = new_emb_edge1\n                emb_edge2 = new_emb_edge2\n                '''\n\n        data_dict.update({\n            'ds_mat': ss[-1],\n            'perm_mat': hungarian(ss[-1], ns_src, ns_tgt)\n        })\n        return data_dict\n\n# ==========================================\n# File: src/evaluation_metric.py\n# Function/Context: objective_score\n# ==========================================\nimport torch\nfrom torch import Tensor\nfrom itertools import combinations\nfrom src.utils.config import cfg\n\ndef objective_score(pmat_pred: Tensor, affmtx: Tensor) -> Tensor:\n    r\"\"\"\n    Objective score given predicted permutation matrix and affinity matrix from the problem.\n\n    .. math::\n        \\text{objective score} = \\mathrm{vec}(\\mathbf{X})^\\top \\mathbf{K} \\mathrm{vec}(\\mathbf{X})\n\n    where :math:`\\mathrm{vec}(\\cdot)` means column-wise vectorization.\n\n    :param pmat_pred: predicted permutation matrix :math:`(\\mathbf{X})`\n    :param affmtx: affinity matrix of the quadratic assignment problem :math:`(\\mathbf{K})`\n    :return: objective scores\n\n    .. note::\n        The most general mathematical form of graph matching is known as Quadratic Assignment Problem (QAP), which is an\n        NP-hard combinatorial optimization problem. Objective score reflects the power of the graph matching/QAP solver\n        concerning the objective score of the QAP.\n    \"\"\"\n    batch_num = pmat_pred.shape[0]\n\n    p_vec = pmat_pred.transpose(1, 2).contiguous().view(batch_num, -1, 1)\n    obj_score = torch.matmul(torch.matmul(p_vec.transpose(1, 2), affmtx), p_vec).view(-1)\n\n    return obj_score\n\n# ==========================================\n# File: src/gconv.py\n# Function/Context: ChannelIndependentConv, Siamese_ChannelIndependentConv\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Tuple, Optional, List, Union\n\n\nclass ChannelIndependentConv(nn.Module):\n    r\"\"\"\n    Channel Independent Embedding Convolution.\n    Proposed by `\"Yu et al. Learning deep graph matching with channel-independent embedding and Hungarian attention.\n    ICLR 2020.\" <https://openreview.net/forum?id=rJgBd2NYPH>`_\n\n    :param in_features: the dimension of input node features\n    :param out_features: the dimension of output node features\n    :param in_edges: the dimension of input edge features\n    :param out_edges: (optional) the dimension of output edge features. It needs to be the same as ``out_features``\n    \"\"\"\n    def __init__(self, in_features: int, out_features: int, in_edges: int, out_edges: int=None):\n        super(ChannelIndependentConv, self).__init__()\n        if out_edges is None:\n            out_edges = out_features\n        self.in_features = in_features\n        self.out_features = out_features\n        self.out_edges = out_edges\n        # self.node_fc = nn.Linear(in_features, out_features // self.out_edges)\n        self.node_fc = nn.Linear(in_features, out_features)\n        self.node_sfc = nn.Linear(in_features, out_features)\n        self.edge_fc = nn.Linear(in_edges, self.out_edges)\n\n    def forward(self, A: Tensor, emb_node: Tensor, emb_edge: Tensor, mode: int=1) -> Tuple[Tensor, Tensor]:\n        r\"\"\"\n        :param A: :math:`(b\\times n\\times n)` {0,1} adjacency matrix. :math:`b`: batch size, :math:`n`: number of nodes\n        :param emb_node: :math:`(b\\times n\\times d_n)` input node embedding. :math:`d_n`: node feature dimension\n        :param emb_edge: :math:`(b\\times n\\times n\\times d_e)` input edge embedding. :math:`d_e`: edge feature dimension\n        :param mode: 1 or 2, refer to the paper for details\n        :return: :math:`(b\\times n\\times d^\\prime)` new node embedding,\n         :math:`(b\\times n\\times n\\times d^\\prime)` new edge embedding\n        \"\"\"\n        if mode == 1:\n            node_x = self.node_fc(emb_node)\n            node_sx = self.node_sfc(emb_node)\n            edge_x = self.edge_fc(emb_edge)\n\n            A = A.unsqueeze(-1)\n            A = torch.mul(A.expand_as(edge_x), edge_x)\n\n            node_x = torch.matmul(A.transpose(2, 3).transpose(1, 2),\n                                  node_x.unsqueeze(2).transpose(2, 3).transpose(1, 2))\n            node_x = node_x.squeeze(-1).transpose(1, 2)\n            node_x = F.relu(node_x) + F.relu(node_sx)\n            edge_x = F.relu(edge_x)\n\n            return node_x, edge_x\n\n        elif mode == 2:\n            node_x = self.node_fc(emb_node)\n            node_sx = self.node_sfc(emb_node)\n            edge_x = self.edge_fc(emb_edge)\n\n            d_x = node_x.unsqueeze(1) - node_x.unsqueeze(2)\n            d_x = torch.sum(d_x ** 2, dim=3, keepdim=False)\n            d_x = torch.exp(-d_x)\n\n            A = A.unsqueeze(-1)\n            A = torch.mul(A.expand_as(edge_x), edge_x)\n\n            node_x = torch.matmul(A.transpose(2, 3).transpose(1, 2),\n                                  node_x.unsqueeze(2).transpose(2, 3).transpose(1, 2))\n            node_x = node_x.squeeze(-1).transpose(1, 2)\n            node_x = F.relu(node_x) + F.relu(node_sx)\n            edge_x = F.relu(edge_x)\n            return node_x, edge_x\n\n        else:\n            raise ValueError('Unknown mode {}. Possible options: 1 or 2'.format(mode))\n\n\nclass Siamese_ChannelIndependentConv(nn.Module):\n    r\"\"\"\n    Siamese Channel Independent Conv neural network for processing arbitrary number of graphs.\n\n    :param in_features: the dimension of input node features\n    :param num_features: the dimension of output node features\n    :param in_edges: the dimension of input edge features\n    :param out_edges: (optional) the dimension of output edge features. It needs to be the same as ``num_features``\n    \"\"\"\n    def __init__(self, in_features, num_features, in_edges, out_edges=None):\n        super(Siamese_ChannelIndependentConv, self).__init__()\n        self.in_feature = in_features\n        self.gconv = ChannelIndependentConv(in_features, num_features, in_edges, out_edges)\n\n    def forward(self, g1: Tuple[Tensor, Tensor, Optional[bool]], *args) -> List[Tensor]:\n        r\"\"\"\n        Forward computation of Siamese Channel Independent Conv.\n\n        :param g1: The first graph, which is a tuple of (:math:`(b\\times n\\times n)` {0,1} adjacency matrix,\n         :math:`(b\\times n\\times d_n)` input node embedding, :math:`(b\\times n\\times n\\times d_e)` input edge embedding,\n         mode (``1`` or ``2``))\n        :param args: Other graphs\n        :return: A list of tensors composed of new node embeddings :math:`(b\\times n\\times d^\\prime)`, appended with new\n         edge embeddings :math:`(b\\times n\\times n\\times d^\\prime)`\n        \"\"\"\n        emb1, emb_edge1 = self.gconv(*g1)\n        embs = [emb1]\n        emb_edges = [emb_edge1]\n        for g in args:\n            emb2, emb_edge2 = self.gconv(*g)\n            embs.append(emb2), emb_edges.append(emb_edge2)\n        return embs + emb_edges\n\n# ==========================================\n# File: src/lap_solvers/hungarian.py\n# Function/Context: hungarian\n# ==========================================\nimport torch\nimport scipy.optimize as opt\nimport numpy as np\nfrom multiprocessing import Pool\nfrom torch import Tensor\n\n\ndef hungarian(s: Tensor, n1: Tensor=None, n2: Tensor=None, nproc: int=1) -> Tensor:\n    r\"\"\"\n    Solve optimal LAP permutation by hungarian algorithm. The time cost is :math:`O(n^3)`.\n\n    :param s: :math:`(b\\times n_1 \\times n_2)` input 3d tensor. :math:`b`: batch size\n    :param n1: :math:`(b)` number of objects in dim1\n    :param n2: :math:`(b)` number of objects in dim2\n    :param nproc: number of parallel processes (default: ``nproc=1`` for no parallel)\n    :return: :math:`(b\\times n_1 \\times n_2)` optimal permutation matrix\n\n    .. note::\n        We support batched instances with different number of nodes, therefore ``n1`` and ``n2`` are\n        required to specify the exact number of objects of each dimension in the batch. If not specified, we assume\n        the batched matrices are not padded.\n    \"\"\"\n    if len(s.shape) == 2:\n        s = s.unsqueeze(0)\n        matrix_input = True\n    elif len(s.shape) == 3:\n        matrix_input = False\n    else:\n        raise ValueError('input data shape not understood: {}'.format(s.shape))\n\n    device = s.device\n    batch_num = s.shape[0]\n\n    perm_mat = s.cpu().detach().numpy() * -1\n    if n1 is not None:\n        n1 = n1.cpu().numpy()\n    else:\n        n1 = [None] * batch_num\n    if n2 is not None:\n        n2 = n2.cpu().numpy()\n    else:\n        n2 = [None] * batch_num\n\n    if nproc > 1:\n        with Pool(processes=nproc) as pool:\n            mapresult = pool.starmap_async(_hung_kernel, zip(perm_mat, n1, n2))\n            perm_mat = np.stack(mapresult.get())\n    else:\n        perm_mat = np.stack([_hung_kernel(perm_mat[b], n1[b], n2[b]) for b in range(batch_num)])\n\n    perm_mat = torch.from_numpy(perm_mat).to(device)\n\n    if matrix_input:\n        perm_mat.squeeze_(0)\n\n    return perm_mat\n\ndef _hung_kernel(s: torch.Tensor, n1=None, n2=None):\n    if n1 is None:\n        n1 = s.shape[0]\n    if n2 is None:\n        n2 = s.shape[1]\n    row, col = opt.linear_sum_assignment(s[:n1, :n2])\n    perm_mat = np.zeros_like(s)\n    perm_mat[row, col] = 1\n    return perm_mat\n\n# ==========================================\n# File: src/lap_solvers/sinkhorn.py\n# Function/Context: Sinkhorn.forward_log\n# ==========================================\nimport torch\nimport torch.nn as nn\nfrom torch import Tensor\nimport pygmtools as pygm\n\n\nclass Sinkhorn(nn.Module):\n    r\"\"\"\n    Sinkhorn algorithm turns the input matrix into a bi-stochastic matrix.\n\n    Sinkhorn algorithm firstly applies an ``exp`` function with temperature :math:`\\tau`:\n\n    .. math::\n        \\mathbf{S}_{i,j} = \\exp \\left(\\frac{\\mathbf{s}_{i,j}}{\\tau}\\right)\n\n    And then turns the matrix into doubly-stochastic matrix by iterative row- and column-wise normalization:\n\n    .. math::\n        \\mathbf{S} &= \\mathbf{S} \\oslash (\\mathbf{1}_{n_2} \\mathbf{1}_{n_2}^\\top \\cdot \\mathbf{S}) \\\\\n        \\mathbf{S} &= \\mathbf{S} \\oslash (\\mathbf{S} \\cdot \\mathbf{1}_{n_2} \\mathbf{1}_{n_2}^\\top)\n\n    where :math:`\\oslash` means element-wise division, :math:`\\mathbf{1}_n` means a column-vector with length :math:`n`\n    whose elements are all :math:`1`\\ s.\n\n    :param max_iter: maximum iterations (default: ``10``)\n    :param tau: the hyper parameter :math:`\\tau` controlling the temperature (default: ``1``)\n    :param epsilon: a small number for numerical stability (default: ``1e-4``)\n    :param log_forward: apply log-scale computation for better numerical stability (default: ``True``)\n    :param batched_operation: apply batched_operation for better efficiency (but may cause issues for back-propagation,\n     default: ``False``)\n\n    .. note::\n        ``tau`` is an important hyper parameter to be set for Sinkhorn algorithm. ``tau`` controls the distance between\n        the predicted doubly-stochastic matrix, and the discrete permutation matrix computed by Hungarian algorithm (see\n        :func:`~src.lap_solvers.hungarian.hungarian`). Given a small ``tau``, Sinkhorn performs more closely to\n        Hungarian, at the cost of slower convergence speed and reduced numerical stability.\n\n    .. note::\n        We recommend setting ``log_forward=True`` because it is more numerically stable. It provides more precise\n        gradient in back propagation and helps the model to converge better and faster.\n\n    .. note::\n        Setting ``batched_operation=True`` may be preferred when you are doing inference with this module and do not\n        need the gradient.\n    \"\"\"\n    def __init__(self, max_iter: int=10, tau: float=1., epsilon: float=1e-4,\n                 log_forward: bool=True, batched_operation: bool=False):\n        super(Sinkhorn, self).__init__()\n        self.max_iter = max_iter\n        self.tau = tau\n        self.epsilon = epsilon\n        self.log_forward = log_forward\n        if not log_forward:\n            print('Warning: Sinkhorn algorithm without log forward is deprecated because log_forward is more stable.')\n        self.batched_operation = batched_operation # batched operation may cause instability in backward computation,\n                                                   # but will boost computation.\n\n    def forward(self, s: Tensor, nrows: Tensor=None, ncols: Tensor=None, dummy_row: bool=False) -> Tensor:\n        r\"\"\"\n        :param s: :math:`(b\\times n_1 \\times n_2)` input 3d tensor. :math:`b`: batch size\n        :param nrows: :math:`(b)` number of objects in dim1\n        :param ncols: :math:`(b)` number of objects in dim2\n        :param dummy_row: whether to add dummy rows (rows whose elements are all 0) to pad the matrix to square matrix.\n         default: ``False``\n        :return: :math:`(b\\times n_1 \\times n_2)` the computed doubly-stochastic matrix\n\n        .. note::\n            We support batched instances with different number of nodes, therefore ``nrows`` and ``ncols`` are\n            required to specify the exact number of objects of each dimension in the batch. If not specified, we assume\n            the batched matrices are not padded.\n\n        .. note::\n            The original Sinkhorn algorithm only works for square matrices. To handle cases where the graphs to be\n            matched have different number of nodes, it is a common practice to add dummy rows to construct a square\n            matrix. After the row and column normalizations, the padded rows are discarded.\n\n        .. note::\n            We assume row number <= column number. If not, the input matrix will be transposed.\n        \"\"\"\n        if self.log_forward:\n            return self.forward_log(s, nrows, ncols, dummy_row)\n        else:\n            return self.forward_ori(s, nrows, ncols, dummy_row) # deprecated\n\n    def forward_log(self, s, nrows=None, ncols=None, dummy_row=False):\n        \"\"\"Compute sinkhorn with row/column normalization in the log space.\"\"\"\n        return pygm.sinkhorn(s, n1=nrows, n2=ncols, dummy_row=dummy_row, max_iter=self.max_iter, tau=self.tau, batched_operation=self.batched_operation, backend='pytorch')\n\n# ==========================================\n# File: src/loss_func.py\n# Function/Context: PermutationLossHung\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom src.lap_solvers.hungarian import hungarian\nfrom src.lap_solvers.ILP import ILP_solver\nfrom torch import Tensor\n\nclass PermutationLossHung(nn.Module):\n    r\"\"\"\n    Binary cross entropy loss between two permutations with Hungarian attention. The vanilla version without Hungarian\n    attention is :class:`~src.loss_func.PermutationLoss`.\n\n    .. math::\n        L_{hung} &=-\\sum_{i\\in\\mathcal{V}_1,j\\in\\mathcal{V}_2}\\mathbf{Z}_{ij}\\left(\\mathbf{X}^\\text{gt}_{ij}\\log \\mathbf{S}_{ij}+\\left(1-\\mathbf{X}^{\\text{gt}}_{ij}\\right)\\log\\left(1-\\mathbf{S}_{ij}\\right)\\right) \\\\\n        \\mathbf{Z}&=\\mathrm{OR}\\left(\\mathrm{Hungarian}(\\mathbf{S}),\\mathbf{X}^\\text{gt}\\right)\n\n    where :math:`\\mathcal{V}_1, \\mathcal{V}_2` are vertex sets for two graphs.\n\n    Hungarian attention highlights the entries where the model makes wrong decisions after the Hungarian step (which is\n    the default discretization step during inference).\n\n    Proposed by `\"Yu et al. Learning deep graph matching with channel-independent embedding and Hungarian attention.\n    ICLR 2020.\" <https://openreview.net/forum?id=rJgBd2NYPH>`_\n\n    .. note::\n        For batched input, this loss function computes the averaged loss among all instances in the batch.\n\n    A working example for Hungarian attention:\n\n    .. image:: ../../images/hungarian_attention.png\n    \"\"\"\n    def __init__(self):\n        super(PermutationLossHung, self).__init__()\n\n    def forward(self, pred_dsmat: Tensor, gt_perm: Tensor, src_ns: Tensor, tgt_ns: Tensor) -> Tensor:\n        r\"\"\"\n        :param pred_dsmat: :math:`(b\\times n_1 \\times n_2)` predicted doubly-stochastic matrix :math:`(\\mathbf{S})`\n        :param gt_perm: :math:`(b\\times n_1 \\times n_2)` ground truth permutation matrix :math:`(\\mathbf{X}^{gt})`\n        :param src_ns: :math:`(b)` number of exact pairs in the first graph (also known as source graph).\n        :param tgt_ns: :math:`(b)` number of exact pairs in the second graph (also known as target graph).\n        :return: :math:`(1)` averaged permutation loss\n\n        .. note::\n            We support batched instances with different number of nodes, therefore ``src_ns`` and ``tgt_ns`` are\n            required to specify the exact number of nodes of each instance in the batch.\n        \"\"\"\n        batch_num = pred_dsmat.shape[0]\n\n        assert torch.all((pred_dsmat >= 0) * (pred_dsmat <= 1))\n        assert torch.all((gt_perm >= 0) * (gt_perm <= 1))\n\n        dis_pred = hungarian(pred_dsmat, src_ns, tgt_ns)\n        ali_perm = dis_pred + gt_perm\n        ali_perm[ali_perm > 1.0] = 1.0 # Hung\n        pred_dsmat = torch.mul(ali_perm, pred_dsmat)\n        gt_perm = torch.mul(ali_perm, gt_perm)\n        loss = torch.tensor(0.).to(pred_dsmat.device)\n        n_sum = torch.zeros_like(loss)\n        for b in range(batch_num):\n            loss += F.binary_cross_entropy(\n                pred_dsmat[b, :src_ns[b], :tgt_ns[b]],\n                gt_perm[b, :src_ns[b], :tgt_ns[b]],\n                reduction='sum')\n            n_sum += src_ns[b].to(n_sum.dtype).to(pred_dsmat.device)\n        return loss / n_sum",
  "description": "Combined Analysis:\n- [models/CIE/model.py]: This file implements the core optimization logic of the CIE (Channel-Independent Embedding) model for graph matching. The implementation directly corresponds to the paper's algorithm: 1) Uses Siamese GNN layers (Siamese_ChannelIndependentConv) for channel-independent embedding of node and edge features. 2) Computes affinity matrices (x^T K x formulation) via Affinity layers at each GNN layer. 3) Applies Sinkhorn normalization to obtain doubly-stochastic matrices. 4) Uses Hungarian algorithm for final permutation matrix discretization. The cross-graph attention mechanism (lines 98-101) aligns with the paper's Hungarian attention concept, though the exact loss computation with Hungarian attention would be in the training code.\n- [src/evaluation_metric.py]: This file implements the core objective function evaluation for the Quadratic Assignment Problem (QAP) formulation of graph matching. The objective_score function directly computes x^T K x where x is the vectorized permutation matrix and K is the affinity matrix, which matches exactly with the optimization objective described in the paper. While the file contains other evaluation metrics (PCK, matching accuracy, clustering accuracy), the objective_score function is the direct implementation of the QAP objective function that the paper aims to maximize.\n- [src/gconv.py]: This file implements the Channel-Independent Embedding (CIE) module, which is a core contribution of the paper. The CIE module simultaneously learns node and edge embeddings through two modes: Mode 1 uses adjacency-weighted edge features, while Mode 2 incorporates a Gaussian kernel on node differences. The Siamese_ChannelIndependentConv extends this to process two graphs in parallel, forming the backbone of the Siamese GNN architecture. This directly corresponds to the paper's algorithmic step of using a Siamese GNN with CIE to extract features for constructing the affinity matrix K. However, the file does not contain the Sinkhorn normalization, Hungarian attention loss, or final discretization steps—these are implemented elsewhere in the repository.\n- [src/lap_solvers/hungarian.py]: This file implements the Hungarian algorithm (Kuhn-Munkres algorithm) for solving the Linear Assignment Problem (LAP), which is a key component in the paper's graph matching pipeline. The paper uses this algorithm during test time to discretize the doubly-stochastic matrix (output from Sinkhorn normalization) into a permutation matrix that satisfies the one-to-one matching constraints. The implementation handles batched inputs with variable graph sizes and supports parallel processing. The mathematical constraints enforced are: 1) each row sums to 1, 2) each column sums to 1, and 3) binary {0,1} entries, exactly matching the permutation matrix constraints described in the paper's optimization model.\n- [src/lap_solvers/sinkhorn.py]: This file implements the Sinkhorn algorithm, which is a key component in the paper's deep graph matching pipeline. The paper uses Sinkhorn to normalize the cross-graph similarity matrix into a doubly-stochastic matrix, relaxing the discrete permutation constraints to continuous ones for differentiable training. Specifically, the Sinkhorn class provides a differentiable layer that applies iterative row and column normalization to produce a bi-stochastic matrix from raw similarity scores. The forward_log method (the default) implements this in log-space for numerical stability, matching the paper's use of Sinkhorn normalization before applying Hungarian attention during training. The implementation handles batched inputs with variable graph sizes through padding and supports temperature parameter τ to control the sharpness of the output distribution.\n- [src/loss_func.py]: This file implements the Hungarian attention loss function from the paper 'LEARNING DEEP GRAPH MATCHING VIA CHANNEL-INDEPENDENT EMBEDDING AND HUNGARIAN ATTENTION'. The PermutationLossHung class computes the binary cross-entropy loss between predicted doubly-stochastic matrices and ground truth permutations, using Hungarian algorithm attention to focus on entries where the model makes incorrect decisions after the Hungarian discretization step. This aligns with the paper's key algorithmic contribution of incorporating Hungarian attention into the training loss to improve gradient flow and focus on hard matching cases.",
  "dependencies": [
    "Affinity",
    "CNN",
    "src.lap_solvers.ILP.ILP_solver",
    "scipy.optimize.linear_sum_assignment",
    "pygmtools",
    "typing.Union",
    "multiprocessing.Pool",
    "typing.Tuple",
    "torch.nn.functional",
    "src.utils.config",
    "typing.Optional",
    "Siamese_ChannelIndependentConv",
    "_hung_kernel",
    "cfg",
    "typing.List",
    "numpy",
    "feature_align",
    "hungarian",
    "torch.nn",
    "Sinkhorn",
    "torch",
    "src.lap_solvers.hungarian.hungarian"
  ]
}