{
  "file_path": "src/models/HalideModel.py, src/models/jspModel.py, src/models/rewriter/HalideRewriter.py, src/models/rewriter/vrpRewriter.py, src/models/vrpModel.py",
  "function_name": "HalideModel, jspModel.forward, HalideRewriter, vrpRewriter.move, vrpModel.forward",
  "code_snippet": "\n\n# ==========================================\n# File: src/models/HalideModel.py\n# Function/Context: HalideModel\n# ==========================================\nimport numpy as np\nimport operator\nimport random\nimport time\nfrom multiprocessing.pool import ThreadPool\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch import cuda\nfrom torch.autograd import Variable\nfrom torch.nn.utils import clip_grad_norm\nimport torch.nn.functional as F\nfrom torch.distributions.categorical import Categorical\n\nfrom .data_utils import data_utils\nfrom .modules import HalideInputEncoder, mlp\nfrom .rewriter import HalideRewriter\nfrom .BaseModel import BaseModel\n\neps = 1e-3\nlog_eps = np.log(eps)\n\n\nclass HalideModel(BaseModel):\n\t\"\"\"\n\tModel for expression simplification.\n\t\"\"\"\n\tdef __init__(self, args, term_vocab, term_vocab_list, op_vocab, op_vocab_list):\n\t\tsuper(HalideModel, self).__init__(args)\n\t\tself.term_vocab = term_vocab\n\t\tself.term_vocab_list = term_vocab_list\n\t\tself.op_vocab = op_vocab\n\t\tself.op_vocab_list = op_vocab_list\n\t\tself.term_vocab_size = args.term_vocab_size\n\t\tself.op_vocab_size = args.op_vocab_size\n\t\tself.embedding_size = args.embedding_size\n\t\tself.num_actions = args.num_actions\n\t\tself.reward_thres = -0.05\n\t\tself.rewriter = HalideRewriter(args, term_vocab, term_vocab_list, op_vocab, op_vocab_list)\n\t\tself.input_encoder = HalideInputEncoder.TreeLSTM(args, term_vocab, term_vocab_list, op_vocab, op_vocab_list)\n\t\tself.policy = mlp.MLPModel(self.num_MLP_layers, self.LSTM_hidden_size * 2, self.MLP_hidden_size, self.num_actions, self.cuda_flag, self.dropout_rate)\n\t\tself.value_estimator = mlp.MLPModel(self.num_MLP_layers, self.LSTM_hidden_size * 2, self.MLP_hidden_size, 1, self.cuda_flag, self.dropout_rate)\n\n\t\tif args.optimizer == 'adam':\n\t\t\tself.optimizer = optim.Adam(self.parameters(), lr=self.lr)\n\t\telif args.optimizer == 'sgd':\n\t\t\tself.optimizer = optim.SGD(self.parameters(), lr=self.lr)\n\t\telif args.optimizer == 'rmsprop':\n\t\t\tself.optimizer = optim.RMSprop(self.parameters(), lr=self.lr)\n\t\telse:\n\t\t\traise ValueError('optimizer undefined: ', args.optimizer)\n\n\n\tdef rewrite(self, tm, ac_logprobs, trace_rec, expr_rec, candidate_rewrite_pos, pending_actions, eval_flag, max_search_pos, reward_thres=None):\n\t\tif len(candidate_rewrite_pos) == 0:\n\t\t\treturn [], [], [], [], [], []\n\n\t\tcandidate_rewrite_pos.sort(reverse=True, key=operator.itemgetter(0))\n\t\tif not eval_flag:\n\t\t\tsample_exp_reward_tensor = []\n\t\t\tfor idx, (cur_pred_reward, cur_pred_reward_tensor, cur_ac_prob, rewrite_pos, tensor_idx) in enumerate(candidate_rewrite_pos):\n\t\t\t\tsample_exp_reward_tensor.append(cur_pred_reward_tensor)\n\t\t\tsample_exp_reward_tensor = torch.cat(sample_exp_reward_tensor, 0)\n\t\t\tsample_exp_reward_tensor = torch.exp(sample_exp_reward_tensor * 10)\n\t\t\tsample_exp_reward = sample_exp_reward_tensor.data.cpu().numpy()\n\n\t\texpr = expr_rec[-1]\n\t\textra_reward_rec = []\n\t\textra_action_rec = []\n\t\tcandidate_tree_managers = []\n\t\tcandidate_update_tree_idxes = []\n\t\tcandidate_rewrite_rec = []\n\t\tcandidate_expr_rec = []\n\t\tcandidate_pending_actions = []\n\n\t\tif len(pending_actions) > 0:\n\t\t\tfor idx, (pred_reward, cur_pred_reward_tensor, cur_ac_prob, rewrite_pos, tensor_idx) in enumerate(candidate_rewrite_pos):\n\t\t\t\tif len(candidate_tree_managers) > 0 and idx >= max_search_pos:\n\t\t\t\t\tbreak\n\t\t\t\tif reward_thres is not None and pred_reward < reward_thres:\n\t\t\t\t\tif eval_flag:\n\t\t\t\t\t\tbreak\n\t\t\t\t\telif np.random.random() > self.cont_prob:\n\t\t\t\t\t\tcontinue\n\t\t\t\tinit_expr = tm.to_string(rewrite_pos)\n\t\t\t\top_idx = pending_actions[0]\n\t\t\t\top_list = self.rewriter.get_rewrite_seq(op_idx)\n\t\t\t\top = self.rewriter.get_rewrite_op(op_list[0])\n\t\t\t\tnew_tm, cur_update_tree_idxes = op(tm, rewrite_pos)\n\t\t\t\tif len(cur_update_tree_idxes) == 0:\n\t\t\t\t\textra_action_rec.append((ac_logprobs[tensor_idx], op_idx))\n\t\t\t\t\tcontinue\n\t\t\t\tcur_expr = str(new_tm)\n\t\t\t\tif cur_expr in candidate_expr_rec:\n\t\t\t\t\tcontinue\n\t\t\t\tcandidate_expr_rec.append(cur_expr)\n\t\t\t\tcandidate_update_tree_idxes.append(cur_update_tree_idxes)\n\t\t\t\tcandidate_tree_managers.append(new_tm)\n\t\t\t\tcandidate_rewrite_rec.append((ac_logprobs[tensor_idx], pred_reward, cur_pred_reward_tensor, rewrite_pos, init_expr, int(op_idx)))\n\t\t\t\tcandidate_pending_actions.append(pending_actions[1:])\n\t\t\t\tif len(candidate_tree_managers) >= max_search_pos:\n\t\t\t\t\tbreak\n\t\t\tif len(candidate_tree_managers) > 0:\n\t\t\t\treturn candidate_tree_managers, candidate_update_tree_idxes, candidate_rewrite_rec, candidate_pending_actions, extra_reward_rec, extra_action_rec\n\n\t\tif not eval_flag:\n\t\t\tsample_rewrite_pos_dist = Categorical(sample_exp_reward_tensor)\n\t\t\tsample_rewrite_pos = sample_rewrite_pos_dist.sample(sample_shape=[len(candidate_rewrite_pos)])\n\t\t\t#sample_rewrite_pos = torch.multinomial(sample_exp_reward_tensor, len(candidate_rewrite_pos))\n\t\t\tsample_rewrite_pos = sample_rewrite_pos.data.cpu().numpy()\n\t\t\tindexes = np.unique(sample_rewrite_pos, return_index=True)[1]\n\t\t\tsample_rewrite_pos = [sample_rewrite_pos[i] for i in sorted(indexes)]\n\t\t\tsample_rewrite_pos = sample_rewrite_pos[:self.num_sample_rewrite_pos]\n\t\t\tsample_exp_reward = [sample_exp_reward[i] for i in sample_rewrite_pos]\n\t\t\tsample_rewrite_pos = [candidate_rewrite_pos[i] for i in sample_rewrite_pos]\n\t\telse:\n\t\t\tsample_rewrite_pos = candidate_rewrite_pos.copy()\n\n\t\tfor idx, (pred_reward, cur_pred_reward_tensor, cur_ac_prob, rewrite_pos, tensor_idx) in enumerate(sample_rewrite_pos):\n\t\t\tif len(candidate_tree_managers) > 0 and idx >= max_search_pos:\n\t\t\t\tbreak\n\t\t\tif reward_thres is not None and pred_reward < reward_thres:\n\t\t\t\tif eval_flag:\n\t\t\t\t\tbreak\n\t\t\t\telif np.random.random() > self.cont_prob:\n\t\t\t\t\tcontinue\n\t\t\tinit_expr = tm.to_string(rewrite_pos)\n\t\t\tif eval_flag:\n\t\t\t\t_, candidate_acs = torch.sort(cur_ac_prob)\n\t\t\t\tcandidate_acs = candidate_acs.data.cpu().numpy()\n\t\t\t\tcandidate_acs = candidate_acs[::-1]\n\t\t\telse:\n\t\t\t\tcandidate_acs_dist = Categorical(cur_ac_prob)\n\t\t\t\tcandidate_acs = candidate_acs_dist.sample(sample_shape=[self.num_actions])\n\t\t\t\t#candidate_acs = torch.multinomial(cur_ac_prob, self.num_actions)\n\t\t\t\tcandidate_acs = candidate_acs.data.cpu().numpy()\n\t\t\t\tindexes = np.unique(candidate_acs, return_index=True)[1]\n\t\t\t\tcandidate_acs = [candidate_acs[i] for i in sorted(indexes)]\n\t\t\tcur_active = False\n\t\t\tcur_ac_prob = cur_ac_prob.data.cpu().numpy()\n\t\t\tfor i, op_idx in enumerate(candidate_acs):\n\t\t\t\tif (expr, init_expr, op_idx) in trace_rec:\n\t\t\t\t\tcontinue\n\t\t\t\top_list = self.rewriter.get_rewrite_seq(op_idx)\n\t\t\t\top = self.rewriter.get_rewrite_op(op_list[0])\n\t\t\t\tnew_tm, cur_update_tree_idxes = op(tm, rewrite_pos)\n\t\t\t\tif len(cur_update_tree_idxes) == 0:\n\t\t\t\t\textra_action_rec.append((ac_logprobs[tensor_idx], op_idx))\n\t\t\t\t\tcontinue\n\t\t\t\tcur_expr = str(new_tm)\n\t\t\t\tif cur_expr in candidate_expr_rec:\n\t\t\t\t\tcontinue\n\t\t\t\tcandidate_expr_rec.append(cur_expr)\n\t\t\t\tcandidate_update_tree_idxes.append(cur_update_tree_idxes)\n\t\t\t\tcandidate_tree_managers.append(new_tm)\n\t\t\t\tcandidate_rewrite_rec.append((ac_logprobs[tensor_idx], pred_reward, cur_pred_reward_tensor, rewrite_pos, init_expr, int(op_list[0])))\n\t\t\t\tcandidate_pending_actions.append(op_list[1:])\n\t\t\t\tcur_active = True\n\t\t\t\tif len(candidate_tree_managers) >= max_search_pos:\n\t\t\t\t\tbreak\n\t\t\tif not cur_active:\n\t\t\t\textra_reward_rec.append(cur_pred_reward_tensor)\n\t\treturn candidate_tree_managers, candidate_update_tree_idxes, candidate_rewrite_rec, candidate_pending_actions, extra_reward_rec, extra_action_rec\n\n\n\tdef batch_rewrite(self, tree_managers, ac_logprobs, trace_rec, expr_rec, candidate_rewrite_pos, pending_actions, eval_flag, max_search_pos, reward_thres):\n\t\tcandidate_tree_managers = []\n\t\tcandidate_update_tree_idxes = []\n\t\tcandidate_rewrite_rec = []\n\t\tcandidate_pending_actions = []\n\t\textra_reward_rec = []\n\t\textra_action_rec = []\n\t\tfor i in range(len(tree_managers)):\n\t\t\tcur_candidate_tree_managers, cur_candidate_update_tree_idxes, cur_candidate_rewrite_rec, cur_candidate_pending_actions, cur_extra_reward_rec, cur_extra_action_rec = self.rewrite(tree_managers[i], ac_logprobs, trace_rec[i], expr_rec[i], candidate_rewrite_pos[i], pending_actions[i], eval_flag, max_search_pos, reward_thres)\n\t\t\tcandidate_tree_managers.append(cur_candidate_tree_managers)\n\t\t\tcandidate_update_tree_idxes.append(cur_candidate_update_tree_idxes)\n\t\t\tcandidate_rewrite_rec.append(cur_candidate_rewrite_rec)\n\t\t\tcandidate_pending_actions.append(cur_candidate_pending_actions)\n\t\t\textra_reward_rec = extra_reward_rec + cur_extra_reward_rec\n\t\t\textra_action_rec = extra_action_rec + cur_extra_action_rec\n\t\treturn candidate_tree_managers, candidate_update_tree_idxes, candidate_rewrite_rec, candidate_pending_actions, extra_reward_rec, extra_action_rec\n\n\n\tdef calc_dependency(self, tm, cur_idx=None):\n\t\tif cur_idx is None:\n\t\t\tcur_idx = tm.root\n\t\t\ttm.trees[cur_idx].depth = 0\n\t\ttm.trees[cur_idx].dependency_parent = cur_idx\n\t\tcur_tree = tm.get_tree(cur_idx)\n\t\tif len(cur_tree.children) == 0:\n\t\t\treturn []\n\t\tnonterm_idxes = []\n\t\tnonterm_idxes += [cur_idx]\n\t\tfor child in cur_tree.children:\n\t\t\ttm.trees[child].depth = cur_tree.depth + 1\n\t\t\tchild_tree = tm.get_tree(child)\n\t\t\tif child_tree.parent != cur_idx:\n\t\t\t\traise ValueError('invalid edge: ' + str(cur_idx) + ' ' + cur_tree.root + ' ' + str(cur_tree.children) + ' ' + str(child) + ' ' + str(child_tree.parent))\n\t\t\tnonterm_idxes += self.calc_dependency(tm, child)\n\t\ttm.trees[cur_idx].dependency_parent = tm.root\n\t\treturn nonterm_idxes\n\n\n\tdef forward(self, batch_data, eval_flag=False):\n\t\ttree_managers = []\n\t\tbatch_size = len(batch_data)\n\t\tfor trace, tm in batch_data:\n\t\t\ttree_managers.append(tm)\n\t\ttree_managers = self.input_encoder.calc_embedding(tree_managers, eval_flag)\n\n\t\tactive = True\n\t\treduce_steps = 0\n\n\t\ttrace_rec = [[] for _ in range(batch_size)]\n\t\trewrite_rec = [[] for _ in range(batch_size)]\n\t\ttm_rec = [[] for _ in range(batch_size)]\n\t\texpr_rec = [[] for _ in range(batch_size)]\n\t\textra_reward_rec = []\n\t\textra_action_rec = []\n\n\t\tfor idx in range(batch_size):\n\t\t\texpr_rec[idx].append(str(tree_managers[idx]))\n\t\t\ttrace_rec[idx].append((expr_rec[idx][-1], '', -1))\n\t\t\ttm_rec[idx].append(tree_managers[idx])\n\n\t\tpending_actions = [[] for _ in range(batch_size)]\n\t\twhile active and ((self.max_reduce_steps is None) or reduce_steps < self.max_reduce_steps):\n\t\t\tactive = False\n\t\t\treduce_steps += 1\n\t\t\tnonterm_idxes = []\n\t\t\ttree_embeddings = []\n\t\t\troot_embeddings = []\n\t\t\tfor tm_idx in range(batch_size):\n\t\t\t\ttm = tree_managers[tm_idx]\n\t\t\t\tcur_nonterm_idxes = self.calc_dependency(tm)\n\t\t\t\tif len(cur_nonterm_idxes) == 0:\n\t\t\t\t\tcontinue\n\t\t\t\tfor tree_idx in cur_nonterm_idxes:\n\t\t\t\t\tcur_tree = tm.get_tree(tree_idx)\n\t\t\t\t\tnonterm_idxes.append((tm_idx, tree_idx))\n\t\t\t\t\ttree_embeddings.append(cur_tree.state[0])\n\t\t\t\t\troot_embedding = tm.get_tree(cur_tree.dependency_parent).state[0]\n\t\t\t\t\troot_embeddings.append(root_embedding)\n\t\t\tif len(nonterm_idxes) == 0:\n\t\t\t\tbreak\n\t\t\tac_logits = []\n\t\t\tpred_rewards = []\n\t\t\tfor st in range(0, len(nonterm_idxes), self.batch_size):\n\t\t\t\tcur_tree_embeddings = tree_embeddings[st: st + self.batch_size]\n\t\t\t\tcur_tree_embeddings = torch.cat(cur_tree_embeddings, 0)\n\t\t\t\tcur_root_embeddings = root_embeddings[st: st + self.batch_size]\n\t\t\t\tcur_root_embeddings = torch.cat(cur_root_embeddings, 0)\n\t\t\t\tcur_inputs = torch.cat([cur_root_embeddings, cur_tree_embeddings], 1)\n\t\t\t\tcur_ac_logits = self.policy(cur_inputs)\n\t\t\t\tcur_pred_rewards = self.value_estimator(cur_inputs)\n\t\t\t\tac_logits.append(cur_ac_logits)\n\t\t\t\tpred_rewards.append(cur_pred_rewards)\n\t\t\tac_logits = torch.cat(ac_logits, 0)\n\t\t\tac_logprobs = nn.LogSoftmax()(ac_logits)\n\t\t\tac_probs = nn.Softmax()(ac_logits)\n\t\t\tpred_rewards = torch.cat(pred_rewards, 0)\n\t\t\tcandidate_rewrite_pos = [[] for _ in range(batch_size)]\n\t\t\tfor idx, (tm_idx, tree_idx) in enumerate(nonterm_idxes):\n\t\t\t\tcandidate_rewrite_pos[tm_idx].append((pred_rewards[idx].data[0], pred_rewards[idx], ac_probs[idx], tree_idx, idx))\n\n\t\t\tupdate_tree_idxes = [[] for _ in range(batch_size)]\n\t\t\tcandidate_tree_managers, candidate_update_tree_idxes, candidate_rewrite_rec, candidate_pending_actions, cur_extra_reward_rec, cur_extra_action_rec = self.batch_rewrite(tree_managers, ac_logprobs, trace_rec, expr_rec, candidate_rewrite_pos, pending_actions, eval_flag, max_search_pos=1, reward_thres=self.reward_thres)\n\t\t\tfor tm_idx in range(batch_size):\n\t\t\t\tcur_candidate_tree_managers = candidate_tree_managers[tm_idx]\n\t\t\t\tcur_candidate_update_tree_idxes = candidate_update_tree_idxes[tm_idx]\n\t\t\t\tcur_candidate_rewrite_rec = candidate_rewrite_rec[tm_idx]\n\t\t\t\tcur_candidate_pending_actions = candidate_pending_actions[tm_idx]\n\t\t\t\tif len(cur_candidate_tree_managers) > 0:\n\t\t\t\t\tactive = True\n\t\t\t\t\tcur_tree_manager = cur_candidate_tree_managers[0]\n\t\t\t\t\tcur_update_tree_idxes = cur_candidate_update_tree_idxes[0]\n\t\t\t\t\tcur_rewrite_rec = cur_candidate_rewrite_rec[0]\n\t\t\t\t\tcur_pending_actions = cur_candidate_pending_actions[0]\n\t\t\t\t\ttree_managers[tm_idx] = cur_tree_manager\n\t\t\t\t\tupdate_tree_idxes[tm_idx] = cur_update_tree_idxes\n\t\t\t\t\tac_logprob, pred_reward, cur_pred_reward_tensor, rewrite_pos, init_expr, applied_op = cur_rewrite_rec\n\t\t\t\t\ttrace_rec[tm_idx][-1] = (expr_rec[tm_idx][-1], init_expr, applied_op)\n\t\t\t\t\trewrite_rec[tm_idx].append(cur_rewrite_rec)\n\t\t\t\t\tpending_actions[tm_idx] = cur_pending_actions\n\t\t\t\t\tif cur_pending_actions[0] < 0:\n\t\t\t\t\t\tac_logprob_st, pred_reward_st, cur_pred_reward_tensor_st, rewrite_pos_st, init_expr_st, applied_op_st = rewrite_rec[tm_idx][cur_pending_actions[0]]\n\t\t\t\t\t\texpr_st, init_expr_st, applied_op_st = trace_rec[tm_idx][cur_pending_actions[0]]\n\t\t\t\t\t\trewrite_rec[tm_idx][cur_pending_actions[0]] = (ac_logprob_st, pred_reward_st, cur_pred_reward_tensor_st, rewrite_pos_st, init_expr_st, cur_pending_actions[1])\n\t\t\t\t\t\ttrace_rec[tm_idx][cur_pending_actions[0]] = (expr_st, init_expr_st, cur_pending_actions[1])\n\t\t\t\t\t\tif cur_pending_actions[0] < -1:\n\t\t\t\t\t\t\trewrite_rec[tm_idx] = rewrite_rec[tm_idx][:cur_pending_actions[0] + 1]\n\t\t\t\t\t\t\ttrace_rec[tm_idx] = trace_rec[tm_idx][:cur_pending_actions[0] + 1]\n\t\t\t\t\t\t\texpr_rec[tm_idx] = expr_rec[tm_idx][:cur_pending_actions[0] + 1]\n\t\t\t\t\t\t\ttm_rec[tm_idx] = tm_rec[tm_idx][:cur_pending_actions[0] + 1]\n\t\t\t\t\t\tpending_actions[tm_idx] = []\n\t\t\textra_reward_rec = extra_reward_rec + cur_extra_reward_rec\n\t\t\textra_action_rec = extra_action_rec + cur_extra_action_rec\n\t\t\tif not active:\n\t\t\t\tbreak\n\t\t\tupdated_tm = self.input_encoder.update_embedding(tree_managers, update_tree_idxes, eval_flag)\n\t\t\tfor i in range(batch_size):\n\t\t\t\ttree_managers[i] = updated_tm[i]\n\t\t\t\tif len(update_tree_idxes[i]) > 0:\n\t\t\t\t\texpr_rec[i].append(str(updated_tm[i]))\n\t\t\t\t\ttrace_rec[i].append((expr_rec[i][-1], '', -1))\n\t\t\t\t\ttm_rec[i].append(updated_tm[i])\n\n\t\ttotal_policy_loss = data_utils.np_to_tensor(np.zeros(1), 'float', self.cuda_flag)\n\t\ttotal_value_loss = data_utils.np_to_tensor(np.zeros(1), 'float', self.cuda_flag)\n\t\t\n\t\tpred_actions_rec = []\n\t\tpred_actions_logprob_rec = []\n\t\tpred_value_rec = []\n\t\tvalue_target_rec = []\n\t\tpred_dependency_rec = []\n\t\tdependency_target_rec = []\n\t\ttotal_reward = 0\n\t\tfor tm_idx, cur_trace_rec in enumerate(trace_rec):\n\t\t\tpred_trace_len = []\n\t\t\tfor i, (expr, i\n\n# ==========================================\n# File: src/models/jspModel.py\n# Function/Context: jspModel.forward\n# ==========================================\nimport numpy as np\nimport operator\nimport random\nimport time\nfrom multiprocessing.pool import ThreadPool\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch import cuda\nfrom torch.autograd import Variable\nfrom torch.nn.utils import clip_grad_norm\nimport torch.nn.functional as F\nfrom torch.distributions.categorical import Categorical\n\nfrom .data_utils import data_utils\nfrom .modules import jspInputEncoder, mlp\nfrom .rewriter import jspRewriter\nfrom .BaseModel import BaseModel\n\neps = 1e-3\nlog_eps = np.log(eps)\n\n\nclass jspModel(BaseModel):\n\t\"\"\"\n\tModel for job scheduling.\n\t\"\"\"\n\tdef __init__(self, args):\n\t\tsuper(jspModel, self).__init__(args)\n\t\tself.input_format = args.input_format\n\t\tself.max_resource_size = args.max_resource_size\n\t\tself.job_horizon = args.job_horizon\n\t\tself.num_res = args.num_res\n\t\tself.max_time_horizon = args.max_time_horizon\n\t\tself.max_job_len = args.max_job_len\n\t\tself.embedding_size = args.embedding_size\n\t\tself.num_actions = self.job_horizon * 2\n\t\tself.reward_thres = -0.01\n\t\tif self.input_format == 'seq':\n\t\t\tself.input_encoder = jspInputEncoder.SeqLSTM(args)\n\t\telse:\n\t\t\tself.input_encoder = jspInputEncoder.DagLSTM(args)\n\t\tself.policy_embedding = mlp.MLPModel(self.num_MLP_layers, self.LSTM_hidden_size * 2, self.MLP_hidden_size, self.LSTM_hidden_size, self.cuda_flag, self.dropout_rate)\n\t\tself.policy = mlp.MLPModel(self.num_MLP_layers, self.LSTM_hidden_size * (self.job_horizon * 2), self.MLP_hidden_size, self.num_actions, self.cuda_flag, self.dropout_rate)\n\t\tself.value_estimator = mlp.MLPModel(self.num_MLP_layers, self.LSTM_hidden_size, self.MLP_hidden_size, 1, self.cuda_flag, self.dropout_rate)\n\t\tself.rewriter = jspRewriter()\n\n\t\tif args.optimizer == 'adam':\n\t\t\tself.optimizer = optim.Adam(self.parameters(), lr=self.lr)\n\t\telif args.optimizer == 'sgd':\n\t\t\tself.optimizer = optim.SGD(self.parameters(), lr=self.lr)\n\t\telif args.optimizer == 'rmsprop':\n\t\t\tself.optimizer = optim.RMSprop(self.parameters(), lr=self.lr)\n\t\telse:\n\t\t\traise ValueError('optimizer undefined: ', args.optimizer)\n\n\n\tdef rewrite(self, dm, trace_rec, candidate_rewrite_pos, eval_flag, max_search_pos, reward_thres=None):\n\t\t\n\t\tcandidate_rewrite_pos.sort(reverse=True, key=operator.itemgetter(0))\n\t\tif not eval_flag:\n\t\t\tsample_exp_reward_tensor = []\n\t\t\tfor idx, (cur_pred_reward, cur_pred_reward_tensor, rewrite_pos) in enumerate(candidate_rewrite_pos):\n\t\t\t\tsample_exp_reward_tensor.append(cur_pred_reward_tensor)\n\t\t\tsample_exp_reward_tensor = torch.cat(sample_exp_reward_tensor, 0)\n\t\t\tsample_exp_reward_tensor = torch.exp(sample_exp_reward_tensor * 10)\n\t\t\tsample_exp_reward = sample_exp_reward_tensor.data.cpu().numpy()\n\n\t\tcandidate_dag_managers = []\n\t\tcandidate_update_node_idxes = []\n\t\tcandidate_rewrite_rec = []\n\t\textra_reward_rec = []\n\t\t\n\t\tif not eval_flag:\n\t\t\tsample_rewrite_pos_dist = Categorical(sample_exp_reward_tensor)\n\t\t\tsample_rewrite_pos = sample_rewrite_pos_dist.sample(sample_shape=[len(candidate_rewrite_pos)])\n\t\t\t#sample_rewrite_pos = torch.multinomial(sample_exp_reward_tensor, len(candidate_rewrite_pos))\n\t\t\tsample_rewrite_pos = sample_rewrite_pos.data.cpu().numpy()\n\t\t\tindexes = np.unique(sample_rewrite_pos, return_index=True)[1]\n\t\t\tsample_rewrite_pos = [sample_rewrite_pos[i] for i in sorted(indexes)]\n\t\t\tsample_rewrite_pos = sample_rewrite_pos[:self.num_sample_rewrite_pos]\n\t\t\tsample_exp_reward = [sample_exp_reward[i] for i in sample_rewrite_pos]\n\t\t\tsample_rewrite_pos = [candidate_rewrite_pos[i] for i in sample_rewrite_pos]\n\t\telse:\n\t\t\tsample_rewrite_pos = candidate_rewrite_pos.copy()\n\n\t\tfor idx, (pred_reward, cur_pred_reward_tensor, rewrite_pos) in enumerate(sample_rewrite_pos):\n\t\t\tif len(candidate_dag_managers) > 0 and idx >= max_search_pos:\n\t\t\t\tbreak\n\t\t\tif reward_thres is not None and pred_reward < reward_thres:\n\t\t\t\tif eval_flag:\n\t\t\t\t\tbreak\n\t\t\t\telif np.random.random() > self.cont_prob:\n\t\t\t\t\tcontinue\n\t\t\tparent_idxes = dm.get_parent_idxes(rewrite_pos, self.job_horizon)\n\t\t\tchildren_idxes = dm.get_children_idxes(rewrite_pos, self.job_horizon)\n\t\t\tpolicy_embedding_inputs = []\n\t\t\tcur_input = dm.get_node(rewrite_pos).state[0]\n\t\t\tcur_inputs = []\n\t\t\tfor i in parent_idxes:\n\t\t\t\tpolicy_embedding_inputs.append(dm.get_node(i).state[0])\n\t\t\t\tcur_inputs.append(cur_input.clone())\n\t\t\twhile len(policy_embedding_inputs) < self.job_horizon:\n\t\t\t\tzero_state = Variable(torch.zeros(1, self.LSTM_hidden_size))\n\t\t\t\tif self.cuda_flag:\n\t\t\t\t\tzero_state = zero_state.cuda()\n\t\t\t\tpolicy_embedding_inputs.append(zero_state)\n\t\t\t\tcur_inputs.append(zero_state.clone())\n\t\t\tfor i in children_idxes:\n\t\t\t\tpolicy_embedding_inputs.append(dm.get_node(i).state[0])\n\t\t\t\tcur_inputs.append(cur_input.clone())\n\t\t\twhile len(policy_embedding_inputs) < self.job_horizon * 2:\n\t\t\t\tzero_state = Variable(torch.zeros(1, self.LSTM_hidden_size))\n\t\t\t\tif self.cuda_flag:\n\t\t\t\t\tzero_state = zero_state.cuda()\n\t\t\t\tpolicy_embedding_inputs.append(zero_state)\n\t\t\t\tcur_inputs.append(zero_state.clone())\n\t\t\tpolicy_embedding_inputs = torch.cat(policy_embedding_inputs, 0)\n\t\t\tcur_inputs = torch.cat(cur_inputs, 0)\n\t\t\tpolicy_embedding_inputs = torch.cat([cur_inputs, policy_embedding_inputs], 1)\n\t\t\tpolicy_inputs = self.policy_embedding(policy_embedding_inputs)\n\t\t\tpolicy_inputs = policy_inputs.view(1, self.LSTM_hidden_size * (self.job_horizon * 2))\n\t\t\tac_logits = self.policy(policy_inputs)\n\t\t\tac_logprobs = nn.LogSoftmax()(ac_logits)\n\t\t\tac_probs = nn.Softmax()(ac_logits)\n\t\t\tac_logits = ac_logits.squeeze(0)\n\t\t\tac_logprobs = ac_logprobs.squeeze(0)\n\t\t\tac_probs = ac_probs.squeeze(0)\n\t\t\tif eval_flag:\n\t\t\t\t_, candidate_acs = torch.sort(ac_logprobs, descending=True)\n\t\t\t\tcandidate_acs = candidate_acs.data.cpu().numpy()\n\t\t\telse:\n\t\t\t\tcandidate_acs_dist = Categorical(ac_probs)\n\t\t\t\tcandidate_acs = candidate_acs_dist.sample(sample_shape=[ac_probs.size()[0]])\n\t\t\t\t#candidate_acs = torch.multinomial(ac_probs, ac_probs.size()[0])\n\t\t\t\tcandidate_acs = candidate_acs.data.cpu().numpy()\n\t\t\t\tindexes = np.unique(candidate_acs, return_index=True)[1]\n\t\t\t\tcandidate_acs = [candidate_acs[i] for i in sorted(indexes)]\n\t\t\tcur_active = False\n\t\t\tfor i, op_idx in enumerate(candidate_acs):\n\t\t\t\tif op_idx < self.job_horizon:\n\t\t\t\t\tif op_idx >= len(parent_idxes):\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tneighbor_idx = parent_idxes[op_idx]\n\t\t\t\telse:\n\t\t\t\t\tif op_idx - self.job_horizon >= len(children_idxes):\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tneighbor_idx = children_idxes[op_idx - self.job_horizon]\n\t\t\t\tif (rewrite_pos, neighbor_idx) in trace_rec or (neighbor_idx, rewrite_pos) in trace_rec:\n\t\t\t\t\tcontinue\n\t\t\t\tnew_dm, cur_update_node_idxes = self.rewriter.move(dm, rewrite_pos, neighbor_idx)\n\t\t\t\tif len(cur_update_node_idxes) == 0:\n\t\t\t\t\tcontinue\n\t\t\t\tcandidate_update_node_idxes.append(cur_update_node_idxes)\n\t\t\t\tcandidate_dag_managers.append(new_dm)\n\t\t\t\tcandidate_rewrite_rec.append((ac_logprobs, pred_reward, cur_pred_reward_tensor, rewrite_pos, op_idx, neighbor_idx))\n\t\t\t\tcur_active = True\n\t\t\t\tif len(candidate_dag_managers) >= max_search_pos:\n\t\t\t\t\tbreak\n\t\t\tif not cur_active:\n\t\t\t\textra_reward_rec.append(cur_pred_reward_tensor)\n\t\treturn candidate_dag_managers, candidate_update_node_idxes, candidate_rewrite_rec, extra_reward_rec\n\n\n\tdef batch_rewrite(self, dag_managers, trace_rec, candidate_rewrite_pos, eval_flag, max_search_pos, reward_thres):\n\t\tcandidate_dag_managers = []\n\t\tcandidate_update_node_idxes = []\n\t\tcandidate_rewrite_rec = []\n\t\textra_reward_rec = []\n\t\tfor i in range(len(dag_managers)):\n\t\t\tcur_candidate_dag_managers, cur_candidate_update_node_idxes, cur_candidate_rewrite_rec, cur_extra_reward_rec = self.rewrite(dag_managers[i], trace_rec[i], candidate_rewrite_pos[i], eval_flag, max_search_pos, reward_thres)\n\t\t\tcandidate_dag_managers.append(cur_candidate_dag_managers)\n\t\t\tcandidate_update_node_idxes.append(cur_candidate_update_node_idxes)\n\t\t\tcandidate_rewrite_rec.append(cur_candidate_rewrite_rec)\n\t\t\textra_reward_rec = extra_reward_rec + cur_extra_reward_rec\n\t\treturn candidate_dag_managers, candidate_update_node_idxes, candidate_rewrite_rec, extra_reward_rec\n\n\n\tdef forward(self, batch_data, eval_flag=False):\n\t\tdag_managers = []\n\t\tbatch_size = len(batch_data)\n\t\tfor dm in batch_data:\n\t\t\tdag_managers.append(dm)\n\t\tdag_managers = self.input_encoder.calc_embedding(dag_managers, eval_flag)\n\n\t\tactive = True\n\t\treduce_steps = 0\n\n\t\ttrace_rec = [[] for _ in range(batch_size)]\n\t\trewrite_rec = [[] for _ in range(batch_size)]\n\t\tdm_rec = [[] for _ in range(batch_size)]\n\t\textra_reward_rec = []\n\t\t\n\t\tfor idx in range(batch_size):\n\t\t\tdm_rec[idx].append(dag_managers[idx])\n\n\t\twhile active and ((self.max_reduce_steps is None) or reduce_steps < self.max_reduce_steps):\n\t\t\tactive = False\n\t\t\treduce_steps += 1\n\t\t\tnode_idxes = []\n\t\t\tnode_embeddings = []\n\t\t\troot_embeddings = []\n\t\t\tfor dm_idx in range(batch_size):\n\t\t\t\tdm = dag_managers[dm_idx]\n\t\t\t\troot_embedding = dm.get_node(0).state[0]\n\t\t\t\tfor i in range(1, dm.num_nodes):\n\t\t\t\t\tcur_node = dm.get_node(i)\n\t\t\t\t\tnode_idxes.append((dm_idx, i))\n\t\t\t\t\tnode_embeddings.append(cur_node.state[0])\n\t\t\t\t\troot_embeddings.append(root_embedding.clone())\n\t\t\tpred_rewards = []\n\t\t\tfor st in range(0, len(node_idxes), self.batch_size):\n\t\t\t\tcur_node_embeddings = node_embeddings[st: st + self.batch_size]\n\t\t\t\tcur_node_embeddings = torch.cat(cur_node_embeddings, 0)\n\t\t\t\tcur_pred_rewards = self.value_estimator(cur_node_embeddings)\n\t\t\t\tpred_rewards.append(cur_pred_rewards)\n\t\t\tpred_rewards = torch.cat(pred_rewards, 0)\n\t\t\tcandidate_rewrite_pos = [[] for _ in range(batch_size)]\n\t\t\tfor idx, (dm_idx, node_idx) in enumerate(node_idxes):\n\t\t\t\tcandidate_rewrite_pos[dm_idx].append((pred_rewards[idx].data[0], pred_rewards[idx], node_idx))\n\n\t\t\tupdate_node_idxes = [[] for _ in range(batch_size)]\n\t\t\tcandidate_dag_managers, candidate_update_node_idxes, candidate_rewrite_rec, cur_extra_reward_rec = self.batch_rewrite(dag_managers, trace_rec, candidate_rewrite_pos, eval_flag, max_search_pos=1, reward_thres=self.reward_thres)\n\t\t\tfor dm_idx in range(batch_size):\n\t\t\t\tcur_candidate_dag_managers = candidate_dag_managers[dm_idx]\n\t\t\t\tcur_candidate_update_node_idxes = candidate_update_node_idxes[dm_idx]\n\t\t\t\tcur_candidate_rewrite_rec = candidate_rewrite_rec[dm_idx]\n\t\t\t\tif len(cur_candidate_dag_managers) > 0:\n\t\t\t\t\tactive = True\n\t\t\t\t\tcur_dag_manager = cur_candidate_dag_managers[0]\n\t\t\t\t\tcur_update_node_idxes = cur_candidate_update_node_idxes[0]\n\t\t\t\t\tcur_rewrite_rec = cur_candidate_rewrite_rec[0]\n\t\t\t\t\tdag_managers[dm_idx] = cur_dag_manager\n\t\t\t\t\tupdate_node_idxes[dm_idx] = cur_update_node_idxes\n\t\t\t\t\tac_logprob, pred_reward, cur_pred_reward_tensor, rewrite_pos, applied_op, neighbor_idx = cur_rewrite_rec\n\t\t\t\t\trewrite_rec[dm_idx].append(cur_rewrite_rec)\n\t\t\t\t\ttrace_rec[dm_idx].append((rewrite_pos, neighbor_idx))\n\t\t\tif not active:\n\t\t\t\tbreak\n\n\t\t\tupdated_dm = self.input_encoder.calc_embedding(dag_managers, eval_flag)\n\n\t\t\tfor i in range(batch_size):\n\t\t\t\tdag_managers[i] = updated_dm[i]\n\t\t\t\tif len(update_node_idxes[i]) > 0:\n\t\t\t\t\tdm_rec[i].append(updated_dm[i])\n\n\t\ttotal_policy_loss = data_utils.np_to_tensor(np.zeros(1), 'float', self.cuda_flag)\n\t\ttotal_value_loss = data_utils.np_to_tensor(np.zeros(1), 'float', self.cuda_flag)\n\t\t\n\t\tpred_actions_rec = []\n\t\tpred_actions_logprob_rec = []\n\t\tpred_value_rec = []\n\t\tvalue_target_rec = []\n\t\ttotal_reward = 0\n\t\ttotal_completion_time = 0\n\t\ttotal_slow_down = 0\n\t\tfor dm_idx, cur_dm_rec in enumerate(dm_rec):\n\t\t\tpred_avg_slow_down = []\n\t\t\tpred_avg_completion_time = []\n\t\t\tfor dm in cur_dm_rec:\n\t\t\t\tpred_avg_slow_down.append(dm.avg_slow_down)\n\t\t\t\tpred_avg_completion_time.append(dm.avg_completion_time)\n\t\t\tmin_slow_down = pred_avg_slow_down[0]\n\t\t\tmin_completion_time = pred_avg_completion_time[0]\n\t\t\tbest_reward = min_slow_down\n\t\t\tfor idx, (ac_logprob, pred_reward, cur_pred_reward_tensor, rewrite_pos, applied_op, neighbor_idx) in enumerate(rewrite_rec[dm_idx]):\n\t\t\t\tcur_reward = pred_avg_slow_down[idx] - pred_avg_slow_down[idx + 1] - 0.01\n\t\t\t\tbest_reward = min(best_reward, pred_avg_slow_down[idx + 1])\n\t\t\t\tmin_slow_down = min(min_slow_down, pred_avg_slow_down[idx + 1])\n\t\t\t\tmin_completion_time = min(min_completion_time, pred_avg_completion_time[idx + 1])\n\n\t\t\t\tif self.gamma > 0.0:\n\t\t\t\t\tdecay_coef = 1.0\n\t\t\t\t\tnum_rollout_steps = len(cur_dm_rec) - idx - 1\n\t\t\t\t\tfor i in range(idx + 1, idx + 1 + num_rollout_steps):\n\t\t\t\t\t\tcur_reward = max(decay_coef * (pred_avg_slow_down[idx] - pred_avg_slow_down[i] - (i - idx) * 0.01), cur_reward)\n\t\t\t\t\t\tdecay_coef *= self.gamma\n\n\t\t\t\tcur_reward = cur_reward * 1.0 / pred_avg_slow_down[0]\n\t\t\t\tcur_reward_tensor = data_utils.np_to_tensor(np.array([cur_reward], dtype=np.float32), 'float', self.cuda_flag, eval_flag)\n\t\t\t\tif ac_logprob.data.cpu().numpy()[0] > log_eps or cur_reward - pred_reward > 0:\n\t\t\t\t\tac_mask = np.zeros(self.num_actions)\n\t\t\t\t\tac_mask[applied_op] = cur_reward - pred_reward\n\t\t\t\t\tac_mask = data_utils.np_to_tensor(ac_mask, 'float', self.cuda_flag, eval_flag)\n\t\t\t\t\tac_mask = ac_mask.unsqueeze(0)\n\t\t\t\t\tpred_actions_rec.append(ac_mask)\n\t\t\t\t\tpred_actions_logprob_rec.append(ac_logprob.unsqueeze(0))\n\t\t\t\tpred_value_rec.append(cur_pred_reward_tensor)\n\t\t\t\tvalue_target_rec.append(cur_reward_tensor)\n\t\t\ttotal_reward += best_reward\n\t\t\ttotal_completion_time += min_completion_time\n\t\t\ttotal_slow_down += min_slow_down\n\n\t\tfor cur_pred_reward in extra_reward_rec:\n\t\t\tpred_value_rec.append(cur_pred_reward)\n\t\t\tvalue_target = data_utils.np_to_tensor(np.zeros(1), 'float', self.cuda_flag)\n\t\t\tvalue_target_rec.append(value_target)\n\n\t\tif len(pred_actions_rec) > 0:\n\t\t\tpred_actions_rec = torch.cat(pred_actions_rec, 0)\n\t\t\tpred_actions_logprob_rec = torch.cat(pred_actions_logprob_rec, 0)\n\t\t\tpred_value_rec = torch.cat(pred_value_rec, 0)\n\t\t\tvalue_target_rec = torch.cat(value_target_rec, 0)\n\t\t\tpred_value_rec = pred_value_rec.unsqueeze(1)\n\t\t\tvalue_target_rec = value_target_rec.unsqueeze(1)\n\t\t\ttotal_policy_loss = -torch.sum(pred_actions_logprob_rec * pred_actions_rec)\n\t\t\ttotal_value_loss = F.smooth_l1_loss(pred_value_rec, value_target_rec, size_average=False)\n\t\ttotal_policy_loss /= batch_size\n\t\ttotal_value_loss /= batch_size\n\t\ttotal_loss = total_policy_loss + total_value_loss * self.value_loss_coef\n\t\ttotal_reward = total_reward * 1.0 / batch_size\n\t\ttotal_completion_time = total_completion_time * 1.0 / batch_size\n\t\ttotal_slow_down = total_slow_down * 1.0 / batch_size\n\t\treturn total_loss, total_reward, total_completion_time, dm_rec\n\n# ==========================================\n# File: src/models/rewriter/HalideRewriter.py\n# Function/Context: HalideRewriter\n# ==========================================\n# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nimport numpy as np\nimport operator\nimport random\nimport time\nimport copy\n\nfrom ..data_utils import data_utils\nfrom ..data_utils import Tree\nfrom ..data_utils.utils import *\n\n\nclass HalideRewriter(object):\n\t\"\"\"\n\tThe rewriter for expression simplification, according to the rewriting rules of Halide.\n\t\"\"\"\n\tdef __init__(self, args, term_vocab, term_vocab_list, op_vocab, op_vocab_list):\n\t\tself.term_vocab = term_vocab\n\t\tself.term_vocab_list = term_vocab_list\n\t\tself.op_vocab = op_vocab\n\t\tself.op_vocab_list = op_vocab_list\n\t\tself.term_vocab_size = args.term_vocab_size\n\t\tself.op_vocab_size = args.op_vocab_size\n\t\tself.rewrite_rules = [self.not_symbol_rewrite, self.simple_bool_rewrite, self.eq_bool_rewrite, \\\n\t\tself.var_bound_rewrite, self.alg_const_calculation, self.cmp_const_calculation, \\\n\t\tself.left_association, self.association, \\\n\t\tself.muldiv_elimination, self.div_reduction, self.muldiv_to_mod_transformation, \\\n\t\tself.muldiv_association, self.muldiv_distribution, \\\n\t\tself.select_simplification, self.select_association, self.select_distribution, \\\n\t\tself.minmax_simplification, self.minmax_alg_distribution, self.minmax_distribution]\n\t\tself.rewrite_seqs = [[i, -1, i] for i in range(len(self.rewrite_rules))]\n\t\tself.add_rewrite_seqs()\n\n\n\tdef add_rewrite_seqs(self):\n\t\tself.rewrite_seqs += [[12, 4, -2, len(self.rewrite_seqs)]]\n\t\tself.rewrite_seqs += [[9, 4, -2, len(self.rewrite_seqs)]]\n\t\tself.rewrite_seqs += [[7, 5, -2, len(self.rewrite_seqs)]]\n\t\tself.rewrite_seqs += [[17, 4, -2, len(self.rewrite_seqs)]]\n\t\tself.rewrite_seqs += [[18, 5, -2, len(self.rewrite_seqs)]]\n\t\tself.rewrite_seqs += [[10, 4, -2, len(self.rewrite_seqs)]]\n\t\tself.rewrite_seqs += [[15, 0, -2, len(self.rewrite_seqs)]]\n\t\tself.rewrite_seqs += [[15, 5, -2, len(self.rewrite_seqs)]]\n\t\tself.rewrite_seqs += [[11, 4, -2, len(self.rewrite_seqs)]]\n\t\tself.rewrite_seqs += [[11, 11, -2, len(self.rewrite_seqs)]]\n\n\n\tdef get_rewrite_op(self, idx):\n\t\treturn self.rewrite_rules[idx]\n\n\n\tdef get_rewrite_seq(self, idx):\n\t\treturn self.rewrite_seqs[idx]\n\n\n\tdef not_symbol_rewrite(self, tm, tree_idx): #return (tm, tree idxes to update embedding)\n\t\tcur_tree = tm.get_tree(tree_idx)\n\t\top = cur_tree.root\n\t\tif op != '!':\n\t\t\treturn tm, []\n\t\tchild = tm.get_tree(cur_tree.children[0])\n\t\tif child.is_const:\n\t\t\tres = tm.clone()\n\t\t\tparent_idx = cur_tree.parent\n\t\t\tchild_idx = res.find_child_idx(parent_idx, tree_idx)\n\t\t\tres.update_edge(parent_idx, cur_tree.children[0], child_idx)\n\t\t\tif res.trees[cur_tree.children[0]].root == '1':\n\t\t\t\tres.trees[cur_tree.children[0]].root = '0'\n\t\t\telse:\n\t\t\t\tres.trees[cur_tree.children[0]].root = '1'\n\t\t\treturn res, [cur_tree.children[0]]\n\t\tif child.root == '!': # !!v0 -> v0\n\t\t\tres = tm.clone()\n\t\t\tparent_idx = cur_tree.parent\n\t\t\tchild_idx = res.find_child_idx(parent_idx, tree_idx)\n\t\t\tres.update_edge(parent_idx, child.children[0], child_idx)\n\t\t\treturn res, [parent_idx]\n\t\tif child.root == '<':\n\t\t\tres = tm.clone()\n\t\t\tparent_idx = cur_tree.parent\n\t\t\tchild_idx = res.find_child_idx(parent_idx, tree_idx)\n\t\t\tres.update_edge(parent_idx, cur_tree.children[0], child_idx)\n\t\t\tres.trees[cur_tree.children[0]].children.reverse()\n\t\t\treturn res, [cur_tree.children[0]]\n\t\treturn tm, []\n\n\n\tdef simple_bool_rewrite(self, tm, tree_idx):\n\t\tcur_tree = tm.get_tree(tree_idx)\n\t\top = cur_tree.root\n\t\tif not (op in ['&&', '||']):\n\t\t\treturn tm, []\n\t\tltree_idx = cur_tree.children[0]\n\t\trtree_idx = cur_tree.children[1]\n\t\tltree = tm.get_tree(ltree_idx)\n\t\trtree = tm.get_tree(rtree_idx)\n\t\tif ltree.is_const and rtree.is_const:\n\t\t\tres = tm.clone()\n\t\t\tres.trees[tree_idx].root = calc(op, ltree.root, rtree.root)\n\t\t\tres.trees[tree_idx].is_const = True\n\t\t\tres.trees[tree_idx].children = []\n\t\t\treturn res, [tree_idx]\n\t\tif rtree.is_const:\n\t\t\tif rtree.root == '1' and op == '||':\n\t\t\t\tres = tm.clone()\n\t\t\t\tres.trees[tree_idx].root = '1'\n\t\t\t\tres.trees[tree_idx].is_const = True\n\t\t\t\tres.trees[tree_idx].children = []\n\t\t\t\treturn res, [tree_idx]\n\t\t\tif rtree.root == '0' and op == '&&':\n\t\t\t\tres = tm.clone()\n\t\t\t\tres.trees[tree_idx].root = '0'\n\t\t\t\tres.trees[tree_idx].is_const = True\n\t\t\t\tres.trees[tree_idx].children = []\n\t\t\t\treturn res, [tree_idx]\n\t\t\tif (rtree.root == '1' and op == '&&') or (rtree.root == '0' and op == '||'):\n\t\t\t\tres = tm.clone()\n\t\t\t\tparent_idx = cur_tree.parent\n\t\t\t\tchild_idx = res.find_child_idx(parent_idx, tree_idx)\n\t\t\t\tres.update_edge(parent_idx, ltree_idx, child_idx)\n\t\t\t\treturn res, [parent_idx]\n\t\tif ltree.is_const:\n\t\t\tif ltree.root == '1' and op == '||':\n\t\t\t\tres = tm.clone()\n\t\t\t\tres.trees[tree_idx].root = '1'\n\t\t\t\tres.trees[tree_idx].is_const = True\n\t\t\t\tres.trees[tree_idx].children = []\n\t\t\t\treturn res, [tree_idx]\n\t\t\tif ltree.root == '0' and op == '&&':\n\t\t\t\tres = tm.clone()\n\t\t\t\tres.trees[tree_idx].root = '0'\n\t\t\t\tres.trees[tree_idx].is_const = True\n\t\t\t\tres.trees[tree_idx].children = []\n\t\t\t\treturn res, [tree_idx]\n\t\t\tif (ltree.root == '1' and op == '&&') or (ltree.root == '0' and op == '||'):\n\t\t\t\tres = tm.clone()\n\t\t\t\tparent_idx = cur_tree.parent\n\t\t\t\tchild_idx = res.find_child_idx(parent_idx, tree_idx)\n\t\t\t\tres.update_edge(parent_idx, rtree_idx, child_idx)\n\t\t\t\treturn res, [parent_idx]\n\t\tsame_tree_idx = tm.find_subtree(ltree_idx, rtree_idx, op)\n\t\tif same_tree_idx == -1:\n\t\t\treturn tm, []\n\t\tres = tm.clone()\n\t\tparent_idx = cur_tree.parent\n\t\tchild_idx = res.find_child_idx(parent_idx, tree_idx)\n\t\tres.update_edge(parent_idx, ltree_idx, child_idx)\n\t\treturn res, [parent_idx]\n\n\n\tdef eq_bool_rewrite(self, tm, tree_idx):\n\t\tcur_tree = tm.get_tree(tree_idx)\n\t\top = cur_tree.root\n\t\tif not (op in ['&&', '||']):\n\t\t\treturn tm, []\n\t\tltree_idx = cur_tree.children[0]\n\t\trtree_idx = cur_tree.children[1]\n\t\tltree = tm.get_tree(ltree_idx)\n\t\trtree = tm.get_tree(rtree_idx)\n\t\tif ltree.root == '!':\n\t\t\tltree_child_idx = ltree.children[0]\n\t\t\tltree_child = tm.get_tree(ltree_child_idx)\n\t\t\tif tm.equal_tree(ltree_child, rtree):\n\t\t\t\tres = tm.clone()\n\t\t\t\tif op == '||':\n\t\t\t\t\tres.trees[tree_idx].root = '1'\n\t\t\t\telse:\n\t\t\t\t\tres.trees[tree_idx].root = '0'\n\t\t\t\tres.trees[tree_idx].is_const = True\n\t\t\t\tres.trees[tree_idx].children = []\n\t\t\t\treturn res, [tree_idx]\n\t\tif rtree.root == '!':\n\t\t\trtree_child_idx = rtree.children[0]\n\t\t\trtree_child = tm.get_tree(rtree_child_idx)\n\t\t\tif tm.equal_tree(rtree_child, ltree):\n\t\t\t\tres = tm.clone()\n\t\t\t\tif op == '||':\n\t\t\t\t\tres.trees[tree_idx].root = '1'\n\t\t\t\telse:\n\t\t\t\t\tres.trees[tree_idx].root = '0'\n\t\t\t\tres.trees[tree_idx].is_const = True\n\t\t\t\tres.trees[tree_idx].children = []\n\t\t\t\treturn res, [tree_idx]\n\t\tif (ltree.root == rtree.root) and (ltree.root in ['<', '<=']):\n\t\t\tltree_lchild_idx = ltree.children[0]\n\t\t\tltree_lchild = tm.get_tree(ltree_lchild_idx)\n\t\t\tltree_rchild_idx = ltree.children[1]\n\t\t\tltree_rchild = tm.get_tree(ltree_rchild_idx)\n\t\t\trtree_lchild_idx = rtree.children[0]\n\t\t\trtree_lchild = tm.get_tree(rtree_lchild_idx)\n\t\t\trtree_rchild_idx = rtree.children[1]\n\t\t\trtree_rchild = tm.get_tree(rtree_rchild_idx)\n\t\t\tif tm.equal_tree(ltree_lchild, rtree_rchild) and tm.equal_tree(ltree_rchild, rtree_lchild):\n\t\t\t\tres = tm.clone()\n\t\t\t\tif op == '||':\n\t\t\t\t\tres.trees[tree_idx].root = '1'\n\t\t\t\telse:\n\t\t\t\t\tres.trees[tree_idx].root = '0'\n\t\t\t\tres.trees[tree_idx].is_const = True\n\t\t\t\tres.trees[tree_idx].children = []\n\t\t\t\treturn res, [tree_idx]\n\t\tif (op == '&&') and (ltree.root in ['<', '<=']) and (rtree.root in ['<', '<=']):\n\t\t\tltree_lchild_idx = ltree.children[0]\n\t\t\tltree_lchild = tm.get_tree(ltree_lchild_idx)\n\t\t\tltree_rchild_idx = ltree.children[1]\n\t\t\tltree_rchild = tm.get_tree(ltree_rchild_idx)\n\t\t\trtree_lchild_idx = rtree.children[0]\n\t\t\trtree_lchild = tm.get_tree(rtree_lchild_idx)\n\t\t\trtree_rchild_idx = rtree.children[1]\n\t\t\trtree_rchild = tm.get_tree(rtree_rchild_idx)\n\t\t\tif tm.equal_tree(ltree_lchild, rtree_rchild) and tm.equal_tree(ltree_rchild, rtree_lchild):\n\t\t\t\tres = tm.clone()\n\t\t\t\tres.trees[tree_idx].root = '0'\n\t\t\t\tres.trees[tree_idx].is_const = True\n\t\t\t\tres.trees[tree_idx].children = []\n\t\t\t\treturn res, [tree_idx]\n\t\treturn tm, []\n\n\n\tdef var_bound_rewrite(self, tm, tree_idx):\n\t\tcur_tree = tm.get_tree(tree_idx)\n\t\top = cur_tree.root\n\t\tif not (op in ['&&', '||']):\n\t\t\treturn tm, []\n\t\tltree_idx = cur_tree.children[0]\n\t\trtree_idx = cur_tree.children[1]\n\t\tltree = tm.get_tree(ltree_idx)\n\t\trtree = tm.get_tree(rtree_idx)\n\t\tl_reverse = False\n\t\tr_reverse = False\n\t\tif ltree.root == '!':\n\t\t\tl_reverse = True\n\t\t\tltree_idx = ltree.children[0]\n\t\t\tltree = tm.get_tree(ltree_idx)\n\t\tif rtree.root == '!':\n\t\t\tr_reverse = True\n\t\t\trtree_idx = rtree.children[0]\n\t\t\trtree = tm.get_tree(rtree_idx)\t\t\n\t\tif not ((ltree.root in ['<', '<=']) and (rtree.root in ['<', '<='])):\n\t\t\treturn tm, []\n\t\tltree_lchild_idx = ltree.children[0]\n\t\tltree_lchild = tm.get_tree(ltree_lchild_idx)\n\t\tltree_rchild_idx = ltree.children[1]\n\t\tltree_rchild = tm.get_tree(ltree_rchild_idx)\n\t\trtree_lchild_idx = rtree.children[0]\n\t\trtree_lchild = tm.get_tree(rtree_lchild_idx)\n\t\trtree_rchild_idx = rtree.children[1]\n\t\trtree_rchild = tm.get_tree(rtree_rchild_idx)\n\n\t\tif tm.equal_tree(ltree_lchild, rtree_lchild) and ltree_rchild.root == '+' and rtree_rchild.root == '+':\n\t\t\tltree_rchild_ltree_idx = ltree_rchild.children[0]\n\t\t\tltree_rchild_ltree = tm.get_tree(ltree_rchild_ltree_idx)\n\t\t\tltree_rchild_rtree_idx = ltree_rchild.children[1]\n\t\t\tltree_rchild_rtree = tm.get_tree(ltree_rchild_rtree_idx)\n\t\t\trtree_rchild_ltree_idx = rtree_rchild.children[0]\n\t\t\trtree_rchild_ltree = tm.get_tree(rtree_rchild_ltree_idx)\n\t\t\trtree_rchild_rtree_idx = rtree_rchild.children[1]\n\t\t\trtree_rchild_rtree = tm.get_tree(rtree_rchild_rtree_idx)\n\t\t\tif ltree_rchild_rtree.is_const and rtree_rchild_rtree.is_const \\\n\t\t\tand (not ltree_rchild_ltree.is_const) and tm.equal_tree(ltree_rchild_ltree, rtree_rchild_ltree):\n\t\t\t\tres = tm.clone()\n\t\t\t\tres.trees[ltree_rchild_idx].root = '-'\n\t\t\t\tres.trees[rtree_rchild_idx].root = '-'\n\t\t\t\tres.update_edge(ltree_idx, ltree_rchild_idx, 0)\n\t\t\t\tres.update_edge(rtree_idx, rtree_rchild_idx, 0)\n\t\t\t\tres.update_edge(ltree_idx, ltree_rchild_rtree_idx, 1)\n\t\t\t\tres.update_edge(rtree_idx, rtree_rchild_rtree_idx, 1)\n\t\t\t\tres.update_edge(ltree_rchild_idx, ltree_lchild_idx, 0)\n\t\t\t\tres.update_edge(ltree_rchild_idx, ltree_rchild_ltree_idx, 1)\n\t\t\t\tres.update_edge(rtree_rchild_idx, rtree_lchild_idx, 0)\n\t\t\t\tres.update_edge(rtree_rchild_idx, rtree_rchild_ltree_idx, 1)\n\t\t\t\treturn res, [ltree_rchild_idx, rtree_rchild_idx]\n\n\t\tif tm.equal_tree(ltree_lchild, rtree_rchild) and ltree_rchild.root == '+' and rtree_lchild.root == '+':\n\t\t\tltree_rchild_ltree_idx = ltree_rchild.children[0]\n\t\t\tltree_rchild_ltree = tm.get_tree(ltree_rchild_ltree_idx)\n\t\t\tltree_rchild_rtree_idx = ltree_rchild.children[1]\n\t\t\tltree_rchild_rtree = tm.get_tree(ltree_rchild_rtree_idx)\n\t\t\trtree_lchild_ltree_idx = rtree_lchild.children[0]\n\t\t\trtree_lchild_ltree = tm.get_tree(rtree_lchild_ltree_idx)\n\t\t\trtree_lchild_rtree_idx = rtree_lchild.children[1]\n\t\t\trtree_lchild_rtree = tm.get_tree(rtree_lchild_rtree_idx)\n\t\t\tif ltree_rchild_rtree.is_const and rtree_lchild_rtree.is_const \\\n\t\t\tand (not ltree_rchild_ltree.is_const) and tm.equal_tree(ltree_rchild_ltree, rtree_lchild_ltree):\n\t\t\t\tres = tm.clone()\n\t\t\t\tres.trees[ltree_rchild_idx].root = '-'\n\t\t\t\tres.trees[rtree_lchild_idx].root = '-'\n\t\t\t\tres.update_edge(ltree_idx, ltree_rchild_idx, 0)\n\t\t\t\tres.update_edge(rtree_idx, rtree_lchild_idx, 1)\n\t\t\t\tres.update_edge(ltree_idx, ltree_rchild_rtree_idx, 1)\n\t\t\t\tres.update_edge(rtree_idx, rtree_lchild_rtree_idx, 0)\n\t\t\t\tres.update_edge(ltree_rchild_idx, ltree_lchild_idx, 0)\n\t\t\t\tres.update_edge(ltree_rchild_idx, ltree_rchild_ltree_idx, 1)\n\t\t\t\tres.update_edge(rtree_lchild_idx, rtree_rchild_idx, 0)\n\t\t\t\tres.update_edge(rtree_lchild_idx, rtree_lchild_ltree_idx, 1)\n\t\t\t\treturn res, [ltree_rchild_idx, rtree_lchild_idx]\n\n\t\tif tm.equal_tree(ltree_rchild, rtree_lchild) and ltree_lchild.root == '+' and rtree_rchild.root == '+':\n\t\t\tltree_lchild_ltree_idx = ltree_lchild.children[0]\n\t\t\tltree_lchild_ltree = tm.get_tree(ltree_lchild_ltree_idx)\n\t\t\tltree_lchild_rtree_idx = ltree_lchild.children[1]\n\t\t\tltree_lchild_rtree = tm.get_tree(ltree_lchild_rtree_idx)\n\t\t\trtree_rchild_ltree_idx = rtree_rchild.children[0]\n\t\t\trtree_rchild_ltree = tm.get_tree(rtree_rchild_ltree_idx)\n\t\t\trtree_rchild_rtree_idx = rtree_rchild.children[1]\n\t\t\trtree_rchild_rtree = tm.get_tree(rtree_rchild_rtree_idx)\n\t\t\tif ltree_lchild_rtree.is_const and rtree_rchild_rtree.is_const \\\n\t\t\tand (not ltree_lchild_ltree.is_const) and tm.equal_tree(ltree_lchild_ltree, rtree_rchild_ltree):\n\t\t\t\tres = tm.clone()\n\t\t\t\tres.trees[ltree_lchild_idx].root = '-'\n\t\t\t\tres.trees[rtree_rchild_idx].root = '-'\n\t\t\t\tres.update_edge(ltree_idx, ltree_lchild_idx, 1)\n\t\t\t\tres.update_edge(rtree_idx, rtree_rchild_idx, 0)\n\t\t\t\tres.update_edge(ltree_idx, ltree_lchild_rtree_idx, 0)\n\t\t\t\tres.update_edge(rtree_idx, rtree_rchild_rtree_idx, 1)\n\t\t\t\tres.update_edge(ltree_lchild_idx, ltree_rchild_idx, 0)\n\t\t\t\tres.update_edge(ltree_lchild_idx, ltree_lchild_ltree_idx, 1)\n\t\t\t\tres.update_edge(rtree_rchild_idx, rtree_lchild_idx, 0)\n\t\t\t\tres.update_edge(rtree_rchild_idx, rtree_rchild_ltree_idx, 1)\n\t\t\t\treturn res, [ltree_lchild_idx, rtree_rchild_idx]\n\n\t\tif tm.equal_tree(ltree_rchild, rtree_rchild) and ltree_lchild.root == '+' and rtree_lchild.root == '+':\n\t\t\tltree_lchild_ltree_idx = ltree_lchild.children[0]\n\t\t\tltree_lchild_ltree = tm.get_tree(ltree_lchild_ltree_idx)\n\t\t\tltree_lchild_rtree_idx = ltree_lchild.children[1]\n\t\t\tltree_lchild_rtree = tm.get_tree(ltree_lchild_rtree_idx)\n\t\t\trtree_lchild_ltree_idx = rtree_lchild.children[0]\n\t\t\trtree_lchild_ltree = tm.get_tree(rtree_lchild_ltree_idx)\n\t\t\trtree_lchild_rtree_idx = rtree_lchild.children[1]\n\t\t\trtree_lchild_rtree = tm.get_tree(rtree_lchild_rtree_idx)\n\t\t\tif ltree_lchild_rtree.is_const and rtree_lchild_rtree.is_const \\\n\t\t\tand (not ltree_lchild_ltree.is_const) and tm.equal_tree(ltree_lchild_ltree, rtree_lchild_ltree):\n\t\t\t\tres = tm.clone()\n\t\t\t\tres.trees[ltree_lchild_idx].root = '-'\n\t\t\t\tres.trees[rtree_lchild_idx].root = '-'\n\t\t\t\tres.update_edge(ltree_idx, ltree_lchild_idx, 1)\n\t\t\t\tres.update_edge(rtree_idx, rtree_lchild_idx, 1)\n\t\t\t\tres.update_edge(ltree_idx, ltree_lchild_rtree_idx, 0)\n\t\t\t\tres.update_edge(rtree_idx, rtree_lchild_rtree_idx, 0)\n\t\t\t\tres.update_edge(ltree_lchild_idx, ltree_rchild_idx, 0)\n\t\t\t\tres.update_edge(ltree_lchild_idx, ltree_lchild_ltree_idx, 1)\n\t\t\t\tres.update_edge(rtree_lchild_idx, rtree_rchild_idx, 0)\n\t\t\t\tres.update_edge(rtree_lchild_idx, rtree_lchild_ltree_idx, 1)\n\t\t\t\treturn res, [ltree_lchild_idx, rtree_lchild_idx]\n\n\t\tif ltree_lchild.is_const + ltree_rchild.is_const != 1:\n\t\t\treturn tm, []\n\t\tif ltree_lchild.is_const:\n\t\t\tvar = ltree_rchild\n\t\telse:\n\t\t\tvar = ltree_lchild\n\t\tif rtree_lchild.is_const + rtree_rchild.is_const != 1:\n\t\t\treturn tm, []\n\t\tr_max_value = None\n\t\tr_min_value = None\n\t\tl_max_value = None\n\t\tl_min_value = None\n\t\tif rtree_lchild.is_const:\n\t\t\tif not tm.equal_tree(var, rtree_rchild):\n\t\t\t\treturn tm, []\n\t\t\tif r_reverse:\n\t\t\t\tr_max_value = int(rtree_lchild.root)\n\t\t\telse:\n\t\t\t\tr_min_value = int(rtree_lchild.root)\n\t\tif rtree_rchild.is_const:\n\t\t\tif not tm.\n\n# ==========================================\n# File: src/models/rewriter/vrpRewriter.py\n# Function/Context: vrpRewriter.move\n# ==========================================\n# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nimport numpy as np\nimport operator\nimport random\nimport time\nimport copy\n\nfrom ..data_utils import data_utils\n\n\nclass vrpRewriter(object):\n    \"\"\"\n    Rewriter for vehicle routing.\n    \"\"\"\n    def move(self, dm, cur_route_idx, neighbor_route_idx):\n        min_update_idx = min(cur_route_idx, neighbor_route_idx)\n        res = dm.clone()\n        old_vehicle_state = res.vehicle_state[:]\n        old_vehicle_state[cur_route_idx], old_vehicle_state[neighbor_route_idx] = old_vehicle_state[neighbor_route_idx], old_vehicle_state[cur_route_idx]\n        if old_vehicle_state[neighbor_route_idx][0] == 0:\n            del old_vehicle_state[neighbor_route_idx]\n        res.vehicle_state = res.vehicle_state[:min_update_idx]\n        res.route = res.route[:min_update_idx]\n        res.tot_dis = res.tot_dis[:min_update_idx]\n        cur_node_idx, cur_capacity = res.vehicle_state[-1]\n        for t in range(min_update_idx, len(old_vehicle_state)):\n            new_node_idx, new_capacity = old_vehicle_state[t]\n            new_node = res.get_node(new_node_idx)\n            if new_node_idx != 0 and cur_capacity < new_node.demand:\n                res.add_route_node(0)\n            res.add_route_node(new_node_idx)\n            cur_capacity = res.vehicle_state[-1][1]\n        return res\n\n# ==========================================\n# File: src/models/vrpModel.py\n# Function/Context: vrpModel.forward\n# ==========================================\nimport numpy as np\nimport operator\nimport random\nimport time\nfrom multiprocessing.pool import ThreadPool\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch import cuda\nfrom torch.autograd import Variable\nfrom torch.nn.utils import clip_grad_norm\nimport torch.nn.functional as F\nfrom torch.distributions.categorical import Categorical\n\nfrom .data_utils import data_utils\nfrom .modules import vrpInputEncoder, mlp\nfrom .rewriter import vrpRewriter\nfrom .BaseModel import BaseModel\n\neps = 1e-3\nlog_eps = np.log(eps)\n\n\nclass vrpModel(BaseModel):\n\t\"\"\"\n\tModel architecture for vehicle routing.\n\t\"\"\"\n\tdef __init__(self, args):\n\t\tsuper(vrpModel, self).__init__(args)\n\t\tself.input_format = args.input_format\n\t\tself.embedding_size = args.embedding_size\n\t\tself.attention_size = args.attention_size\n\t\tself.sqrt_attention_size = int(np.sqrt(self.attention_size))\n\t\tself.reward_thres = -0.01\n\t\tself.input_encoder = vrpInputEncoder.SeqLSTM(args)\n\t\tself.policy_embedding = mlp.MLPModel(self.num_MLP_layers, self.LSTM_hidden_size * 6 + self.embedding_size * 2, self.MLP_hidden_size, self.attention_size, self.cuda_flag, self.dropout_rate)\n\t\tself.policy = mlp.MLPModel(self.num_MLP_layers, self.LSTM_hidden_size * 4, self.MLP_hidden_size, self.attention_size, self.cuda_flag, self.dropout_rate)\n\t\tself.value_estimator = mlp.MLPModel(self.num_MLP_layers, self.LSTM_hidden_size * 4, self.MLP_hidden_size, 1, self.cuda_flag, self.dropout_rate)\n\t\tself.rewriter = vrpRewriter()\n\n\t\tif args.optimizer == 'adam':\n\t\t\tself.optimizer = optim.Adam(self.parameters(), lr=self.lr)\n\t\telif args.optimizer == 'sgd':\n\t\t\tself.optimizer = optim.SGD(self.parameters(), lr=self.lr)\n\t\telif args.optimizer == 'rmsprop':\n\t\t\tself.optimizer = optim.RMSprop(self.parameters(), lr=self.lr)\n\t\telse:\n\t\t\traise ValueError('optimizer undefined: ', args.optimizer)\n\n\n\tdef rewrite(self, dm, trace_rec, candidate_rewrite_pos, eval_flag, max_search_pos, reward_thres=None):\n\n\t\tcandidate_rewrite_pos.sort(reverse=True, key=operator.itemgetter(0))\n\t\tif not eval_flag:\n\t\t\tsample_exp_reward_tensor = []\n\t\t\tfor idx, (cur_pred_reward, cur_pred_reward_tensor, rewrite_pos) in enumerate(candidate_rewrite_pos):\n\t\t\t\tsample_exp_reward_tensor.append(cur_pred_reward_tensor)\n\t\t\tsample_exp_reward_tensor = torch.cat(sample_exp_reward_tensor, 0)\n\t\t\tsample_exp_reward_tensor = torch.exp(sample_exp_reward_tensor * 10)\n\t\t\tsample_exp_reward = sample_exp_reward_tensor.data.cpu()\n\n\t\tcandidate_dm = []\n\t\tcandidate_rewrite_rec = []\n\t\tcandidate_trace_rec = []\n\t\tcandidate_scores = []\n\n\t\tif not eval_flag:\n\t\t\tsample_rewrite_pos_dist = Categorical(sample_exp_reward_tensor)\n\t\t\tsample_rewrite_pos = sample_rewrite_pos_dist.sample(sample_shape=[len(candidate_rewrite_pos)])\n\t\t\t#sample_rewrite_pos = torch.multinomial(sample_exp_reward_tensor, len(candidate_rewrite_pos))\n\t\t\tsample_rewrite_pos = sample_rewrite_pos.data.cpu().numpy()\n\t\t\tindexes = np.unique(sample_rewrite_pos, return_index=True)[1]\n\t\t\tsample_rewrite_pos = [sample_rewrite_pos[i] for i in sorted(indexes)]\n\t\t\tsample_rewrite_pos = sample_rewrite_pos[:self.num_sample_rewrite_pos]\n\t\t\tsample_exp_reward = [sample_exp_reward[i] for i in sample_rewrite_pos]\n\t\t\tsample_rewrite_pos = [candidate_rewrite_pos[i] for i in sample_rewrite_pos]\n\t\telse:\n\t\t\tsample_rewrite_pos = candidate_rewrite_pos.copy()\n\n\t\tfor idx, (pred_reward, cur_pred_reward_tensor, rewrite_pos) in enumerate(sample_rewrite_pos):\n\t\t\tif len(candidate_dm) > 0 and idx >= max_search_pos:\n\t\t\t\tbreak\n\t\t\tif reward_thres is not None and pred_reward < reward_thres:\n\t\t\t\tif eval_flag:\n\t\t\t\t\tbreak\n\t\t\t\telif np.random.random() > self.cont_prob:\n\t\t\t\t\tcontinue\n\t\t\tcandidate_neighbor_idxes = dm.get_neighbor_idxes(rewrite_pos)\n\t\t\tcur_node_idx = dm.vehicle_state[rewrite_pos][0]\n\t\t\tcur_node = dm.get_node(cur_node_idx)\n\t\t\tpre_node_idx = dm.vehicle_state[rewrite_pos - 1][0]\n\t\t\tpre_node = dm.get_node(pre_node_idx)\n\t\t\tpre_capacity = dm.vehicle_state[rewrite_pos - 1][1]\n\t\t\tdepot = dm.get_node(0)\n\t\t\tdepot_state = dm.encoder_outputs[0].unsqueeze(0)\n\t\t\tcur_state = dm.encoder_outputs[rewrite_pos].unsqueeze(0)\n\t\t\tcur_states_0 = []\n\t\t\tcur_states_1 = []\n\t\t\tcur_states_2 = []\n\t\t\tnew_embeddings_0 = []\n\t\t\tnew_embeddings_1 = []\n\t\t\tfor i in candidate_neighbor_idxes:\n\t\t\t\tneighbor_idx = dm.vehicle_state[i][0]\n\t\t\t\tneighbor_node = dm.get_node(neighbor_idx)\n\t\t\t\tcur_states_0.append(depot_state.clone())\n\t\t\t\tcur_states_1.append(cur_state.clone())\n\t\t\t\tcur_states_2.append(dm.encoder_outputs[i].unsqueeze(0))\n\t\t\t\tif pre_capacity >= neighbor_node.demand:\n\t\t\t\t\tnew_embedding = [neighbor_node.x, neighbor_node.y, neighbor_node.demand * 1.0 / dm.capacity, pre_node.x, pre_node.y, neighbor_node.demand * 1.0 / pre_capacity, dm.get_dis(pre_node, neighbor_node)]\n\t\t\t\telse:\n\t\t\t\t\tnew_embedding = [neighbor_node.x, neighbor_node.y, neighbor_node.demand * 1.0 / dm.capacity, pre_node.x, pre_node.y, neighbor_node.demand * 1.0 / dm.capacity, dm.get_dis(pre_node, depot) + dm.get_dis(depot, neighbor_node)]\n\t\t\t\tnew_embeddings_0.append(new_embedding[:])\n\t\t\t\tif pre_capacity >= neighbor_node.demand:\n\t\t\t\t\tnew_embedding = [(neighbor_node.x - depot.x) * (pre_node.x - depot.x), (neighbor_node.y - depot.y) * (pre_node.y - depot.y), (neighbor_node.demand - cur_node.demand) * 1.0 / pre_capacity, pre_node.px, pre_node.py, \\\n\t\t\t\t\t(neighbor_node.demand - cur_node.demand) * 1.0 / dm.capacity, dm.get_dis(pre_node, depot) + dm.get_dis(depot, neighbor_node)]\n\t\t\t\telse:\n\t\t\t\t\tnew_embedding = [(neighbor_node.x - depot.x) * (pre_node.x - depot.x), (neighbor_node.y - depot.y) * (pre_node.y - depot.y), (neighbor_node.demand - cur_node.demand) * 1.0 / dm.capacity, pre_node.px, pre_node.py, \\\n\t\t\t\t\t(neighbor_node.demand - cur_node.demand) * 1.0 / dm.capacity, dm.get_dis(pre_node, depot) + dm.get_dis(depot, neighbor_node)]\t\t\t\t\n\t\t\t\tnew_embeddings_1.append(new_embedding[:])\n\t\t\tcur_states_0 = torch.cat(cur_states_0, 0)\n\t\t\tcur_states_1 = torch.cat(cur_states_1, 0)\n\t\t\tcur_states_2 = torch.cat(cur_states_2, 0)\n\t\t\tnew_embeddings_0 = data_utils.np_to_tensor(new_embeddings_0, 'float', self.cuda_flag)\n\t\t\tnew_embeddings_1 = data_utils.np_to_tensor(new_embeddings_1, 'float', self.cuda_flag)\n\t\t\tpolicy_inputs = torch.cat([cur_states_0, cur_states_1, cur_states_2, new_embeddings_0, new_embeddings_1], 1)\n\t\t\tctx_embeddings = self.policy_embedding(policy_inputs)\n\t\t\tcur_state_key = self.policy(torch.cat([cur_state, depot_state], dim=1))\n\t\t\tac_logits = torch.matmul(cur_state_key, torch.transpose(ctx_embeddings, 0, 1)) / self.sqrt_attention_size\n\t\t\tac_logprobs = nn.LogSoftmax()(ac_logits)\n\t\t\tac_probs = nn.Softmax()(ac_logits)\n\t\t\tac_logits = ac_logits.squeeze(0)\n\t\t\tac_logprobs = ac_logprobs.squeeze(0)\n\t\t\tac_probs = ac_probs.squeeze(0)\n\t\t\tif eval_flag:\n\t\t\t\t_, candidate_acs = torch.sort(ac_logprobs, descending=True)\n\t\t\t\tcandidate_acs = candidate_acs.data.cpu().numpy()\n\t\t\telse:\n\t\t\t\tcandidate_acs_dist = Categorical(ac_probs)\n\t\t\t\tcandidate_acs = candidate_acs_dist.sample(sample_shape=[ac_probs.size()[0]])\n\t\t\t\t#candidate_acs = torch.multinomial(ac_probs, ac_probs.size()[0])\n\t\t\t\tcandidate_acs = candidate_acs.data.cpu().numpy()\n\t\t\t\tindexes = np.unique(candidate_acs, return_index=True)[1]\n\t\t\t\tcandidate_acs = [candidate_acs[i] for i in sorted(indexes)]\n\t\n\t\t\tfor i in candidate_acs:\n\t\t\t\tneighbor_idx = candidate_neighbor_idxes[i]\n\t\t\t\tnew_dm = self.rewriter.move(dm, rewrite_pos, neighbor_idx)\n\t\t\t\tif new_dm.tot_dis[-1] in trace_rec:\n\t\t\t\t\tcontinue\n\t\t\t\tcandidate_dm.append(new_dm)\n\t\t\t\tcandidate_rewrite_rec.append((ac_logprobs, pred_reward, cur_pred_reward_tensor, rewrite_pos, i, new_dm.tot_dis[-1]))\n\t\t\t\tif len(candidate_dm) >= max_search_pos:\n\t\t\t\t\tbreak\n\n\t\treturn candidate_dm, candidate_rewrite_rec\n\n\n\tdef batch_rewrite(self, dm, trace_rec, candidate_rewrite_pos, eval_flag, max_search_pos, reward_thres):\n\t\tcandidate_dm = []\n\t\tcandidate_rewrite_rec = []\n\t\tfor i in range(len(dm)):\n\t\t\tcur_candidate_dm, cur_candidate_rewrite_rec = self.rewrite(dm[i], trace_rec[i], candidate_rewrite_pos[i], eval_flag, max_search_pos, reward_thres)\n\t\t\tcandidate_dm.append(cur_candidate_dm)\n\t\t\tcandidate_rewrite_rec.append(cur_candidate_rewrite_rec)\n\t\treturn candidate_dm, candidate_rewrite_rec\n\n\n\tdef forward(self, batch_data, eval_flag=False):\n\t\ttorch.set_grad_enabled(not eval_flag)\n\t\tdm_list = []\n\t\tbatch_size = len(batch_data)\n\t\tfor dm in batch_data:\n\t\t\tdm_list.append(dm)\n\t\tdm_list = self.input_encoder.calc_embedding(dm_list, eval_flag)\n\n\t\tactive = True\n\t\treduce_steps = 0\n\n\t\ttrace_rec = [{} for _ in range(batch_size)]\n\t\trewrite_rec = [[] for _ in range(batch_size)]\n\t\tdm_rec = [[] for _ in range(batch_size)]\n\n\t\tfor idx in range(batch_size):\n\t\t\tdm_rec[idx].append(dm_list[idx])\n\t\t\ttrace_rec[idx][dm_list[idx].tot_dis[-1]] = 0\n\n\t\twhile active and (self.max_reduce_steps is None or reduce_steps < self.max_reduce_steps):\n\t\t\tactive = False\n\t\t\treduce_steps += 1\n\t\t\tnode_idxes = []\n\t\t\tnode_states = []\n\t\t\tdepot_states = []\n\t\t\tfor dm_idx in range(batch_size):\n\t\t\t\tdm = dm_list[dm_idx]\n\t\t\t\tfor i in range(1, len(dm.vehicle_state) - 1):\n\t\t\t\t\tcur_node_idx = dm.vehicle_state[i][0]\n\t\t\t\t\tcur_node = dm.get_node(cur_node_idx)\n\t\t\t\t\tnode_idxes.append((dm_idx, i))\n\t\t\t\t\tnode_states.append(dm.encoder_outputs[i].unsqueeze(0))\n\t\t\t\t\tdepot_states.append(dm.encoder_outputs[0].clone().unsqueeze(0))\n\t\t\tpred_rewards = []\n\t\t\tfor st in range(0, len(node_idxes), self.batch_size):\n\t\t\t\tcur_node_states = node_states[st: st + self.batch_size]\n\t\t\t\tcur_node_states = torch.cat(cur_node_states, 0)\n\t\t\t\tcur_depot_states = depot_states[st: st + self.batch_size]\n\t\t\t\tcur_depot_states = torch.cat(cur_depot_states, 0)\n\t\t\t\tcur_pred_rewards = self.value_estimator(torch.cat([cur_node_states, cur_depot_states], dim=1))\n\t\t\t\tpred_rewards.append(cur_pred_rewards)\n\t\t\tpred_rewards = torch.cat(pred_rewards, 0)\n\t\t\tcandidate_rewrite_pos = [[] for _ in range(batch_size)]\n\t\t\tfor idx, (dm_idx, node_idx) in enumerate(node_idxes):\n\t\t\t\tcandidate_rewrite_pos[dm_idx].append((pred_rewards[idx].data[0], pred_rewards[idx], node_idx))\n\n\t\t\tcandidate_dm, candidate_rewrite_rec = self.batch_rewrite(dm_list, trace_rec, candidate_rewrite_pos, eval_flag, max_search_pos=1, reward_thres=self.reward_thres)\n\t\t\tfor dm_idx in range(batch_size):\n\t\t\t\tcur_candidate_dm = candidate_dm[dm_idx]\n\t\t\t\tcur_candidate_rewrite_rec = candidate_rewrite_rec[dm_idx]\n\t\t\t\tif len(cur_candidate_dm) > 0:\n\t\t\t\t\tactive = True\n\t\t\t\t\tcur_dm = cur_candidate_dm[0]\n\t\t\t\t\tcur_rewrite_rec = cur_candidate_rewrite_rec[0]\n\t\t\t\t\tdm_list[dm_idx] = cur_dm\n\t\t\t\t\trewrite_rec[dm_idx].append(cur_rewrite_rec)\n\t\t\t\t\ttrace_rec[dm_idx][cur_dm.tot_dis[-1]] = 0\n\t\t\tif not active:\n\t\t\t\tbreak\n\n\t\t\tupdated_dm = self.input_encoder.calc_embedding(dm_list, eval_flag)\n\t\t\tfor i in range(batch_size):\n\t\t\t\tif updated_dm[i].tot_dis[-1] != dm_rec[i][-1].tot_dis[-1]:\n\t\t\t\t\tdm_rec[i].append(updated_dm[i])\n\n\t\ttotal_policy_loss = data_utils.np_to_tensor(np.zeros(1), 'float', self.cuda_flag)\n\t\ttotal_value_loss = data_utils.np_to_tensor(np.zeros(1), 'float', self.cuda_flag)\n\n\t\tpred_value_rec = []\n\t\tvalue_target_rec = []\n\t\ttotal_reward = 0\n\t\ttotal_rewrite_steps = 0\n\t\tfor dm_idx, cur_dm_rec in enumerate(dm_rec):\n\t\t\tpred_dis = []\n\t\t\tfor dm in cur_dm_rec:\n\t\t\t\tpred_dis.append(dm.tot_dis[-1])\n\t\t\tbest_reward = pred_dis[0]\n\t\t\t\n\t\t\tfor idx, (ac_logprob, pred_reward, cur_pred_reward_tensor, rewrite_pos, applied_op, new_dis) in enumerate(rewrite_rec[dm_idx]):\n\t\t\t\tcur_reward = pred_dis[idx] - pred_dis[idx + 1]\n\t\t\t\tbest_reward = min(best_reward, pred_dis[idx + 1])\n\n\t\t\t\tif self.gamma > 0.0:\n\t\t\t\t\tdecay_coef = 1.0\n\t\t\t\t\tnum_rollout_steps = len(cur_dm_rec) - idx - 1\n\t\t\t\t\tfor i in range(idx + 1, idx + 1 + num_rollout_steps):\n\t\t\t\t\t\tcur_reward = max(decay_coef * (pred_dis[idx] - pred_dis[i]), cur_reward)\n\t\t\t\t\t\tdecay_coef *= self.gamma\n\n\t\t\t\tcur_reward_tensor = data_utils.np_to_tensor(np.array([cur_reward], dtype=np.float32), 'float', self.cuda_flag, volatile_flag=True)\n\t\t\t\tif ac_logprob.data.cpu().numpy()[0] > log_eps or cur_reward - pred_reward > 0:\n\t\t\t\t\tac_mask = np.zeros(ac_logprob.size()[0])\n\t\t\t\t\tac_mask[applied_op] = cur_reward - pred_reward\n\t\t\t\t\tac_mask = data_utils.np_to_tensor(ac_mask, 'float', self.cuda_flag, eval_flag)\n\t\t\t\t\ttotal_policy_loss -= ac_logprob[applied_op] * ac_mask[applied_op]\n\t\t\t\tpred_value_rec.append(cur_pred_reward_tensor)\n\t\t\t\tvalue_target_rec.append(cur_reward_tensor)\n\t\t\t\n\t\t\ttotal_reward += best_reward\n\n\t\tif len(pred_value_rec) > 0:\n\t\t\tpred_value_rec = torch.cat(pred_value_rec, 0)\n\t\t\tvalue_target_rec = torch.cat(value_target_rec, 0)\n\t\t\tpred_value_rec = pred_value_rec.unsqueeze(1)\n\t\t\tvalue_target_rec = value_target_rec.unsqueeze(1)\n\t\t\ttotal_value_loss = F.smooth_l1_loss(pred_value_rec, value_target_rec, size_average=False)\n\t\ttotal_policy_loss /= batch_size\n\t\ttotal_value_loss /= batch_size\n\t\ttotal_loss = total_policy_loss * self.value_loss_coef + total_value_loss\n\t\ttotal_reward = total_reward * 1.0 / batch_size\n\n\t\treturn total_loss, total_reward, dm_rec",
  "description": "Combined Analysis:\n- [src/models/HalideModel.py]: This file implements the core NeuRewriter algorithm for expression simplification, matching the paper's description of iterative local rewriting. The HalideModel class contains: 1) A TreeLSTM encoder for expression trees, 2) An MLP policy network for selecting rewrite actions, 3) An MLP value estimator for reward prediction, 4) The rewrite() method that implements the rule application logic with sampling during training and greedy selection during evaluation, 5) The forward() method that performs the iterative rewriting process by repeatedly selecting subexpressions (regions) and applying rewrite rules. The implementation uses reinforcement learning (actor-critic) with policy and value networks, exactly as described in the paper's algorithm steps for expression simplification.\n- [src/models/jspModel.py]: This file implements the core NeuRewriter algorithm for job scheduling (JSP) as described in the paper. The jspModel class contains the neural network architecture (input encoder, policy network, value estimator) and the iterative rewriting logic. The forward method executes the main algorithm: (1) encodes the initial job schedule state, (2) iteratively selects rewrite positions using a value estimator (region-picking), (3) applies local rewriting actions via a policy network (rule-picking), and (4) computes reinforcement learning losses (policy + value) with reward shaping based on slowdown reduction. The rewrite method implements the detailed action selection and state transition logic. This directly corresponds to the paper's NeuRewriter framework applied to job scheduling optimization.\n- [src/models/rewriter/HalideRewriter.py]: This file implements the core rewriting logic for expression simplification in the Halide domain. The HalideRewriter class contains 19 specific rewriting rules (not_symbol_rewrite, simple_bool_rewrite, etc.) that correspond to the local rewriting operations described in the paper. These rules implement the mathematical transformations needed to simplify expressions while preserving semantics. The class maintains rewrite sequences and provides methods to apply individual rules to expression trees. This directly implements the 'local rewriting operations' component of the NeuRewriter algorithm for expression simplification.\n- [src/models/rewriter/vrpRewriter.py]: This file implements a local rewriting operation for Vehicle Routing Problem (VRP) as described in the NeuRewriter paper. The 'move' method performs a route-swapping operation between two routes (cur_route_idx and neighbor_route_idx) while maintaining feasibility constraints. It handles capacity constraints by checking if current capacity (cur_capacity) is less than the demand of the next node (new_node.demand) and inserting depot (node 0) when needed. This corresponds to the paper's approach of learning to select which region (routes) to modify and which rewriting rule (this specific move operation) to apply for iterative solution improvement.\n- [src/models/vrpModel.py]: This file implements the core NeuRewriter algorithm for the Vehicle Routing Problem (VRP) as described in the paper. The vrpModel class contains the neural network architecture (LSTM encoder, MLP policy/value networks) and the iterative rewriting logic. The forward() method implements the main algorithm: starting from initial solutions, it repeatedly selects rewrite positions (region-picking via value estimator), applies local moves (rule-picking via policy network), and updates the solution until convergence. The rewrite() method handles individual rewriting operations, including neighbor selection and solution modification. The implementation uses reinforcement learning (actor-critic) with policy gradients and value estimation, directly corresponding to the NeuRewriter approach of learning local rewriting policies for combinatorial optimization.",
  "dependencies": [
    "calc",
    "torch.cuda",
    "copy",
    "multiprocessing.pool.ThreadPool",
    "torch.autograd.Variable",
    "modules.mlp",
    "torch.optim",
    "data_utils",
    "torch.distributions.categorical.Categorical",
    "time",
    "rewriter.jspRewriter",
    "HalideRewriter",
    "dm.clone()",
    "torch.nn.functional",
    "data_utils.data_utils",
    "operator",
    "torch.nn.utils",
    "res.add_route_node()",
    "rewriter.vrpRewriter",
    "torch.distributions.categorical",
    "modules.vrpInputEncoder",
    "BaseModel.BaseModel",
    "mlp",
    "multrocessing.pool.ThreadPool",
    "random",
    "res.get_node()",
    "numpy",
    "utils",
    "modules.jspInputEncoder",
    "torch.nn.utils.clip_grad_norm",
    "multiprocessing.pool",
    "torch.nn",
    "torch.autograd",
    "BaseModel",
    "HalideInputEncoder",
    "torch",
    "Tree"
  ]
}