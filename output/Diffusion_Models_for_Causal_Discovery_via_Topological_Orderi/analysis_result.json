{
  "paper_id": "Diffusion_Models_for_Causal_Discovery_via_Topological_Orderi",
  "title": "DIFFUSION MODELS FOR CAUSAL DISCOVERY VIA TOPOLOGICAL ORDERING",
  "abstract": "Discovering causal relations from observational data becomes possible with additional assumptions such as considering the functional relations to be constrained as nonlinear with additive noise (ANM). Even with strong assumptions, causal discovery involves an expensive search problem over the space of directed acyclic graphs (DAGs). Topological ordering approaches reduce the optimisation space of causal discovery by searching over a permutation rather than graph space. For ANMs, the Hessian of the data log-likelihood can be used for finding leaf nodes in a causal graph, allowing its topological ordering. However, existing computational methods for obtaining the Hessian still do not scale as the number of variables and the number of samples are increased. Therefore, inspired by recent innovations in diffusion probabilistic models (DPMs), we propose DiffAN, a topological ordering algorithm that leverages DPMs for learning a Hessian function. We introduce theory for updating the learned Hessian without re-training the neural network, and we show that computing with a subset of samples gives a more accurate approximation of the ordering, which allows scaling to datasets with more samples and variables. We show empirically that our method scales exceptionally well to datasets with up to 500 nodes and up to 10^5 samples while still performing on par over small datasets with state-of-the-art causal discovery methods.",
  "problem_description_natural": "The paper addresses the problem of causal discovery from observational data under the assumption of a nonlinear additive noise model (ANM). The goal is to recover the underlying directed acyclic graph (DAG) that represents causal relationships among variables. This is framed as a combinatorial optimization problem over the space of DAGs, which is NP-hard. To make the problem tractable, the authors reformulate it as a topological ordering task—finding a permutation of variables consistent with the DAG’s directionality—by iteratively identifying and removing leaf nodes using properties of the Hessian of the data log-likelihood. The key challenge is efficiently and scalably estimating this Hessian for high-dimensional data with many samples, which prior kernel-based methods cannot handle. The proposed solution uses diffusion probabilistic models to learn the score function (gradient of log-density) and its Jacobian (Hessian of log-density) via neural networks, enabling scalable leaf identification and topological sorting without retraining at each iteration.",
  "problem_type": "combinatorial optimization",
  "datasets": [
    "Sachs",
    "SynTReN",
    "Generated ER Graphs",
    "Generated SF Graphs"
  ],
  "performance_metrics": [
    "Structural Hamming Distance (SHD)",
    "Structural Intervention Distance (SID)",
    "Order Divergence",
    "Run time in seconds"
  ],
  "lp_model": {
    "objective": "Not explicitly given. The algorithm greedily minimizes the variance of the diagonal of the Hessian at each step: $\\text{leaf} = \\arg\\min_{x_i \\in \\mathbf{x}} \\text{Var}_{\\mathcal{B}} \\left[ \\nabla_{\\mathbf{x}} \\left( \\text{score}(\\mathbf{M}_\\pi \\odot \\mathcal{B}, t) \\right) \\right]$ (Equation 9).",
    "constraints": [
      "$\\pi$ is a permutation of $\\{1,\\dots,d\\}$",
      "$\\pi$ is a topological order of the true DAG $\\mathcal{G}$: $\\pi_i < \\pi_j \\iff j \\in De_{\\mathcal{G}}(x_i)$",
      "Under the ANM, for a leaf node $l$, $\\text{Var}_{\\mathbf{X}} \\left[ \\mathbf{H}_{l,l} (\\log p(\\mathbf{x})) \\right] = 0$ (Lemma 1, Equation 2)"
    ],
    "variables": [
      "$\\pi$: topological order (permutation of nodes)",
      "$\\mathbf{A} \\in \\{0,1\\}^{d \\times d}$: adjacency matrix of the DAG"
    ]
  },
  "raw_latex_model": "The paper does not provide a single optimization model. The key condition for leaf identification is from Lemma 1: $$\\text{Var}_{\\mathbf{X}} \\left[ \\mathbf{H}_{l,l} (\\log p(\\mathbf{x})) \\right] = 0$$ for a leaf node $l$. The topological order $\\pi$ is defined by $$\\pi_i < \\pi_j \\iff j \\in De_{\\mathcal{G}}(x_i).$$",
  "algorithm_description": "DiffAN uses a diffusion probabilistic model (DPM) to approximate the score (gradient of log-likelihood) and its Jacobian (Hessian) via backpropagation. It iteratively finds leaf nodes by selecting the node with the smallest variance of the diagonal of the Hessian (computed on a subsample). After finding a leaf, it removes the node and updates the score using the deciduous score (Theorem 1) without retraining the neural network. This process repeats until all nodes are ordered. Finally, a pruning step (e.g., feature selection) is applied to obtain the DAG adjacency matrix."
}