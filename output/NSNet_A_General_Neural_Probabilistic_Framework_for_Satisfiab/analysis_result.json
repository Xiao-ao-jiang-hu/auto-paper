{
  "paper_id": "NSNet_A_General_Neural_Probabilistic_Framework_for_Satisfiab",
  "title": "NSNet: A General Neural Probabilistic Framework for Satisfiability Problems",
  "abstract": "We present the Neural Satisfiability Network (NSNet), a general neural framework that models satisfiability problems as probabilistic inference and meanwhile exhibits proper explainability. Inspired by the Belief Propagation (BP), NSNet uses a novel graph neural network (GNN) to parameterize BP in the latent space, where its hidden representations maintain the same probabilistic interpretation as BP. NSNet can be flexibly configured to solve both SAT and #SAT problems by applying different learning objectives. For SAT, instead of directly predicting a satisfying assignment, NSNet performs marginal inference among all satisfying solutions, which we empirically find is more feasible for neural networks to learn. With the estimated marginals, a satisfying assignment can be efficiently generated by rounding and executing a stochastic local search. For #SAT, NSNet performs approximate model counting by learning the Bethe approximation of the partition function. Our evaluations show that NSNet achieves competitive results in terms of inference accuracy and time efficiency on multiple SAT and #SAT datasets.",
  "problem_description_natural": "The paper addresses the Boolean Satisfiability Problem (SAT) and the Sharp Satisfiability Problem (#SAT or model counting). SAT asks whether there exists an assignment of Boolean variables that satisfies a given propositional formula in conjunctive normal form (CNF), while #SAT aims to count the total number of such satisfying assignments. The authors frame both problems as instances of probabilistic inference over factor graphs derived from CNF formulas, where satisfying assignments correspond to configurations with non-zero probability under a uniform distribution over the solution space. NSNet leverages a neural extension of Belief Propagation to estimate variable marginals (for SAT) and the partition function (for #SAT) in a data-driven manner.",
  "problem_type": "combinatorial optimization",
  "datasets": [
    "SR",
    "3-SAT",
    "Community Attachment (CA)",
    "BIRD",
    "SATLIB",
    "RND3SAT",
    "BMS",
    "CBS",
    "GCP",
    "SW-GCP"
  ],
  "performance_metrics": [
    "Solving accuracy (%)",
    "Root mean square error (RMSE)",
    "Runtime (s)",
    "Average number of flips"
  ],
  "lp_model": {
    "objective": "$\\text{find } \\boldsymbol{x} \\in \\{0,1\\}^n \\text{ such that } \\Phi(\\boldsymbol{x}) = 1$",
    "constraints": [
      "$\\sum_{j \\in P_a} x_j + \\sum_{j \\in N_a} (1 - x_j) \\geq 1 \\quad \\forall a = 1,\\dots,m$"
    ],
    "variables": [
      "$x_i \\in \\{0,1\\}$ for $i=1,\\dots,n$: binary assignment variable for variable $X_i$"
    ]
  },
  "raw_latex_model": "$$\\begin{aligned} \\text{Given a CNF formula } \\Phi \\text{ with } n \\text{ variables } X_1,\\dots,X_n \\text{ and } m \\text{ clauses } C_1,\\dots,C_m. \\\\ \\text{Let } P_a \\text{ be the set of variables appearing positively in clause } C_a, \\text{ and } N_a \\text{ be the set appearing negatively.} \\\\ \\text{The SAT problem is to find an assignment } \\boldsymbol{x} = (x_1,\\dots,x_n) \\in \\{0,1\\}^n \\text{ such that:} \\\\ \\sum_{j \\in P_a} x_j + \\sum_{j \\in N_a} (1 - x_j) \\geq 1 \\quad \\forall a = 1,\\dots,m. \\end{aligned}$$",
  "algorithm_description": "NSNet is a neural framework that uses a graph neural network to parameterize Belief Propagation in the latent space. For SAT, it performs marginal inference to estimate variable assignment distributions among all satisfying solutions, and then uses rounding and stochastic local search to generate a satisfying assignment. For #SAT, it learns the Bethe approximation of the partition function to estimate the number of satisfying assignments."
}