{
  "file_path": "model/Siamese/layers.py, model/Siamese/model.py, model/Siamese/model_regression.py",
  "function_name": "GraphConvolutionAttention, Attention, Dot, SLM, Model, SiameseRegressionModel",
  "code_snippet": "\n\n# ==========================================\n# File: model/Siamese/layers.py\n# Function/Context: GraphConvolutionAttention, Attention, Dot, SLM\n# ==========================================\nimport tensorflow as tf\nfrom config import FLAGS\nfrom utils_siamese import dot\nfrom inits import *\n\nclass GraphConvolutionAttention(GraphConvolution):\n    \"\"\" Graph convolution with attention layer. \"\"\"\n\n    def __init__(self, input_dim, output_dim, dropout, sparse_inputs, act,\n                 bias, featureless, num_supports, **kwargs):\n        super(GraphConvolutionAttention, self).__init__(input_dim, output_dim,\n                                                        dropout, sparse_inputs, act, bias, featureless, num_supports,\n                                                        **kwargs)\n\n    def _call_one_graph(self, input):\n        x = super(GraphConvolutionAttention, self)._call_one_graph(input)\n        L = tf.sparse_tensor_dense_matmul(self.laplacians[0], tf.eye(tf.shape(x)[0]))\n        degree_att = -tf.log(tf.reshape(tf.diag_part(L), [-1, 1]))\n        output = tf.multiply(x, degree_att)\n        return output\n\nclass Attention(Average):\n    \"\"\" Attention layer.\"\"\"\n\n    def __init__(self, input_dim, att_times, att_num, att_style, att_weight, **kwargs):\n        super(Attention, self).__init__(**kwargs)\n        self.emb_dim = input_dim  # same dimension D as input embeddings\n        self.att_times = att_times\n        self.att_num = att_num\n        self.att_style = att_style\n        self.att_weight = att_weight\n        assert (self.att_times >= 1)\n        assert (self.att_num >= 1)\n        assert (self.att_style == 'dot' or self.att_style == 'slm' or\n                'ntn_' in self.att_style)\n\n        with tf.variable_scope(self.name + '_vars'):\n            for i in range(self.att_num):\n                self.vars['W_' + str(i)] = \\\n                    glorot([self.emb_dim, self.emb_dim],\n                           name='W_' + str(i))\n                if self.att_style == 'slm':\n                    self.interact_dim = 1\n                    self.vars['NTN_V_' + str(i)] = \\\n                        glorot([self.interact_dim, 2 * self.emb_dim],\n                               name='NTN_V_' + str(i))\n                if 'ntn_' in self.att_style:\n                    self.interact_dim = int(self.att_style[4])\n                    self.vars['NTN_V_' + str(i)] = \\\n                        glorot([self.interact_dim, 2 * self.emb_dim],\n                               name='NTN_V_' + str(i))\n                    self.vars['NTN_W_' + str(i)] = \\\n                        glorot([self.interact_dim, self.emb_dim, self.emb_dim],\n                               name='NTN_W_' + str(i))\n                    self.vars['NTN_U_' + str(i)] = \\\n                        glorot([self.interact_dim, 1],\n                               name='NTN_U_' + str(i))\n                    self.vars['NTN_b_' + str(i)] = \\\n                        zeros([self.interact_dim],\n                              name='NTN_b_' + str(i))\n\n        self._log_vars()\n\n    def produce_node_atts(self):\n        return True\n\n    def _call_one_mat(self, inputs):\n        outputs = []\n        for i in range(self.att_num):\n            acts = [inputs]\n            assert (self.att_times >= 1)\n            output = None\n            for _ in range(self.att_times):\n                x = acts[-1]  # x is N*D\n                temp = tf.reshape(tf.reduce_mean(x, 0), [1, -1])  # (1, D)\n                h_avg = tf.tanh(tf.matmul(temp, self.vars['W_' + str(i)])) if \\\n                    self.att_weight else temp\n                self.att = self._gen_att(x, h_avg, i)\n                output = tf.matmul(tf.reshape(self.att, [1, -1]), x)  # (1, D)\n                x_new = tf.multiply(x, self.att)\n                acts.append(x_new)\n            outputs.append(output)\n        return tf.concat(outputs, 1)\n\n    def _gen_att(self, x, h_avg, i):\n        if self.att_style == 'dot':\n            return interact_two_sets_of_vectors(\n                x, h_avg, 1,  # interact only once\n                W=[tf.eye(self.emb_dim)],\n                act=tf.sigmoid)\n        elif self.att_style == 'slm':\n            # return tf.sigmoid(tf.matmul(concat, self.vars['a_' + str(i)]))\n            return interact_two_sets_of_vectors(\n                x, h_avg, self.interact_dim,\n                V=self.vars['NTN_V_' + str(i)],\n                act=tf.sigmoid)\n        else:\n            assert ('ntn_' in self.att_style)\n            return interact_two_sets_of_vectors(\n                x, h_avg, self.interact_dim,\n                V=self.vars['NTN_V_' + str(i)],\n                W=self.vars['NTN_W_' + str(i)],\n                b=self.vars['NTN_b_' + str(i)],\n                act=tf.sigmoid,\n                U=self.vars['NTN_U_' + str(i)])\n\nclass Dot(Merge):\n    \"\"\" Dot layer. \"\"\"\n\n    def __init__(self, output_dim, **kwargs):\n        super(Dot, self).__init__(**kwargs)\n        self.output_dim = output_dim\n        assert (output_dim == 1 or output_dim == 2)\n\n    def _call_one_pair(self, input):\n        x_1 = input[0]\n        x_2 = input[1]\n        assert (x_1.shape == x_2.shape)\n        assert (x_1.shape[0] == 1)\n        emb_dim = x_1.get_shape().as_list()[1]\n\n        # one pair comparison\n        sim_score = interact_two_sets_of_vectors(\n            x_1, x_2, 1,  # interact only once\n            W=[tf.eye(emb_dim)],\n            act=tf.sigmoid)  # use sigmoid to squash the score between (0, 1)\n\n        if self.output_dim == 2:\n            output = tf.concat([sim_score, 1 - sim_score], 1)\n            assert (output.shape == (1, 2))\n        else:\n            assert (self.output_dim == 1)\n            output = tf.reshape(sim_score, [1, 1])\n        return output\n\nclass SLM(Merge):\n    \"\"\" Single Layer model.\n    (Socher, Richard, et al.\n    \"Reasoning with neural tensor networks for knowledge base completion.\"\n    NIPS. 2013.). \"\"\"\n\n    def __init__(self, input_dim, output_dim, act, dropout, bias, **kwargs):\n        super(SLM, self).__init__(**kwargs)\n        self.output_dim = output_dim\n        self.act = act\n        self.bias = bias\n        with tf.variable_scope(self.name + '_vars'):\n            self.vars['V'] = glorot([2 * input_dim, output_dim], name='V')\n            if self.bias:\n                self.vars['b'] = zeros([output_dim], name='b')\n        self._log_vars()\n\n    def _call_one_pair(self, input):\n        x_1 = input[0]\n        x_2 = input[1]\n        concat = tf.concat([x_1, x_2], 1)\n        output = tf.matmul(concat, self.vars['V'])\n        if self.bias:\n            output += self.vars['b']\n        return self.act(output)\n\n# ==========================================\n# File: model/Siamese/model.py\n# Function/Context: Model\n# ==========================================\nfrom config import FLAGS\nfrom layers_factory import create_layers\nimport numpy as np\nimport tensorflow as tf\nfrom warnings import warn\n\n\nclass Model(object):\n    def __init__(self, **kwargs):\n        allowed_kwargs = {'name'}\n        for kwarg in kwargs.keys():\n            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n        name = kwargs.get('name')\n        if not name:\n            name = self.__class__.__name__.lower()\n        self.name = name\n\n        self.vars = {}\n        self.layers = []\n        self.train_loss = 0\n        self.val_test_loss = 0\n        self.optimizer = None\n        self.opt_op = None\n\n        self.batch_size = FLAGS.batch_size\n        self.weight_decay = FLAGS.weight_decay\n        self.optimizer = tf.train.AdamOptimizer(\n            learning_rate=FLAGS.learning_rate)\n\n        self._build()\n        print('Flow built')\n        # Build metrics\n        self._loss()\n        print('Loss built')\n        self.opt_op = self.optimizer.minimize(self.train_loss)\n        print('Optimizer built')\n\n    def _build(self):\n        # Create layers according to FLAGS.\n        self.layers = create_layers(self)\n        assert (len(self.layers) > 0)\n        print('Created {} layers: {}'.format(\n            len(self.layers), ', '.join(l.get_name() for l in self.layers)))\n\n        # Build the siamese model for train and val_test, respectively,\n        for tvt in ['train', 'val_test']:\n            print(tvt)\n            # Go through each layer except the last one.\n            acts = [self._get_ins(self.layers[0], tvt)]\n            outs = None\n            for k, layer in enumerate(self.layers):\n                ins = self._proc_ins(acts[-1], k, layer, tvt)\n                print(layer.name)\n                outs = layer(ins)\n                outs = self._proc_outs(outs, k, layer, tvt)\n                acts.append(outs)\n            if tvt == 'train':\n                self.train_outputs = outs\n                self.train_acts = acts\n            else:\n                self.val_test_output = outs\n                self.val_test_pred_score = self._val_test_pred_score()\n                self.val_test_acts = acts\n\n        self.node_embeddings = self._get_last_gcn_layer_outputs('val_test')\n\n        # Store model variables for easy access.\n        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n        self.vars = {var.name: var for var in variables}\n\n    def _loss(self):\n        self.train_loss = self._loss_helper('train')\n        self.val_test_loss = self._loss_helper('val')\n\n    def _loss_helper(self, tvt):\n        rtn = 0\n\n        # Weight decay loss.\n        wdl = 0\n        for layer in self.layers:\n            for var in layer.vars.values():\n                wdl = self.weight_decay * tf.nn.l2_loss(var)\n                rtn += wdl\n        if tvt == 'train':\n            tf.summary.scalar('weight_decay_loss', wdl)\n\n        loss, loss_label = self._task_loss(tvt)\n        rtn += loss\n        if tvt == 'train':\n            tf.summary.scalar(loss_label, loss)\n\n        if FLAGS.graph_loss == '1st':\n            node_emb_list = self._get_last_gcn_layer_outputs(tvt)\n            laplacian_list = self._get_laplacians_for_graph_loss(tvt)\n            gl = 0\n            for i, node_emb_mat in enumerate(node_emb_list):\n                # gli = 2 * tf.trace(\n                #     dot(tf.transpose(\n                #         dot(laplacian_list[i], node_emb_mat, sparse=True)),\n                #         node_emb_mat))\n                # gl += gli\n                mat = tf.matmul(node_emb_mat, tf.transpose(node_emb_mat))\n                gl += tf.sqrt(tf.reduce_sum(tf.square(tf.sparse_add(-mat, laplacian_list[i][0]))))\n            gl /= FLAGS.batch_size\n            gl *= FLAGS.graph_loss_alpha\n            rtn += gl\n            if tvt == 'train':\n                tf.summary.scalar('1st_order_graph_loss', gl)\n\n        if tvt == 'train':\n            tf.summary.scalar('total_loss', rtn)\n        return rtn\n\n    def pred_sim_without_act(self):\n        raise NotImplementedError()\n\n    def apply_final_act_np(self, score):\n        raise NotImplementedError()\n\n    def get_feed_dict_for_train(self, data):\n        raise NotImplementedError()\n\n    def get_feed_dict_for_val_test(self, g1, g2, true_sim):\n        raise NotImplementedError()\n\n    def get_true_sim(self, i, j, true_result):\n        raise NotImplementedError()\n\n    def get_eval_metrics_for_val(self):\n        raise NotImplementedError()\n\n    def get_eval_metrics_for_test(self):\n        raise NotImplementedError()\n\n    def _get_determining_result_for_val(self):\n        raise NotImplementedError()\n\n    def _val_need_max(self):\n        raise NotImplementedError()\n\n    def find_load_best_model(self, sess, saver, val_results_dict):\n        cur_max_metric = -float('inf')\n        cur_min_metric = float('inf')\n        cur_best_iter = 1\n        metric_list = []\n        early_thresh = int(FLAGS.iters * 0.1)\n        deter_r_name = self._get_determining_result_for_val()\n        for iter, val_results in val_results_dict.items():\n            metric = val_results[deter_r_name]\n            metric_list.append(metric)\n            if iter >= early_thresh:\n                if self._val_need_max():\n                    if metric >= cur_max_metric:\n                        cur_max_metric = metric\n                        cur_best_iter = iter\n                else:\n                    if metric <= cur_min_metric:\n                        cur_min_metric = metric\n                        cur_best_iter = iter\n        if self._val_need_max():\n            argfunc = np.argmax\n            takefunc = np.max\n            best_metric = cur_max_metric\n        else:\n            argfunc = np.argmin\n            takefunc = np.min\n            best_metric = cur_min_metric\n        global_best_iter = list(val_results_dict.items()) \\\n            [int(argfunc(metric_list))][0]\n        global_best_metirc = takefunc(metric_list)\n        if global_best_iter != cur_best_iter:\n            warn(\n                'The global best iter is {} with {}={:.5f},\\nbut the '\n                'best iter after first 10% iterations is {} with {}={:.5f}'.format(\n                    global_best_iter, deter_r_name, global_best_metirc,\n                    cur_best_iter, deter_r_name, best_metric))\n        lp = '{}/models/{}.ckpt'.format(saver.get_log_dir(), cur_best_iter)\n        self.load(sess, lp)\n        print('Loaded the best model at iter {} with {} {:.5f}'.format(\n            cur_best_iter, deter_r_name, best_metric))\n        return cur_best_iter\n        # return None\n\n    def _get_ins(self, layer, tvt):\n        raise NotImplementedError()\n\n    def _supply_laplacians_etc_to_ins(self, ins, tvt, gcn_id):\n        if not FLAGS.coarsening:\n            gcn_id = 0\n        for i, (laplacians, num_nonzero) in \\\n                enumerate(zip(\n                    self._get_plhdr('laplacians_1', tvt) +\n                    self._get_plhdr('laplacians_2', tvt),\n                    self._get_plhdr('num_nonzero_1', tvt) +\n                    self._get_plhdr('num_nonzero_2', tvt))):\n            ins[i] = [ins[i], laplacians[gcn_id], num_nonzero]\n        return ins\n\n    def _proc_ins_for_merging_layer(self, ins, tvt):\n        raise NotImplementedError()\n\n    def _val_test_pred_score(self):\n        raise NotImplementedError()\n\n    def _task_loss(self, tvt):\n        raise NotImplementedError()\n\n    def _proc_ins(self, ins, k, layer, tvt):\n        ln = layer.__class__.__name__\n        ins_mat = None\n        if k != 0 and tvt == 'train':\n            # sparse matrices (k == 0; the first layer) cannot be logged.\n            need_log = True\n        else:\n            need_log = False\n        if ln == 'GraphConvolution' or ln == 'GraphConvolutionAttention':\n            gcn_count = int(layer.name.split('_')[-1])\n            assert (gcn_count >= 1)  # 1-based\n            gcn_id = gcn_count - 1\n            ins = self._supply_laplacians_etc_to_ins(ins, tvt, gcn_id)\n            if need_log:\n                ins_mat = self._stack_concat([i[0] for i in ins])\n        else:\n            ins_mat = self._stack_concat(ins)\n            if layer.merge_graph_level_embs():\n                ins = self._proc_ins_for_merging_layer(ins, tvt)\n            if ln == 'Dense':\n                # Use matrix operations instead of iterating through list\n                # after the merging layer.\n                ins = ins_mat\n        if need_log:\n            self._log_mat(ins_mat, layer, 'ins')\n        return ins\n\n    def _proc_outs(self, outs, k, layer, tvt):\n        outs_mat = self._stack_concat(outs)\n        ln = layer.__class__.__name__\n        if tvt == 'train':\n            self._log_mat(outs_mat, layer, 'outs')\n        if tvt == 'val_test' and layer.produce_graph_level_emb() and \\\n                not FLAGS.coarsening:\n            if ln != 'ANPM' and ln != 'ANPMD' and ln != 'ANNH':\n                embs = outs\n            else:\n                embs = layer.embeddings\n            assert (len(embs) == 2)\n            # Note: some architecture may NOT produce\n            # any graph-level embeddings.\n            self.graph_embeddings = embs\n            s = embs[0].get_shape().as_list()\n            assert (s[0] == 1)\n            self.embed_dim = s[1]\n        if tvt == 'val_test' and layer.produce_node_atts():\n            if ln == 'Attention':\n                assert (len(outs) == 2)\n            self.attentions = layer.att\n            s = self.attentions.get_shape().as_list()\n            assert (s[1] == 1)\n        return outs\n\n    def _get_plhdr(self, key, tvt):\n        if tvt == 'train':\n            return self.__dict__[key]\n        else:\n            assert (tvt == 'test' or tvt == 'val' or tvt == 'val_test')\n            return self.__dict__['val_test_' + key]\n\n    def _get_last_gcn_layer_outputs(self, tvt):\n        acts = self.train_acts if tvt == 'train' else self.val_test_acts\n        assert (len(acts) == len(self.layers) + 1)\n        idx = None\n        for k, layer in enumerate(self.layers):\n            if 'GraphConvolution' not in layer.__class__.__name__:\n                idx = k\n                break\n        assert (idx)\n        return acts[idx]\n\n    def _get_laplacians_for_graph_loss(self, tvt):\n        rtn = []\n        for laplacians in (self._get_plhdr('laplacians_1', tvt) +\n                           self._get_plhdr('laplacians_2', tvt)):\n            assert (len(laplacians) == 1)\n            rtn.append(laplacians[0])\n        return rtn\n\n    def _stack_concat(self, x):\n        if type(x) is list:\n            list_of_tensors = x\n            assert (list_of_tensors)\n            s = list_of_tensors[0].get_shape()\n            if s != ():\n                return tf.concat(list_of_tensors, 0)\n            else:\n                return tf.stack(list_of_tensors)\n        else:\n            # assert(len(x.get_shape()) == 2) # should be a 2-D matrix\n            return x\n\n    def _log_mat(self, mat, layer, label):\n        tf.summary.histogram(layer.name + '/' + label, mat)\n\n    def save(self, sess, saver, iter):\n        logdir = saver.get_log_dir()\n        sp = '{}/models/{}.ckpt'.format(logdir, iter)\n        tf.train.Saver(self.vars).save(sess, sp)\n\n    def load(self, sess, load_path):\n        tf.train.Saver(self.vars).restore(sess, load_path)\n\n# ==========================================\n# File: model/Siamese/model_regression.py\n# Function/Context: SiameseRegressionModel\n# ==========================================\nfrom config import FLAGS\nfrom model import Model\nfrom samplers import SelfShuffleList\nfrom utils_siamese import get_coarsen_level, get_flags\nfrom similarity import create_sim_kernel\nfrom dist_calculator import get_gs_dist_mat\nimport numpy as np\nimport tensorflow as tf\nfrom fake_generator import graph_generator\nfrom data_siamese import ModelGraph\nfrom utils import save, load, get_save_path\n\n\nclass SiameseRegressionModel(Model):\n    def __init__(self, input_dim, data, dist_calculator):\n        self.input_dim = input_dim\n        print('original_input_dim', self.input_dim)\n        self.laplacians_1, self.laplacians_2, self.features_1, self.features_2, \\\n        self.num_nonzero_1, self.num_nonzero_2, self.dropout, \\\n        self.val_test_laplacians_1, self.val_test_laplacians_2, \\\n        self.val_test_features_1, self.val_test_features_2, \\\n        self.val_test_num_nonzero_1, self.val_test_num_nonzero_2 = \\\n            self._create_basic_placeholders(FLAGS.batch_size, FLAGS.batch_size,\n                                            level=get_coarsen_level())\n        self.train_y_true = tf.placeholder(\n            tf.float32, shape=(FLAGS.batch_size, 1))\n        self.val_test_y_true = tf.placeholder(\n            tf.float32, shape=(1, 1))\n        # Build the model.\n        super(SiameseRegressionModel, self).__init__()\n        self.sim_kernel = create_sim_kernel(\n            FLAGS.sim_kernel, get_flags('yeta'), get_flags('scale'))\n        self.train_triples = self._load_train_triples(data, dist_calculator)\n\n    def pred_sim_without_act(self):\n        return self.val_test_pred_score\n\n    def apply_final_act_np(self, score):\n        return score\n\n    def get_feed_dict_for_train(self, data):\n        rtn = {}\n        pairs = []\n        y_true = np.zeros((FLAGS.batch_size, 1))\n        for i in range(FLAGS.batch_size):\n            g1, g2, true_sim = self._sample_train_pair(data)\n            pairs.append((g1, g2))\n            y_true[i] = true_sim\n        rtn[self.train_y_true] = y_true\n        rtn[self.dropout] = FLAGS.dropout\n        return self._supply_laplacians_etc_to_feed_dict(rtn, pairs, 'train')\n\n    def get_feed_dict_for_val_test(self, g1, g2, true_sim):\n        rtn = {}\n        pairs = [(g1, g2)]\n        y_true = np.zeros((1, 1))\n        y_true[0] = true_sim\n        rtn[self.val_test_y_true] = y_true\n        return self._supply_laplacians_etc_to_feed_dict(rtn, pairs, 'val_test')\n\n    def get_true_sim(self, i, j, true_result):\n        assert (true_result.dist_or_sim() == 'dist')\n        _, d = true_result.dist_sim(i, j, FLAGS.dist_norm)\n        true_sim = self.sim_kernel.dist_to_sim_np(d)\n        return true_sim\n\n    def get_eval_metrics_for_val(self):\n        return ['loss', 'mse']\n\n    def get_eval_metrics_for_test(self):\n        return ['mse', 'prec@k', 'prec@k_0.005', 'mrr', 'kendalls_tau', 'spearmans_rho', 'time',\n                'groundtruth', 'ranking', 'attention', 'emb_vis_gradual', 'draw_heat_hist',\n                'draw_gt_rk']\n\n    def _get_determining_result_for_val(self):\n        return 'val_mse'\n\n    def _val_need_max(self):\n        return False\n\n    def _get_ins(self, layer, tvt):\n        assert (layer.__class__.__name__ == 'GraphConvolution' or\n                layer.__class__.__name__ == 'GraphConvolutionAttention')\n        ins = []\n        for features in (self._get_plhdr('features_1', tvt) +\n                         self._get_plhdr('features_2', tvt)):\n            ins.append(features)\n        return ins\n\n    def _proc_ins_for_merging_layer(self, ins, _):\n        assert (len(ins) % 2 == 0)\n        proc_ins = []\n        i = 0\n        j = len(ins) // 2\n        for _ in range(len(ins) // 2):\n            proc_ins.append([ins[i], ins[j]])\n            i += 1\n            j += 1\n        ins = proc_ins\n        return ins\n\n    def _val_test_pred_score(self):\n        assert (self.val_test_output.get_shape().as_list() == [1, 1])\n        return tf.squeeze(self.val_test_output)\n\n    def _task_loss(self, tvt):\n        if tvt == 'train':\n            y_pred = self._stack_concat(self.train_outputs)\n            y_true = self.train_y_true\n        else:\n            y_pred = self._stack_concat(self.val_test_output)\n            y_true = self.val_test_y_true\n        assert (y_true.get_shape() == y_pred.get_shape())\n        loss = tf.reduce_mean(tf.nn.l2_loss(y_true - y_pred))\n        return loss, 'mse_loss'\n\n    def _create_basic_placeholders(self, num1, num2, level):\n        laplacians_1 = \\\n            [[[tf.sparse_placeholder(tf.float32)]\n              for _ in range(level)] for _ in range(num1)]\n        laplacians_2 = \\\n            [[[tf.sparse_placeholder(tf.float32)]\n              for _ in range(level)] for _ in range(num2)]\n        features_1 = \\\n            [tf.sparse_placeholder(tf.float32) for _ in range(num1)]\n        features_2 = \\\n            [tf.sparse_placeholder(tf.float32) for _ in range(num2)]\n        num_nonzero_1 = \\\n            [tf.placeholder(tf.int32) for _ in range(num1)]\n        num_nonzero_2 = \\\n            [tf.placeholder(tf.int32) for _ in range(num2)]\n        dropout = tf.placeholder_with_default(0., shape=())\n        val_test_laplacians_1 = [[[tf.sparse_placeholder(tf.float32)] for _ in range(level)]]\n        val_test_laplacians_2 = [[[tf.sparse_placeholder(tf.float32)] for _ in range(level)]]\n        val_test_features_1 = [tf.sparse_placeholder(tf.float32)]\n        val_test_features_2 = [tf.sparse_placeholder(tf.float32)]\n        val_test_num_nonzero_1 = [tf.placeholder(tf.int32)]\n        val_test_num_nonzero_2 = [tf.placeholder(tf.int32)]\n        return laplacians_1, laplacians_2, features_1, features_2, \\\n               num_nonzero_1, num_nonzero_2, dropout, \\\n               val_test_laplacians_1, val_test_laplacians_2, \\\n               val_test_features_1, val_test_features_2, \\\n               val_test_num_nonzero_1, val_test_num_nonzero_2\n\n    def _load_train_triples(self, data, dist_calculator):\n        gs = [g.nxgraph for g in data.train_gs]\n        dist_mat = get_gs_dist_mat(\n            gs, gs, dist_calculator, 'train', 'train',\n            FLAGS.dataset, FLAGS.dist_metric, FLAGS.dist_algo, FLAGS.dist_norm)\n        m, n = dist_mat.shape\n        triples = []\n\n        generate_flag = FLAGS.fake_generation is not None\n        repeat_flag = FLAGS.top_repeater is not None\n        if generate_flag:\n            assert ('fake_' in FLAGS.fake_generation)\n            assert (not FLAGS.top_repeater)\n            fake_num = int(FLAGS.fake_generation.split('_')[1])\n            filepath = get_save_path() + '/{}_fake_{}'.format(\n                FLAGS.dataset, fake_num)\n            load_data = load(filepath)\n            if load_data:\n                print('Loaded from {} with {} triples'.format(\n                    filepath, len(load_data.li)))\n                return load_data\n            node_feat_encoder = data.node_feat_encoder\n        elif repeat_flag:\n            assert ('_repeat_' in FLAGS.top_repeater)\n            assert (not FLAGS.fake_generation)\n            top_num = int(FLAGS.top_repeater.split('_')[0])\n            repeat_num = int(FLAGS.top_repeater.split('_')[2])\n\n        dist_mat_idx = np.argsort(dist_mat, axis=1)\n        for i in range(m):\n            g1 = data.train_gs[i]\n            if generate_flag:\n                sample_graphs, sample_geds = graph_generator(g1.nxgraph, fake_num)\n                print(i, m, sample_geds)\n                for sample_g, sample_ged in zip(sample_graphs, sample_geds):\n                    triples.append((ModelGraph(g1.nxgraph, node_feat_encoder),\n                                    ModelGraph(sample_g, node_feat_encoder),\n                                    self.sim_kernel.dist_to_sim_np(sample_ged)))\n            for j in range(n):\n                col = dist_mat_idx[i][j]\n                g2, ged = data.train_gs[col], dist_mat[i][col]\n                triples.append((g1, g2, self.sim_kernel.dist_to_sim_np(ged)))\n                if repeat_flag and j <= top_num:\n                    for _ in range(repeat_num):\n                        triples.append((g1, g2, self.sim_kernel.dist_to_sim_np(ged)))\n        rtn = SelfShuffleList(triples)\n        if generate_flag:\n            save(filepath, rtn)\n            print('Saved to {} with {} triples'.format(filepath, len(rtn.li)))\n        return rtn\n\n    def _sample_train_pair(self, data):\n        x, y, true_sim = self.train_triples.get_next()\n        # print(x, y, true_sim)\n        # return data.train_gs[x], data.train_gs[y], true_sim\n        return x, y, true_sim\n\n    def _supply_laplacians_etc_to_feed_dict(self, feed_dict, pairs, tvt):\n        for i, (g1, g2) in enumerate(pairs):\n            feed_dict[self._get_plhdr('features_1', tvt)[i]] = \\\n                g1.get_node_inputs()\n            feed_dict[self._get_plhdr('features_2', tvt)[i]] = \\\n                g2.get_node_inputs()\n            feed_dict[self._get_plhdr('num_nonzero_1', tvt)[i]] = \\\n                g1.get_node_inputs_num_nonzero()\n            feed_dict[self._get_plhdr('num_nonzero_2', tvt)[i]] = \\\n                g2.get_node_inputs_num_nonzero()\n            num_laplacians = 1\n            for j in range(get_coarsen_level()):\n                for k in range(num_laplacians):\n                    feed_dict[self._get_plhdr('laplacians_1', tvt)[i][j][k]] = \\\n                        g1.get_laplacians(j)[k]\n                    feed_dict[self._get_plhdr('laplacians_2', tvt)[i][j][k]] = \\\n                        g2.get_laplacians(j)[k]\n        return feed_dict",
  "description": "Combined Analysis:\n- [model/Siamese/layers.py]: This file implements core neural network layers for SimGNN's architecture: 1) GraphConvolutionAttention adds degree-based attention to GCNs for node embedding enhancement. 2) Attention layer generates graph-level embeddings via multiple attention mechanisms (dot/SLM/NTN) that weight important nodes. 3) Dot and SLM layers compute similarity scores between graph embeddings using dot product and single-layer neural tensor networks respectively. These layers collectively enable the key algorithm steps: node embedding via attentive GCNs, graph-level embedding aggregation, and pairwise graph comparison for similarity prediction.\n- [model/Siamese/model.py]: This file implements the core neural network architecture for SimGNN. It defines the Model class that constructs a siamese network using Graph Convolutional Networks (GCNs) and attention mechanisms to compute graph similarity scores. The _build method creates the dual-branch architecture for processing graph pairs, while the _loss method implements the optimization objective combining task-specific loss (approximating GED) and graph regularization loss. The architecture is representation-invariant (via GCNs) and inductive (via learned parameters). The model trains using Adam optimizer to minimize the differentiable loss function, avoiding direct combinatorial optimization of edit operations.\n- [model/Siamese/model_regression.py]: This file implements the SiameseRegressionModel class, which is the core neural network model for approximating graph similarity (specifically Graph Edit Distance) as described in the SimGNN paper. The model uses a siamese architecture with graph convolutional networks (GCNs) and attention mechanisms to learn a regression function that maps pairs of graphs to similarity scores. Key aspects include: 1) Placeholder setup for graph Laplacians and features, 2) Training with pairwise graph triples and ground-truth GED converted to similarity via a kernel, 3) Mean squared error loss for regression, 4) Support for data augmentation through fake graph generation and top pair repetition. The implementation directly corresponds to the paper's algorithm of using GCNs with attention to learn a fast, differentiable approximation of GED without solving the NP-complete optimization problem explicitly.",
  "dependencies": [
    "numpy",
    "data_siamese",
    "utils_siamese",
    "fake_generator",
    "config.FLAGS",
    "model",
    "utils",
    "config",
    "warnings",
    "inits.zeros",
    "inits.glorot",
    "similarity",
    "samplers",
    "utils_siamese.interact_two_sets_of_vectors",
    "dist_calculator",
    "utils_siamese.dot",
    "layers_factory",
    "tensorflow"
  ]
}