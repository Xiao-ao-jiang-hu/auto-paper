{
  "file_path": "pytorch/dknn_layer.py, pytorch/neuralsort.py, pytorch/pl.py, pytorch/run_dknn.py, tf/run_median.py, tf/run_sort.py, tf/sinkhorn.py, tf/util.py",
  "function_name": "DKNN.forward, NeuralSort, PL, dknn_loss, gumbel_sinkhorn",
  "code_snippet": "\n\n# ==========================================\n# File: pytorch/dknn_layer.py\n# Function/Context: DKNN.forward\n# ==========================================\nimport torch\nfrom pl import PL\nfrom neuralsort import NeuralSort\n\n\nclass DKNN (torch.nn.Module):\n    def __init__(self, k, tau=1.0, hard=False, method='deterministic', num_samples=-1):\n        super(DKNN, self).__init__()\n        self.k = k\n        self.soft_sort = NeuralSort(tau=tau, hard=hard)\n        self.method = method\n        self.num_samples = num_samples\n\n    # query: M x p\n    # neighbors: N x p\n    #\n    # returns:\n    def forward(self, query, neighbors, tau=1.0):\n        diffs = (query.unsqueeze(1) - neighbors.unsqueeze(0))\n        squared_diffs = diffs ** 2\n        l2_norms = squared_diffs.sum(2)\n        norms = l2_norms\n        scores = -norms\n\n        if self.method == 'deterministic':\n            P_hat = self.soft_sort(scores)\n            top_k = P_hat[:, :self.k, :].sum(1)\n            return top_k\n        if self.method == 'stochastic':\n            pl_s = PL(scores, tau, hard=False)\n            P_hat = pl_s.sample((self.num_samples,))\n            top_k = P_hat[:, :, :self.k, :].sum(2)\n            return top_k\n\n# ==========================================\n# File: pytorch/neuralsort.py\n# Function/Context: NeuralSort\n# ==========================================\nimport torch\nfrom torch import Tensor\n\n\nclass NeuralSort (torch.nn.Module):\n    def __init__(self, tau=1.0, hard=False):\n        super(NeuralSort, self).__init__()\n        self.hard = hard\n        self.tau = tau\n\n    def forward(self, scores: Tensor):\n        \"\"\"\n        scores: elements to be sorted. Typical shape: batch_size x n x 1\n        \"\"\"\n        scores = scores.unsqueeze(-1)\n        bsize = scores.size()[0]\n        dim = scores.size()[1]\n        one = torch.cuda.FloatTensor(dim, 1).fill_(1)\n\n        A_scores = torch.abs(scores - scores.permute(0, 2, 1))\n        B = torch.matmul(A_scores, torch.matmul(\n            one, torch.transpose(one, 0, 1)))\n        scaling = (dim + 1 - 2 * (torch.arange(dim) + 1)\n                   ).type(torch.cuda.FloatTensor)\n        C = torch.matmul(scores, scaling.unsqueeze(0))\n\n        P_max = (C-B).permute(0, 2, 1)\n        sm = torch.nn.Softmax(-1)\n        P_hat = sm(P_max / self.tau)\n\n        if self.hard:\n            P = torch.zeros_like(P_hat, device='cuda')\n            b_idx = torch.arange(bsize).repeat([1, dim]).view(dim, bsize).transpose(\n                dim0=1, dim1=0).flatten().type(torch.cuda.LongTensor)\n            r_idx = torch.arange(dim).repeat(\n                [bsize, 1]).flatten().type(torch.cuda.LongTensor)\n            c_idx = torch.argmax(P_hat, dim=-1).flatten()  # this is on cuda\n            brc_idx = torch.stack((b_idx, r_idx, c_idx))\n\n            P[brc_idx[0], brc_idx[1], brc_idx[2]] = 1\n            P_hat = (P-P_hat).detach() + P_hat\n        return P_hat\n\n# ==========================================\n# File: pytorch/pl.py\n# Function/Context: PL\n# ==========================================\n'''\nRelaxed Plackett-Luce distribution.\n'''\n\nfrom numbers import Number\n\nimport torch\nfrom torch.distributions.distribution import Distribution\nfrom torch.distributions.utils import broadcast_all\nfrom torch.distributions import constraints\n\n# use GPU if available\nUSE_CUDA = torch.cuda.is_available()\nFloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\nLongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\nByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor\n\n\nclass PL(Distribution):\n\n    arg_constraints = {'scores': constraints.positive,\n                       'tau': constraints.positive}\n    has_rsample = True\n\n    @property\n    def mean(self):\n        # mode of the PL distribution\n        return self.relaxed_sort(self.scores)\n\n    def __init__(self, scores, tau, hard=True, validate_args=None):\n        \"\"\"\n        scores. Shape: (batch_size x) n \n        tau: temperature for the relaxation. Scalar.\n        hard: use straight-through estimation if True\n        \"\"\"\n        self.scores = scores.unsqueeze(-1)\n        self.tau = tau\n        self.hard = hard\n        self.n = self.scores.size()[1]\n\n        if isinstance(scores, Number):\n            batch_shape = torch.Size()\n        else:\n            batch_shape = self.scores.size()\n        super(PL, self).__init__(batch_shape, validate_args=validate_args)\n\n        if self._validate_args:\n            if not torch.gt(self.scores, torch.zeros_like(self.scores)).all():\n                raise ValueError(\"PL is not defined when scores <= 0\")\n\n    def relaxed_sort(self, inp):\n        \"\"\"\n        inp: elements to be sorted. Typical shape: batch_size x n x 1\n        \"\"\"\n        bsize = inp.size()[0]\n        dim = inp.size()[1]\n        one = FloatTensor(dim, 1).fill_(1)\n\n        A_inp = torch.abs(inp - inp.permute(0, 2, 1))\n        B = torch.matmul(A_inp, torch.matmul(one, torch.transpose(one, 0, 1)))\n        scaling = (dim + 1 - 2 * (torch.arange(dim) + 1)).type(FloatTensor)\n        C = torch.matmul(inp, scaling.unsqueeze(0))\n\n        P_max = (C-B).permute(0, 2, 1)\n        sm = torch.nn.Softmax(-1)\n        P_hat = sm(P_max / self.tau)\n\n        if self.hard:\n            P = torch.zeros_like(P_hat)\n            b_idx = torch.arange(bsize).repeat([1, dim]).view(\n                dim, bsize).transpose(dim0=1, dim1=0).flatten().type(LongTensor)\n            r_idx = torch.arange(dim).repeat(\n                [bsize, 1]).flatten().type(LongTensor)\n            c_idx = torch.argmax(P_hat, dim=-1).flatten()  # this is on cuda\n            brc_idx = torch.stack((b_idx, r_idx, c_idx))\n\n            P[brc_idx[0], brc_idx[1], brc_idx[2]] = 1\n            P_hat = (P-P_hat).detach() + P_hat\n        return P_hat\n\n    def rsample(self, sample_shape, log_score=True):\n        \"\"\"\n        sample_shape: number of samples from the PL distribution. Scalar.\n        \"\"\"\n        with torch.enable_grad():  # torch.distributions turns off autograd\n            n_samples = sample_shape[0]\n\n            def sample_gumbel(samples_shape, eps=1e-20):\n                U = torch.zeros(samples_shape, device='cuda').uniform_()\n                return -torch.log(-torch.log(U + eps) + eps)\n            if not log_score:\n                log_s_perturb = torch.log(self.scores.unsqueeze(\n                    0)) + sample_gumbel([n_samples, 1, self.n, 1])\n            else:\n                log_s_perturb = self.scores.unsqueeze(\n                    0) + sample_gumbel([n_samples, 1, self.n, 1])\n            log_s_perturb = log_s_perturb.view(-1, self.n, 1)\n            P_hat = self.relaxed_sort(log_s_perturb)\n            P_hat = P_hat.view(n_samples, -1, self.n, self.n)\n\n            return P_hat.squeeze()\n\n    def log_prob(self, value):\n        \"\"\"\n        value: permutation matrix. shape: batch_size x n x n\n        \"\"\"\n        permuted_scores = torch.squeeze(torch.matmul(value, self.scores))\n        log_numerator = torch.sum(torch.log(permuted_scores), dim=-1)\n        idx = LongTensor([i for i in range(self.n-1, -1, -1)])\n        invert_permuted_scores = permuted_scores.index_select(-1, idx)\n        denominators = torch.cumsum(invert_permuted_scores, dim=-1)\n        log_denominator = torch.sum(torch.log(denominators), dim=-1)\n        return (log_numerator - log_denominator)\n\n# ==========================================\n# File: pytorch/run_dknn.py\n# Function/Context: dknn_loss\n# ==========================================\nimport torch\nimport argparse\nimport os\nimport random\n\nimport numpy as np\n\nfrom itertools import islice\nfrom torch import Tensor, FloatTensor, LongTensor\nfrom pl import PL\nfrom utils import one_hot, generate_nothing\nfrom models.preact_resnet import PreActResNet18\nfrom models.easy_net import ConvNet\nfrom dataset import DataSplit\nfrom neuralsort import NeuralSort\nfrom dknn_layer import DKNN\n\ntorch.manual_seed(94305)\ntorch.cuda.manual_seed(94305)\nnp.random.seed(94305)\nrandom.seed(94305)\n\nparser = argparse.ArgumentParser(\n    description=\"Differentiable k-nearest neighbors.\")\nparser.add_argument(\"--k\", type=int, metavar=\"k\", required=True)\nparser.add_argument(\"--tau\", type=float, metavar=\"tau\", default=16.)\nparser.add_argument(\"--nloglr\", type=float, metavar=\"-log10(beta)\", default=3.)\nparser.add_argument(\"--method\", type=str, default=\"deterministic\")\nparser.add_argument(\"-resume\", action='store_true')\nparser.add_argument(\"--dataset\", type=str, required=True)\n\nparser.add_argument(\"--num_train_queries\", type=int, default=100)\n# no effect on training, but massive effect on memory usage\nparser.add_argument(\"--num_test_queries\", type=int, default=10)\nparser.add_argument(\"--num_train_neighbors\", type=int, default=100)\nparser.add_argument(\"--num_samples\", type=int, default=5)\nparser.add_argument(\"--num_epochs\", type=int, default=200)\n\nargs = parser.parse_args()\ndataset = args.dataset\nsplit = DataSplit(dataset)\nprint(args)\n\nk = args.k\ntau = args.tau\nNUM_TRAIN_QUERIES = args.num_train_queries\nNUM_TEST_QUERIES = args.num_test_queries\nNUM_TRAIN_NEIGHBORS = args.num_train_neighbors\nLEARNING_RATE = 10 ** -args.nloglr\nNUM_SAMPLES = args.num_samples\nresume = args.resume\nmethod = args.method\nNUM_EPOCHS = args.num_epochs\nEMBEDDING_SIZE = 500 if dataset == 'mnist' else 512\n\n\ndef experiment_id(dataset, k, tau, nloglr, method):\n    return 'dknn-resnet-%s-%s-k%d-t%d-b%d' % (dataset, method, k, tau * 100, nloglr)\n\n\ne_id = experiment_id(dataset, k, tau, args.nloglr, method)\n\n\ndknn_layer = DKNN(k, tau, method=method, num_samples=NUM_SAMPLES)\n\n\ndef dknn_loss(query, neighbors, query_label, neighbor_labels, method=method):\n    # query: batch_size x p\n    # neighbors: 10k x p\n    # query_labels: batch_size x [10] one-hot\n    # neighbor_labels: n x [10] one-hot\n    if method == 'deterministic':\n        top_k_ness = dknn_layer(query, neighbors)\n        correct = (query_label.unsqueeze(1) *\n                   neighbor_labels.unsqueeze(0)).sum(-1)\n        correct_in_top_k = (correct * top_k_ness).sum(-1)\n        loss = -correct_in_top_k\n        return loss\n    elif method == 'stochastic':\n        top_k_ness = dknn_layer(query, neighbors)\n        correct = (query_label.unsqueeze(1) *\n                   neighbor_labels.unsqueeze(0)).sum(-1)\n        correct_in_top_k = (correct.unsqueeze(0) * top_k_ness).sum(-1)\n        loss = -correct_in_top_k\n        return loss\n    else:\n        raise ValueError(method)\n\n\ngpu = torch.device('cuda')\n\nif dataset == 'mnist':\n    h_phi = ConvNet().to(gpu)\nelse:\n    h_phi = PreActResNet18(num_channels=3 if dataset ==\n                           'cifar10' else 1).to(gpu)\n\nif resume:\n    # Load checkpoint.\n    print('==> Resuming from checkpoint..')\n    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n    checkpoint = torch.load('./checkpoint/ckpt-%s.t7' % e_id)\n    h_phi.load_state_dict(checkpoint['net'])\n    best_acc = checkpoint['acc']\n    start_epoch = checkpoint['epoch']\nelse:\n    best_acc = 0\n    start_epoch = 0\n\n\noptimizer = torch.optim.SGD(\n    h_phi.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=5e-4)\n\nunit_test_linear_layer = torch.nn.Linear(EMBEDDING_SIZE, 10).to(device=gpu)\nunit_test_ce_loss = torch.nn.CrossEntropyLoss()\n\nema_factor = .999\nema_num = 0\n\n\nbatched_query_train = split.get_train_loader(NUM_TRAIN_QUERIES)\nbatched_neighbor_train = split.get_train_loader(NUM_TRAIN_NEIGHBORS)\n\n\ndef train(epoch):\n    h_phi.train()\n    to_average = []\n    # train\n    for query, candidates in zip(batched_query_train, batched_neighbor_train):\n        optimizer.zero_grad()\n        cand_x, cand_y = candidates\n        query_x, query_y = query\n\n        cand_x = cand_x.to(device=gpu)\n        cand_y = cand_y.to(device=gpu)\n        query_x = query_x.to(device=gpu)\n        query_y = query_y.to(device=gpu)\n\n        neighbor_e = h_phi(cand_x).reshape(NUM_TRAIN_NEIGHBORS, EMBEDDING_SIZE)\n        query_e = h_phi(query_x).reshape(NUM_TRAIN_QUERIES, EMBEDDING_SIZE)\n\n        neighbor_y_oh = one_hot(cand_y).reshape(NUM_TRAIN_NEIGHBORS, 10)\n        query_y_oh = one_hot(query_y).reshape(NUM_TRAIN_QUERIES, 10)\n\n        losses = dknn_loss(query_e, neighbor_e, query_y_oh, neighbor_y_oh)\n        loss = losses.mean()\n        loss.backward()\n        optimizer.step()\n        to_average.append((-loss).item() / k)\n\n    print('Avg. train correctness of top k:',\n          sum(to_average) / len(to_average))\n    print('Avg. train correctness of top k:', sum(\n        to_average) / len(to_average), file=logfile)\n    logfile.flush()\n\n# ==========================================\n# File: tf/run_median.py\n# Function/Context: \n# ==========================================\nimport tensorflow as tf\nimport numpy as np\nimport mnist_input\nimport multi_mnist_cnn\nfrom matplotlib import pyplot as plt\nfrom sinkhorn import gumbel_sinkhorn, sinkhorn_operator\n\nimport util\nimport random\n\ntf.set_random_seed(94305)\nrandom.seed(94305)\n\nflags = tf.app.flags\nflags.DEFINE_integer('M', 1, 'batch size')\nflags.DEFINE_integer('n', 3, 'number of elements to compare at a time')\nflags.DEFINE_integer('l', 5, 'number of digits')\nflags.DEFINE_float('tau', 5, 'temperature (dependent meaning)')\nflags.DEFINE_string('method', 'deterministic_neuralsort',\n                    'which method to use?')\nflags.DEFINE_integer('n_s', 5, 'number of samples')\nflags.DEFINE_integer('num_epochs', 200, 'number of epochs to train')\nflags.DEFINE_float('lr', 1e-4, 'initial learning rate')\n\nFLAGS = flags.FLAGS\n\nn_s = FLAGS.n_s\nNUM_EPOCHS = FLAGS.num_epochs\nM = FLAGS.M\nn = FLAGS.n\nl = FLAGS.l\ntau = FLAGS.tau\nmethod = FLAGS.method\ninitial_rate = FLAGS.lr\n\ntrain_iterator, val_iterator, test_iterator = mnist_input.get_iterators(\n    l, n, 10 ** l - 1, minibatch_size=M)\n\nfalse_tensor = tf.convert_to_tensor(False)\nevaluation = tf.placeholder_with_default(false_tensor, ())\ntemp = tf.cond(evaluation,\n               false_fn=lambda: tf.convert_to_tensor(tau, dtype=tf.float32),\n               true_fn=lambda: tf.convert_to_tensor(1e-10, dtype=tf.float32)\n               )\n\nexperiment_id = 'median-%s-M%d-n%d-l%d-t%d' % (method, M, n, l, tau * 10)\ncheckpoint_path = 'checkpoints/%s/' % experiment_id\n\nhandle = tf.placeholder(tf.string, ())\nX_iterator = tf.data.Iterator.from_string_handle(\n    handle,\n    (tf.float32, tf.float32, tf.float32, tf.float32),\n    ((M, n, l * 28, 28), (M,), (M, n), (M, n))\n)\n\nX, y, median_scores, true_scores = X_iterator.get_next()\n\ntrue_scores = tf.expand_dims(true_scores, 2)\nP_true = util.neuralsort(true_scores, 1e-10)\nn_prime = n\n\n\ndef get_median_probs(P):\n    median_strip = P[:, n // 2, :]\n    median_total = tf.reduce_sum(median_strip, axis=1, keepdims=True)\n    probs = median_strip / median_total\n    # print(probs)\n    return probs\n\n\nif method == 'vanilla':\n    with tf.variable_scope(\"phi\"):\n        representations = multi_mnist_cnn.deepnn(l, X, 10)\n    representations = tf.reshape(representations, [M, n * 10])\n    fc1 = tf.layers.dense(representations, 10, tf.nn.relu)\n    fc2 = tf.layers.dense(fc1, 10, tf.nn.relu)\n    fc3 = tf.layers.dense(fc2, 10, tf.nn.relu)\n    y_hat = tf.layers.dense(fc3, 1)\n    y_hat = tf.squeeze(y_hat)\n    loss_phi = tf.reduce_sum(tf.squared_difference(y_hat, y))\n    loss_theta = loss_phi\n    prob_median_eval = 0\n\nelif method == 'sinkhorn':\n    with tf.variable_scope('phi'):\n        representations = multi_mnist_cnn.deepnn(l, X, n)\n        pre_sinkhorn = tf.reshape(representations, [M, n, n])\n    with tf.variable_scope('theta'):\n        regression_candidates = multi_mnist_cnn.deepnn(l, X, 1)\n        regression_candidates = tf.reshape(\n            regression_candidates, [M, n])\n\n    P_hat = sinkhorn_operator(pre_sinkhorn, temp=temp)\n    prob_median = get_median_probs(P_hat)\n\n    point_estimates = tf.reduce_sum(\n        prob_median * regression_candidates, axis=1)\n    exp_loss = tf.squared_difference(y, point_estimates)\n\n    loss_phi = tf.reduce_mean(exp_loss)\n    loss_theta = loss_phi\n\n    P_hat_eval = sinkhorn_operator(pre_sinkhorn, temp=1e-20)\n    prob_median_eval = get_median_probs(P_hat_eval)\n\nelif method == 'gumbel_sinkhorn':\n    with tf.variable_scope('phi'):\n        representations = multi_mnist_cnn.deepnn(l, X, n)\n        pre_sinkhorn_orig = tf.reshape(representations, [M, n, n])\n        pre_sinkhorn = tf.tile(pre_sinkhorn_orig, [\n                               n_s, 1, 1])\n        pre_sinkhorn += util.sample_gumbel([n_s * M, n, n])\n\n    with tf.variable_scope('theta'):\n        regression_candidates = multi_mnist_cnn.deepnn(l, X, 1)\n        regression_candidates = tf.reshape(\n            regression_candidates, [M, n])\n\n    P_hat = sinkhorn_operator(pre_sinkhorn, temp=temp)\n    prob_median = get_median_probs(P_hat)\n    prob_median = tf.reshape(prob_median, [n_s, M, n])\n\n    point_estimates = tf.reduce_sum(\n        prob_median * regression_candidates, axis=2)\n    exp_loss = tf.squared_difference(y, point_estimates)\n\n    loss_phi = tf.reduce_mean(exp_loss)\n    loss_theta = loss_phi\n\n    P_hat_eval = sinkhorn_operator(pre_sinkhorn_orig, temp=1e-20)\n    prob_median_eval = get_median_probs(P_hat_eval)\n\nelif method == 'deterministic_neuralsort':\n    with tf.variable_scope('phi'):\n        scores = multi_mnist_cnn.deepnn(l, X, 1)\n        scores = tf.reshape(scores, [M, n, 1])\n\n    P_hat = util.neuralsort(scores, temp)\n    P_hat_eval = util.neuralsort(scores, 1e-20)\n\n    with tf.variable_scope('theta'):\n        regression_candidates = multi_mnist_cnn.deepnn(l, X, 1)\n        regression_candidates = tf.reshape(\n            regression_candidates, [M, n])\n\n    losses = tf.squared_difference(\n        regression_candidates, tf.expand_dims(y, 1))\n    prob_median = get_median_probs(P_hat)\n    prob_median_eval = get_median_probs(P_hat_eval)\n\n    point_estimates = tf.reduce_sum(\n        prob_median * regression_candidates, axis=1)\n    exp_loss = tf.squared_difference(y, point_estimates)\n\n    point_estimates_eval = tf.reduce_sum(\n        prob_median_eval * regression_candidates, axis=1)\n    exp_loss_eval = tf.squared_difference(y, point_estimates)\n\n    loss_phi = tf.reduce_mean(exp_loss)\n    loss_theta = tf.reduce_mean(exp_loss_eval)\n\nelif method == 'stochastic_neuralsort':\n    with tf.variable_scope('phi'):\n        scores = multi_mnist_cnn.deepnn(l, X, 1)\n        scores = tf.reshape(scores, [M, n, 1])\n        scores = tf.tile(scores, [n_s, 1, 1])\n        scores += util.sample_gumbel([M * n_s, n, 1])\n\n    P_hat = util.neuralsort(scores, temp)\n    P_hat_eval = util.neuralsort(scores, 1e-20)\n\n    with tf.variable_scope('theta'):\n        regression_candidates = multi_mnist_cnn.deepnn(l, X, 1)\n        regression_candidates = tf.reshape(\n            regression_candidates, [M, n])\n\n    res_y = tf.expand_dims(y, 1)\n\n    losses = tf.squared_difference(regression_candidates, res_y)\n\n    prob_median = get_median_probs(P_hat)\n    prob_median = tf.reshape(prob_median, [n_s, M, n])\n    prob_median_eval = get_median_probs(P_hat_eval)\n    prob_median_eval = tf.reshape(prob_median_eval, [n_s, M, n])\n\n    exp_losses = tf.reduce_sum(prob_median * losses, axis=2)\n    exp_losses_eval = tf.reduce_sum(\n        prob_median_eval * losses, axis=2)\n\n    loss_phi = tf.reduce_mean(exp_losses)\n    loss_theta = tf.reduce_mean(exp_losses_eval)\nelse:\n    raise ValueError(\"No such method.\")\n\nnum_losses = M * n_s if method == 'stochastic_neuralsort' or method == 'gumbel_sinkhorn' else M\n\ncorrectly_identified = tf.reduce_sum(\n    prob_median_eval * median_scores) / num_losses\n\nphi = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='phi')\ntheta = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='theta')\n\ntrain_phi = tf.train.AdamOptimizer(\n    initial_rate).minimize(loss_phi, var_list=phi)\n\nif method != 'vanilla':\n    train_theta = tf.train.AdamOptimizer(initial_rate).minimize(\n        loss_phi, var_list=theta)\n    train_step = tf.group(train_phi, train_theta)\nelse:\n    train_step = train_phi\n\nsaver = tf.train.Saver()\n\nsess = tf.Session()\nlogfile = open('./logs/%s.log' % experiment_id, 'w')\n\n\ndef prnt(*args):\n    print(*args)\n    print(*args, file=logfile)\n\n\nsess.run(tf.global_variables_initializer())\ntrain_sh, validate_sh, test_sh = sess.run([\n    train_iterator.string_handle(),\n    val_iterator.string_handle(),\n    test_iterator.string_handle()\n])\n\nTRAIN_PER_EPOCH = mnist_input.TRAIN_SET_SIZE // (l * M)\nVAL_PER_EPOCH = mnist_input.VAL_SET_SIZE // (l * M)\nTEST_PER_EPOCH = mnist_input.TEST_SET_SIZE // (l * M)\nbest_val = float('inf')\ntiebreaker_val = -1\n\n\ndef save_model(epoch):\n    saver.save(sess, checkpoint_path + 'checkpoint', global_step=epoch)\n\n\ndef load_model():\n    filename = tf.train.latest_checkpoint(checkpoint_path)\n    if filename == None:\n        raise Exception(\"No model found.\")\n    print(\"Loaded model %s.\" % filename)\n    saver.restore(sess, filename)\n\n\ndef train(epoch):\n    loss_train = []\n    for _ in range(TRAIN_PER_EPOCH):\n        _, l = sess.run([train_step, loss_phi],\n                        feed_dict={handle: train_sh})\n        loss_train.append(l)\n    prnt('Average loss:', sum(loss_train) / len(loss_train))\n\n\ndef test(epoch, val=False):\n    global best_correct_val\n    c_is = []\n    l_vs = []\n    for _ in range(VAL_PER_EPOCH if val else TEST_PER_EPOCH):\n        c_i, l_v = sess.run([correctly_identified, loss_phi], feed_dict={\n                            handle: validate_sh if val else test_sh, evaluation: True})\n        c_is.append(c_i)\n        l_vs.append(l_v)\n\n    c_i = sum(c_is) / len(c_is)\n    l_v = sum(l_vs) / len(l_vs)\n\n    if val:\n        prnt(\"Validation set: correctly identified %f, mean squared error %f\" %\n             (c_i, l_v))\n        if l_v < best_val:\n            best_val = l_v\n            prnt('Saving...')\n            save_model(epoch)\n    else:\n        prnt(\"Test set: correctly identified %f, mean squared error %f\" %\n             (c_i, l_v))\n\n\nfor epoch in range(1, NUM_EPOCHS + 1):\n    prnt('Epoch', epoch, '(%s)' % experiment_id)\n    train(epoch)\n    test(epoch, val=True)\n    logfile.flush()\nload_model()\ntest(epoch, val=False)\n\nsess.close()\nlogfile.close()\n\n# ==========================================\n# File: tf/run_sort.py\n# Function/Context: \n# ==========================================\nimport tensorflow as tf\nimport numpy as np\nimport mnist_input\nimport multi_mnist_cnn\nfrom sinkhorn import gumbel_sinkhorn, sinkhorn_operator\n\nimport util\nimport random\n\ntf.set_random_seed(94305)\nrandom.seed(94305)\n\nflags = tf.app.flags\nflags.DEFINE_integer('M', 1, 'batch size')\nflags.DEFINE_integer('n', 3, 'number of elements to compare at a time')\nflags.DEFINE_integer('l', 4, 'number of digits')\nflags.DEFINE_integer('tau', 5, 'temperature (dependent meaning)')\nflags.DEFINE_string('method', 'deterministic_neuralsort',\n                    'which method to use?')\nflags.DEFINE_integer('n_s', 5, 'number of samples')\nflags.DEFINE_integer('num_epochs', 200, 'number of epochs to train')\nflags.DEFINE_float('lr', 1e-4, 'initial learning rate')\n\nFLAGS = flags.FLAGS\n\nn_s = FLAGS.n_s\nNUM_EPOCHS = FLAGS.num_epochs\nM = FLAGS.M\nn = FLAGS.n\nl = FLAGS.l\ntau = FLAGS.tau\nmethod = FLAGS.method\ninitial_rate = FLAGS.lr\n\ntrain_iterator, val_iterator, test_iterator = mnist_input.get_iterators(\n    l, n, 10 ** l - 1, minibatch_size=M)\n\nfalse_tensor = tf.convert_to_tensor(False)\nevaluation = tf.placeholder_with_default(false_tensor, ())\ntemperature = tf.cond(evaluation,\n                      false_fn=lambda: tf.convert_to_tensor(\n                          tau, dtype=tf.float32),\n                      true_fn=lambda: tf.convert_to_tensor(\n                          1e-10, dtype=tf.float32)  # simulate hard sort\n                      )\n\nexperiment_id = 'sort-%s-M%d-n%d-l%d-t%d' % (method, M, n, l, tau * 10)\ncheckpoint_path = 'checkpoints/%s/' % experiment_id\n\nhandle = tf.placeholder(tf.string, ())\nX_iterator = tf.data.Iterator.from_string_handle(\n    handle,\n    (tf.float32, tf.float32, tf.float32, tf.float32),\n    ((M, n, l * 28, 28), (M,), (M, n), (M, n))\n)\n\nX, y, median_scores, true_scores = X_iterator.get_next()\ntrue_scores = tf.expand_dims(true_scores, 2)\nP_true = util.neuralsort(true_scores, 1e-10)\n\nif method == 'vanilla':\n    representations = multi_mnist_cnn.deepnn(l, X, n)\n    concat_reps = tf.reshape(representations, [M, n * n])\n    fc1 = tf.layers.dense(concat_reps, n * n)\n    fc2 = tf.layers.dense(fc1, n * n)\n    P_hat_raw = tf.layers.dense(fc2, n * n)\n    P_hat_raw_square = tf.reshape(P_hat_raw, [M, n, n])\n\n    P_hat = tf.nn.softmax(P_hat_raw_square, dim=-1)  # row-stochastic!\n\n    losses = tf.nn.softmax_cross_entropy_with_logits_v2(\n        labels=P_true, logits=P_hat_raw_square, dim=2)\n    losses = tf.reduce_mean(losses, axis=-1)\n    loss = tf.reduce_mean(losses)\n\nif method == 'sinkhorn':\n    representations = multi_mnist_cnn.deepnn(l, X, n)\n    pre_sinkhorn = tf.reshape(representations, [M, n, n])\n    P_hat = sinkhorn_operator(pre_sinkhorn, temp=temperature)\n    P_hat_logit = tf.log(P_hat)\n\n    losses = tf.nn.softmax_cross_entropy_with_logits_v2(\n        labels=P_true, logits=P_hat_logit, dim=2)\n    losses = tf.reduce_mean(losses, axis=-1)\n    loss = tf.reduce_mean(losses)\n\nif method == 'gumbel_sinkhorn':\n    representations = multi_mnist_cnn.deepnn(l, X, n)\n    pre_sinkhorn = tf.reshape(representations, [M, n, n])\n    P_hat = sinkhorn_operator(pre_sinkhorn, temp=temperature)\n\n    P_hat_sample, _ = gumbel_sinkhorn(\n        pre_sinkhorn, temp=temperature, n_samples=n_s)\n    P_hat_sample_logit = tf.log(P_hat_sample)\n\n    P_true_sample = tf.expand_dims(P_true, 1)\n    P_true_sample = tf.tile(P_true_sample, [1, n_s, 1, 1])\n\n    losses = tf.nn.softmax_cross_entropy_with_logits_v2(\n        labels=P_true_sample, logits=P_hat_sample_logit, dim=3)\n    losses = tf.reduce_mean(losses, axis=-1)\n    losses = tf.reshape(losses, [-1])\n    loss = tf.reduce_mean(losses)\n\nif method == 'deterministic_neuralsort':\n    scores = multi_mnist_cnn.deepnn(l, X, 1)\n    scores = tf.reshape(scores, [M, n, 1])\n    P_hat = util.neuralsort(scores, temperature)\n\n    losses = tf.nn.softmax_cross_entropy_with_logits_v2(\n        labels=P_true, logits=tf.log(P_hat + 1e-20), dim=2)\n    losses = tf.reduce_mean(losses, axis=-1)\n    loss = tf.reduce_mean(losses)\n\nif method == 'stochastic_neuralsort':\n    scores = multi_mnist_cnn.deepnn(l, X, 1)\n    scores = tf.reshape(scores, [M, n, 1])\n    P_hat = util.neuralsort(scores, temperature)\n\n    scores_sample = tf.tile(scores, [n_s, 1, 1])\n    scores_sample += util.sample_gumbel([M * n_s, n, 1])\n    P_hat_sample = util.neuralsort(\n        scores_sample, temperature)\n\n    P_true_sample = tf.tile(P_true, [n_s, 1, 1])\n    losses = tf.nn.softmax_cross_entropy_with_logits_v2(\n        labels=P_true_sample, logits=tf.log(P_hat_sample + 1e-20), dim=2)\n    losses = tf.reduce_mean(losses, axis=-1)\n    loss = tf.reduce_mean(losses)\nelse:\n    raise ValueError(\"No such method.\")\n\n\ndef vec_gradient(l):  # l is a scalar\n    gradient = tf.gradients(l, tf.trainable_variables())\n    vec_grads = [tf.reshape(grad, [-1]) for grad in gradient]  # flatten\n    z = tf.concat(vec_grads, 0)  # n_params\n    return z\n\n\nprop_correct = util.prop_correct(P_true, P_hat)\nprop_any_correct = util.prop_any_correct(P_true, P_hat)\n\nopt = tf.train.AdamOptimizer(initial_rate)\ntrain_step = opt.minimize(loss)\nsaver = tf.train.Saver()\n\n# ==========================================\n# File: tf/sinkhorn.py\n# Function/Context: gumbel_sinkhorn\n# ==========================================\nimport tensorflow as tf\n\ndef sinkhorn_operator(log_alpha, n_iters=20, temp=1.0):\n  \"\"\"Performs incomplete Sinkhorn normalization to log_alpha.\n  By a theorem by Sinkhorn and Knopp [1], a sufficiently well-behaved  matrix\n  with positive entries can be turned into a doubly-stochastic matrix\n  (i.e. its rows and columns add up to one) via the succesive row and column\n  normalization.\n  -To ensure positivity, the effective input to sinkhorn has to be\n  exp(log_alpha) (elementwise).\n  -However, for stability, sinkhorn works in the log-space. It is only at\n   return time that entries are exponentiated.\n  [1] Sinkhorn, Richard and Knopp, Paul.\n  Concerning nonnegative matrices and doubly stochastic\n  matrices. Pacific Journal of Mathematics, 1967\n  Args:\n    log_alpha: 2D tensor (a matrix of shape [N, N])\n      or 3D tensor (a batch of matrices of shape = [batch_size, N, N])\n    n_iters: number of sinkhorn iterations (in practice, as little as 20\n      iterations are needed to achieve decent convergence for N~100)\n  Returns:\n    A 3D tensor of close-to-doubly-stochastic matrices (2D tensors are\n      converted to 3D tensors with batch_size equals to 1)\n  \"\"\"\n\n  n = tf.shape(log_alpha)[1]\n  log_alpha = tf.reshape(log_alpha, [-1, n, n]) / temp\n\n  for _ in range(n_iters):\n    log_alpha -= tf.reshape(tf.reduce_logsumexp(log_alpha, axis=2), [-1, n, 1])\n    log_alpha -= tf.reshape(tf.reduce_logsumexp(log_alpha, axis=1), [-1, 1, n])\n  return tf.exp(log_alpha)\n\ndef gumbel_sinkhorn(log_alpha,\n                    temp=1.0, n_samples=1, noise_factor=1.0, n_iters=20,\n                    squeeze=True):\n  \"\"\"Random doubly-stochastic matrices via gumbel noise.\n  In the zero-temperature limit sinkhorn(log_alpha/temp) approaches\n  a permutation matrix. Therefore, for low temperatures this method can be\n  seen as an approximate sampling of permutation matrices, where the\n  distribution is parameterized by the matrix log_alpha\n  The deterministic case (noise_factor=0) is also interesting: it can be\n  shown that lim t->0 sinkhorn(log_alpha/t) = M, where M is a\n  permutation matrix, the solution of the\n  matching problem M=arg max_M sum_i,j log_alpha_i,j M_i,j.\n  Therefore, the deterministic limit case of gumbel_sinkhorn can be seen\n  as approximate solving of a matching problem, otherwise solved via the\n  Hungarian algorithm.\n  Warning: the convergence holds true in the limit case n_iters = infty.\n  Unfortunately, in practice n_iter is finite which can lead to numerical\n  instabilities, mostly if temp is very low. Those manifest as\n  pseudo-convergence or some row-columns to fractional entries (e.g.\n  a row having two entries with 0.5, instead of a single 1.0)\n  To minimize those effects, try increasing n_iter for decreased temp.\n  On the other hand, too-low temperature usually lead to high-variance in\n  gradients, so better not choose too low temperatures.\n  Args:\n    log_alpha: 2D tensor (a matrix of shape [N, N])\n      or 3D tensor (a batch of matrices of shape = [batch_size, N, N])\n    temp: temperature parameter, a float.\n    n_samples: number of samples\n    noise_factor: scaling factor for the gumbel samples. Mostly to explore\n      different degrees of randomness (and the absence of randomness, with\n      noise_factor=0)\n    n_iters: number of sinkhorn iterations. Should be chosen carefully, in\n      inverse corresponde with temp to avoid numerical stabilities.\n    squeeze: a boolean, if True and there is a single sample, the output will\n      remain being a 3D tensor.\n  Returns:\n    sink: a 4D tensor of [batch_size, n_samples, N, N] i.e.\n      batch_size *n_samples doubly-stochastic matrices. If n_samples = 1 and\n      squeeze = True then the output is 3D.\n    log_alpha_w_noise: a 4D tensor of [batch_size, n_samples, N, N] of\n      noisy samples of log_alpha, divided by the temperature parameter. If\n      n_samples = 1 then the output is 3D.\n  \"\"\"\n  n = tf.shape(log_alpha)[1]\n  log_alpha = tf.reshape(log_alpha, [-1, n, n])\n  batch_size = tf.shape(log_alpha)[0]\n  log_alpha_w_noise = tf.tile(log_alpha, [n_samples, 1, 1])\n  if noise_factor == 0:\n    noise = 0.0\n  else:\n    noise = sample_gumbel([n_samples*batch_size, n, n])*noise_factor\n  log_alpha_w_noise += noise\n  log_alpha_w_noise /= temp\n  sink = sinkhorn_operator(log_alpha_w_noise, n_iters)\n  if n_samples > 1 or squeeze is False:\n    sink = tf.reshape(sink, [n_samples, batch_size, n, n])\n    sink = tf.transpose(sink, [1, 0, 2, 3])\n    log_alpha_w_noise = tf.reshape(\n        log_alpha_w_noise, [n_samples, batch_size, n, n])\n    log_alpha_w_noise = tf.transpose(log_alpha_w_noise, [1, 0, 2, 3])\n  return sink, log_alpha_w_noise\n\n# ==========================================\n# File: tf/util.py\n# Function/Context: \n# ==========================================\nimport tensorflow as tf\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n# M: minibatch size\n# n: number of items in each sequence\n# s: scores\n\nnp.set_printoptions(precision=4, suppress=True)\neps = 1e-20\n\ndef bl_matmul(A, B):\n  return tf.einsum('mij,jk->mik', A, B)\n\ndef br_matmul(A, B):\n  return tf.einsum('ij,mjk->mik', A, B)\n\n# s: M x n x 1\n# neuralsort(s): M x n x n\ndef neuralsort(s, tau = 1):\n  A_s = s - tf.transpose(s, perm=[0, 2, 1])\n  A_s = tf.abs(A_s) \n  # As_ij = |s_i - s_j|\n\n  n = tf.shape(s)[1] \n  one = tf.ones((n, 1), dtype = tf.float32)\n\n  B = bl_matmul(A_s, one @ tf.transpose(one))\n  # B_:k = (A_s)(one)\n\n  K = tf.range(n) + 1\n  # K_k = k\n\n  C = bl_matmul(\n    s, tf.expand_dims(tf.cast(n + 1 - 2 * K, dtype = tf.float32), 0)\n  )\n  # C_:k = (n + 1 - 2k)s\n\n  P = tf.transpose(C - B, perm=[0, 2, 1])\n  # P_k: = (n + 1 - 2k)s - (A_s)(one)\n \n  P = tf.nn.softmax(P / tau, -1)\n  # P_k: = softmax( ((n + 1 - 2k)s - (A_s)(one)) / tau )\n\n  return P\n\n# Pi: M x n x n row-stochastic\ndef prop_any_correct (P1, P2):\n  z1 = tf.argmax(P1, axis=-1) \n  z2 = tf.argmax(P2, axis=-1)\n  eq = tf.equal(z1, z2)\n  eq = tf.cast(eq, dtype=tf.float32)\n  correct = tf.reduce_mean(eq, axis=-1)\n  return tf.reduce_mean(correct)\n\n# Pi: M x n x n row-stochastic\ndef prop_correct (P1, P2):\n  z1 = tf.argmax(P1, axis=-1) \n  z2 = tf.argmax(P2, axis=-1)\n  eq = tf.equal(z1, z2)\n  correct = tf.reduce_all(eq, axis=-1)\n  return tf.reduce_mean(tf.cast(correct, tf.float32))\n\ndef sample_gumbel(shape, eps = 1e-20): \n\tU = tf.random_uniform(shape, minval=0, maxval=1)\n\treturn -tf.log(-tf.log(U + eps) + eps)\n\n# s: M x n\n# P: M x n x n\n# returns: M\ndef pl_log_density(log_s, P):\n  log_s = tf.expand_dims(log_s, 2) # M x n x 1\n  ordered_log_s = P @ log_s # M x n x 1\n  ordered_log_s = tf.squeeze(ordered_log_s, squeeze_dims=[-1]) # M x n\n  potentials = tf.exp(ordered_log_s)\n  n = log_s.get_shape().as_list()[1]\n  max_log_s = [\n    tf.reduce_max(ordered_log_s[:, k:], axis=1, keepdims=True)\n    for k in range(n)\n  ] # [M x 1] x n\n  adj_log_s = [\n    ordered_log_s - max_log_s[k]\n    for k in range(n)\n  ] # [M x n] x n\n  potentials = [\n    tf.exp(adj_log_s[k][:, k:])\n    for k in range(n)\n  ] # [M x n] x n\n  denominators = [\n    tf.reduce_sum(potentials[k], axis=1, keepdims=True)\n    for k in range(n)\n  ] # [M x 1] x n\n  log_denominators = [\n    tf.squeeze(tf.log(denominators[k]) + max_log_s[k], squeeze_dims=[1])\n    for k in range(n)\n  ] # [M] x n\n  log_denominator = tf.add_n(log_denominators) # M\n  log_potentials = ordered_log_s # M x n x 1\n  log_potential = tf.reduce_sum(log_potentials, 1) # M\n  log_likelihood = log_potential - log_denominator\n  return log_likelihood",
  "description": "Combined Analysis:\n- [pytorch/dknn_layer.py]: This file implements the core differentiable k-nearest neighbors (kNN) layer using NeuralSort, which directly applies the paper's continuous relaxation of sorting. The DKNN class computes pairwise distances between queries and neighbors, converts them to scores, and then uses either deterministic NeuralSort (continuous relaxation) or stochastic PL sampling (Plackett-Luce distribution) to obtain soft top-k selection matrices. This enables gradient-based learning of kNN classifiers by relaxing the non-differentiable sorting operation, aligning with the paper's goal of enabling end-to-end learning with sorting operations.\n- [pytorch/neuralsort.py]: This file implements the core NeuralSort algorithm from the paper. The NeuralSort class provides a continuous relaxation of the sorting operator that maps input scores to unimodal row-stochastic matrices approximating permutation matrices. The forward method computes the relaxed permutation matrix P_hat using the formula: P_hat = softmax(((n+1-2i)s - A_s1_n1_n^T)/Ï„), where A_s is the matrix of absolute differences between scores. The implementation includes both deterministic (soft) and hard (straight-through estimator) modes. The code directly corresponds to the mathematical formulation in the paper, enabling gradient-based optimization through sorting operations.\n- [pytorch/pl.py]: This file implements the core NeuralSort relaxation and Plackett-Luce distribution from the paper. The PL class provides: 1) A differentiable sorting operator (relaxed_sort) that maps input scores to unimodal row-stochastic matrices approximating permutation matrices (NeuralSort). 2) Reparameterized sampling via Gumbel perturbations (rsample) for low-variance gradient estimation. 3) Exact log-probability computation for the Plackett-Luce distribution (log_prob). The implementation directly corresponds to the paper's mathematical formulation, enabling gradient-based optimization over permutations through continuous relaxations.\n- [pytorch/run_dknn.py]: This file implements the differentiable k-nearest neighbors (DKNN) model using NeuralSort as described in the paper. The core logic is in the dknn_loss function which uses the DKNN layer (that internally uses NeuralSort) to compute a differentiable top-k selection. The DKNN layer provides a continuous relaxation of the sorting operation via the temperature parameter tau, enabling gradient-based optimization. The training loop optimizes the embedding network h_phi to maximize the correctness of top-k neighbors using the differentiable sorting relaxation. The method parameter supports both deterministic and stochastic versions (the latter using Gumbel perturbations for the Plackett-Luce distribution).\n- [tf/run_median.py]: This file implements the core optimization logic from the paper for quantile regression using NeuralSort. It contains implementations of both deterministic and stochastic NeuralSort methods, which are continuous relaxations of sorting networks. The code demonstrates how to use the NeuralSort operator (util.neuralsort) to obtain unimodal row-stochastic matrices that approximate permutation matrices, enabling gradient-based optimization. The stochastic variant uses Gumbel perturbations (util.sample_gumbel) for reparameterized gradient estimation. The implementation includes the full training pipeline with temperature annealing, loss computation, and evaluation metrics for median regression on multi-digit MNIST data.\n- [tf/run_sort.py]: This file implements the core optimization logic from the paper for both deterministic and stochastic NeuralSort. The key components are:\n1. The NeuralSort relaxation (via `util.neuralsort`) is used to map input scores to unimodal row-stochastic matrices approximating permutation matrices.\n2. The stochastic variant (`stochastic_neuralsort`) adds Gumbel perturbations to scores before applying NeuralSort, implementing the reparameterized gradient estimator for the Plackett-Luce distribution.\n3. The objective minimizes cross-entropy between predicted permutation matrices (P_hat) and true permutation matrices (P_true), aligning with the paper's objective of minimizing negative log-likelihood.\n4. Temperature annealing is implemented: during training, temperature is set to `tau` for relaxed sorting; during evaluation, it approaches 0 for hard sorting.\n5. The optimization uses Adam to update parameters via backpropagation through the differentiable sorting operator.\n- [tf/sinkhorn.py]: This file implements the Gumbel-Sinkhorn operator, which is a core component of the paper's stochastic optimization approach. The gumbel_sinkhorn function combines Gumbel perturbations with Sinkhorn normalization to produce doubly-stochastic matrices that approximate permutation matrices in the low-temperature limit. This enables reparameterized gradient estimation for stochastic optimization over permutations, directly addressing the paper's challenge of non-differentiable sorting operations. The implementation matches the paper's description of using continuous relaxations (via Sinkhorn iterations) and Gumbel noise for low-variance gradient estimation in permutation learning tasks.\n- [tf/util.py]: This file directly implements the core NeuralSort algorithm from the paper. The neuralsort() function provides the continuous relaxation mapping scores to unimodal row-stochastic matrices (Equation 3 in paper). The pl_log_density() function computes the log-likelihood under the Plackett-Luce distribution, enabling stochastic optimization over permutations. Helper functions include batch matrix multiplications (bl_matmul, br_matmul), Gumbel sampling (sample_gumbel), and permutation evaluation metrics (prop_any_correct, prop_correct). The implementation matches the mathematical formulation exactly: A_s computes pairwise score differences, B computes row sums of A_s, C incorporates the (n+1-2k) coefficients, and the final softmax produces the relaxed permutation matrix P.",
  "dependencies": [
    "sinkhorn.gumbel_sinkhorn",
    "util.prop_any_correct",
    "util",
    "multi_mnist_cnn",
    "matplotlib.pyplot",
    "mnist_input",
    "matplotlib",
    "dataset.DataSplit",
    "tensorflow",
    "util.neuralsort",
    "sinkhorn",
    "utils.one_hot",
    "sample_gumbel (external function not defined in this file)",
    "torch.distributions.Distribution",
    "pl.PL",
    "torch.nn.Softmax",
    "utils.generate_nothing",
    "torch.distributions.utils.broadcast_all",
    "torch.distributions.constraints",
    "os",
    "sinkhorn.sinkhorn_operator",
    "argparse",
    "random",
    "torch.nn.Module",
    "util.prop_correct",
    "numpy",
    "models.preact_resnet.PreActResNet18",
    "util.sample_gumbel",
    "torch",
    "dknn_layer.DKNN",
    "neuralsort.NeuralSort",
    "models.easy_net.ConvNet",
    "itertools"
  ]
}