{
  "file_path": "portfolio_optimization/main.py, portfolio_optimization/portfolio_utils.py, resource_provisioning/util.py, synthetic_linear_programming/util.py",
  "function_name": "ours_train_portfolio, getconstrlist, merge_constraints, getconstrlist, merge_constraints",
  "code_snippet": "\n\n# ==========================================\n# File: portfolio_optimization/main.py\n# Function/Context: \n# ==========================================\nimport os\nimport sys\nimport pandas as pd\nimport torch\nimport numpy as np\nimport qpth\nimport scipy\nimport cvxpy as cp\nimport random\nimport argparse\nimport tqdm\nimport time\nimport math\nimport datetime as dt\nfrom cvxpylayers.torch import CvxpyLayer\n\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom data_utils import SP500DataLoader\nfrom portfolio_utils import computeCovariance, generateDataset\nfrom model import PortfolioModel, CovarianceModel\nfrom portfolio_utils import train_portfolio, validate_portfolio, test_portfolio, ours_train_portfolio, ours_validate_portfolio, ours_test_portfolio\nfrom utils import normalize_matrix, normalize_matrix_positive, normalize_vector, normalize_matrix_qr, normalize_projection\nfrom config import OUTPUT_NAME\n\ndef sparsify_matrix(x, sparsity=0.5):\n    if x.ndimension() != 2:\n        raise ValueError(\"Not a matrix: dim %d,  should be 2-dim\", x.ndimension())\n\n    rows, cols = x.shape\n    num_zeros = int(math.ceil(sparsity * rows))\n\n    for col_idx in range(cols):\n        row_indices = torch.randperm(rows)\n        zero_indices = row_indices[:num_zeros]\n        x[zero_indices, col_idx] = 0\n    return x\n\nif __name__ == '__main__':\n    # ==================== Parser setting ==========================\n    parser = argparse.ArgumentParser(description='Portfolio Optimization')\n    parser.add_argument('--filepath', type=str, default='', help='filename under folder results')\n    parser.add_argument('--method', type=int, default=3, help='0: two-stage(L1), 1: decision-focused,2=two-stage(L2), 3:ours')\n    parser.add_argument('--T-size', type=int, default=10, help='the size of reparameterization metrix')\n    parser.add_argument('--epochs', type=int, default=20, help='number of training epochs')\n    parser.add_argument('--n', type=int, default=100, help='number of items')\n    parser.add_argument('--num-samples', type=int, default=0, help='number of samples, 0 -> all')\n    parser.add_argument('--lr', type=float, default=0.01, help='learning rate')\n    parser.add_argument('--seed', type=int, default=471298479,help='random seed')\n    args = parser.parse_args()\n\n    SEED = args.seed \n    print(\"Random seed: {}\".format(SEED))\n    torch.manual_seed(SEED)\n    np.random.seed(SEED)\n    random.seed(SEED)\n\n    portfolio_opt_dir = os.path.abspath(os.path.dirname(__file__))\n    print(\"portfolio_opt_dir:\", portfolio_opt_dir)\n\n    sp500_data_dir = os.path.join(portfolio_opt_dir, \"data\", \"sp500\")\n    sp500_data = SP500DataLoader(sp500_data_dir, \"sp500\",\n                                 start_date=dt.datetime(2004, 1, 1),\n                                 end_date=dt.datetime(2017, 1, 1),\n                                 collapse=\"daily\",\n                                 overwrite=False,\n                                 verbose=True)\n\n    filepath = args.filepath\n    n = args.n\n    num_samples = args.num_samples if args.num_samples != 0 else 1000000\n    num_epochs = args.epochs\n    lr = args.lr\n    method_id = args.method\n    if method_id == 0:\n        training_method = 'two-stage'\n    elif method_id == 1:\n        training_method = 'decision-focused'\n    elif method_id == 2:\n        training_method = 'two-stage2'\n    elif method_id == 3:\n        training_method = 'ours'\n    else:\n        raise ValueError('Not implemented methods')\n    des = open(\"res/K100/\"+training_method + \"N\"+str(args.n)+\"newwithMSE_\"+str(args.seed-471298479)+\".txt\", \"w\")\n    print(\"Training method: {}\".format(training_method))\n\n    train_dataset, validate_dataset, test_dataset = generateDataset(sp500_data, n=n, num_samples=num_samples)\n    feature_size = train_dataset.dataset[0][0].shape[1]\n\n    model = PortfolioModel(input_size=feature_size, output_size=1, seed=args.seed)\n    covariance_model = CovarianceModel(n=n, seed=args.seed)\n\n    optimizer = torch.optim.Adam(list(model.parameters()) + list(covariance_model.parameters()), lr=lr)\n    scheduler = ReduceLROnPlateau(optimizer, 'min')\n\n    if training_method == 'surrogate':\n        T_size = args.T_size\n        init_T = normalize_matrix_positive(torch.rand(n, T_size))\n        T = torch.tensor(init_T, requires_grad=True)\n        T_lr = lr\n        T_optimizer = torch.optim.Adam([T], lr=T_lr)\n        T_scheduler = ReduceLROnPlateau(T_optimizer, 'min')\n\n    train_loss_list, train_obj_list = [], []\n    test_loss_list,  test_obj_list  = [], []\n    validate_loss_list,  validate_obj_list = [], []\n\n    # generate C0, d0 and alpha0\n    M1 = int(n * 0.4)\n    C0 = sparsify_matrix(torch.ones(M1, n), sparsity=0.9)\n    d0 = torch.from_numpy(np.random.random((M1, 1))) * 0.2\n    alpha0 = torch.from_numpy(np.random.random((M1, 1)) * 0.3 * 50 / args.n)\n    softcon = (C0, d0, alpha0)\n\n    print('n: {}, lr: {}'.format(n,lr))\n    print('Start training...')\n    evaluate = True\n    total_forward_time, total_inference_time, total_qp_time, total_backward_time = 0, 0, 0, 0\n    forward_time_list, inference_time_list, qp_time_list, backward_time_list = [], [], [], []\n    for epoch in range(-1, num_epochs):\n        evaluate = True\n        start_time = time.time()\n        forward_time, inference_time, qp_time, backward_time = 0, 0, 0, 0\n        if training_method in ['decision-focused' ,'two-stage','two-stage2']:\n            if epoch == -1:\n                print('Testing the optimal solution...')\n                train_loss, train_obj = test_portfolio(model, covariance_model, epoch, train_dataset, evaluate=True, softcon=(C0, d0, alpha0))\n            elif epoch == 0:\n                print('Testing the initial solution quality...')\n                train_loss, train_obj = test_portfolio(model, covariance_model, epoch, train_dataset, evaluate=evaluate, softcon=(C0, d0, alpha0))\n            else:\n                train_loss, train_obj, (forward_time, inference_time, qp_time, backward_time) = train_portfolio(model, covariance_model, optimizer, epoch, train_dataset, training_method=training_method, evaluate=evaluate, softcon=softcon)\n        elif training_method == \"ours\":\n            if epoch == -1:\n                print(\"Testing the optimal.\")\n                train_loss, train_obj = ours_test_portfolio(model, covariance_model, epoch, train_dataset, evaluate=True, softcon=(C0, d0, alpha0))\n            elif epoch == 0:\n                print(\"Testing the initial.\")\n                train_loss, train_obj = ours_test_portfolio(model, covariance_model, epoch, train_dataset, evaluate=evaluate, softcon=(C0, d0, alpha0))\n            else:\n                train_loss, train_obj, (forward_time, inference_time, qp_time, backward_time) = ours_train_portfolio(model, covariance_model, optimizer, epoch, train_dataset, evaluate=evaluate, softcon=softcon)\n\n        else:\n            raise ValueError('Not implemented')\n        total_forward_time   += forward_time\n        total_inference_time += inference_time\n        total_qp_time        += qp_time\n        total_backward_time  += backward_time\n\n        forward_time_list.append(forward_time)\n        inference_time_list.append(inference_time)\n        qp_time_list.append(qp_time)\n        backward_time_list.append(backward_time)\n\n        # ================ validating ==================\n        if training_method == \"ours\":\n            if epoch == -1:\n                validate_loss, validate_obj = ours_test_portfolio(model, covariance_model, epoch, validate_dataset, evaluate=True, softcon=(C0, d0, alpha0))\n            else:\n                validate_loss, validate_obj = ours_validate_portfolio(model, covariance_model, scheduler, epoch, validate_dataset, softcon=(C0, d0, alpha0))\n        else:\n            if epoch == -1:\n                validate_loss, validate_obj = test_portfolio(model, covariance_model, epoch, validate_dataset, evaluate=True, softcon=(C0, d0, alpha0))\n            else:\n                validate_loss, validate_obj = validate_portfolio(model, covariance_model, scheduler, epoch, validate_dataset, training_method=training_method, evaluate=evaluate, softcon=(C0, d0, alpha0))\n\n        # ================== testing ===================\n        if training_method == \"ours\":\n            if epoch == -1:\n                test_loss, test_obj = ours_test_portfolio(model, covariance_model, epoch, test_dataset, evaluate=True, softcon=(C0, d0, alpha0))\n            else:\n                test_loss, test_obj = ours_test_portfolio(model, covariance_model, epoch, test_dataset, evaluate=evaluate, softcon=(C0, d0, alpha0))\n        else:\n            if epoch == -1:\n                test_loss, test_obj = test_portfolio(model, covariance_model, epoch, test_dataset, evaluate=True, softcon=(C0, d0, alpha0))\n            else:\n                test_loss, test_obj = test_portfolio(model, covariance_model, epoch, test_dataset, evaluate=evaluate, softcon=(C0, d0, alpha0))\n\n        # =============== printing data ================\n        des.write('Epoch {} | Train Loss:    \\t {} \\t | Train Objective Value:    \\t {}% \\n'.format(epoch, train_loss, train_obj * 100))\n        des.write('Epoch {} | Validate Loss: \\t {} \\t | Validate Objective Value: \\t {}% \\n'.format(epoch, validate_loss, validate_obj * 100))\n        des.write('Epoch {} | Test Loss:     \\t {} \\t | Test Objective Value:     \\t {}% \\n'.format(epoch, test_loss, test_obj * 100))\n        des.flush()\n        # ============== recording data ================\n        end_time = time.time()\n        print(\"Epoch {}, elapsed time: {}, forward time: {}, inference time: {}, qp time: {}, backward time: {}\".format(epoch, end_time - start_time, forward_time, inference_time, qp_time, backward_time))\n\n        train_loss_list.append(train_loss)\n        train_obj_list.append(train_obj)\n        test_loss_list.append(test_loss)\n        test_obj_list.append(test_obj)\n        validate_loss_list.append(validate_loss)\n        validate_obj_list.append(validate_obj)\n\n        # record the data every epoch\n        f_output = open('results/performance/' + filepath + \"{}-SEED{}.csv\".format(training_method,SEED), 'w')\n        f_output.write('Epoch, {}\\n'.format(epoch))\n        f_output.write('training loss,' + ','.join([str(x) for x in train_loss_list]) + '\\n')\n        f_output.write('training obj,'  + ','.join([str(x) for x in train_obj_list])  + '\\n')\n        f_output.write('validating loss,' + ','.join([str(x) for x in validate_loss_list]) + '\\n')\n        f_output.write('validating obj,'  + ','.join([str(x) for x in validate_obj_list])  + '\\n')\n        f_output.write('testing loss,'  + ','.join([str(x) for x in test_loss_list])  + '\\n')\n        f_output.write('testing obj,'   + ','.join([str(x) for x in test_obj_list])   + '\\n')\n        f_output.close()\n\n        f_time = open('results/time/' + filepath + \"{}-SEED{}.csv\".format(training_method, SEED), 'w')\n        f_time.write('Epoch, {}\\n'.format(epoch))\n        f_time.write('Random seed, {}, forward time, {}, inference time, {}, qp time, {}, backward_time, {}\\n'.format(str(seed), total_forward_time, total_inference_time, total_qp_time, total_backward_time))\n        f_time.write('forward time,'   + ','.join([str(x) for x in forward_time_list]) + '\\n')\n        f_time.write('inference time,' + ','.join([str(x) for x in inference_time_list]) + '\\n')\n        f_time.write('qp time,'        + ','.join([str(x) for x in qp_time_list]) + '\\n')\n        f_time.write('backward time,'  + ','.join([str(x) for x in backward_time_list]) + '\\n')\n        f_time.close()\n    des.close()\n\n# ==========================================\n# File: portfolio_optimization/portfolio_utils.py\n# Function/Context: ours_train_portfolio\n# ==========================================\nimport torch\nimport tqdm\nimport time\nfrom utils import computeCovariance\nimport numpy as np\nimport cvxpy as cp\nfrom cvxpylayers.torch import CvxpyLayer\n\nimport sys\nimport pandas as pd\nimport torch\nimport numpy as np\nimport qpth\nimport scipy\nimport cvxpy as cp\nimport random\nimport argparse\nimport tqdm\nimport time\nimport math\nimport datetime as dt\nfrom cvxpylayers.torch import CvxpyLayer\nimport matplotlib.pyplot as plt\nimport torch.nn\nimport torch.utils.data as data_utils\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torchvision import transforms\nimport gurobipy as gp\nfrom gurobipy import GRB\nfrom utils import normalize_matrix, normalize_matrix_positive, normalize_vector, normalize_matrix_qr, normalize_projection\nfrom sqrtm import sqrtm\nfrom calc_ours import quad_surro_tensor, getval, getopt, drawpic\nfrom config import RATIO, OUTPUT_NAME\nalp = 2\nREG = 0.1\nsolver = 'cvxpy'\n\nMAX_NORM = 0.1\nT_MAX_NORM = 0.1\n\ndef symsqrt(matrix):\n    \"\"\"Compute the square root of a positive definite matrix.\"\"\"\n    _, s, v = matrix.svd()\n    good = s > s.max(-1, True).values * s.size(-1) * torch.finfo(s.dtype).eps\n    components = good.sum(-1)\n    common = components.max()\n    unbalanced = common != components.min()\n    if common < s.size(-1):\n        s = s[..., :common]\n        v = v[..., :common]\n        if unbalanced:\n            good = good[..., :common]\n    if unbalanced:\n        s = s.where(good, torch.zeros((), device=s.device, dtype=s.dtype))\n    return (v * s.sqrt().unsqueeze(-2)) @ v.transpose(-2, -1)\n\ndef computeCovariance(covariance_mat):\n    cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n    n = len(covariance_mat)\n    cosine_matrix = torch.zeros((n,n))\n    for i in range(n):\n        cosine_matrix[i] = cos(covariance_mat, covariance_mat[i].repeat(n,1))\n    return cosine_matrix\n\ndef ours_train_portfolio(model, covariance_model, optimizer, epoch, dataset, device='cpu', evaluate=False, KK=None, AP=None, softcon=None):\n    global plts, mseQ, maeQ, msetheta, maetheta, grdnm, opt_mae\n    if softcon is not None:\n        C0, d0, alpha0 = softcon\n        C0, d0, alpha0 = C0.double(), d0.double(), alpha0.double()\n    opts = []\n    model.train()\n    covariance_model.train()\n    loss_fn = torch.nn.MSELoss() # torch.nn.MSELoss()\n    train_losses, train_objs = [], []\n    global turn, rec_features, rec_covmat, rec_labels\n    forward_time, inference_time, qp_time, backward_time = 0, 0, 0, 0\n    repeat, lianxu = False, True # for debugging. repeat - training only with the first data point; lianxu - training Q and theta simutaneously (False is separately in turn).\n    if AP is None: AP = 0.2 # a constant for the generation of alpha. This coefficient is much smaller and practical than theoretical upper bound, yet still working.\n    with tqdm.tqdm(dataset) as tqdm_loader:\n\n        batchloss = torch.zeros(1)\n        for batch_idx, (features, covariance_mat, labels) in enumerate(tqdm_loader):\n            forward_start_time = time.time()\n            if rec_features is None:\n                rec_features, rec_covmat, rec_labels = features, covariance_mat, labels\n            if repeat:\n                features, covariance_mat, labels = rec_features, rec_covmat, rec_labels\n            features, covariance_mat, labels = features[0].to(device), covariance_mat[0].to(device), labels[0,:,0].to(device).double() # only one single data\n\n            n = len(covariance_mat)\n            Q_real = (1/RATIO) * alp/2 * (computeCovariance(covariance_mat) * (1 - REG) + torch.eye(n).double() * REG)\n            predictions = model(features.double())[:,0]\n            loss = loss_fn(predictions, labels)\n            Q = (1/RATIO) * alp/2 * (covariance_model() * (1 - REG) + torch.eye(n).double() * REG)  # TODO\n            A, b = torch.cat((-torch.ones(1, n), torch.ones(1, n), -torch.eye(n)), dim=0).double(), torch.cat(\n                (-RATIO*torch.ones(1, 1), RATIO*torch.ones(1, 1), torch.zeros(n, 1)), dim=0).double()\n            if softcon is None:\n                C, d, alpha = A, b, (AP * math.sqrt(n) * torch.ones(n + 2, 1)).double()\n            else:\n                C, d, alpha = torch.cat((A, C0), dim=0).double(), torch.cat((b, d0), dim=0).double(), torch.cat(((AP * math.sqrt(n) * torch.ones(n + 2, 1).double()), alpha0), dim=0).double()\n                softcon = C0, d0, alpha0\n            if repeat: # control the variant\n                predictions = labels\n                pass\n            elif not lianxu:\n                if batch_idx % 2 == 0: # train the two components in turn.\n                    predictions = labels\n                else:\n                    Q = Q_real\n            x, opt_x = getopt(A.numpy(), b.numpy(), Q.detach().numpy(), predictions.view(-1, 1).detach().numpy(), softcon), getopt(A.numpy(), b.numpy(), Q_real.numpy(), labels.view(-1, 1).numpy(), softcon)\n            val = getval(Q_real, labels.view(-1, 1), torch.from_numpy(x).double(), softcon)\n            if val > getval(Q_real, labels.view(-1, 1), torch.from_numpy(opt_x).double(), softcon):\n                exit(0)\n            optval = getval(Q_real, labels.view(-1, 1), torch.from_numpy(opt_x).double(), softcon)\n            opt_mae.append(torch.nn.L1Loss()(torch.from_numpy(x).double(), torch.from_numpy(opt_x).double()).item())\n            plts.append((optval - val)/RATIO)\n            if evaluate:\n                forward_time += time.time() - forward_start_time\n                inference_start_time = time.time()\n                # FIXME:solve with gurobi and finally set obj = xxx.\n                if softcon is None:\n                    C0, d0, alpha0 = 0, 0, 0\n                obj = quad_surro_tensor.apply(A, b, C, d, alpha, C0, d0, alpha0, Q, Q_real, predictions, labels, KK)\n                val = getval(Q_real, labels.view(-1, 1), torch.from_numpy(getopt(A.numpy(), b.numpy(), Q.detach().numpy(), predictions.view(-1, 1).detach().numpy(), softcon)).double(), softcon)\n                inference_time += time.time() - inference_start_time\n            else:\n                obj = torch.Tensor([0])\n\n            # ====================== back-prop =====================\n            optimizer.zero_grad()\n            backward_start_time = time.time()\n            BATCH_SIZE = 1 if not repeat else 1\n            reg = 0\n            for parameter in model.parameters():\n                reg += torch.norm(parameter, 1)\n            for parameter in covariance_model.parameters():\n                reg += torch.norm(parameter, 1)\n            batchloss = batchloss - obj #+ 0.0001 * reg # + 1 * torch.nn.L1Loss()(Q, Q_real) + 10 * torch.nn.L1Loss()(predictions, labels) # now with regularization term.\n            train_objs.append(val.item() / RATIO)\n            opts.append(optval / RATIO)\n            if batch_idx % BATCH_SIZE == BATCH_SIZE - 1 or batch_idx % 2028 == 2027:\n                batchloss.backward()\n                gradnorm = 0\n                if repeat:\n                    #for parameter in model.parameters():\n                    #    parameter.grad = torch.clamp(parameter.grad, min=-0.2 * MAX_NORM, max=0.2 * MAX_NORM)\n                    msetheta.append(torch.nn.MSELoss()(predictions, labels).item())\n                    maetheta.append(torch.nn.L1Loss()(predictions, labels).item())\n\n                    for parameter in covariance_model.parameters():\n                        parameter.grad = torch.clamp(parameter.grad, min=-0.2*MAX_NORM, max=0.2*MAX_NORM)\n                        gradnorm += torch.norm(parameter.grad, 2)\n                    grdnm.append(gradnorm.item())\n                    mseQ.append(torch.nn.MSELoss()((RATIO ** 2) * Q,(RATIO ** 2) * Q_real).item())\n                    maeQ.append(torch.nn.L1Loss()((RATIO ** 2) * Q, (RATIO ** 2) * Q_real).item())\n                else:\n                    if not lianxu:\n                        if batch_idx % 2 == 1:\n                            for parameter in model.parameters():\n                                parameter.grad = torch.clamp(parameter.grad, min=-0.2*MAX_NORM, max=0.2*MAX_NORM)\n                            msetheta.append(torch.nn.MSELoss()(RATIO * predictions, RATIO * labels).item())\n                            maetheta.append(torch.nn.L1Loss()(RATIO * predictions, RATIO * labels).item())\n                        else:\n                            for parameter in covariance_model.parameters():\n                                parameter.grad = torch.clamp(parameter.grad, min=-0.1*MAX_NORM, max=0.1*MAX_NORM)\n                            mseQ.append(torch.nn.MSELoss()((RATIO ** 2) * Q, (RATIO ** 2) * Q_real).item())\n                            maeQ.append(torch.nn.L1Loss()((RATIO ** 2) * Q, (RATIO ** 2) * Q_real).item())\n                    else:\n                        for parameter in model.parameters():\n                            parameter.grad = torch.clamp(parameter.grad, min=-0.2 * MAX_NORM, max=0.2 * MAX_NORM)\n                            gradnorm += torch.norm(parameter.grad, 2)\n                        for parameter in covariance_model.parameters():\n                            parameter.grad = torch.clamp(parameter.grad, min=-0.2 * MAX_NORM, max=0.2 * MAX_NORM)\n                            gradnorm += torch.norm(parameter.grad, 2)\n                        grdnm.append(gradnorm.item())\n                        msetheta.append(torch.nn.MSELoss()(predictions, labels).item())\n                        maetheta.append(torch.nn.L1Loss()(predictions, labels).item())\n                        mseQ.append(torch.nn.MSELoss()(Q, Q_real).item())\n                        maeQ.append(torch.nn.L1Loss()(Q, Q_real).item())\n                optimizer.step()\n                backward_time += time.time() - backward_start_time\n                train_losses.append(loss.item())\n\n                tqdm_loader.set_postfix(loss=f'{loss.item():.6f}', obj=f'{val.item()/RATIO*100:.6f}%',optimal=f'{optval.item()/RATIO*100:.6f}%') # this is val! not obj!\n            batchloss = torch.zeros(1)\n            if batch_idx % 2028 == 2027: # for debugging.\n                print(\"maeQ:\", sum(maeQ[-2028:])/2028, \"maetheta:\", sum(maetheta[-2028:])/2028)\n                drawpic()\n                print(\"turn: #\",turn)\n                plt.title(\"MAE of matrix Q\")\n                plt.plot([i for i in range(len(maeQ))], maeQ)\n                name=\"onetarget\"\n                plt.savefig(\"results/pic/\"+OUTPUT_NAME+\"/\"+name+\"_maeQ_\"+str(turn)+\".pdf\")\n                plt.cla()\n                plt.title(\"The total L2 norm of gradients\")\n                plt.semilogy([i for i in range(len(grdnm))], grdnm)\n                plt.savefig(\"results/pic/\"+OUTPUT_NAME+\"/\"+name+\"_gradnorm_\"+str(turn)+\".pdf\")\n                plt.cla()\n                plt.title(\"MAE of theta\")\n                plt.plot([i for i in range(len(maetheta))], maetheta)\n                plt.savefig(\"results/pic/\"+OUTPUT_NAME+\"/\"+name+\"_maetheta_\"+str(turn)+\".pdf\")\n                plt.cla()\n                plt.title(\"MAE of optimal x (sum(x)=20)\")\n                plt.semilogy([i for i in range(len(opt_mae))], opt_mae)\n                plt.savefig(\"results/pic/\"+OUTPUT_NAME+\"/\" + name + \"_optMAE_\" + str(turn) + \".pdf\")\n                plt.cla()\n                plt.title(\"MSE of Q\")\n                plt.plot([i for i in range(len(maeQ))], mseQ)\n                plt.savefig(\"results/pic/\"+OUTPUT_NAME+\"/\"+name+\"_mseQ_\" + str(turn) + \".pdf\")\n                plt.cla()\n                plt.title(\"MSE of theta\")\n                plt.plot([i for i in range(len(maetheta))], msetheta)\n                plt.savefig(\"results/pic/\"+OUTPUT_NAME+\"/\"+name+\"_msetheta_\" + str(turn) + \".pdf\")\n                plt.cla()\n                plt.title(\"regret\")\n                plt.semilogy([i for i in range(len(plts))], plts)\n                plt.savefig(\"results/pic/\"+OUTPUT_NAME+\"/\" + name + \"_regret_\" + str(turn) + \".pdf\")\n                plt.cla()\n                print(\"total regret:\", sum(plts).item(), \"avg per case:\", sum(plts).item() / 2028 * 100,\"%\")\n                # plts, maeQ, mseQ, maetheta, msetheta = [], [], [], [], []\n        turn += 1\n    average_loss    = np.mean(train_losses)\n    average_obj     = np.mean(train_objs)\n    print(\"train_objs:\", train_objs, \"opts:\", np.mean(opts), len(train_objs), len(opts))\n    return average_loss, average_obj, (forward_time, inference_time, qp_time, backward_time)\n\n# ==========================================\n# File: resource_provisioning/util.py\n# Function/Context: getconstrlist, merge_constraints\n# ==========================================\nimport torch.nn as nn\nimport time\nimport numpy as np\nimport torch\nimport math\nfrom config import *\n\ntorch.random.manual_seed(179090813)\n\ndef rounding(x, eps):\n    if math.fmod(x, 1) < eps: x = math.floor(x)\n    elif math.fmod(x, 1) >= 1 - eps: x = math.ceil(x)\n    return x\n\ndef getconstrlist(x, C, d):\n    # print(C.shape, x.shape, d.shape)\n    constr = np.matmul(C, x.reshape(-1, 1)) - d.reshape(-1, 1)\n    idx_none, idx_linear, idx_quad = [], [], []\n    for i in range(C.shape[0]):\n        if constr[i] < - 1 / (4 * QUAD_SOFT_K): idx_none.append(i)\n        if -1 / (4 * QUAD_SOFT_K) <= constr[i] and constr[i] <= 1 / (4 * QUAD_SOFT_K): idx_quad.append(i)\n        if constr[i] > 1 / (4 * QUAD_SOFT_K): idx_linear.append(i)\n    return idx_quad, idx_linear, idx_none\n\ndef merge_constraints(A0, b0, C0, d0, alpha1, alpha2):\n    N, M1, M2 = A0.shape[1], A0.shape[0], C0.shape[0]\n    # merged data\n    # print(C0.shape, A0.shape)\n    C = np.concatenate((C0, -C0, A0, -np.identity(N)), axis=0)\n    alpha = np.concatenate((alpha1, alpha2, math.sqrt(N) * 100 * np.ones((M1, 1)), 100 * np.ones((N, 1))), axis=0)\n    d = np.concatenate((d0, -d0, b0, np.zeros((N, 1))), axis=0)\n    return C, d, alpha\n\n# ==========================================\n# File: synthetic_linear_programming/util.py\n# Function/Context: getconstrlist, merge_constraints\n# ==========================================\nimport torch.nn as nn\nimport time\nimport numpy as np\nimport torch\nimport math\nfrom config import *\n\ndef getconstrlist(x, C, d):\n    # print(C.shape, x.shape, d.shape)\n    constr = np.matmul(C, x.reshape(-1, 1)) - d.reshape(-1, 1)\n    idx_none, idx_linear, idx_quad = [], [], []\n    for i in range(C.shape[0]):\n        if constr[i] < - 1 / (4 * QUAD_SOFT_K): idx_none.append(i)\n        if -1 / (4 * QUAD_SOFT_K) <= constr[i] and constr[i] <= 1 / (4 * QUAD_SOFT_K): idx_quad.append(i)\n        if constr[i] > 1 / (4 * QUAD_SOFT_K): idx_linear.append(i)\n    return idx_quad, idx_linear, idx_none\n\ndef merge_constraints(A0, b0, C0, d0, alpha0, theta):\n    N, M1, M2 = A0.shape[1], A0.shape[0], C0.shape[0]\n    # merged data\n    C = np.concatenate((C0, A0, -np.identity(N)), axis=0)\n    alpha = np.concatenate((alpha0, math.sqrt(N) * 5 * np.max(np.abs(theta)) * np.ones((M1, 1)), np.zeros((N, 1))), axis=0)\n    d = np.concatenate((d0, b0, np.zeros((N, 1))), axis=0)\n    alpha[M1 + M2: M1 + M2 + N] = 20 * np.ones((theta.shape[0], 1))\n    return C, d, alpha",
  "description": "Combined Analysis:\n- [portfolio_optimization/main.py]: This file implements the main training pipeline for the portfolio optimization experiment using the surrogate objective framework. Key aspects: 1) It sets up soft constraints (C0, d0, alpha0) exactly matching the paper's objective with penalty terms α^T max(Cx-d,0). 2) It implements the 'ours' method (method=3) which corresponds to the surrogate objective framework, calling specialized training functions (ours_train_portfolio, ours_validate_portfolio, ours_test_portfolio). 3) The code handles the prediction+optimization pipeline where models predict parameters for the optimization problem. 4) It uses differentiable optimization layers (qpth, cvxpylayers) for gradient computation. 5) The soft constraint generation (C0, d0, alpha0) and training loop structure directly align with the paper's algorithm steps for surrogate-based training.\n- [portfolio_optimization/portfolio_utils.py]: This file implements the core training logic for the portfolio optimization experiment using the surrogate objective framework. The key function 'ours_train_portfolio' handles: 1) Prediction of parameters (returns theta and covariance matrix Q), 2) Formulation of hard constraints (A, b) and soft constraints (C, d, alpha), 3) Computation of optimal decisions using getopt(), 4) Application of the surrogate objective via quad_surro_tensor.apply() for gradient computation, 5) Backpropagation through both prediction models. The code matches the paper's approach for handling quadratic programming with soft constraints in prediction+optimization problems.\n- [resource_provisioning/util.py]: This file implements two key components of the surrogate objective framework for soft constraints. 1) getconstrlist: Implements the constraint categorization step (Algorithm 1) by evaluating Cx-d and classifying constraints into three regions (none/quadratic/linear) based on the piecewise surrogate function S(z) with threshold 1/(4*QUAD_SOFT_K). This directly corresponds to determining active constraint sets for gradient computation. 2) merge_constraints: Implements the constraint softening transformation (Section 3.1) by converting hard constraints Ax≤b and x≥0 into soft constraints with large penalty weights (100*sqrt(N) and 100). The function returns the combined C, d, α for the surrogate objective g(θ,x) - α^T S(Cx-d).\n- [synthetic_linear_programming/util.py]: This file implements key components of the surrogate objective framework: 1) getconstrlist() classifies constraints into three regions (quadratic, linear, inactive) based on the value of Cx-d relative to thresholds derived from QUAD_SOFT_K (the K parameter in the piecewise surrogate function). This corresponds to determining which constraints are active and in which region for gradient computation. 2) merge_constraints() implements Step 1 (Soften Hard Constraints) by converting hard constraints Ax≤b and x≥0 into soft constraints with large penalty coefficients α, creating a unified optimization problem with only soft constraints.",
  "dependencies": [
    "portfolio_utils.ours_validate_portfolio",
    "tqdm",
    "utils.normalize_matrix_qr",
    "argparse",
    "utils.normalize_vector",
    "pandas",
    "data_utils.SP500DataLoader",
    "portfolio_utils.computeCovariance",
    "portfolio_utils.train_portfolio",
    "calc_ours.quad_surro_tensor",
    "cvxpylayers",
    "numpy",
    "config.QUAD_SOFT_K",
    "portfolio_utils.test_portfolio",
    "torch.utils.data",
    "time",
    "config.RATIO",
    "model.CovarianceModel",
    "torch",
    "calc_ours.drawpic",
    "datetime",
    "portfolio_utils.validate_portfolio",
    "utils.normalize_matrix_positive",
    "utils.normalize_matrix",
    "cvxpy",
    "sqrtm.sqrtm",
    "math",
    "calc_ours.getopt",
    "model.PortfolioModel",
    "utils.normalize_projection",
    "torch.nn",
    "qpth",
    "portfolio_utils.ours_test_portfolio",
    "config.OUTPUT_NAME",
    "scipy",
    "calc_ours.getval",
    "portfolio_utils.generateDataset",
    "random",
    "portfolio_utils.ours_train_portfolio",
    "gurobipy",
    "matplotlib.pyplot"
  ]
}