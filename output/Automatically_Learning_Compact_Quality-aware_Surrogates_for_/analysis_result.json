{
  "paper_id": "Automatically_Learning_Compact_Quality-aware_Surrogates_for_",
  "title": "Automatically Learning Compact Quality-aware Surrogates for Optimization Problems",
  "abstract": "Solving optimization problems with unknown parameters often requires learning a predictive model to predict the values of the unknown parameters and then solving the problem using these values. Recent work has shown that including the optimization problem as a layer in the model training pipeline results in predictions of the unobserved parameters that lead to higher decision quality. Unfortunately, this process comes at a large computational cost because the optimization problem must be solved and differentiated through in each training iteration; furthermore, it may also sometimes fail to improve solution quality due to non-smoothness issues that arise when training through a complex optimization layer. To address these shortcomings, we learn a low-dimensional surrogate model of a large optimization problem by representing the feasible space in terms of meta-variables, each of which is a linear combination of the original variables. By training a low-dimensional surrogate model end-to-end, and jointly with the predictive model, we achieve: i) a large reduction in training and inference time; and ii) improved performance by focusing attention on the more important variables in the optimization and learning in a smoother space. Empirically, we demonstrate these improvements on a non-convex adversary modeling task, a submodular recommendation task and a convex portfolio optimization task.",
  "problem_description_natural": "The paper addresses decision-making under uncertainty where key parameters of an optimization problem are unknown at decision time but can be predicted from available features. The goal is to learn a predictive model that maps features to parameter estimates such that, when these estimates are plugged into the optimization problem, the resulting decisions perform well on the true (unknown) parameters. Instead of solving the full optimization problem during training—which is computationally expensive and may cause non-smooth gradients—the authors propose replacing it with a compact surrogate problem defined over a low-dimensional space of 'meta-variables', each a linear combination of the original decision variables. This surrogate is learned jointly with the predictive model to maximize decision quality while reducing computational cost.",
  "problem_type": "Mixed-type optimization (includes convex, submodular, and non-convex problems)",
  "datasets": [
    "MovieLens",
    "Quandl WIKI dataset",
    "Random geometric graphs"
  ],
  "performance_metrics": [
    "Regret"
  ],
  "lp_model": {
    "objective": "\\min_{\\mathbf{y}} f(P\\mathbf{y}, \\theta)",
    "constraints": [
      "h(P\\mathbf{y}) \\leq 0",
      "A P \\mathbf{y} = b"
    ],
    "variables": [
      "\\mathbf{y} \\in \\mathbb{R}^m: surrogate decision variables, where m is the reduced dimensionality"
    ]
  },
  "raw_latex_model": "\\min_{\\mathbf{y}} \\ g_P(\\mathbf{y}, \\theta) := f(P\\mathbf{y}, \\theta) \\quad \\text{s.t.} \\quad h(P\\mathbf{y}) \\leq 0, \\quad AP\\mathbf{y} = b",
  "algorithm_description": "1. Initialize the predictive model parameters w and the reparameterization matrix P. 2. For each training iteration: a. Predict parameters θ from features ξ using the model Φ(ξ, w). b. Solve the surrogate optimization problem: minimize f(Py, θ) subject to h(Py) ≤ 0 and APy = b to obtain optimal surrogate solution y*. c. Compute the original solution x* = Py*. d. Evaluate the loss based on the true parameters θ_true: f(x*, θ_true). e. Compute gradients of the loss with respect to w and P by differentiating through the KKT conditions of the surrogate problem. f. Update w and P using gradient descent (e.g., Adam optimizer). 3. Repeat until convergence (e.g., early stopping on validation set). After training, for inference: use the learned P and Φ to predict θ and solve the surrogate problem to obtain decisions."
}