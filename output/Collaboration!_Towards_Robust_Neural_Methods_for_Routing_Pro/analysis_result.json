{
  "paper_id": "Collaboration!_Towards_Robust_Neural_Methods_for_Routing_Pro",
  "title": "Collaboration! Towards Robust Neural Methods for Routing Problems",
  "abstract": "Despite enjoying desirable efficiency and reduced reliance on domain expertise, existing neural methods for vehicle routing problems (VRPs) suffer from severe robustness issues – their performance significantly deteriorates on clean instances with crafted perturbations. To enhance robustness, we propose an ensemble-based Collaborative Neural Framework (CNF) w.r.t. the defense of neural VRP methods, which is crucial yet underexplored in the literature. Given a neural VRP method, we adversarially train multiple models in a collaborative manner to synergistically promote robustness against attacks, while boosting standard generalization on clean instances. A neural router is designed to adeptly distribute training instances among models, enhancing overall load balancing and collaborative efficacy. Extensive experiments verify the effectiveness and versatility of CNF in defending against various attacks across different neural VRP methods. Notably, our approach also achieves impressive out-of-distribution generalization on benchmark instances.",
  "problem_description_natural": "The paper addresses the vulnerability of neural methods for vehicle routing problems (VRPs) to adversarial perturbations—small, crafted modifications to input instances that cause significant performance degradation. The goal is to improve both standard generalization (performance on clean, unperturbed instances) and adversarial robustness (performance on perturbed instances) without sacrificing one for the other. The authors focus on classic VRP variants including the Traveling Salesman Problem (TSP), Asymmetric TSP (ATSP), and Capacitated Vehicle Routing Problem (CVRP), where the objective is to find optimal or near-optimal tours that minimize total travel cost while satisfying problem-specific constraints (e.g., visiting each node exactly once in TSP, or respecting vehicle capacity in CVRP).",
  "problem_type": "combinatorial optimization",
  "datasets": [
    "TSPLIB",
    "CVRPLIB",
    "Rotation (100)",
    "Explosion (100)",
    "Uniform (50)",
    "Uniform (200)",
    "Rotation (200)",
    "Explosion (200)"
  ],
  "performance_metrics": [
    "Optimality Gap",
    "Inference Time"
  ],
  "lp_model": {
    "objective": "\\min_{\\tau \\in \\Phi} c(\\tau | x)",
    "constraints": [
      "For TSP: \\tau must visit each node exactly once and return to the starting node.",
      "For CVRP: \\tau must consist of multiple sub-tours starting and ending at the depot, each customer node visited exactly once, and the total demand in each sub-tour \\leq Q."
    ],
    "variables": [
      "Tour \\tau as a sequence of nodes."
    ]
  },
  "raw_latex_model": "\\tau^* = \\arg\\min_{\\tau \\in \\Phi} c(\\tau | x)",
  "algorithm_description": "The Collaborative Neural Framework (CNF) algorithm for adversarial training of neural VRP methods involves the following steps:\n1. Initialize M models from a pretrained model and initialize the neural router.\n2. For each training step e from 1 to E:\n   a. Sample a batch of clean instances.\n   b. Initialize local and global adversarial instances to the clean instances.\n   c. For t from 1 to T (attack steps):\n      i. For each model, update local adversarial instances by maximizing the loss (inner maximization).\n      ii. Update global adversarial instances by attacking the best-performing model among all models.\n   d. Collect clean, local adversarial, and global adversarial instances into a set.\n   e. Evaluate all models on these instances to obtain a cost matrix.\n   f. The neural router takes the instances and cost matrix as input, outputs a logit matrix, and converts it to a probability matrix via Softmax.\n   g. For each model, select instances with the TopK largest probabilities for training.\n   h. Train each model on its selected instances using reinforcement learning (e.g., REINFORCE).\n   i. Evaluate all models again on the instances to obtain a new cost matrix.\n   j. Update the neural router using the difference in minimum costs (before and after training) as a reward signal, via gradient ascent.\n3. After training, output the set of trained models for inference, discarding the neural router."
}