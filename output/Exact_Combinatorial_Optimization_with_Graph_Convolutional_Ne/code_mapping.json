{
  "file_path": "03_train_gcnn.py, 04_test.py, 05_evaluate.py, models/baseline/model.py, models/mean_convolution/model.py, models/no_prenorm/model.py",
  "function_name": "process, process, PolicyBranching.branchexeclp, GCNPolicy, GCNPolicy, GCNPolicy",
  "code_snippet": "\n\n# ==========================================\n# File: 03_train_gcnn.py\n# Function/Context: process\n# ==========================================\nimport os\nimport importlib\nimport argparse\nimport sys\nimport pathlib\nimport pickle\nimport numpy as np\nfrom time import strftime\nfrom shutil import copyfile\nimport gzip\n\nimport tensorflow as tf\nimport tensorflow.contrib.eager as tfe\n\nimport utilities\nfrom utilities import log\n\nfrom utilities_tf import load_batch_gcnn\n\n\ndef load_batch_tf(x):\n    return tf.py_func(\n        load_batch_gcnn,\n        [x],\n        [tf.float32, tf.int32, tf.float32, tf.float32, tf.int32, tf.int32, tf.int32, tf.int32, tf.int32, tf.float32])\n\n\ndef pretrain(model, dataloader):\n    \"\"\"\n    Pre-normalizes a model (i.e., PreNormLayer layers) over the given samples.\n\n    Parameters\n    ----------\n    model : model.BaseModel\n        A base model, which may contain some model.PreNormLayer layers.\n    dataloader : tf.data.Dataset\n        Dataset to use for pre-training the model.\n    Return\n    ------\n    number of PreNormLayer layers processed.\n    \"\"\"\n    model.pre_train_init()\n    i = 0\n    while True:\n        for batch in dataloader:\n            c, ei, ev, v, n_cs, n_vs, n_cands, cands, best_cands, cand_scores = batch\n            batched_states = (c, ei, ev, v, n_cs, n_vs)\n\n            if not model.pre_train(batched_states, tf.convert_to_tensor(True)):\n                break\n\n        res = model.pre_train_next()\n        if res is None:\n            break\n        else:\n            layer, name = res\n\n        i += 1\n\n    return i\n\n\ndef process(model, dataloader, top_k, optimizer=None):\n    mean_loss = 0\n    mean_kacc = np.zeros(len(top_k))\n\n    n_samples_processed = 0\n    for batch in dataloader:\n        c, ei, ev, v, n_cs, n_vs, n_cands, cands, best_cands, cand_scores = batch\n        batched_states = (c, ei, ev, v, tf.reduce_sum(n_cs, keepdims=True), tf.reduce_sum(n_vs, keepdims=True))  # prevent padding\n        batch_size = len(n_cs.numpy())\n\n        if optimizer:\n            with tf.GradientTape() as tape:\n                logits = model(batched_states, tf.convert_to_tensor(True)) # training mode\n                logits = tf.expand_dims(tf.gather(tf.squeeze(logits, 0), cands), 0)  # filter candidate variables\n                logits = model.pad_output(logits, n_cands.numpy())  # apply padding now\n                loss = tf.losses.sparse_softmax_cross_entropy(labels=best_cands, logits=logits)\n            grads = tape.gradient(target=loss, sources=model.variables)\n            optimizer.apply_gradients(zip(grads, model.variables))\n        else:\n            logits = model(batched_states, tf.convert_to_tensor(False))  # eval mode\n            logits = tf.expand_dims(tf.gather(tf.squeeze(logits, 0), cands), 0)  # filter candidate variables\n            logits = model.pad_output(logits, n_cands.numpy())  # apply padding now\n            loss = tf.losses.sparse_softmax_cross_entropy(labels=best_cands, logits=logits)\n\n        true_scores = model.pad_output(tf.reshape(cand_scores, (1, -1)), n_cands)\n        true_bestscore = tf.reduce_max(true_scores, axis=-1, keepdims=True)\n        true_scores = true_scores.numpy()\n        true_bestscore = true_bestscore.numpy()\n\n        kacc = []\n        for k in top_k:\n            pred_top_k = tf.nn.top_k(logits, k=k)[1].numpy()\n            pred_top_k_true_scores = np.take_along_axis(true_scores, pred_top_k, axis=1)\n            kacc.append(np.mean(np.any(pred_top_k_true_scores == true_bestscore, axis=1)))\n        kacc = np.asarray(kacc)\n\n        mean_loss += loss.numpy() * batch_size\n        mean_kacc += kacc * batch_size\n        n_samples_processed += batch_size\n\n    mean_loss /= n_samples_processed\n    mean_kacc /= n_samples_processed\n\n    return mean_loss, mean_kacc\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        'problem',\n        help='MILP instance type to process.',\n        choices=['setcover', 'cauctions', 'facilities', 'indset'],\n    )\n    parser.add_argument(\n        '-m', '--model',\n        help='GCNN model to be trained.',\n        type=str,\n        default='baseline',\n    )\n    parser.add_argument(\n        '-s', '--seed',\n        help='Random generator seed.',\n        type=utilities.valid_seed,\n        default=0,\n    )\n    parser.add_argument(\n        '-g', '--gpu',\n        help='CUDA GPU id (-1 for CPU).',\n        type=int,\n        default=0,\n    )\n    args = parser.parse_args()\n\n    ### HYPER PARAMETERS ###\n    max_epochs = 1000\n    epoch_size = 312\n    batch_size = 32\n    pretrain_batch_size = 128\n    valid_batch_size = 128\n    lr = 0.001\n    patience = 10\n    early_stopping = 20\n    top_k = [1, 3, 5, 10]\n    train_ncands_limit = np.inf\n    valid_ncands_limit = np.inf\n\n    problem_folders = {\n        'setcover': 'setcover/500r_1000c_0.05d',\n        'cauctions': 'cauctions/100_500',\n        'facilities': 'facilities/100_100_5',\n        'indset': 'indset/500_4',\n    }\n    problem_folder = problem_folders[args.problem]\n\n    running_dir = f\"trained_models/{args.problem}/{args.model}/{args.seed}\"\n\n    os.makedirs(running_dir)\n\n    ### LOG ###\n    logfile = os.path.join(running_dir, 'log.txt')\n\n    log(f\"max_epochs: {max_epochs}\", logfile)\n    log(f\"epoch_size: {epoch_size}\", logfile)\n    log(f\"batch_size: {batch_size}\", logfile)\n    log(f\"pretrain_batch_size: {pretrain_batch_size}\", logfile)\n    log(f\"valid_batch_size : {valid_batch_size }\", logfile)\n    log(f\"lr: {lr}\", logfile)\n    log(f\"patience : {patience }\", logfile)\n    log(f\"early_stopping : {early_stopping }\", logfile)\n    log(f\"top_k: {top_k}\", logfile)\n    log(f\"problem: {args.problem}\", logfile)\n    log(f\"gpu: {args.gpu}\", logfile)\n    log(f\"seed {args.seed}\", logfile)\n\n    ### NUMPY / TENSORFLOW SETUP ###\n    if args.gpu == -1:\n        os.environ['CUDA_VISIBLE_DEVICES'] = ''\n    else:\n        os.environ['CUDA_VISIBLE_DEVICES'] = f'{args.gpu}'\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    tf.enable_eager_execution(config)\n    tf.executing_eagerly()\n\n    rng = np.random.RandomState(args.seed)\n    tf.set_random_seed(rng.randint(np.iinfo(int).max))\n\n    ### SET-UP DATASET ###\n    train_files = list(pathlib.Path(f'data/samples/{problem_folder}/train').glob('sample_*.pkl'))\n    valid_files = list(pathlib.Path(f'data/samples/{problem_folder}/valid').glob('sample_*.pkl'))\n\n\n    def take_subset(sample_files, cands_limit):\n        nsamples = 0\n        ncands = 0\n        for filename in sample_files:\n            with gzip.open(filename, 'rb') as file:\n                sample = pickle.load(file)\n\n            _, _, _, cands, _ = sample['data']\n            ncands += len(cands)\n            nsamples += 1\n\n            if ncands >= cands_limit:\n                log(f\"  dataset size limit reached ({cands_limit} candidate variables)\", logfile)\n                break\n\n        return sample_files[:nsamples]\n\n\n    if train_ncands_limit < np.inf:\n        train_files = take_subset(rng.permutation(train_files), train_ncands_limit)\n    log(f\"{len(train_files)} training samples\", logfile)\n    if valid_ncands_limit < np.inf:\n        valid_files = take_subset(valid_files, valid_ncands_limit)\n    log(f\"{len(valid_files)} validation samples\", logfile)\n\n    train_files = [str(x) for x in train_files]\n    valid_files = [str(x) for x in valid_files]\n\n    valid_data = tf.data.Dataset.from_tensor_slices(valid_files)\n    valid_data = valid_data.batch(valid_batch_size)\n    valid_data = valid_data.map(load_batch_tf)\n    valid_data = valid_data.prefetch(1)\n\n    pretrain_files = [f for i, f in enumerate(train_files) if i % 10 == 0]\n    pretrain_data = tf.data.Dataset.from_tensor_slices(pretrain_files)\n    pretrain_data = pretrain_data.batch(pretrain_batch_size)\n    pretrain_data = pretrain_data.map(load_batch_tf)\n    pretrain_data = pretrain_data.prefetch(1)\n\n    ### MODEL LOADING ###\n    sys.path.insert(0, os.path.abspath(f'models/{args.model}'))\n    import model\n    importlib.reload(model)\n    model = model.GCNPolicy()\n    del sys.path[0]\n\n    ### TRAINING LOOP ###\n    optimizer = tf.train.AdamOptimizer(learning_rate=lambda: lr)  # dynamic LR trick\n    best_loss = np.inf\n    for epoch in range(max_epochs + 1):\n        log(f\"EPOCH {epoch}...\", logfile)\n        epoch_loss_avg = tfe.metrics.Mean()\n        epoch_accuracy = tfe.metrics.Accuracy()\n\n        # TRAIN\n        if epoch == 0:\n            n = pretrain(model=model, dataloader=pretrain_data)\n            log(f\"PRETRAINED {n} LAYERS\", logfile)\n            # model compilation\n            model.call = tfe.defun(model.call, input_signature=model.input_signature)\n        else:\n            # bugfix: tensorflow's shuffle() seems broken...\n            epoch_train_files = rng.choice(train_files, epoch_size * batch_size, replace=True)\n            train_data = tf.data.Dataset.from_tensor_slices(epoch_train_files)\n            train_data = train_data.batch(batch_size)\n            train_data = train_data.map(load_batch_tf)\n            train_data = train_data.prefetch(1)\n            train_loss, train_kacc = process(model, train_data, top_k, optimizer)\n            log(f\"TRAIN LOSS: {train_loss:0.3f} \" + \"\".join([f\" acc@{k}: {acc:0.3f}\" for k, acc in zip(top_k, train_kacc)]), logfile)\n\n        # TEST\n        valid_loss, valid_kacc = process(model, valid_data, top_k, None)\n        log(f\"VALID LOSS: {valid_loss:0.3f} \" + \"\".join([f\" acc@{k}: {acc:0.3f}\" for k, acc in zip(top_k, valid_kacc)]), logfile)\n\n        if valid_loss < best_loss:\n            plateau_count = 0\n            best_loss = valid_loss\n            model.save_state(os.path.join(running_dir, 'best_params.pkl'))\n            log(f\"  best model so far\", logfile)\n        else:\n            plateau_count += 1\n            if plateau_count % early_stopping == 0:\n                log(f\"  {plateau_count} epochs without improvement, early stopping\", logfile)\n                break\n            if plateau_count % patience == 0:\n                lr *= 0.2\n                log(f\"  {plateau_count} epochs without improvement, decreasing learning rate to {lr}\", logfile)\n\n    model.restore_state(os.path.join(running_dir, 'best_params.pkl'))\n    valid_loss, valid_kacc = process(model, valid_data, top_k, None)\n    log(f\"BEST VALID LOSS: {valid_loss:0.3f} \" + \"\".join([f\" acc@{k}: {acc:0.3f}\" for k, acc in zip(top_k, valid_kacc)]), logfile)\n\n# ==========================================\n# File: 04_test.py\n# Function/Context: process\n# ==========================================\nimport os\nimport sys\nimport importlib\nimport argparse\nimport csv\nimport numpy as np\nimport time\nimport pickle\nimport pathlib\nimport gzip\n\nimport tensorflow as tf\nimport tensorflow.contrib.eager as tfe\n\nimport svmrank\n\nimport utilities\n\nfrom utilities_tf import load_batch_gcnn\n\n\ndef load_batch_flat(sample_files, feats_type, augment_feats, normalize_feats):\n    cand_features = []\n    cand_choices = []\n    cand_scoress = []\n\n    for i, filename in enumerate(sample_files):\n        cand_states, cand_scores, cand_choice = utilities.load_flat_samples(filename, feats_type, 'scores', augment_feats, normalize_feats)\n\n        cand_features.append(cand_states)\n        cand_choices.append(cand_choice)\n        cand_scoress.append(cand_scores)\n\n    n_cands_per_sample = [v.shape[0] for v in cand_features]\n\n    cand_features = np.concatenate(cand_features, axis=0).astype(np.float32, copy=False)\n    cand_choices = np.asarray(cand_choices).astype(np.int32, copy=False)\n    cand_scoress = np.concatenate(cand_scoress, axis=0).astype(np.float32, copy=False)\n    n_cands_per_sample = np.asarray(n_cands_per_sample).astype(np.int32, copy=False)\n\n    return cand_features, n_cands_per_sample, cand_choices, cand_scoress\n\n\ndef padding(output, n_vars_per_sample, fill=-1e8):\n    n_vars_max = tf.reduce_max(n_vars_per_sample)\n\n    output = tf.split(\n        value=output,\n        num_or_size_splits=n_vars_per_sample,\n        axis=1,\n    )\n    output = tf.concat([\n        tf.pad(\n            x,\n            paddings=[[0, 0], [0, n_vars_max - tf.shape(x)[1]]],\n            mode='CONSTANT',\n            constant_values=fill)\n        for x in output\n    ], axis=0)\n\n    return output\n\n\ndef process(policy, dataloader, top_k):\n    mean_kacc = np.zeros(len(top_k))\n\n    n_samples_processed = 0\n    for batch in dataloader:\n\n        if policy['type'] == 'gcnn':\n            c, ei, ev, v, n_cs, n_vs, n_cands, cands, best_cands, cand_scores = batch\n\n            pred_scores = policy['model']((c, ei, ev, v, tf.reduce_sum(n_cs, keepdims=True), tf.reduce_sum(n_vs, keepdims=True)), tf.convert_to_tensor(False))\n\n            # filter candidate variables\n            pred_scores = tf.expand_dims(tf.gather(tf.squeeze(pred_scores, 0), cands), 0)\n\n        elif policy['type'] == 'ml-competitor':\n            cand_feats, n_cands, best_cands, cand_scores = batch\n\n            # move to numpy\n            cand_feats = cand_feats.numpy()\n            n_cands = n_cands.numpy()\n\n            # feature normalization\n            cand_feats = (cand_feats - policy['feat_shift']) / policy['feat_scale']\n\n            pred_scores = policy['model'].predict(cand_feats)\n\n            # move back to TF\n            pred_scores = tf.convert_to_tensor(pred_scores.reshape((1, -1)), dtype=tf.float32)\n\n        # padding\n        pred_scores = padding(pred_scores, n_cands)\n        true_scores = padding(tf.reshape(cand_scores, (1, -1)), n_cands)\n        true_bestscore = tf.reduce_max(true_scores, axis=-1, keepdims=True)\n\n        assert all(true_bestscore.numpy() == np.take_along_axis(true_scores.numpy(), best_cands.numpy().reshape((-1, 1)), axis=1))\n\n        kacc = []\n        for k in top_k:\n            pred_top_k = tf.nn.top_k(pred_scores, k=k)[1].numpy()\n            pred_top_k_true_scores = np.take_along_axis(true_scores.numpy(), pred_top_k, axis=1)\n            kacc.append(np.mean(np.any(pred_top_k_true_scores == true_bestscore.numpy(), axis=1)))\n        kacc = np.asarray(kacc)\n\n        batch_size = int(n_cands.shape[0])\n        mean_kacc += kacc * batch_size\n        n_samples_processed += batch_size\n\n    mean_kacc /= n_samples_processed\n\n    return mean_kacc\n\n# ==========================================\n# File: 05_evaluate.py\n# Function/Context: PolicyBranching.branchexeclp\n# ==========================================\nimport os\nimport sys\nimport importlib\nimport argparse\nimport csv\nimport numpy as np\nimport time\nimport pickle\n\nimport pyscipopt as scip\n\nimport tensorflow as tf\nimport tensorflow.contrib.eager as tfe\n\nimport svmrank\n\nimport utilities\n\n\nclass PolicyBranching(scip.Branchrule):\n\n    def __init__(self, policy):\n        super().__init__()\n\n        self.policy_type = policy['type']\n        self.policy_name = policy['name']\n\n        if self.policy_type == 'gcnn':\n            model = policy['model']\n            model.restore_state(policy['parameters'])\n            self.policy = tfe.defun(model.call, input_signature=model.input_signature)\n\n        elif self.policy_type == 'internal':\n            self.policy = policy['name']\n\n        elif self.policy_type == 'ml-competitor':\n            self.policy = policy['model']\n\n            # feature parameterization\n            self.feat_shift = policy['feat_shift']\n            self.feat_scale = policy['feat_scale']\n            self.feat_specs = policy['feat_specs']\n\n        else:\n            raise NotImplementedError\n\n    def branchinitsol(self):\n        self.ndomchgs = 0\n        self.ncutoffs = 0\n        self.state_buffer = {}\n        self.khalil_root_buffer = {}\n\n    def branchexeclp(self, allowaddcons):\n\n        # SCIP internal branching rule\n        if self.policy_type == 'internal':\n            result = self.model.executeBranchRule(self.policy, allowaddcons)\n\n        # custom policy branching\n        else:\n            candidate_vars, *_ = self.model.getPseudoBranchCands()\n            candidate_mask = [var.getCol().getLPPos() for var in candidate_vars]\n\n            # initialize root buffer for Khalil features extraction\n            if self.model.getNNodes() == 1 \\\n                    and self.policy_type == 'ml-competitor' \\\n                    and self.feat_specs['type'] in ('khalil', 'all'):\n                utilities.extract_khalil_variable_features(self.model, [], self.khalil_root_buffer)\n\n            if len(candidate_vars) == 1:\n                best_var = candidate_vars[0]\n\n            elif self.policy_type == 'gcnn':\n                state = utilities.extract_state(self.model, self.state_buffer)\n\n                # convert state to tensors\n                c, e, v = state\n                state = (\n                    tf.convert_to_tensor(c['values'], dtype=tf.float32),\n                    tf.convert_to_tensor(e['indices'], dtype=tf.int32),\n                    tf.convert_to_tensor(e['values'], dtype=tf.float32),\n                    tf.convert_to_tensor(v['values'], dtype=tf.float32),\n                    tf.convert_to_tensor([c['values'].shape[0]], dtype=tf.int32),\n                    tf.convert_to_tensor([v['values'].shape[0]], dtype=tf.int32),\n                )\n\n                var_logits = self.policy(state, tf.convert_to_tensor(False)).numpy().squeeze(0)\n\n                candidate_scores = var_logits[candidate_mask]\n                best_var = candidate_vars[candidate_scores.argmax()]\n\n            elif self.policy_type == 'ml-competitor':\n\n                # build candidate features\n                candidate_states = []\n                if self.feat_specs['type'] in ('all', 'gcnn_agg'):\n                    state = utilities.extract_state(self.model, self.state_buffer)\n                    candidate_states.append(utilities.compute_extended_variable_features(state, candidate_mask))\n                if self.feat_specs['type'] in ('all', 'khalil'):\n                    candidate_states.append(utilities.extract_khalil_variable_features(self.model, candidate_vars, self.khalil_root_buffer))\n                candidate_states = np.concatenate(candidate_states, axis=1)\n\n                # feature preprocessing\n                candidate_states = utilities.preprocess_variable_features(candidate_states, self.feat_specs['augment'], self.feat_specs['qbnorm'])\n\n                # feature normalization\n                candidate_states =  (candidate_states - self.feat_shift) / self.feat_scale\n\n                candidate_scores = self.policy.predict(candidate_states)\n                best_var = candidate_vars[candidate_scores.argmax()]\n\n            else:\n                raise NotImplementedError\n\n            self.model.branchVar(best_var)\n            result = scip.SCIP_RESULT.BRANCHED\n\n        # fair node counting\n        if result == scip.SCIP_RESULT.REDUCEDDOM:\n            self.ndomchgs += 1\n        elif result == scip.SCIP_RESULT.CUTOFF:\n            self.ncutoffs += 1\n\n        return {'result': result}\n\n# ==========================================\n# File: models/baseline/model.py\n# Function/Context: GCNPolicy\n# ==========================================\nimport tensorflow as tf\nimport tensorflow.keras as K\nimport numpy as np\nimport pickle\n\n\nclass PreNormException(Exception):\n    pass\n\nclass PreNormLayer(K.layers.Layer):\n    \"\"\"\n    Our pre-normalization layer, whose purpose is to normalize an input layer\n    to zero mean and unit variance to speed-up and stabilize GCN training. The\n    layer's parameters are aimed to be computed during the pre-training phase.\n    \"\"\"\n\n    def __init__(self, n_units, shift=True, scale=True):\n        super().__init__()\n        assert shift or scale\n\n        if shift:\n            self.shift = self.add_weight(\n                name=f'{self.name}/shift',\n                shape=(n_units,),\n                trainable=False,\n                initializer=tf.keras.initializers.constant(value=np.zeros((n_units,)),\n                dtype=tf.float32),\n            )\n        else:\n            self.shift = None\n\n        if scale:\n            self.scale = self.add_weight(\n                name=f'{self.name}/scale',\n                shape=(n_units,),\n                trainable=False,\n                initializer=tf.keras.initializers.constant(value=np.ones((n_units,)),\n                dtype=tf.float32),\n            )\n        else:\n            self.scale = None\n\n        self.n_units = n_units\n        self.waiting_updates = False\n        self.received_updates = False\n\n    def build(self, input_shapes):\n        self.built = True\n\n    def call(self, input):\n        if self.waiting_updates:\n            self.update_stats(input)\n            self.received_updates = True\n            raise PreNormException\n\n        if self.shift is not None:\n            input = input + self.shift\n\n        if self.scale is not None:\n            input = input * self.scale\n\n        return input\n\n    def start_updates(self):\n        \"\"\"\n        Initializes the pre-training phase.\n        \"\"\"\n        self.avg = 0\n        self.var = 0\n        self.m2 = 0\n        self.count = 0\n        self.waiting_updates = True\n        self.received_updates = False\n\n    def update_stats(self, input):\n        \"\"\"\n        Online mean and variance estimation. See: Chan et al. (1979) Updating\n        Formulae and a Pairwise Algorithm for Computing Sample Variances.\n        https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online_algorithm\n        \"\"\"\n        assert self.n_units == 1 or input.shape[-1] == self.n_units, f\"Expected input dimension of size {self.n_units}, got {input.shape[-1]}.\"\n\n        input = tf.reshape(input, [-1, self.n_units])\n        sample_avg = tf.reduce_mean(input, 0)\n        sample_var = tf.reduce_mean((input - sample_avg) ** 2, axis=0)\n        sample_count = tf.cast(tf.size(input=input) / self.n_units, tf.float32)\n\n        delta = sample_avg - self.avg\n\n        self.m2 = self.var * self.count + sample_var * sample_count + delta ** 2 * self.count * sample_count / (\n                self.count + sample_count)\n\n        self.count += sample_count\n        self.avg += delta * sample_count / self.count\n        self.var = self.m2 / self.count if self.count > 0 else 1\n\n    def stop_updates(self):\n        \"\"\"\n        Ends pre-training for that layer, and fixes the layers's parameters.        \n        \"\"\"\n        assert self.count > 0\n        if self.shift is not None:\n            self.shift.assign(-self.avg)\n        \n        if self.scale is not None:\n            self.var = tf.where(tf.equal(self.var, 0), tf.ones_like(self.var), self.var)  # NaN check trick\n            self.scale.assign(1 / np.sqrt(self.var))\n        \n        del self.avg, self.var, self.m2, self.count\n        self.waiting_updates = False\n        self.trainable = False\n\n\nclass BipartiteGraphConvolution(K.Model):\n    \"\"\"\n    Partial bipartite graph convolution (either left-to-right or right-to-left).\n    \"\"\"\n\n    def __init__(self, emb_size, activation, initializer, right_to_left=False):\n        super().__init__()\n        self.emb_size = emb_size\n        self.activation = activation\n        self.initializer = initializer\n        self.right_to_left = right_to_left\n\n        # feature layers\n        self.feature_module_left = K.Sequential([\n            K.layers.Dense(units=self.emb_size, activation=None, use_bias=True, kernel_initializer=self.initializer)\n        ])\n        self.feature_module_edge = K.Sequential([\n            K.layers.Dense(units=self.emb_size, activation=None, use_bias=False, kernel_initializer=self.initializer)\n        ])\n        self.feature_module_right = K.Sequential([\n            K.layers.Dense(units=self.emb_size, activation=None, use_bias=False, kernel_initializer=self.initializer)\n        ])\n        self.feature_module_final = K.Sequential([\n            PreNormLayer(1, shift=False),  # normalize after summation trick\n            K.layers.Activation(self.activation),\n            K.layers.Dense(units=self.emb_size, activation=None, kernel_initializer=self.initializer)\n        ])\n\n        self.post_conv_module = K.Sequential([\n            PreNormLayer(1, shift=False),  # normalize after convolution\n        ])\n\n        # output_layers\n        self.output_module = K.Sequential([\n            K.layers.Dense(units=self.emb_size, activation=None, kernel_initializer=self.initializer),\n            K.layers.Activation(self.activation),\n            K.layers.Dense(units=self.emb_size, activation=None, kernel_initializer=self.initializer),\n        ])\n\n    def build(self, input_shapes):\n        l_shape, ei_shape, ev_shape, r_shape = input_shapes\n\n        self.feature_module_left.build(l_shape)\n        self.feature_module_edge.build(ev_shape)\n        self.feature_module_right.build(r_shape)\n        self.feature_module_final.build([None, self.emb_size])\n        self.post_conv_module.build([None, self.emb_size])\n        self.output_module.build([None, self.emb_size + (l_shape[1] if self.right_to_left else r_shape[1])])\n        self.built = True\n\n    def call(self, inputs, training):\n        \"\"\"\n        Perfoms a partial graph convolution on the given bipartite graph.\n\n        Inputs\n        ------\n        left_features: 2D float tensor\n            Features of the left-hand-side nodes in the bipartite graph\n        edge_indices: 2D int tensor\n            Edge indices in left-right order\n        edge_features: 2D float tensor\n            Features of the edges\n        right_features: 2D float tensor\n            Features of the right-hand-side nodes in the bipartite graph\n        scatter_out_size: 1D int tensor\n            Output size (left_features.shape[0] or right_features.shape[0], unknown at compile time)\n\n        Other parameters\n        ----------------\n        training: boolean\n            Training mode indicator\n        \"\"\"\n        left_features, edge_indices, edge_features, right_features, scatter_out_size = inputs\n\n        if self.right_to_left:\n            scatter_dim = 0\n            prev_features = left_features\n        else:\n            scatter_dim = 1\n            prev_features = right_features\n\n        # compute joint features\n        joint_features = self.feature_module_final(\n            tf.gather(\n                self.feature_module_left(left_features),\n                axis=0,\n                indices=edge_indices[0]\n            ) +\n            self.feature_module_edge(edge_features) +\n            tf.gather(\n                self.feature_module_right(right_features),\n                axis=0,\n                indices=edge_indices[1])\n        )\n\n        # perform convolution\n        conv_output = tf.scatter_nd(\n            updates=joint_features,\n            indices=tf.expand_dims(edge_indices[scatter_dim], axis=1),\n            shape=[scatter_out_size, self.emb_size]\n        )\n        conv_output = self.post_conv_module(conv_output)\n\n        # apply final module\n        output = self.output_module(tf.concat([\n            conv_output,\n            prev_features,\n        ], axis=1))\n\n        return output\n\n\nclass BaseModel(K.Model):\n    \"\"\"\n    Our base model class, which implements basic save/restore and pre-training\n    methods.\n    \"\"\"\n\n    def pre_train_init(self):\n        self.pre_train_init_rec(self, self.name)\n\n    @staticmethod\n    def pre_train_init_rec(model, name):\n        for layer in model.layers:\n            if isinstance(layer, K.Model):\n                BaseModel.pre_train_init_rec(layer, f\"{name}/{layer.name}\")\n            elif isinstance(layer, PreNormLayer):\n                layer.start_updates()\n\n    def pre_train_next(self):\n        return self.pre_train_next_rec(self, self.name)\n\n    @staticmethod\n    def pre_train_next_rec(model, name):\n        for layer in model.layers:\n            if isinstance(layer, K.Model):\n                result = BaseModel.pre_train_next_rec(layer, f\"{name}/{layer.name}\")\n                if result is not None:\n                    return result\n            elif isinstance(layer, PreNormLayer) and layer.waiting_updates and layer.received_updates:\n                layer.stop_updates()\n                return layer, f\"{name}/{layer.name}\"\n        return None\n\n    def pre_train(self, *args, **kwargs):\n        try:\n            self.call(*args, **kwargs)\n            return False\n        except PreNormException:\n            return True\n\n    def save_state(self, path):\n        with open(path, 'wb') as f:\n            for v_name in self.variables_topological_order:\n                v = [v for v in self.variables if v.name == v_name][0]\n                pickle.dump(v.numpy(), f)\n\n    def restore_state(self, path):\n        with open(path, 'rb') as f:\n            for v_name in self.variables_topological_order:\n                v = [v for v in self.variables if v.name == v_name][0]\n                v.assign(pickle.load(f))\n\n\nclass GCNPolicy(BaseModel):\n    \"\"\"\n    Our bipartite Graph Convolutional neural Network (GCN) model.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n        self.emb_size = 64\n        self.cons_nfeats = 5\n        self.edge_nfeats = 1\n        self.var_nfeats = 19\n\n        self.activation = K.activations.relu\n        self.initializer = K.initializers.Orthogonal()\n\n        # CONSTRAINT EMBEDDING\n        self.cons_embedding = K.Sequential([\n            PreNormLayer(n_units=self.cons_nfeats),\n            K.layers.Dense(units=self.emb_size, activation=self.activation, kernel_initializer=self.initializer),\n            K.layers.Dense(units=self.emb_size, activation=self.activation, kernel_initializer=self.initializer),\n        ])\n\n        # EDGE EMBEDDING\n        self.edge_embedding = K.Sequential([\n            PreNormLayer(self.edge_nfeats),\n        ])\n\n        # VARIABLE EMBEDDING\n        self.var_embedding = K.Sequential([\n            PreNormLayer(n_units=self.var_nfeats),\n            K.layers.Dense(units=self.emb_size, activation=self.activation, kernel_initializer=self.initializer),\n            K.layers.Dense(units=self.emb_size, activation=self.activation, kernel_initializer=self.initializer),\n        ])\n\n        # GRAPH CONVOLUTIONS\n        self.conv_v_to_c = BipartiteGraphConvolution(self.emb_size, self.activation, self.initializer, right_to_left=True)\n        self.conv_c_to_v = BipartiteGraphConvolution(self.emb_size, self.activation, self.initializer)\n\n        # OUTPUT\n        self.output_module = K.Sequential([\n            K.layers.Dense(units=self.emb_size, activation=self.activation, kernel_initializer=self.initializer),\n            K.layers.Dense(units=1, activation=None, kernel_initializer=self.initializer, use_bias=False),\n        ])\n\n        # build model right-away\n        self.build([\n            (None, self.cons_nfeats),\n            (2, None),\n            (None, self.edge_nfeats),\n            (None, self.var_nfeats),\n            (None, ),\n            (None, ),\n        ])\n\n        # save / restore fix\n        self.variables_topological_order = [v.name for v in self.variables]\n\n        # save input signature for compilation\n        self.input_signature = [\n            (\n                tf.contrib.eager.TensorSpec(shape=[None, self.cons_nfeats], dtype=tf.float32),\n                tf.contrib.eager.TensorSpec(shape=[2, None], dtype=tf.int32),\n                tf.contrib.eager.TensorSpec(shape=[None, self.edge_nfeats], dtype=tf.float32),\n                tf.contrib.eager.TensorSpec(shape=[None, self.var_nfeats], dtype=tf.float32),\n                tf.contrib.eager.TensorSpec(shape=[None], dtype=tf.int32),\n                tf.contrib.eager.TensorSpec(shape=[None], dtype=tf.int32),\n            ),\n            tf.contrib.eager.TensorSpec(shape=[], dtype=tf.bool),\n        ]\n\n    def build(self, input_shapes):\n        c_shape, ei_shape, ev_shape, v_shape, nc_shape, nv_shape = input_shapes\n        emb_shape = [None, self.emb_size]\n\n        if not self.built:\n            self.cons_embedding.build(c_shape)\n            self.edge_embedding.build(ev_shape)\n            self.var_embedding.build(v_shape)\n            self.conv_v_to_c.build((emb_shape, ei_shape, ev_shape, emb_shape))\n            self.conv_c_to_v.build((emb_shape, ei_shape, ev_shape, emb_shape))\n            self.output_module.build(emb_shape)\n            self.built = True\n\n    @staticmethod\n    def pad_output(output, n_vars_per_sample, pad_value=-1e8):\n        n_vars_max = tf.reduce_max(n_vars_per_sample)\n\n        output = tf.split(\n            value=output,\n            num_or_size_splits=n_vars_per_sample,\n            axis=1,\n        )\n        output = tf.concat([\n            tf.pad(\n                x,\n                paddings=[[0, 0], [0, n_vars_max - tf.shape(x)[1]]],\n                mode='CONSTANT',\n                constant_values=pad_value)\n            for x in output\n        ], axis=0)\n\n        return output\n\n    def call(self, inputs, training):\n        \"\"\"\n        Accepts stacked mini-batches, i.e. several bipartite graphs aggregated\n        as one. In that case the number of variables per samples has to be\n        provided, and the output consists in a padded dense tensor.\n\n        Parameters\n        ----------\n        inputs: list of tensors\n            Model input as a bipartite graph. May be batched into a stacked graph.\n\n        Inputs\n        ------\n        constraint_features: 2D float tensor\n            Constraint node features (n_constraints x n_constraint_features)\n        edge_indices: 2D int tensor\n            Edge constraint and variable indices (2, n_edges)\n        edge_features: 2D float tensor\n            Edge features (n_edges, n_edge_features)\n        variable_features: 2D float tensor\n            Variable node features (n_variables, n_variable_features)\n        n_cons_per_sample: 1D int tensor\n            Number of constraints for each of the samples stacked in the batch.\n        n_vars_per_sample: 1D int tensor\n            Number of variables for each of the samples stacked in the batch.\n\n        Other parameters\n        ----------------\n        training: boolean\n            Training mode indicator\n        \"\"\"\n        constraint_features, edge_indices, edge_features, variable_features, n_cons_per_sample, n_vars_per_sample = inputs\n        n_cons_total = tf.reduce_sum(n_cons_per_sample)\n        n_vars_total = tf.reduce_sum(n_vars_per_sample)\n\n        # Embeddings\n        constraint_features = self.cons_embedding(constraint_features)\n        edge_features = self.edge_embedding(edge_features)\n        variable_features = self.var_embedding(variable_features)\n\n        # Graph convolutions\n        constraint_features = self.conv_v_to_c(\n            (variable_features, edge_indices, edge_features, constraint_features, n_cons_total),\n            training\n        )\n        variable_features = self.conv_c_to_v(\n            (constraint_features, edge_indices, edge_features, variable_features, n_vars_total),\n            training\n        )\n\n        # Output\n        output = self.output_module(variable_features)\n        output = self.pad_output(output, n_vars_per_sample)\n\n        return output\n\n# ==========================================\n# File: models/mean_convolution/model.py\n# Function/Context: GCNPolicy\n# ==========================================\nimport tensorflow as tf\nimport tensorflow.keras as K\nimport numpy as np\nimport pickle\n\n\nclass PreNormException(Exception):\n    pass\n\nclass PreNormLayer(K.layers.Layer):\n    \"\"\"\n    Our pre-normalization layer, whose purpose is to normalize an input layer\n    to zero mean and unit variance to speed-up and stabilize GCN training. The\n    layer's parameters are aimed to be computed during the pre-training phase.\n    \"\"\"\n\n    def __init__(self, n_units, shift=True, scale=True):\n        super().__init__()\n        assert shift or scale\n\n        if shift:\n            self.shift = self.add_weight(\n                name=f'{self.name}/shift',\n                shape=(n_units,),\n                trainable=False,\n                initializer=tf.keras.initializers.constant(value=np.zeros((n_units,)),\n                dtype=tf.float32),\n            )\n        else:\n            self.shift = None\n\n        if scale:\n            self.scale = self.add_weight(\n                name=f'{self.name}/scale',\n                shape=(n_units,),\n                trainable=False,\n                initializer=tf.keras.initializers.constant(value=np.ones((n_units,)),\n                dtype=tf.float32),\n            )\n        else:\n            self.scale = None\n\n        self.n_units = n_units\n        self.waiting_updates = False\n        self.received_updates = False\n\n    def build(self, input_shapes):\n        self.built = True\n\n    def call(self, input):\n        if self.waiting_updates:\n            self.update_stats(input)\n            self.received_updates = True\n            raise PreNormException\n\n        if self.shift is not None:\n            input = input + self.shift\n\n        if self.scale is not None:\n            input = input * self.scale\n\n        return input\n\n    def start_updates(self):\n        \"\"\"\n        Initializes the pre-training phase.\n        \"\"\"\n        self.avg = 0\n        self.var = 0\n        self.m2 = 0\n        self.count = 0\n        self.waiting_updates = True\n        self.received_updates = False\n\n    def update_stats(self, input):\n        \"\"\"\n        Online mean and variance estimation. See: Chan et al. (1979) Updating\n        Formulae and a Pairwise Algorithm for Computing Sample Variances.\n        https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online_algorithm\n        \"\"\"\n        assert self.n_units == 1 or input.shape[-1] == self.n_units, f\"Expected input dimension of size {self.n_units}, got {input.shape[-1]}.\"\n\n        input = tf.reshape(input, [-1, self.n_units])\n        sample_avg = tf.reduce_mean(input, 0)\n        sample_var = tf.reduce_mean((input - sample_avg) ** 2, axis=0)\n        sample_count = tf.cast(tf.size(input=input) / self.n_units, tf.float32)\n\n        delta = sample_avg - self.avg\n\n        self.m2 = self.var * self.count + sample_var * sample_count + delta ** 2 * self.count * sample_count / (\n                self.count + sample_count)\n\n        self.count += sample_count\n        self.avg += delta * sample_count / self.count\n        self.var = self.m2 / self.count if self.count > 0 else 1\n\n    def stop_updates(self):\n        \"\"\"\n        Ends pre-training for that layer, and fixes the layers's parameters.        \n        \"\"\"\n        assert self.count > 0\n        if self.shift is not None:\n            self.shift.assign(-self.avg)\n        \n        if self.scale is not None:\n            self.var = tf.where(tf.equal(self.var, 0), tf.ones_like(self.var), self.var)  # NaN check trick\n            self.scale.assign(1 / np.sqrt(self.var))\n        \n        del self.avg, self.var, self.m2, self.count\n        self.waiting_updates = False\n        self.trainable = False\n\n\nclass BipartiteGraphConvolution(K.Model):\n    \"\"\"\n    Partial bipartite graph convolution (either left-to-right or right-to-left).\n    \"\"\"\n\n    def __init__(self, emb_size, activation, initializer, right_to_left=False):\n        super().__init__()\n        self.emb_size = emb_size\n        self.activation = activation\n        self.initializer = initializer\n        self.right_to_left = right_to_left\n\n        # feature layers\n        self.feature_module_left = K.Sequential([\n            K.layers.Dense(units=self.emb_size, activation=None, use_bias=True, kernel_initializer=self.initializer)\n        ])\n        self.feature_module_edge = K.Sequential([\n            K.layers.Dense(units=self.emb_size, activation=None, use_bias=False, kernel_initializer=self.initializer)\n        ])\n        self.feature_module_right = K.Sequential([\n            K.layers.Dense(units=self.emb_size, activation=None, use_bias=False, kernel_initializer=self.initializer)\n        ])\n        self.feature_module_final = K.Sequential([\n            PreNormLayer(1, shift=False),  # normalize after summation trick\n            K.layers.Activation(self.activation),\n            K.layers.Dense(units=self.emb_size, activation=None, kernel_initializer=self.initializer)\n        ])\n\n        # output_layers\n        self.output_module = K.Sequential([\n            K.layers.Dense(units=self.emb_size, activation=None, kernel_initializer=self.initializer),\n            K.layers.Activation(self.activation),\n            K.layers.Dense(units=self.emb_size, activation=None, kernel_initializer=self.initializer),\n        ])\n\n    def build(self, input_shapes):\n        l_shape, ei_shape, ev_shape, r_shape = input_shapes\n\n        self.feature_module_left.build(l_shape)\n        self.feature_module_edge.build(ev_shape)\n        self.feature_module_right.build(r_shape)\n        self.feature_module_final.build([None, self.emb_size])\n        self.output_module.build([None, self.emb_size + (l_shape[1] if self.right_to_left else r_shape[1])])\n        self.built = True\n\n    def call(self, inputs):\n        \"\"\"\n        Perfoms a partial graph convolution on the given bipartite graph.\n\n        Inputs\n        ------\n        left_features: 2D float tensor\n            Features of the left-hand-side nodes in the bipartite graph\n        edge_indices: 2D int tensor\n            Edge indices in left-right order\n        edge_features: 2D float tensor\n            Features of the edges\n        right_features: 2D float tensor\n            Features of the right-hand-side nodes in the bipartite graph\n        scatter_out_size: 1D int tensor\n            Output size (left_features.shape[0] or right_features.shape[0], unknown at compile time)\n        \"\"\"\n        left_features, edge_indices, edge_features, right_features, scatter_out_size = inputs\n\n        if self.right_to_left:\n            scatter_dim = 0\n            prev_features = left_features\n        else:\n            scatter_dim = 1\n            prev_features = right_features\n\n        # compute joint features\n        joint_features = self.feature_module_final(\n            tf.gather(\n                self.feature_module_left(left_features),\n                axis=0,\n                indices=edge_indices[0]\n            ) +\n            self.feature_module_edge(edge_features) +\n            tf.gather(\n                self.feature_module_right(right_features),\n                axis=0,\n                indices=edge_indices[1])\n        )\n\n        # perform convolution\n        conv_output = tf.scatter_nd(\n            updates=joint_features,\n            indices=tf.expand_dims(edge_indices[scatter_dim], axis=1),\n            shape=[scatter_out_size, self.emb_size]\n        )\n\n        # mean convolution\n        neighbour_count = tf.scatter_nd(\n            updates=tf.ones(shape=[tf.shape(edge_indices)[1], 1], dtype=tf.float32),\n            indices=tf.expand_dims(edge_indices[scatter_dim], axis=1),\n            shape=[scatter_out_size, 1])\n        neighbour_count = tf.where(\n            tf.equal(neighbour_count, 0),\n            tf.ones_like(neighbour_count),\n            neighbour_count)  # NaN safety trick\n        conv_output = conv_output / neighbour_count\n\n        # apply final module\n        output = self.output_module(tf.concat([\n            conv_output,\n            prev_features,\n        ], axis=1))\n\n        return output\n\n\nclass BaseModel(K.Model):\n    \"\"\"\n    Our base model class, which implements basic save/restore and pre-training\n    methods.\n    \"\"\"\n\n    def pre_train_init(self):\n        self.pre_train_init_rec(self, self.name)\n\n    @staticmethod\n    def pre_train_init_rec(model, name):\n        for layer in model.layers:\n            if isinstance(layer, K.Model):\n                BaseModel.pre_train_init_rec(layer, f\"{name}/{layer.name}\")\n            elif isinstance(layer, PreNormLayer):\n                layer.start_updates()\n\n    def pre_train_next(self):\n        return self.pre_train_next_rec(self, self.name)\n\n    @staticmethod\n    def pre_train_next_rec(model, name):\n        for layer in model.layers:\n            if isinstance(layer, K.Model):\n                result = BaseModel.pre_train_next_rec(layer, f\"{name}/{layer.name}\")\n                if result is not None:\n                    return result\n            elif isinstance(layer, PreNormLayer) and layer.waiting_updates and layer.received_updates:\n                layer.stop_updates()\n                return layer, f\"{name}/{layer.name}\"\n        return None\n\n    def pre_train(self, *args, **kwargs):\n        try:\n            self.call(*args, **kwargs)\n            return False\n        except PreNormException:\n            return True\n\n    def save_state(self, path):\n        with open(path, 'wb') as f:\n            for v_name in self.variables_topological_order:\n                v = [v for v in self.variables if v.name == v_name][0]\n                pickle.dump(v.numpy(), f)\n\n    def restore_state(self, path):\n        with open(path, 'rb') as f:\n            for v_name in self.variables_topological_order:\n                v = [v for v in self.variables if v.name == v_name][0]\n                v.assign(pickle.load(f))\n\n\nclass GCNPolicy(BaseModel):\n    \"\"\"\n    Our bipartite Graph Convolutional neural Network (GCN) model.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n        self.emb_size = 64\n        self.cons_nfeats = 5\n        self.edge_nfeats = 1\n        self.var_nfeats = 19\n\n        self.activation = K.activations.relu\n        self.initializer = K.initializers.Orthogonal()\n\n        # CONSTRAINT EMBEDDING\n        self.cons_embedding = K.Sequential([\n            PreNormLayer(n_units=self.cons_nfeats),\n            K.layers.Dense(units=self.emb_size, activation=self.activation, kernel_initializer=self.initializer),\n            K.layers.Dense(units=self.emb_size, activation=self.activation, kernel_initializer=self.initializer),\n        ])\n\n        # EDGE EMBEDDING\n        self.edge_embedding = K.Sequential([\n            PreNormLayer(self.edge_nfeats),\n        ])\n\n        # VARIABLE EMBEDDING\n        self.var_embedding = K.Sequential([\n            PreNormLayer(n_units=self.var_nfeats),\n            K.layers.Dense(units=self.emb_size, activation=self.activation, kernel_initializer=self.initializer),\n            K.layers.Dense(units=self.emb_size, activation=self.activation, kernel_initializer=self.initializer),\n        ])\n\n        # GRAPH CONVOLUTIONS\n        self.conv_v_to_c = BipartiteGraphConvolution(self.emb_size, self.activation, self.initializer, right_to_left=True)\n        self.conv_c_to_v = BipartiteGraphConvolution(self.emb_size, self.activation, self.initializer)\n\n        # OUTPUT\n        self.output_module = K.Sequential([\n            K.layers.Dense(units=self.emb_size, activation=self.activation, kernel_initializer=self.initializer),\n            K.layers.Dense(units=1, activation=None, kernel_initializer=self.initializer, use_bias=False),\n        ])\n\n        # build model right-away\n        self.build([\n            (None, self.cons_nfeats),\n            (2, None),\n            (None, self.edge_nfeats),\n            (None, self.var_nfeats),\n            (None, ),\n            (None, ),\n        ])\n\n        # save / restore fix\n        self.variables_topological_order = [v.name for v in self.variables]\n\n        # save input signature for compilation\n        self.input_signature = [\n            (\n                tf.contrib.eager.TensorSpec(shape=[None, self.cons_nfeats], dtype=tf.float32),\n                tf.contrib.eager.TensorSpec(shape=[2, None], dtype=tf.int32),\n                tf.contrib.eager.TensorSpec(shape=[None, self.edge_nfeats], dtype=tf.float32),\n                tf.contrib.eager.TensorSpec(shape=[None, self.var_nfeats], dtype=tf.float32),\n                tf.contrib.eager.TensorSpec(shape=[None], dtype=tf.int32),\n                tf.contrib.eager.TensorSpec(shape=[None], dtype=tf.int32),\n            ),\n            tf.contrib.eager.TensorSpec(shape=[], dtype=tf.bool),\n        ]\n\n    def build(self, input_shapes):\n        c_shape, ei_shape, ev_shape, v_shape, nc_shape, nv_shape = input_shapes\n        emb_shape = [None, self.emb_size]\n\n        if not self.built:\n            self.cons_embedding.build(c_shape)\n            self.edge_embedding.build(ev_shape)\n            self.var_embedding.build(v_shape)\n            self.conv_v_to_c.build((emb_shape, ei_shape, ev_shape, emb_shape))\n            self.conv_c_to_v.build((emb_shape, ei_shape, ev_shape, emb_shape))\n            self.output_module.build(emb_shape)\n            self.built = True\n\n    @staticmethod\n    def pad_output(output, n_vars_per_sample, pad_value=-1e8):\n        n_vars_max = tf.reduce_max(n_vars_per_sample)\n\n        output = tf.split(\n            value=output,\n            num_or_size_splits=n_vars_per_sample,\n            axis=1,\n        )\n        output = tf.concat([\n            tf.pad(\n                x,\n                paddings=[[0, 0], [0, n_vars_max - tf.shape(x)[1]]],\n                mode='CONSTANT',\n                constant_values=pad_value)\n            for x in output\n        ], axis=0)\n\n        return output\n\n    def call(self, inputs, training):\n        \"\"\"\n        Accepts stacked mini-batches, i.e. several bipartite graphs aggregated\n        as one. In that case the number of variables per samples has to be\n        provided, and the output consists in a padded dense tensor.\n\n        Parameters\n        ----------\n        inputs: list of tensors\n            Model input as a bipartite graph. May be batched into a stacked graph.\n        n_cons_per_sample: list of ints\n            Number of constraints for each of the samples stacked in the batch.\n        n_vars_per_sample: list of ints\n            Number of variables for each of the samples stacked in the batch.\n\n        Inputs\n        ------\n        constraint_features: 2D float tensor\n            Constraint node features (n_constraints x n_constraint_features)\n        edge_indices: 2D int tensor\n            Edge constraint and variable indices (2, n_edges)\n        edge_features: 2D float tensor\n            Edge features (n_edges, n_edge_features)\n        variable_features: 2D float tensor\n            Variable node features (n_variables, n_variable_features)\n        n_cons_per_sample: 1D int tensor\n            Number of constraints for each of the samples stacked in the batch.\n        n_vars_per_sample: 1D int tensor\n        \"\"\"\n        constraint_features, edge_indices, edge_features, variable_features, n_cons_per_sample, n_vars_per_sample = inputs\n        n_cons = tf.shape(constraint_features)[0]\n        n_vars = tf.shape(variable_features)[0]\n        n_edges = tf.shape(edge_indices)[1]\n\n        # EMBEDDINGS\n        constraint_features = self.cons_embedding(constraint_features)\n        edge_features = self.edge_embedding(edge_features)\n        variable_features = self.var_embedding(variable_features)\n\n        # GRAPH CONVOLUTIONS\n        constraint_features = self.conv_v_to_c((constraint_features, edge_indices, edge_features, variable_features, n_cons))\n        variable_features = self.conv_c_to_v((variable_features, edge_indices, edge_features, constraint_features, n_vars))\n\n        # OUTPUT\n        output = self.output_module(variable_features)\n        output = tf.reshape(output, [1, -1])\n\n        # PADDING\n        output = self.pad_output(output, n_vars_per_sample)\n\n        return output\n\n# ==========================================\n# File: models/no_prenorm/model.py\n# Function/Context: GCNPolicy\n# ==========================================\nimport tensorflow as tf\nimport tensorflow.keras as K\nimport numpy as np\nimport pickle\n\n\nclass PreNormException(Exception):\n    pass\n\nclass PreNormLayer(K.layers.Layer):\n    \"\"\"\n    Our pre-normalization layer, whose purpose is to normalize an input layer\n    to zero mean and unit variance to speed-up and stabilize GCN training. The\n    layer's parameters are aimed to be computed during the pre-training phase.\n    \"\"\"\n\n    def __init__(self, n_units, shift=True, scale=True):\n        super().__init__()\n        assert shift or scale\n\n        if shift:\n            self.shift = self.add_weight(\n                name=f'{self.name}/shift',\n                shape=(n_units,),\n                trainable=False,\n                initializer=tf.keras.initializers.constant(value=np.zeros((n_units,)),\n                dtype=tf.float32),\n            )\n        else:\n            self.shift = None\n\n        if scale:\n            self.scale = self.add_weight(\n                name=f'{self.name}/scale',\n                shape=(n_units,),\n                trainable=False,\n                initializer=tf.keras.initializers.constant(value=np.ones((n_units,)),\n                dtype=tf.float32),\n            )\n        else:\n            self.scale = None\n\n        self.n_units = n_units\n        self.waiting_updates = False\n        self.received_updates = False\n\n    def build(self, input_shapes):\n        self.built = True\n\n    def call(self, input):\n        if self.waiting_updates:\n            self.update_stats(input)\n            self.received_updates = True\n            raise PreNormException\n\n        if self.shift is not None:\n            input = input + self.shift\n\n        if self.scale is not None:\n            input = input * self.scale\n\n        return input\n\n    def start_updates(self):\n        \"\"\"\n        Initializes the pre-training phase.\n        \"\"\"\n        self.avg = 0\n        self.var = 0\n        self.m2 = 0\n        self.count = 0\n        self.waiting_updates = True\n        self.received_updates = False\n\n    def update_stats(self, input):\n        \"\"\"\n        Online mean and variance estimation. See: Chan et al. (1979) Updating\n        Formulae and a Pairwise Algorithm for Computing Sample Variances.\n        https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online_algorithm\n        \"\"\"\n        assert self.n_units == 1 or input.shape[-1] == self.n_units, f\"Expected input dimension of size {self.n_units}, got {input.shape[-1]}.\"\n\n        input = tf.reshape(input, [-1, self.n_units])\n        sample_avg = tf.reduce_mean(input, 0)\n        sample_var = tf.reduce_mean((input - sample_avg) ** 2, axis=0)\n        sample_count = tf.cast(tf.size(input=input) / self.n_units, tf.float32)\n\n        delta = sample_avg - self.avg\n\n        self.m2 = self.var * self.count + sample_var * sample_count + delta ** 2 * self.count * sample_count / (\n                self.count + sample_count)\n\n        self.count += sample_count\n        self.avg += delta * sample_count / self.count\n        self.var = self.m2 / self.count if self.count > 0 else 1\n\n    def stop_updates(self):\n        \"\"\"\n        Ends pre-training for that layer, and fixes the layers's parameters.        \n        \"\"\"\n        assert self.count > 0\n        if self.shift is not None:\n            self.shift.assign(-self.avg)\n        \n        if self.scale is not None:\n            self.var = tf.where(tf.equal(self.var, 0), tf.ones_like(self.var), self.var)  # NaN check trick\n            self.scale.assign(1 / np.sqrt(self.var))\n        \n        del self.avg, self.var, self.m2, self.count\n        self.waiting_updates = False\n        self.trainable = False\n\n\nclass BipartiteGraphConvolution(K.Model):\n    \"\"\"\n    Partial bipartite graph convolution (either left-to-right or right-to-left).\n    \"\"\"\n\n    def __init__(self, emb_size, activation, initializer, right_to_left=False):\n        super().__init__()\n        self.emb_size = emb_size\n        self.activation = activation\n        self.initializer = initializer\n        self.right_to_left = right_to_left\n\n        # feature layers\n        self.feature_module_left = K.Sequential([\n            K.layers.Dense(units=self.emb_size, activation=None, use_bias=True, kernel_initializer=self.initializer)\n        ])\n        self.feature_module_edge = K.Sequential([\n            K.layers.Dense(units=self.emb_size, activation=None, use_bias=False, kernel_initializer=self.initializer)\n        ])\n        self.feature_module_right = K.Sequential([\n            K.layers.Dense(units=self.emb_size, activation=None, use_bias=False, kernel_initializer=self.initializer)\n        ])\n        self.feature_module_final = K.Sequential([\n            PreNormLayer(1, shift=False),  # normalize after summation trick\n            K.layers.Activation(self.activation),\n            K.layers.Dense(units=self.emb_size, activation=None, kernel_initializer=self.initializer)\n        ])\n\n        # output_layers\n        self.output_module = K.Sequential([\n            K.layers.Dense(units=self.emb_size, activation=None, kernel_initializer=self.initializer),\n            K.layers.Activation(self.activation),\n            K.layers.Dense(units=self.emb_size, activation=None, kernel_initializer=self.initializer),\n        ])\n\n    def build(self, input_shapes):\n        l_shape, ei_shape, ev_shape, r_shape = input_shapes\n\n        self.feature_module_left.build(l_shape)\n        self.feature_module_edge.build(ev_shape)\n        self.feature_module_right.build(r_shape)\n        self.feature_module_final.build([None, self.emb_size])\n        self.output_module.build([None, self.emb_size + (l_shape[1] if self.right_to_left else r_shape[1])])\n        self.built = True\n\n    def call(self, inputs):\n        \"\"\"\n        Perfoms a partial graph convolution on the given bipartite graph.\n\n        Inputs\n        ------\n        left_features: 2D float tensor\n            Features of the left-hand-side nodes in the bipartite graph\n        edge_indices: 2D int tensor\n            Edge indices in left-right order\n        edge_features: 2D float tensor\n            Features of the edges\n        right_features: 2D float tensor\n            Features of the right-hand-side nodes in the bipartite graph\n        scatter_out_size: 1D int tensor\n            Output size (left_features.shape[0] or right_features.shape[0], unknown at compile time)\n        \"\"\"\n        left_features, edge_indices, edge_features, right_features, scatter_out_size = inputs\n\n        if self.right_to_left:\n            scatter_dim = 0\n            prev_features = left_features\n        else:\n            scatter_dim = 1\n            prev_features = right_features\n\n        # compute joint features\n        joint_features = self.feature_module_final(\n            tf.gather(\n                self.feature_module_left(left_features),\n                axis=0,\n                indices=edge_indices[0]\n            ) +\n            self.feature_module_edge(edge_features) +\n            tf.gather(\n                self.feature_module_right(right_features),\n                axis=0,\n                indices=edge_indices[1])\n        )\n\n        # perform convolution\n        conv_output = tf.scatter_nd(\n            updates=joint_features,\n            indices=tf.expand_dims(edge_indices[scatter_dim], axis=1),\n            shape=[scatter_out_size, self.emb_size]\n        )\n\n        # apply final module\n        output = self.output_module(tf.concat([\n            conv_output,\n            prev_features,\n        ], axis=1))\n\n        return output\n\n\nclass BaseModel(K.Model):\n    \"\"\"\n    Our base model class, which implements basic save/restore and pre-training\n    methods.\n    \"\"\"\n\n    def pre_train_init(self):\n        self.pre_train_init_rec(self, self.name)\n\n    @staticmethod\n    def pre_train_init_rec(model, name):\n        for layer in model.layers:\n            if isinstance(layer, K.Model):\n                BaseModel.pre_train_init_rec(layer, f\"{name}/{layer.name}\")\n            elif isinstance(layer, PreNormLayer):\n                layer.start_updates()\n\n    def pre_train_next(self):\n        return self.pre_train_next_rec(self, self.name)\n\n    @staticmethod\n    def pre_train_next_rec(model, name):\n        for layer in model.layers:\n            if isinstance(layer, K.Model):\n                result = BaseModel.pre_train_next_rec(layer, f\"{name}/{layer.name}\")\n                if result is not None:\n                    return result\n            elif isinstance(layer, PreNormLayer) and layer.waiting_updates and layer.received_updates:\n                layer.stop_updates()\n                return layer, f\"{name}/{layer.name}\"\n        return None\n\n    def pre_train(self, *args, **kwargs):\n        try:\n            self.call(*args, **kwargs)\n            return False\n        except PreNormException:\n            return True\n\n    def save_state(self, path):\n        with open(path, 'wb') as f:\n            for v_name in self.variables_topological_order:\n                v = [v for v in self.variables if v.name == v_name][0]\n                pickle.dump(v.numpy(), f)\n\n    def restore_state(self, path):\n        with open(path, 'rb') as f:\n            for v_name in self.variables_topological_order:\n                v = [v for v in self.variables if v.name == v_name][0]\n                v.assign(pickle.load(f))\n\n\nclass GCNPolicy(BaseModel):\n    \"\"\"\n    Our bipartite Graph Convolutional neural Network (GCN) model.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n        self.emb_size = 64\n        self.cons_nfeats = 5\n        self.edge_nfeats = 1\n        self.var_nfeats = 19\n\n        self.activation = K.activations.relu\n        self.initializer = K.initializers.Orthogonal()\n\n        # CONSTRAINT EMBEDDING\n        self.cons_embedding = K.Sequential([\n            PreNormLayer(n_units=self.cons_nfeats),\n            K.layers.Dense(units=self.emb_size, activation=self.activation, kernel_initializer=self.initializer),\n            K.layers.Dense(units=self.emb_size, activation=self.activation, kernel_initializer=self.initializer),\n        ])\n\n        # EDGE EMBEDDING\n        self.edge_embedding = K.Sequential([\n            PreNormLayer(self.edge_nfeats),\n        ])\n\n        # VARIABLE EMBEDDING\n        self.var_embedding = K.Sequential([\n            PreNormLayer(n_units=self.var_nfeats),\n            K.layers.Dense(units=self.emb_size, activation=self.activation, kernel_initializer=self.initializer),\n            K.layers.Dense(units=self.emb_size, activation=self.activation, kernel_initializer=self.initializer),\n        ])\n\n        # GRAPH CONVOLUTIONS\n        self.conv_v_to_c = BipartiteGraphConvolution(self.emb_size, self.activation, self.initializer, right_to_left=True)\n        self.conv_c_to_v = BipartiteGraphConvolution(self.emb_size, self.activation, self.initializer)\n\n        # OUTPUT\n        self.output_module = K.Sequential([\n            K.layers.Dense(units=self.emb_size, activation=self.activation, kernel_initializer=self.initializer),\n            K.layers.Dense(units=1, activation=None, kernel_initializer=self.initializer, use_bias=False),\n        ])\n\n        # build model right-away\n\n        self.build([\n            (None, self.cons_nfeats),\n            (2, None),\n            (None, self.edge_nfeats),\n            (None, self.var_nfeats),\n            (None, ),\n            (None, ),\n        ])\n\n        # save / restore fix\n        self.variables_topological_order = [v.name for v in self.variables]\n\n        # save input signature for compilation\n        self.input_signature = [\n            (\n                tf.contrib.eager.TensorSpec(shape=[None, self.cons_nfeats], dtype=tf.float32),\n                tf.contrib.eager.TensorSpec(shape=[2, None], dtype=tf.int32),\n                tf.contrib.eager.TensorSpec(shape=[None, self.edge_nfeats], dtype=tf.float32),\n                tf.contrib.eager.TensorSpec(shape=[None, self.var_nfeats], dtype=tf.float32),\n                tf.contrib.eager.TensorSpec(shape=[None], dtype=tf.int32),\n                tf.contrib.eager.TensorSpec(shape=[None], dtype=tf.int32),\n            ),\n            tf.contrib.eager.TensorSpec(shape=[], dtype=tf.bool),\n        ]\n\n    def build(self, input_shapes):\n        c_shape, ei_shape, ev_shape, v_shape, nc_shape, nv_shape = input_shapes\n        emb_shape = [None, self.emb_size]\n\n        if not self.built:\n            self.cons_embedding.build(c_shape)\n            self.edge_embedding.build(ev_shape)\n            self.var_embedding.build(v_shape)\n            self.conv_v_to_c.build((emb_shape, ei_shape, ev_shape, emb_shape))\n            self.conv_c_to_v.build((emb_shape, ei_shape, ev_shape, emb_shape))\n            self.output_module.build(emb_shape)\n            self.built = True\n\n    @staticmethod\n    def pad_output(output, n_vars_per_sample, pad_value=-1e8):\n        n_vars_max = tf.reduce_max(n_vars_per_sample)\n\n        output = tf.split(\n            value=output,\n            num_or_size_splits=n_vars_per_sample,\n            axis=1,\n        )\n        output = tf.concat([\n            tf.pad(\n                x,\n                paddings=[[0, 0], [0, n_vars_max - tf.shape(x)[1]]],\n                mode='CONSTANT',\n                constant_values=pad_value)\n            for x in output\n        ], axis=0)\n\n        return output\n\n    def call(self, inputs, training):\n        \"\"\"\n        Accepts stacked mini-batches, i.e. several bipartite graphs aggregated\n        as one. In that case the number of variables per samples has to be\n        provided, and the output consists in a padded dense tensor.\n\n        Parameters\n        ----------\n        inputs: list of tensors\n            Model input as a bipartite graph. May be batched into a stacked graph.\n        n_cons_per_sample: list of ints\n            Number of constraints for each of the samples stacked in the batch.\n        n_vars_per_sample: list of ints\n            Number of variables for each of the samples stacked in the batch.\n\n        Inputs\n        ------\n        constraint_features: 2D float tensor\n            Constraint node features (n_constraints x n_constraint_features)\n        edge_indices: 2D int tensor\n            Edge constraint and variable indices (2, n_edges)\n        edge_features: 2D float tensor\n            Edge features (n_edges, n_edge_features)\n        variable_features: 2D float tensor\n            Variable node features (n_variables, n_variable_features)\n        n_cons_per_sample: 1D int tensor\n            Number of constraints for each of the samples stacked in the batch.\n        n_vars_per_sample: 1D int tensor\n            Number of variables for each of the samples stacked in the batch.\n\n        Other parameters\n        ----------------\n        training: boolean\n            Training mode indicator\n        \"\"\"\n        constraint_features, edge_indices, edge_features, variable_features, n_cons_per_sample, n_vars_per_sample = inputs\n        n_cons_total = tf.reduce_sum(n_cons_per_sample)\n        n_vars_total = tf.reduce_sum(n_vars_per_sample)\n\n        # EMBEDDINGS\n        constraint_features = self.cons_embedding(constraint_features)\n        edge_features = self.edge_embedding(edge_features)\n        variable_features = self.var_embedding(variable_features)\n\n        # GRAPH CONVOLUTIONS\n        constraint_features = self.conv_v_to_c((variable_features, edge_indices, edge_features, constraint_features, n_cons_total))\n        variable_features = self.conv_c_to_v((constraint_features, edge_indices, edge_features, variable_features, n_vars_total))\n\n        # OUTPUT\n        output = self.output_module(variable_features)\n        output = tf.reshape(output, [1, n_vars_total])\n\n        # PADDING\n        output = self.pad_output(output, n_vars_per_sample)\n\n        return output",
  "description": "Combined Analysis:\n- [03_train_gcnn.py]: This file implements the core training algorithm for the GCNN variable selection policy described in the paper. The key components are: 1) The 'process' function that performs forward/backward propagation using the GCNN model on bipartite graph representations of MILP states, computing cross-entropy loss against strong branching labels and top-k accuracy metrics. 2) The training loop that uses imitation learning (behavioral cloning) to mimic strong branching decisions. 3) Data loading and preprocessing for the four NP-hard problem classes. The code directly corresponds to the paper's Algorithm 1 (training procedure) and Section 4 (experimental setup).\n- [04_test.py]: This file implements the TESTING phase of the core algorithm - evaluating trained variable selection policies (both GCNN and traditional ML competitors) on MILP instances. The key function 'process' executes the policy evaluation by: 1) Loading trained models (GCNN or ML competitors), 2) Processing batches of MILP states represented as bipartite graphs (for GCNN) or flat features (for competitors), 3) Computing variable selection scores using the trained policies, 4) Measuring top-k accuracy against strong branching expert labels. This directly implements the evaluation of the imitation learning approach described in the paper, where policies are trained to mimic strong branching decisions.\n- [05_evaluate.py]: This file implements the core branching logic for integrating learned policies into SCIP's branch-and-bound algorithm. The PolicyBranching class extends SCIP's Branchrule and implements the branchexeclp method, which is called at each branching node. It handles three policy types: 1) GCNN models that process bipartite graph representations of MILP states, 2) internal SCIP branching rules, and 3) ML competitors using handcrafted features. For GCNN policies, it extracts the MILP state as a bipartite graph (variables and constraints), converts it to tensors, and uses the trained GCNN to predict variable scores for branching decisions. This directly implements the key algorithm step of using graph convolutional neural networks for variable selection in branch-and-bound.\n- [models/baseline/model.py]: This file implements the core GCNN architecture from the paper. It defines the bipartite graph convolutional network that processes MILP instances represented as bipartite graphs (variables and constraints). The model takes constraint features, variable features, edge indices, and edge features as input, performs graph convolutions to propagate information between variables and constraints, and outputs scores for each variable. These scores are used for variable selection (branching) in the branch-and-bound algorithm. The implementation includes specialized layers for pre-normalization and bipartite graph convolutions, along with pre-training mechanisms. While this file doesn't implement the full branch-and-bound algorithm or strong branching expert, it implements the key neural network component that learns the branching policy through imitation learning.\n- [models/mean_convolution/model.py]: This file implements the core Graph Convolutional Neural Network (GCNN) model for variable selection in branch-and-bound, as described in the paper. The GCNPolicy class encodes the MILP state as a bipartite graph between variables and constraints, performs graph convolutions via BipartiteGraphConvolution layers, and outputs scores for each variable. The model uses pre-normalization layers (PreNormLayer) for stable training and handles batched graphs with padding. This directly implements the neural network architecture used to approximate the strong branching policy.\n- [models/no_prenorm/model.py]: This file implements the core GCNN architecture for variable selection in branch-and-bound, which is the key algorithmic contribution of the paper. The GCNPolicy class encodes the MILP state as a bipartite graph between constraints and variables, performs graph convolutions to learn node embeddings, and outputs variable scores for branching decisions. The implementation includes: 1) Pre-normalization layers for stable training, 2) Bipartite graph convolution layers for message passing between constraints and variables, 3) A complete GCNN model that processes MILP features and produces branching scores. This directly implements the neural network policy described in the paper's Algorithm 1 (imitation learning framework) and Section 3.2 (graph convolutional architecture).",
  "dependencies": [
    "utilities",
    "shutil",
    "pickle",
    "sys",
    "tensorflow",
    "time",
    "pyscipopt",
    "csv",
    "BipartiteGraphConvolution",
    "importlib",
    "pathlib",
    "utilities_tf.load_batch_gcnn",
    "PreNormLayer",
    "os",
    "tensorflow.contrib.eager",
    "argparse",
    "utilities_tf",
    "svmrank",
    "numpy",
    "tensorflow.keras",
    "BaseModel",
    "gzip"
  ]
}