{
  "file_path": "data/generate_tsp_concorde.py, main.ipynb, models/gcn_layers.py, models/gcn_model.py, utils/beamsearch.py, utils/model_utils.py",
  "function_name": "train_one_epoch, ResidualGatedGCNLayer, NodeFeatures, EdgeFeatures, MLP, ResidualGatedGCNModel.forward, Beamsearch, beamsearch_tour_nodes, beamsearch_tour_nodes_shortest",
  "code_snippet": "\n\n# ==========================================\n# File: data/generate_tsp_concorde.py\n# Function/Context: \n# ==========================================\nimport time\nimport argparse\nimport pprint as pp\nimport os\n\nimport pandas as pd\nimport numpy as np\nfrom concorde.tsp import TSPSolver\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--num_samples\", type=int, default=10000)\n    parser.add_argument(\"--num_nodes\", type=int, default=20)\n    parser.add_argument(\"--node_dim\", type=int, default=2)\n    parser.add_argument(\"--filename\", type=str, default=None)\n    opts = parser.parse_args()\n    \n    if opts.filename is None:\n        opts.filename = f\"tsp{opts.num_nodes}_concorde.txt\"\n    \n    # Pretty print the run args\n    pp.pprint(vars(opts))\n    \n    set_nodes_coord = np.random.random([opts.num_samples, opts.num_nodes, opts.node_dim])\n    with open(opts.filename, \"w\") as f:\n        start_time = time.time()\n        for nodes_coord in set_nodes_coord:\n            solver = TSPSolver.from_data(nodes_coord[:,0], nodes_coord[:,1], norm=\"GEO\")  \n            solution = solver.solve()\n            f.write( \" \".join( str(x)+str(\" \")+str(y) for x,y in nodes_coord) )\n            f.write( str(\" \") + str('output') + str(\" \") )\n            f.write( str(\" \").join( str(node_idx+1) for node_idx in solution.tour) )\n            f.write( str(\" \") + str(solution.tour[0]+1) + str(\" \") )\n            f.write( \"\\n\" )\n        end_time = time.time() - start_time\n    \n    print(f\"Completed generation of {opts.num_samples} samples of TSP{opts.num_nodes}.\")\n    print(f\"Total time: {end_time/3600:.1f}h\")\n    print(f\"Average time: {(end_time/3600)/opts.num_samples:.1f}h\")\n\n# ==========================================\n# File: main.ipynb\n# Function/Context: train_one_epoch\n# ==========================================\nimport os\nimport json\nimport argparse\nimport time\n\nimport numpy as np\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport torch.nn as nn\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nimport networkx as nx\nfrom sklearn.utils.class_weight import compute_class_weight\n\nfrom tensorboardX import SummaryWriter\nfrom fastprogress import master_bar, progress_bar\n\n# Remove warning\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nfrom scipy.sparse import SparseEfficiencyWarning\nwarnings.simplefilter('ignore', SparseEfficiencyWarning)\n\nfrom config import *\nfrom utils.graph_utils import *\nfrom utils.google_tsp_reader import GoogleTSPReader\nfrom utils.plot_utils import *\nfrom models.gcn_model import ResidualGatedGCNModel\nfrom utils.model_utils import *\n\n# GPU Configuration\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = str(config.gpu_id)\n\nif torch.cuda.is_available():\n    print(\"CUDA available, using GPU ID {}\".format(config.gpu_id))\n    dtypeFloat = torch.cuda.FloatTensor\n    dtypeLong = torch.cuda.LongTensor\n    torch.cuda.manual_seed(1)\nelse:\n    print(\"CUDA not available\")\n    dtypeFloat = torch.FloatTensor\n    dtypeLong = torch.LongTensor\n    torch.manual_seed(1)\n\ndef train_one_epoch(net, optimizer, config, master_bar):\n    # Set training mode\n    net.train()\n\n    # Assign parameters\n    num_nodes = config.num_nodes\n    num_neighbors = config.num_neighbors\n    batch_size = config.batch_size\n    batches_per_epoch = config.batches_per_epoch\n    accumulation_steps = config.accumulation_steps\n    train_filepath = config.train_filepath\n\n    # Load TSP data\n    dataset = GoogleTSPReader(num_nodes, num_neighbors, batch_size, train_filepath)\n    if batches_per_epoch != -1:\n        batches_per_epoch = min(batches_per_epoch, dataset.max_iter)\n    else:\n        batches_per_epoch = dataset.max_iter\n\n    # Convert dataset to iterable\n    dataset = iter(dataset)\n    \n    # Initially set loss class weights as None\n    edge_cw = None\n\n    # Initialize running data\n    running_loss = 0.0\n    running_pred_tour_len = 0.0\n    running_gt_tour_len = 0.0\n    running_nb_data = 0\n    running_nb_batch = 0\n\n    start_epoch = time.time()\n    for batch_num in progress_bar(range(batches_per_epoch), parent=master_bar):\n        # Generate a batch of TSPs\n        try:\n            batch = next(dataset)\n        except StopIteration:\n            break\n\n        # Convert batch to torch Variables\n        x_edges = Variable(torch.LongTensor(batch.edges).type(dtypeLong), requires_grad=False)\n        x_edges_values = Variable(torch.FloatTensor(batch.edges_values).type(dtypeFloat), requires_grad=False)\n        x_nodes = Variable(torch.LongTensor(batch.nodes).type(dtypeLong), requires_grad=False)\n        x_nodes_coord = Variable(torch.FloatTensor(batch.nodes_coord).type(dtypeFloat), requires_grad=False)\n        y_edges = Variable(torch.LongTensor(batch.edges_target).type(dtypeLong), requires_grad=False)\n        y_nodes = Variable(torch.LongTensor(batch.nodes_target).type(dtypeLong), requires_grad=False)\n        \n        # Compute class weights (if uncomputed)\n        if type(edge_cw) != torch.Tensor:\n            edge_labels = y_edges.cpu().numpy().flatten()\n            edge_cw = compute_class_weight(\"balanced\", classes=np.unique(edge_labels), y=edge_labels)\n        \n        # Forward pass\n        y_preds, loss = net.forward(x_edges, x_edges_values, x_nodes, x_nodes_coord, y_edges, edge_cw)\n        loss = loss.mean()  # Take mean of loss across multiple GPUs\n        loss = loss / accumulation_steps  # Scale loss by accumulation steps\n        loss.backward()\n\n        # Backward pass\n        if (batch_num+1) % accumulation_steps == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n\n        # Compute error metrics and mean tour lengths\n        pred_tour_len = mean_tour_len_edges(x_edges_values, y_preds)\n        gt_tour_len = np.mean(batch.tour_len)\n\n        # Update running data\n        running_nb_data += batch_size\n        running_loss += batch_size* loss.data.item()* accumulation_steps  # Re-scale loss\n        running_pred_tour_len += batch_size* pred_tour_len\n        running_gt_tour_len += batch_size* gt_tour_len\n        running_nb_batch += 1\n        \n        # Log intermediate statistics\n        result = ('loss:{loss:.4f} pred_tour_len:{pred_tour_len:.3f} gt_tour_len:{gt_tour_len:.3f}'.format(\n            loss=running_loss/running_nb_data,\n            pred_tour_len=running_pred_tour_len/running_nb_data,\n            gt_tour_len=running_gt_tour_len/running_nb_data))\n        master_bar.child.comment = result\n\n    # Compute statistics for full epoch\n    loss = running_loss / running_nb_data\n    pred_tour_len = running_pred_tour_len / running_nb_data\n    gt_tour_len = running_gt_tour_len / running_nb_data\n    time_epoch = time.time() - start_epoch\n    \n    return loss, pred_tour_len, gt_tour_len, time_epoch\n\n# ==========================================\n# File: models/gcn_layers.py\n# Function/Context: ResidualGatedGCNLayer, NodeFeatures, EdgeFeatures, MLP\n# ==========================================\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\n\nimport numpy as np\n\n\nclass BatchNormNode(nn.Module):\n    \"\"\"Batch normalization for node features.\n    \"\"\"\n\n    def __init__(self, hidden_dim):\n        super(BatchNormNode, self).__init__()\n        self.batch_norm = nn.BatchNorm1d(hidden_dim, track_running_stats=False)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Node features (batch_size, num_nodes, hidden_dim)\n\n        Returns:\n            x_bn: Node features after batch normalization (batch_size, num_nodes, hidden_dim)\n        \"\"\"\n        x_trans = x.transpose(1, 2).contiguous()  # Reshape input: (batch_size, hidden_dim, num_nodes)\n        x_trans_bn = self.batch_norm(x_trans)\n        x_bn = x_trans_bn.transpose(1, 2).contiguous()  # Reshape to original shape\n        return x_bn\n\n\nclass BatchNormEdge(nn.Module):\n    \"\"\"Batch normalization for edge features.\n    \"\"\"\n\n    def __init__(self, hidden_dim):\n        super(BatchNormEdge, self).__init__()\n        self.batch_norm = nn.BatchNorm2d(hidden_dim, track_running_stats=False)\n\n    def forward(self, e):\n        \"\"\"\n        Args:\n            e: Edge features (batch_size, num_nodes, num_nodes, hidden_dim)\n\n        Returns:\n            e_bn: Edge features after batch normalization (batch_size, num_nodes, num_nodes, hidden_dim)\n        \"\"\"\n        e_trans = e.transpose(1, 3).contiguous()  # Reshape input: (batch_size, num_nodes, num_nodes, hidden_dim)\n        e_trans_bn = self.batch_norm(e_trans)\n        e_bn = e_trans_bn.transpose(1, 3).contiguous()  # Reshape to original\n        return e_bn\n\n\nclass NodeFeatures(nn.Module):\n    \"\"\"Convnet features for nodes.\n    \n    Using `sum` aggregation:\n        x_i = U*x_i +  sum_j [ gate_ij * (V*x_j) ]\n    \n    Using `mean` aggregation:\n        x_i = U*x_i + ( sum_j [ gate_ij * (V*x_j) ] / sum_j [ gate_ij] )\n    \"\"\"\n    \n    def __init__(self, hidden_dim, aggregation=\"mean\"):\n        super(NodeFeatures, self).__init__()\n        self.aggregation = aggregation\n        self.U = nn.Linear(hidden_dim, hidden_dim, True)\n        self.V = nn.Linear(hidden_dim, hidden_dim, True)\n\n    def forward(self, x, edge_gate):\n        \"\"\"\n        Args:\n            x: Node features (batch_size, num_nodes, hidden_dim)\n            edge_gate: Edge gate values (batch_size, num_nodes, num_nodes, hidden_dim)\n\n        Returns:\n            x_new: Convolved node features (batch_size, num_nodes, hidden_dim)\n        \"\"\"\n        Ux = self.U(x)  # B x V x H\n        Vx = self.V(x)  # B x V x H\n        Vx = Vx.unsqueeze(1)  # extend Vx from \"B x V x H\" to \"B x 1 x V x H\"\n        gateVx = edge_gate * Vx  # B x V x V x H\n        if self.aggregation==\"mean\":\n            x_new = Ux + torch.sum(gateVx, dim=2) / (1e-20 + torch.sum(edge_gate, dim=2))  # B x V x H\n        elif self.aggregation==\"sum\":\n            x_new = Ux + torch.sum(gateVx, dim=2)  # B x V x H\n        return x_new\n\n\nclass EdgeFeatures(nn.Module):\n    \"\"\"Convnet features for edges.\n\n    e_ij = U*e_ij + V*(x_i + x_j)\n    \"\"\"\n\n    def __init__(self, hidden_dim):\n        super(EdgeFeatures, self).__init__()\n        self.U = nn.Linear(hidden_dim, hidden_dim, True)\n        self.V = nn.Linear(hidden_dim, hidden_dim, True)\n        \n    def forward(self, x, e):\n        \"\"\"\n        Args:\n            x: Node features (batch_size, num_nodes, hidden_dim)\n            e: Edge features (batch_size, num_nodes, num_nodes, hidden_dim)\n\n        Returns:\n            e_new: Convolved edge features (batch_size, num_nodes, num_nodes, hidden_dim)\n        \"\"\"\n        Ue = self.U(e)\n        Vx = self.V(x)\n        Wx = Vx.unsqueeze(1)  # Extend Vx from \"B x V x H\" to \"B x V x 1 x H\"\n        Vx = Vx.unsqueeze(2)  # extend Vx from \"B x V x H\" to \"B x 1 x V x H\"\n        e_new = Ue + Vx + Wx\n        return e_new\n\n\nclass ResidualGatedGCNLayer(nn.Module):\n    \"\"\"Convnet layer with gating and residual connection.\n    \"\"\"\n\n    def __init__(self, hidden_dim, aggregation=\"sum\"):\n        super(ResidualGatedGCNLayer, self).__init__()\n        self.node_feat = NodeFeatures(hidden_dim, aggregation)\n        self.edge_feat = EdgeFeatures(hidden_dim)\n        self.bn_node = BatchNormNode(hidden_dim)\n        self.bn_edge = BatchNormEdge(hidden_dim)\n\n    def forward(self, x, e):\n        \"\"\"\n        Args:\n            x: Node features (batch_size, num_nodes, hidden_dim)\n            e: Edge features (batch_size, num_nodes, num_nodes, hidden_dim)\n\n        Returns:\n            x_new: Convolved node features (batch_size, num_nodes, hidden_dim)\n            e_new: Convolved edge features (batch_size, num_nodes, num_nodes, hidden_dim)\n        \"\"\"\n        e_in = e\n        x_in = x\n        # Edge convolution\n        e_tmp = self.edge_feat(x_in, e_in)  # B x V x V x H\n        # Compute edge gates\n        edge_gate = F.sigmoid(e_tmp)\n        # Node convolution\n        x_tmp = self.node_feat(x_in, edge_gate)\n        # Batch normalization\n        e_tmp = self.bn_edge(e_tmp)\n        x_tmp = self.bn_node(x_tmp)\n        # ReLU Activation\n        e = F.relu(e_tmp)\n        x = F.relu(x_tmp)\n        # Residual connection\n        x_new = x_in + x\n        e_new = e_in + e\n        return x_new, e_new\n\n\nclass MLP(nn.Module):\n    \"\"\"Multi-layer Perceptron for output prediction.\n    \"\"\"\n\n    def __init__(self, hidden_dim, output_dim, L=2):\n        super(MLP, self).__init__()\n        self.L = L\n        U = []\n        for layer in range(self.L - 1):\n            U.append(nn.Linear(hidden_dim, hidden_dim, True))\n        self.U = nn.ModuleList(U)\n        self.V = nn.Linear(hidden_dim, output_dim, True)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input features (batch_size, hidden_dim)\n\n        Returns:\n            y: Output predictions (batch_size, output_dim)\n        \"\"\"\n        Ux = x\n        for U_i in self.U:\n            Ux = U_i(Ux)  # B x H\n            Ux = F.relu(Ux)  # B x H\n        y = self.V(Ux)  # B x O\n        return y\n\n# ==========================================\n# File: models/gcn_model.py\n# Function/Context: ResidualGatedGCNModel.forward\n# ==========================================\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\n\nfrom models.gcn_layers import ResidualGatedGCNLayer, MLP\nfrom utils.model_utils import *\n\n\nclass ResidualGatedGCNModel(nn.Module):\n    \"\"\"Residual Gated GCN Model for outputting predictions as edge adjacency matrices.\n\n    References:\n        Paper: https://arxiv.org/pdf/1711.07553v2.pdf\n        Code: https://github.com/xbresson/spatial_graph_convnets\n    \"\"\"\n\n    def __init__(self, config, dtypeFloat, dtypeLong):\n        super(ResidualGatedGCNModel, self).__init__()\n        self.dtypeFloat = dtypeFloat\n        self.dtypeLong = dtypeLong\n        # Define net parameters\n        self.num_nodes = config.num_nodes\n        self.node_dim = config.node_dim\n        self.voc_nodes_in = config['voc_nodes_in']\n        self.voc_nodes_out = config['num_nodes']  # config['voc_nodes_out']\n        self.voc_edges_in = config['voc_edges_in']\n        self.voc_edges_out = config['voc_edges_out']\n        self.hidden_dim = config['hidden_dim']\n        self.num_layers = config['num_layers']\n        self.mlp_layers = config['mlp_layers']\n        self.aggregation = config['aggregation']\n        # Node and edge embedding layers/lookups\n        self.nodes_coord_embedding = nn.Linear(self.node_dim, self.hidden_dim, bias=False)\n        self.edges_values_embedding = nn.Linear(1, self.hidden_dim//2, bias=False)\n        self.edges_embedding = nn.Embedding(self.voc_edges_in, self.hidden_dim//2)\n        # Define GCN Layers\n        gcn_layers = []\n        for layer in range(self.num_layers):\n            gcn_layers.append(ResidualGatedGCNLayer(self.hidden_dim, self.aggregation))\n        self.gcn_layers = nn.ModuleList(gcn_layers)\n        # Define MLP classifiers\n        self.mlp_edges = MLP(self.hidden_dim, self.voc_edges_out, self.mlp_layers)\n        # self.mlp_nodes = MLP(self.hidden_dim, self.voc_nodes_out, self.mlp_layers)\n\n    def forward(self, x_edges, x_edges_values, x_nodes, x_nodes_coord, y_edges, edge_cw):\n        \"\"\"\n        Args:\n            x_edges: Input edge adjacency matrix (batch_size, num_nodes, num_nodes)\n            x_edges_values: Input edge distance matrix (batch_size, num_nodes, num_nodes)\n            x_nodes: Input nodes (batch_size, num_nodes)\n            x_nodes_coord: Input node coordinates (batch_size, num_nodes, node_dim)\n            y_edges: Targets for edges (batch_size, num_nodes, num_nodes)\n            edge_cw: Class weights for edges loss\n            # y_nodes: Targets for nodes (batch_size, num_nodes, num_nodes)\n            # node_cw: Class weights for nodes loss\n\n        Returns:\n            y_pred_edges: Predictions for edges (batch_size, num_nodes, num_nodes)\n            # y_pred_nodes: Predictions for nodes (batch_size, num_nodes)\n            loss: Value of loss function\n        \"\"\"\n        # Node and edge embedding\n        x = self.nodes_coord_embedding(x_nodes_coord)  # B x V x H\n        e_vals = self.edges_values_embedding(x_edges_values.unsqueeze(3))  # B x V x V x H\n        e_tags = self.edges_embedding(x_edges)  # B x V x V x H\n        e = torch.cat((e_vals, e_tags), dim=3)\n        # GCN layers\n        for layer in range(self.num_layers):\n            x, e = self.gcn_layers[layer](x, e)  # B x V x H, B x V x V x H\n        # MLP classifier\n        y_pred_edges = self.mlp_edges(e)  # B x V x V x voc_edges_out\n        # y_pred_nodes = self.mlp_nodes(x)  # B x V x voc_nodes_out\n        \n        # Compute loss\n        edge_cw = torch.Tensor(edge_cw).type(self.dtypeFloat)  # Convert to tensors\n        loss = loss_edges(y_pred_edges, y_edges, edge_cw)\n        \n        return y_pred_edges, loss\n\n# ==========================================\n# File: utils/beamsearch.py\n# Function/Context: Beamsearch\n# ==========================================\nimport numpy as np\nimport torch\n\n\nclass Beamsearch(object):\n    \"\"\"Class for managing internals of beamsearch procedure.\n\n    References:\n        General: https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/translate/beam.py\n        For TSP: https://github.com/alexnowakvila/QAP_pt/blob/master/src/tsp/beam_search.py\n    \"\"\"\n\n    def __init__(self, beam_size, batch_size, num_nodes,\n                 dtypeFloat=torch.FloatTensor, dtypeLong=torch.LongTensor, \n                 probs_type='raw', random_start=False):\n        \"\"\"\n        Args:\n            beam_size: Beam size\n            batch_size: Batch size\n            num_nodes: Number of nodes in TSP tours\n            dtypeFloat: Float data type (for GPU/CPU compatibility)\n            dtypeLong: Long data type (for GPU/CPU compatibility)\n            probs_type: Type of probability values being handled by beamsearch (either 'raw'/'logits'/'argmax'(TODO))\n            random_start: Flag for using fixed (at node 0) vs. random starting points for beamsearch\n        \"\"\"\n        # Beamsearch parameters\n        self.batch_size = batch_size\n        self.beam_size = beam_size\n        self.num_nodes = num_nodes\n        self.probs_type = probs_type\n        # Set data types\n        self.dtypeFloat = dtypeFloat\n        self.dtypeLong = dtypeLong\n        # Set beamsearch starting nodes\n        self.start_nodes = torch.zeros(batch_size, beam_size).type(self.dtypeLong)\n        if random_start == True:\n            # Random starting nodes\n            self.start_nodes = torch.randint(0, num_nodes, (batch_size, beam_size)).type(self.dtypeLong)\n        # Mask for constructing valid hypothesis\n        self.mask = torch.ones(batch_size, beam_size, num_nodes).type(self.dtypeFloat)\n        self.update_mask(self.start_nodes)  # Mask the starting node of the beam search\n        # Score for each translation on the beam\n        self.scores = torch.zeros(batch_size, beam_size).type(self.dtypeFloat)\n        self.all_scores = []\n        # Backpointers at each time-step\n        self.prev_Ks = []\n        # Outputs at each time-step\n        self.next_nodes = [self.start_nodes]\n\n    def get_current_state(self):\n        \"\"\"Get the output of the beam at the current timestep.\n        \"\"\"\n        current_state = (self.next_nodes[-1].unsqueeze(2)\n                         .expand(self.batch_size, self.beam_size, self.num_nodes))\n        return current_state\n\n    def get_current_origin(self):\n        \"\"\"Get the backpointers for the current timestep.\n        \"\"\"\n        return self.prev_Ks[-1]\n\n    def advance(self, trans_probs):\n        \"\"\"Advances the beam based on transition probabilities.\n\n        Args:\n            trans_probs: Probabilities of advancing from the previous step (batch_size, beam_size, num_nodes)\n        \"\"\"\n        # Compound the previous scores (summing logits == multiplying probabilities)\n        if len(self.prev_Ks) > 0:\n            if self.probs_type == 'raw':\n                beam_lk = trans_probs * self.scores.unsqueeze(2).expand_as(trans_probs)\n            elif self.probs_type == 'logits':\n                beam_lk = trans_probs + self.scores.unsqueeze(2).expand_as(trans_probs)\n        else:\n            beam_lk = trans_probs\n            # Only use the starting nodes from the beam\n            if self.probs_type == 'raw':\n                beam_lk[:, 1:] = torch.zeros(beam_lk[:, 1:].size()).type(self.dtypeFloat)\n            elif self.probs_type == 'logits':\n                beam_lk[:, 1:] = -1e20 * torch.ones(beam_lk[:, 1:].size()).type(self.dtypeFloat)\n        # Multiply by mask\n        beam_lk = beam_lk * self.mask\n        beam_lk = beam_lk.view(self.batch_size, -1)  # (batch_size, beam_size * num_nodes)\n        # Get top k scores and indexes (k = beam_size)\n        bestScores, bestScoresId = beam_lk.topk(self.beam_size, 1, True, True)\n        # Update scores\n        self.scores = bestScores\n        # Update backpointers\n        prev_k = bestScoresId / self.num_nodes\n        self.prev_Ks.append(prev_k)\n        # Update outputs\n        new_nodes = bestScoresId - prev_k * self.num_nodes\n        self.next_nodes.append(new_nodes)\n        # Re-index mask\n        perm_mask = prev_k.unsqueeze(2).expand_as(self.mask)  # (batch_size, beam_size, num_nodes)\n        self.mask = self.mask.gather(1, perm_mask)\n        # Mask newly added nodes\n        self.update_mask(new_nodes)\n\n    def update_mask(self, new_nodes):\n        \"\"\"Sets new_nodes to zero in mask.\n        \"\"\"\n        arr = (torch.arange(0, self.num_nodes).unsqueeze(0).unsqueeze(1)\n               .expand_as(self.mask).type(self.dtypeLong))\n        new_nodes = new_nodes.unsqueeze(2).expand_as(self.mask)\n        update_mask = 1 - torch.eq(arr, new_nodes).type(self.dtypeFloat)\n        self.mask = self.mask * update_mask\n        if self.probs_type == 'logits':\n            # Convert 0s in mask to inf\n            self.mask[self.mask == 0] = 1e20\n\n    def sort_best(self):\n        \"\"\"Sort the beam.\n        \"\"\"\n        return torch.sort(self.scores, 0, True)\n\n    def get_best(self):\n        \"\"\"Get the score and index of the best hypothesis in the beam.\n        \"\"\"\n        scores, ids = self.sort_best()\n        return scores[1], ids[1]\n\n    def get_hypothesis(self, k):\n        \"\"\"Walk back to construct the full hypothesis.\n\n        Args:\n            k: Position in the beam to construct (usually 0s for most probable hypothesis)\n        \"\"\"\n        assert self.num_nodes == len(self.prev_Ks) + 1\n\n        hyp = -1 * torch.ones(self.batch_size, self.num_nodes).type(self.dtypeLong)\n        for j in range(len(self.prev_Ks) - 1, -2, -1):\n            hyp[:, j + 1] = self.next_nodes[j + 1].gather(1, k).view(1, self.batch_size)\n            k = self.prev_Ks[j].gather(1, k)\n        return hyp\n\n# ==========================================\n# File: utils/model_utils.py\n# Function/Context: beamsearch_tour_nodes, beamsearch_tour_nodes_shortest\n# ==========================================\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\n\nfrom utils.beamsearch import *\nfrom utils.graph_utils import *\n\n\ndef beamsearch_tour_nodes(y_pred_edges, beam_size, batch_size, num_nodes, dtypeFloat, dtypeLong, probs_type='raw', random_start=False):\n    \"\"\"\n    Performs beamsearch procedure on edge prediction matrices and returns possible TSP tours.\n\n    Args:\n        y_pred_edges: Predictions for edges (batch_size, num_nodes, num_nodes)\n        beam_size: Beam size\n        batch_size: Batch size\n        num_nodes: Number of nodes in TSP tours\n        dtypeFloat: Float data type (for GPU/CPU compatibility)\n        dtypeLong: Long data type (for GPU/CPU compatibility)\n        random_start: Flag for using fixed (at node 0) vs. random starting points for beamsearch\n\n    Returns: TSP tours in terms of node ordering (batch_size, num_nodes)\n\n    \"\"\"\n    if probs_type == 'raw':\n        # Compute softmax over edge prediction matrix\n        y = F.softmax(y_pred_edges, dim=3)  # B x V x V x voc_edges\n        # Consider the second dimension only\n        y = y[:, :, :, 1]  # B x V x V\n    elif probs_type == 'logits':\n        # Compute logits over edge prediction matrix\n        y = F.log_softmax(y_pred_edges, dim=3)  # B x V x V x voc_edges\n        # Consider the second dimension only\n        y = y[:, :, :, 1]  # B x V x V\n        y[y == 0] = -1e-20  # Set 0s (i.e. log(1)s) to very small negative number\n    # Perform beamsearch\n    beamsearch = Beamsearch(beam_size, batch_size, num_nodes, dtypeFloat, dtypeLong, probs_type, random_start)\n    trans_probs = y.gather(1, beamsearch.get_current_state())\n    for step in range(num_nodes - 1):\n        beamsearch.advance(trans_probs)\n        trans_probs = y.gather(1, beamsearch.get_current_state())\n    # Find TSP tour with highest probability among beam_size candidates\n    ends = torch.zeros(batch_size, 1).type(dtypeLong)\n    return beamsearch.get_hypothesis(ends)\n\n\ndef beamsearch_tour_nodes_shortest(y_pred_edges, x_edges_values, beam_size, batch_size, num_nodes,\n                                   dtypeFloat, dtypeLong, probs_type='raw', random_start=False):\n    \"\"\"\n    Performs beamsearch procedure on edge prediction matrices and returns possible TSP tours.\n\n    Final predicted tour is the one with the shortest tour length.\n    (Standard beamsearch returns the one with the highest probability and does not take length into account.)\n\n    Args:\n        y_pred_edges: Predictions for edges (batch_size, num_nodes, num_nodes)\n        x_edges_values: Input edge distance matrix (batch_size, num_nodes, num_nodes)\n        beam_size: Beam size\n        batch_size: Batch size\n        num_nodes: Number of nodes in TSP tours\n        dtypeFloat: Float data type (for GPU/CPU compatibility)\n        dtypeLong: Long data type (for GPU/CPU compatibility)\n        probs_type: Type of probability values being handled by beamsearch (either 'raw'/'logits'/'argmax'(TODO))\n        random_start: Flag for using fixed (at node 0) vs. random starting points for beamsearch\n\n    Returns:\n        shortest_tours: TSP tours in terms of node ordering (batch_size, num_nodes)\n\n    \"\"\"\n    if probs_type == 'raw':\n        # Compute softmax over edge prediction matrix\n        y = F.softmax(y_pred_edges, dim=3)  # B x V x V x voc_edges\n        # Consider the second dimension only\n        y = y[:, :, :, 1]  # B x V x V\n    elif probs_type == 'logits':\n        # Compute logits over edge prediction matrix\n        y = F.log_softmax(y_pred_edges, dim=3)  # B x V x V x voc_edges\n        # Consider the second dimension only\n        y = y[:, :, :, 1]  # B x V x V\n        y[y == 0] = -1e-20  # Set 0s (i.e. log(1)s) to very small negative number\n    # Perform beamsearch\n    beamsearch = Beamsearch(beam_size, batch_size, num_nodes, dtypeFloat, dtypeLong, probs_type, random_start)\n    trans_probs = y.gather(1, beamsearch.get_current_state())\n    for step in range(num_nodes - 1):\n        beamsearch.advance(trans_probs)\n        trans_probs = y.gather(1, beamsearch.get_current_state())\n    # Initially assign shortest_tours as most probable tours i.e. standard beamsearch\n    ends = torch.zeros(batch_size, 1).type(dtypeLong)\n    shortest_tours = beamsearch.get_hypothesis(ends)\n    # Compute current tour lengths\n    shortest_lens = [1e6] * len(shortest_tours)\n    for idx in range(len(shortest_tours)):\n        shortest_lens[idx] = tour_nodes_to_tour_len(shortest_tours[idx].cpu().numpy(),\n                                                    x_edges_values[idx].cpu().numpy())\n    # Iterate over all positions in beam (except position 0 --> highest probability)\n    for pos in range(1, beam_size):\n        ends = pos * torch.ones(batch_size, 1).type(dtypeLong)  # New positions\n        hyp_tours = beamsearch.get_hypothesis(ends)\n        for idx in range(len(hyp_tours)):\n            hyp_nodes = hyp_tours[idx].cpu().numpy()\n            hyp_len = tour_nodes_to_tour_len(hyp_nodes, x_edges_values[idx].cpu().numpy())\n            # Replace tour in shortest_tours if new length is shorter than current best\n            if hyp_len < shortest_lens[idx] and is_valid_tour(hyp_nodes, num_nodes):\n                shortest_tours[idx] = hyp_tours[idx]\n                shortest_lens[idx] = hyp_len\n    return shortest_tours",
  "description": "Combined Analysis:\n- [data/generate_tsp_concorde.py]: This file implements Phase 1, Step 1-2 of the algorithm: data preparation. It generates random 2D node coordinates uniformly in [0,1]Â² (via np.random.random) and uses the Concorde TSP solver to compute optimal tours. The output format includes node coordinates followed by the optimal tour permutation (1-indexed, with the starting node repeated at the end). This creates the training pairs (graph, optimal tour) required for supervised learning. It does not implement the optimization model directly but generates ground-truth solutions that respect the TSP constraints via an exact solver.\n- [main.ipynb]: This file implements the training infrastructure and main loop for the GCN-based TSP solver. It handles data loading, model instantiation, training loop with gradient accumulation, and logging. The core GCN computations are delegated to the ResidualGatedGCNModel class. The file matches Phase 2 (Model Training) of the algorithm steps, specifically implementing mini-batch processing, forward/backward passes, and optimization with Adam.\n- [models/gcn_layers.py]: This file implements the core graph convolutional network layers described in the paper's Phase 2 (Model Training) steps 2c-2d. Specifically, it provides the ResidualGatedGCNLayer class that corresponds to the anisotropic diffusion equations (Eq. 4-5) for updating node and edge embeddings through multiple layers. The NodeFeatures class implements the gated aggregation mechanism (with mean/sum options), EdgeFeatures implements the edge update, and MLP corresponds to the final edge probability classifier (Eq. 6). These components are essential building blocks for the full GCN architecture used in the TSP solution pipeline.\n- [models/gcn_model.py]: This file implements the core neural network architecture described in Phase 2 of the algorithm. Specifically, it implements the Residual Gated GCN model that performs the following key steps: 1) Node and edge embedding layers that process input coordinates and distances (corresponding to step 2c), 2) Multiple graph convolution layers (configurable via num_layers, typically 30 as per the paper) that implement anisotropic diffusion (Eq. 4-5), 3) An MLP classifier that outputs edge probabilities p_ij^TSP (Eq. 6), and 4) Loss computation using weighted binary cross-entropy against ground-truth adjacency matrices. The forward method exactly corresponds to steps 2c-2f of the training phase. However, it does not implement the data preparation (Phase 1), training loop optimization (Adam, learning rate decay), or beam search inference (Phase 3).\n- [utils/beamsearch.py]: This file implements the core beam search decoding algorithm (Phase 3, steps 2-3) from the paper. The Beamsearch class manages the beam expansion, masking of visited nodes, and cumulative probability scoring. It directly corresponds to the inference step where edge probabilities from the trained model are used to construct tours via beam search. The class handles batch processing, supports both raw probabilities and logits, and includes masking to enforce the TSP constraint of visiting each node exactly once. The get_hypothesis method reconstructs the full tour permutation from the beam's backpointers.\n- [utils/model_utils.py]: This file implements the core inference algorithm steps from Phase 3 of the paper. The two beam search functions directly correspond to the 'Beam Search Decoding' phase where edge probability heatmaps are converted into valid TSP tours. beamsearch_tour_nodes implements the standard beam search strategy (highest probability tour), while beamsearch_tour_nodes_shortest implements the 'beam search + shortest tour' strategy mentioned in the paper. Both functions take the model's edge predictions (p_ij^TSP), perform parallelized beam search expansion while masking visited nodes, and return complete tours. The code handles both raw probabilities and logits, and includes the key steps of expanding partial tours, keeping top-b candidates, and selecting final tours based on either probability or actual tour length.",
  "dependencies": [
    "utils.google_tsp_reader",
    "utils.plot_utils",
    "sklearn",
    "argparse",
    "matplotlib",
    "config",
    "utils.model_utils.loss_edges",
    "pandas",
    "numpy",
    "concorde.tsp.TSPSolver",
    "time",
    "utils.beamsearch.Beamsearch",
    "torch",
    "utils.graph_utils.tour_nodes_to_tour_len",
    "torch.nn.functional",
    "os",
    "tensorboardX",
    "models.gcn_model",
    "fastprogress",
    "torch.nn",
    "utils.model_utils",
    "pprint",
    "scipy",
    "networkx",
    "utils.graph_utils",
    "utils.graph_utils.is_valid_tour",
    "models.gcn_layers.ResidualGatedGCNLayer",
    "models.gcn_layers.MLP"
  ]
}