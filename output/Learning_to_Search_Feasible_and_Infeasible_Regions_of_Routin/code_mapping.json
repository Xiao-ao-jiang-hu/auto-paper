{
  "file_path": "GIRE_Example.ipynb, agent/ppo.py, nets/actor_network.py, nets/graph_layers.py, problems/problem_cvrp.py, problems/problem_tsp.py, utils/utils.py",
  "function_name": "PPO.rollout, Actor, kopt_Decoder, CVRP, TSP, augmentation",
  "code_snippet": "\n\n# ==========================================\n# File: GIRE_Example.ipynb\n# Function/Context: \n# ==========================================\nimport os\nimport json\nimport torch\nimport pprint\nimport numpy as np\nimport random\nfrom tensorboard_logger import Logger as TbLogger\nimport warnings\n\nfrom problems.problem_tsp import TSP\nfrom problems.problem_cvrp import CVRP\nfrom agent.ppo import PPO\nfrom tqdm.notebook import tqdm\n\n# Initialize hyperparameters\nfrom options import get_options\nopts = get_options('')\n\n# Configure for CVRP with GIRE\nopts.problem = 'cvrp' \nopts.dummy_rate = 0.4\nopts.wo_feature1 = False\nopts.wo_feature3 = False\nopts.wo_regular = False\nopts.wo_bonus = False\nopts.wo_RNN = False\nopts.wo_MDP = True\nopts.graph_size = 50\nopts.val_dataset='./datasets/cvrp_50.pkl'\nopts.init_val_met = 'random'\nopts.no_saving = True\nopts.no_tb = True\nopts.val_size = 10\nopts.load_path = './pre-trained/cvrp50.pt'\nopts.device = torch.device(\"cuda\" if opts.use_cuda else \"cpu\")\nopts.no_progress_bar = True\n\n# Load problem environment with GIRE configuration\ndef load_problem(name):\n    problem = {\n        'tsp': TSP,\n        'cvrp': CVRP,\n    }.get(name, None)\n    assert problem is not None, \"Currently unsupported problem: {}!\".format(name)\n    return problem\n\n# Disable Tensorboard logging\ntb_logger = None\n\n# Initialize CVRP problem with GIRE parameters\nproblem = load_problem(opts.problem)(\n                        p_size = opts.graph_size,\n                        init_val_met = opts.init_val_met,\n                        with_assert = opts.use_assert,\n                        DUMMY_RATE = opts.dummy_rate,\n                        k = opts.k,\n                        with_bonus = not opts.wo_bonus,\n                        with_regular = not opts.wo_regular)\n\n# Initialize PPO agent with GIRE-enabled neural network\nagent = PPO(problem, opts)\n\n# Load pre-trained model with GIRE capabilities\nassert opts.load_path is None or opts.resume is None, \"Only one of load path and resume can be given\"\nload_path = opts.load_path if opts.load_path is not None else opts.resume\nif load_path is not None:\n    agent.load(load_path)\n\n# Prepare CVRP dataset for inference\nfrom torch.utils.data import DataLoader\nfrom problems.problem_cvrp import CVRPDataset\ndataset = CVRPDataset(size = 50, num_samples = 10, DUMMY_RATE=0.4)\nbatch = next(iter(DataLoader(dataset, batch_size=10)))\ncoordinates_first = batch['coordinates'][0]\ndemand_first = batch['demand'][0]\n\n# Generate initial solution using random initialization (part of D2A strategy)\nrec = problem.get_initial_solutions(batch)\nprint(rec[0])\n\n# ==========================================\n# File: agent/ppo.py\n# Function/Context: PPO.rollout\n# ==========================================\nimport os\nfrom tqdm import tqdm\nimport warnings\nimport torch\nfrom torch.utils.data import DataLoader\nimport torch.multiprocessing as mp\nimport torch.distributed as dist\nfrom tensorboard_logger import Logger as TbLogger\nimport numpy as np\nimport random\n\nfrom utils import clip_grad_norms\nfrom nets.actor_network import Actor\nfrom nets.critic_network import Critic\nfrom utils import torch_load_cpu, get_inner_model, move_to\nfrom utils.logger import log_to_tb_train\nfrom agent.utils import validate,gather_tensor_and_concat\nfrom problems.problem_cvrp import total_history\n\nfeasibility_history_base = [True] * (total_history)\n\nclass Memory:\n    def __init__(self):\n        self.actions = []\n        self.states = []\n        self.logprobs = []\n        self.rewards = [] \n        self.obj = []\n        self.context = []\n        self.context2 = []\n        \n    def clear_memory(self):\n        del self.actions[:]\n        del self.states[:]\n        del self.logprobs[:]\n        del self.rewards[:]\n        del self.obj[:]\n        del self.context[:]\n        del self.context2[:]\n\nclass PPO:\n    def __init__(self, problem, opts):\n        \n        # figure out the options\n        self.opts = opts\n        \n        # figure out the actor\n        self.actor = Actor(\n            problem = problem,\n            embedding_dim = opts.embedding_dim,\n            hidden_dim = opts.hidden_dim,\n            n_heads_actor = opts.actor_head_num,\n            n_layers = opts.n_encode_layers,\n            normalization = opts.normalization,\n            v_range = opts.v_range,\n            seq_length = problem.size,\n            k = opts.k,\n            with_RNN = not opts.wo_RNN,\n            with_feature1 = not opts.wo_feature1,\n            with_feature3 = not opts.wo_feature3,\n            with_simpleMDP = opts.wo_MDP\n        )\n        \n        if not opts.eval_only:\n            # figure out the critic\n            self.critic = Critic(\n                    embedding_dim = opts.embedding_dim,\n                    hidden_dim = opts.hidden_dim,\n                    n_heads = opts.critic_head_num,\n                    n_layers = opts.n_encode_layers,\n                    normalization = opts.normalization,\n                    with_regular = not opts.wo_regular,\n                    with_bonus = not opts.wo_bonus\n                )\n            \n            self.optimizer = torch.optim.Adam(\n                [{'params':  self.actor.parameters(), 'lr': opts.lr_model}] + \n                [{'params':  self.critic.parameters(), 'lr': opts.lr_critic}])\n            \n            self.lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, opts.lr_decay, last_epoch=-1,)\n\n        print(f'Distributed: {opts.distributed}')\n        if opts.use_cuda and not opts.distributed:\n            self.actor.to(opts.device)\n            if not opts.eval_only: self.critic.to(opts.device)\n                \n    \n    def rollout(self, problem, T, val_m, stall_limit, batch, record = False, show_bar = False):\n        bs, gs, _ = batch['coordinates'].size()\n        batch = move_to(batch, self.opts.device)\n        batch_aug_same = problem.augment(batch, val_m, only_copy=True)\n        batch_aug = problem.augment(batch, val_m)\n        batch_feature = problem.input_feature_encoding(batch_aug)\n        \n        solutions = move_to(problem.get_initial_solutions(batch_aug_same), self.opts.device)\n        solution_best = solutions.clone()\n        \n        obj, context = problem.get_costs(batch_aug_same, solutions, get_context = True, check_full_feasibility = True)\n        obj = torch.cat((obj[:,None], obj[:,None],obj[:,None]),-1).clone()\n        context2 = torch.zeros(bs*val_m,9).to(solutions.device);context2[:,-1] = 1\n        feasibility_history = torch.tensor(feasibility_history_base).view(-1,total_history).expand(bs*val_m, total_history).to(obj.device)\n        \n        solution_history = [solutions.clone()]\n        solution_best_history = [solution_best.clone()]\n        obj_history = [obj.clone()]        \n        feasible_history_recorded = [feasibility_history[:,0]]\n        action = None\n        reward = []\n        stall_cnt_ins = torch.zeros(bs * val_m).to(solution_best.device)\n\n        for t in tqdm(range(T), disable = self.opts.no_progress_bar or not show_bar, desc = 'rollout', bar_format='{l_bar}{bar:20}{r_bar}{bar:-20b}'):       \n\n            action = self.actor(problem,\n                                batch_aug_same,\n                                batch_feature,\n                                solutions,\n                                context,\n                                context2,\n                                action)[0]\n\n            solutions, rewards, obj, feasibility_history, context, context2, info = problem.step(batch_aug_same, \n                                                                                                solutions,\n                                                                                                action,\n                                                                                                obj,\n                                                                                                feasibility_history,\n                                                                                                t,\n                                                                                                weights = 0)\n            index = rewards[:,0] > 0.0\n            solution_best[index] = solutions[index].clone()\n\n            # record informations\n            reward.append(rewards[:,0].clone())\n            obj_history.append(obj.clone())\n            if record: \n                solution_history.append(solutions.clone())\n                solution_best_history.append(solution_best.clone())\n                feasible_history_recorded.append(feasibility_history[:,0].clone())\n            \n            # augment if stall>0 (checking every step)\n            if stall_limit > 0:\n                batch_aug_temp = problem.augment(batch, val_m)\n                stall_cnt_ins = stall_cnt_ins * (1 - index.float()) + 1\n                index_aug = stall_cnt_ins >= stall_limit\n                batch_aug['coordinates'][index_aug] = batch_aug_temp['coordinates'][index_aug]\n                batch_feature = problem.input_feature_encoding(batch_aug)\n                stall_cnt_ins[index_aug] *= 0\n\n        \n        # assert\n        best_length = problem.get_costs(batch_aug_same, solution_best, get_context = False, check_full_feasibility = True)\n        assert (best_length - obj[:,1] < 1e-5).all(), (best_length, obj, best_length - obj[:,1])\n        assert (problem.augment(batch, val_m, only_copy=True)['coordinates'] == batch_aug_same['coordinates']).all()\n        out = (obj[:,1].reshape(bs, val_m).min(1)[0], # batch_size, 1\n               torch.stack(obj_history,1).view(bs, val_m, T+1, -1).min(1)[0], # batch_size, T, 2/3\n               torch.stack(reward,1).view(bs, val_m, T).max(1)[0], # batch_size, T\n               None if not record else (solution_history, solution_best_history, feasible_history_recorded)\n              )\n        \n        return out\n\n# ==========================================\n# File: nets/actor_network.py\n# Function/Context: Actor\n# ==========================================\nimport torch\nfrom torch import nn\nfrom nets.graph_layers import MultiHeadEncoder, EmbeddingNet, MultiHeadPosCompat, kopt_Decoder\n\nclass mySequential(nn.Sequential):\n    def forward(self, *inputs):\n        for module in self._modules.values():\n            if type(inputs) == tuple:\n                inputs = module(*inputs)\n            else:\n                inputs = module(inputs)\n        return inputs\n    \nclass Actor(nn.Module):\n\n    def __init__(self,\n                 problem,\n                 embedding_dim,\n                 hidden_dim,\n                 n_heads_actor,\n                 n_layers,\n                 normalization,\n                 v_range,\n                 seq_length,\n                 k,\n                 with_RNN,\n                 with_feature1,\n                 with_feature3,\n                 with_simpleMDP\n                 ):\n        super(Actor, self).__init__()\n\n        problem_name = problem.NAME\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.n_heads_actor = n_heads_actor\n        self.n_layers = n_layers\n        self.normalization = normalization\n        self.range = v_range\n        self.seq_length = seq_length                \n        self.k = k\n        self.with_RNN = with_RNN\n        self.with_feature1 = with_feature1\n        self.with_feature3 = with_feature3\n        self.with_simpleMDP = with_simpleMDP\n        \n        if problem_name == 'tsp':\n            self.node_dim = 2\n        elif problem_name == 'cvrp':\n            self.node_dim = 8 if self.with_feature1 else 6\n        else:\n            raise NotImplementedError()\n            \n        self.embedder = EmbeddingNet(\n                            self.node_dim,\n                            self.embedding_dim,\n                            self.seq_length)\n        \n        self.encoder = mySequential(*(\n                MultiHeadEncoder(self.n_heads_actor, \n                                self.embedding_dim, \n                                self.hidden_dim,\n                                number_aspect = 2,\n                                normalization = self.normalization\n                                )\n            for _ in range(self.n_layers)))\n\n        self.pos_encoder = MultiHeadPosCompat(self.n_heads_actor, \n                                self.embedding_dim, \n                                self.hidden_dim, \n                                )\n        \n        self.decoder = kopt_Decoder(self.n_heads_actor, \n                                    input_dim = self.embedding_dim, \n                                    embed_dim = self.embedding_dim,\n                                    v_range = self.range,\n                                    k = self.k,\n                                    with_RNN = self.with_RNN,\n                                    with_feature3 = self.with_feature3,\n                                    simpleMDP = self.with_simpleMDP\n                                    )\n\n        print('# params in Actor', self.get_parameter_number())\n        \n    def get_parameter_number(self):\n        total_num = sum(p.numel() for p in self.parameters())\n        trainable_num = sum(p.numel() for p in self.parameters() if p.requires_grad)\n        return {'Total': total_num, 'Trainable': trainable_num}\n\n    def forward(self, problem, batch, x_in, solution, context, context2,last_action, fixed_action = None, require_entropy = False, to_critic = False, only_critic  = False):\n        # the embedded input x\n        bs, gs, in_d = x_in.size()\n        \n        if problem.NAME == 'cvrp':\n            \n            visited_time, to_actor = problem.get_dynamic_feature(solution, batch, context)\n            if self.with_feature1:\n                x_in = torch.cat((x_in, to_actor), -1)\n            else:\n                x_in = torch.cat((x_in, to_actor[:,:,:-2]), -1)\n            del context, to_actor\n\n        elif problem.NAME == 'tsp':\n            visited_time = problem.get_order(solution, return_solution = False)\n        else: \n            raise NotImplementedError()\n            \n        h_embed, h_pos = self.embedder(x_in, solution, visited_time)\n        aux_scores = self.pos_encoder(h_pos)\n        \n        h_em_final, _ = self.encoder(h_embed, aux_scores)\n        \n        if only_critic:\n            return (h_em_final)\n        \n        action, log_ll, entropy = self.decoder(problem,\n                                               h_em_final,\n                                               solution,\n                                               context2,\n                                               visited_time,\n                                               last_action,\n                                               fixed_action = fixed_action,\n                                               require_entropy = require_entropy)\n        \n        # assert (visited_time == visited_time_clone).all()\n        if require_entropy:\n            return action, log_ll, (h_em_final) if to_critic else None, entropy\n        else:\n            return action, log_ll, (h_em_final) if to_critic else None\n\n# ==========================================\n# File: nets/graph_layers.py\n# Function/Context: kopt_Decoder\n# ==========================================\nimport torch\nimport torch.nn.functional as F\nfrom torch.distributions import Categorical\nimport numpy as np\nfrom torch import nn\nimport math\n\nclass kopt_Decoder(nn.Module):\n    def __init__(\n            self,\n            n_heads,\n            input_dim,\n            embed_dim=None,\n            val_dim=None,\n            key_dim=None,\n            v_range = 6,\n            k = 5,\n            with_RNN = True,\n            with_feature3 = True,\n            simpleMDP = False\n    ):\n        super(kopt_Decoder, self).__init__()\n        self.n_heads = n_heads\n        self.embed_dim = embed_dim\n        self.input_dim = input_dim\n        self.key_dim = input_dim\n        self.range = v_range\n        self.with_RNN = with_RNN\n        self.with_feature3 = with_feature3\n        self.simpleMDP = simpleMDP\n        print('simpleMDP: ', self.simpleMDP)\n        assert simpleMDP\n        \n        self.linear_K1 = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n        self.linear_K2 = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n        self.linear_K3 = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n        self.linear_K4 = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n        \n        self.linear_Q1 = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n        self.linear_Q2 = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n        self.linear_Q3 = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n        self.linear_Q4 = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n        \n        if self.with_feature3:\n            self.meta_linear = nn.Sequential(\n                            nn.Linear(9, 8),\n                            nn.ReLU(inplace=True),\n                            nn.Linear(8, self.embed_dim * 2))                \n        else:\n            self.linear_V1 = nn.Parameter(torch.Tensor(self.embed_dim))\n            self.linear_V2 = nn.Parameter(torch.Tensor(self.embed_dim))\n        \n        if self.with_RNN:\n            self.init_hidden_W = nn.Linear(self.embed_dim, self.embed_dim)\n            self.init_query_learnable = nn.Parameter(torch.Tensor(self.embed_dim))\n            self.rnn1 = nn.GRUCell(self.embed_dim, self.embed_dim)\n            self.rnn2 = nn.GRUCell(self.embed_dim, self.embed_dim)\n        else:\n            self.init_query_learnable = nn.Parameter(torch.Tensor(self.embed_dim))\n        \n        self.init_parameters()\n\n    def init_parameters(self):\n        for param in self.parameters():\n            stdv = 1. / math.sqrt(param.size(-1))\n            param.data.uniform_(-stdv, stdv)\n            \n    def forward(self, problem, h, rec, context2, visited_time, last_action, fixed_action = None, require_entropy = False):    \n        bs, gs, _, ll, action, entropys = *h.size(), 0.0, None, []\n        action_index = torch.zeros(bs, problem.k_max, dtype=torch.long).to(rec.device)\n        k_action_left = torch.zeros(bs, problem.k_max + 1, dtype=torch.long).to(rec.device)\n        k_action_right = torch.zeros(bs, problem.k_max, dtype=torch.long).to(rec.device)\n        next_of_last_action = torch.zeros_like(rec[:,:1], dtype=torch.long).to(rec.device) - 1\n        mask = torch.zeros_like(rec, dtype=torch.bool).to(rec.device)\n        stopped = torch.ones(bs, dtype=torch.bool).to(rec.device)\n        h_mean = h.mean(1)\n        init_query = self.init_query_learnable.repeat(bs,1)\n        input_q1 = input_q2 = init_query.clone()\n        \n        if self.with_RNN:\n            init_hidden = self.init_hidden_W(h_mean)\n            q1 = q2 = init_hidden.clone()\n            \n        if self.with_feature3:\n            decoder_condition = context2\n            linear_V = self.meta_linear(decoder_condition)\n            linear_V1 = linear_V[:,:self.embed_dim]\n            linear_V2 = linear_V[:,self.embed_dim:]\n        else:\n            linear_V1 = self.linear_V1.view(1, -1).expand(bs, -1)\n            linear_V2 = self.linear_V2.view(1, -1).expand(bs, -1)\n        \n        for i in range(problem.k_max):\n            # GRUs\n            if self.with_RNN:\n                q1 = self.rnn1(input_q1, q1)\n                q2 = self.rnn2(input_q2, q2)\n            else:\n                q1 = input_q1\n                q2 = input_q2\n\n            # Dual-Stream Attention\n            result = (linear_V1.unsqueeze(1) * torch.tanh(self.linear_K1(h) + \n                                                self.linear_Q1(q1).unsqueeze(1) +\n                                                self.linear_K3(h) * self.linear_Q3(q1).unsqueeze(1)\n                                                )).sum(-1)      # \\mu stream\n\n# ==========================================\n# File: problems/problem_cvrp.py\n# Function/Context: CVRP\n# ==========================================\nfrom torch.utils.data import Dataset\nimport torch\nimport pickle\nimport os\nimport numpy as np\nfrom utils import augmentation\n\nCAPACITIES = {\n    20: 30.,\n    50: 40.,\n    100: 50.,\n    200: 70.,\n}\n\nEPSILON = {\n    20: 0.33,\n    50: 0.625,\n    100: 1.0,\n    200: 1.429\n}\n\ntotal_history = 25 # (T_ES in the paper)\n\nclass CVRP(object):\n\n    NAME = 'cvrp'  # Capacitiated Vehicle Routing Problem\n    \n    def __init__(self, p_size, init_val_met = 'random', with_assert = False, DUMMY_RATE = 0.5, k = 5, with_bonus = True, with_regular = True):\n        \n        self.size = int(np.ceil(p_size * (1 + DUMMY_RATE)))   # the number of real nodes plus dummy nodes in cvrp\n        self.real_size = p_size\n        self.dummy_size = self.size - self.real_size\n        self.init_val_met = init_val_met\n        self.k_max = k\n        self.state = 'eval'\n        self.epsilon = EPSILON[p_size]\n        self.with_bonus = with_bonus\n        self.with_regular = with_regular\n        self.with_assert = with_assert\n        assert self.real_size + self.dummy_size == self.size\n        print(f'CVRP with {p_size} nodes and {self.dummy_size} dummy depots (total {self.size}).\\n', \n              f'Regulation: {self.with_regular} Bonus: {self.with_bonus} Do assert: {self.with_assert}.\\n',\n              f'MAX {self.k_max}-opt.\\n')\n    \n    def train(self):\n        self.state = 'train'\n        \n    def eval(self):\n        self.state = 'eval'\n    \n    def augment(self, batch, val_m, only_copy=False):\n        bs, gs, dim = batch['coordinates'].size()\n        if only_copy:\n            coordinates = batch['coordinates'].unsqueeze(1).expand(bs,val_m,gs,dim).clone().reshape(-1,gs,dim)\n        else:\n            coordinates = batch['coordinates'].unsqueeze(1).expand(bs,val_m,gs,dim).clone()\n            coordinates = augmentation(coordinates, val_m).reshape(-1,gs,dim)\n        demand = batch['demand'].unsqueeze(1).expand(bs,val_m,gs).clone().reshape(-1,gs)\n        return {'coordinates': coordinates,\n                'demand':demand}\n    \n    def input_feature_encoding(self, batch):\n        return batch['coordinates'].clone()\n    \n    def get_initial_solutions(self, batch):\n      \n      batch_size = batch['coordinates'].size(0)\n  \n      def get_solution(methods):\n          p_size = self.size\n          \n          if methods == 'random':\n              \n              candidates = torch.ones(batch_size,self.size).bool()\n              candidates[:,:self.dummy_size] = False\n              \n              rec = torch.zeros(batch_size, self.size).long()\n              selected_node = torch.zeros(batch_size, 1).long()\n              cum_demand = torch.zeros(batch_size, 2)\n              \n              demand = batch['demand'].cpu()\n              \n              for i in range(self.size - 1):\n                  \n                  dists = torch.arange(p_size).view(-1, p_size).expand(batch_size, p_size).clone()\n                  dists.scatter_(1, selected_node, 1e5)\n                  dists[~candidates] = 1e5\n                  dists[cum_demand[:,-1:] + demand > 1.] = 1e5\n                  dists.scatter_(1,cum_demand[:,:-1].long() + 1, 1e4)\n                  \n                  next_selected_node = dists.min(-1)[1].view(-1,1)\n                  selected_demand = demand.gather(1,next_selected_node)\n                  cum_demand[:,-1:] = torch.where(selected_demand >0, selected_demand + cum_demand[:,-1:], 0 * cum_demand[:,-1:])\n                  cum_demand[:,:-1] = torch.where(selected_demand >0, cum_demand[:,:-1], cum_demand[:,:-1] + 1)\n    \n                  rec.scatter_(1,selected_node, next_selected_node)\n                  candidates.scatter_(1, next_selected_node, 0)\n                  selected_node = next_selected_node  \n                  \n              return rec\n          \n          \n          elif methods == 'greedy':\n\n              candidates = torch.ones(batch_size,self.size).bool()\n              candidates[:,:self.dummy_size] = False\n              \n              rec = torch.zeros(batch_size, self.size).long()\n              selected_node = torch.zeros(batch_size, 1).long()\n              cum_demand = torch.zeros(batch_size, 2)\n              \n              coor = batch['coordinates'].cpu()\n              demand = batch['demand'].cpu()\n              \n              for i in range(self.size - 1):\n                  \n                  coor_now = batch['coordinates'].cpu().gather(1, selected_node.unsqueeze(-1).expand(batch_size, self.size, 2))\n                  dists = (coor_now - coor).norm(p=2, dim=2)\n                  \n                  dists.scatter_(1, selected_node, 1e5)\n                  dists[~candidates] = 1e5\n                  dists[cum_demand[:,-1:] + demand > 1.] = 1e5\n                  dists.scatter_(1,cum_demand[:,:-1].long() + 1, 1e4)\n                  \n                  next_selected_node = dists.min(-1)[1].view(-1,1)\n                  selected_demand = demand.gather(1,next_selected_node)\n                  cum_demand[:,-1:] = torch.where(selected_demand >0, selected_demand + cum_demand[:,-1:], 0 * cum_demand[:,-1:])\n                  cum_demand[:,:-1] = torch.where(selected_demand >0, cum_demand[:,:-1], cum_demand[:,:-1] + 1)\n                  \n                  rec.scatter_(1,selected_node, next_selected_node)\n                  candidates.scatter_(1, next_selected_node, 0)\n                  selected_node = next_selected_node                         \n\n              return rec\n          \n          else:\n              raise NotImplementedError()\n\n      return get_solution(self.init_val_met).expand(batch_size, self.size).clone()\n  \n    def f(self, p): # The entropy measure in Eq.(5)\n        return torch.clamp(1 - 0.5 * torch.log2(2.5*np.pi*np.e*p*(1-p)+ 1e-5), 0, 1)\n    \n    def step(self, batch, rec, action, obj, feasible_history, t, weights = 0):\n        \n        bs, gs = rec.size()\n        pre_bsf = obj[:,1:].clone() # batch_size, 3 (current, bsf, tsp_bsf)\n        feasible_history = feasible_history.clone() # bs, total_history \n        \n        # k-opt step\n        next_state = self.k_opt(rec, action)\n        next_obj, context = self.get_costs(batch, next_state, True)\n        \n        # MDP step\n        non_feasible_cost_total = torch.clamp_min(context[-1] - 1.00001, 0.0).sum(-1)\n        feasible = non_feasible_cost_total <= 0.0\n        soft_infeasible = (non_feasible_cost_total <= self.epsilon) & (non_feasible_cost_total > 0.)\n        \n        now_obj = pre_bsf.clone()\n        now_obj[feasible,0] = next_obj[feasible].clone()\n        now_obj[soft_infeasible,1] = next_obj[soft_infeasible].clone()\n        now_bsf = torch.min(pre_bsf, now_obj)\n        rewards = (pre_bsf - now_bsf) #bs,2(feasible_reward,infeasible_reward) \n\n        # feasible history step\n        feasible_history[:,1:] = feasible_history[:,:total_history-1].clone()\n        feasible_history[:,0] = feasible.clone()\n        \n        # compute the ES features\n        feasible_history_pre = feasible_history[:,1:]\n        feasible_history_post = feasible_history[:,:total_history-1]\n        f_to_if = ((feasible_history_pre == True) & (feasible_history_post == False)).sum(1,True) / (total_history-1)\n        f_to_f = ((feasible_history_pre == True) & (feasible_history_post == True)).sum(1,True) / (total_history-1)\n        if_to_f = ((feasible_history_pre == False) & (feasible_history_post == True)).sum(1,True) / (total_history-1)\n        if_to_if = ((feasible_history_pre == False) & (feasible_history_post == False)).sum(1,True) / (total_history-1)\n        f_to_if_2 = f_to_if / (f_to_if + f_to_f + 1e-5)\n        f_to_f_2 =  f_to_f / (f_to_if + f_to_f + 1e-5)\n        if_to_f_2 =  if_to_f / (if_to_f + if_to_if + 1e-5)\n        if_to_if_2 =  if_to_if / (if_to_f + if_to_if + 1e-5)\n\n        # update info to decoder\n        active = (t >= (total_history - 2))\n        context2 = torch.cat((\n                      (if_to_if * active),\n                      (if_to_if_2 * active),\n                      (f_to_f * active),\n                      (f_to_f_2 * active),\n                      (if_to_f * active),\n                      (if_to_f_2 * active),\n                      (f_to_if * active),\n                      (f_to_if_2 * active),\n                      feasible.unsqueeze(-1).float(),\n                    ),-1) # 9 ES features\n        \n        # update regulation\n        reg = self.f(f_to_f_2) + self.f(if_to_if_2)\n        \n        reward = torch.cat((rewards[:,:1], # reward\n                            -1 * reg * weights * 0.05 * self.with_regular, # regulation, alpha = 0.05\n                            rewards[:,1:2] * 0.05 * self.with_bonus, # bonus, beta = 0.05\n                           ),-1)\n\n        out = (next_state, \n               reward,\n               torch.cat((next_obj[:,None], now_bsf),-1), \n               feasible_history,\n               context,\n               context2,\n               (if_to_if,if_to_f,f_to_if,f_to_f,if_to_if_2,if_to_f_2,f_to_if_2,f_to_f_2)\n               )\n        \n        return out\n\n    def k_opt(self, rec, action):\n        \n        # action bs * (K_index, K_from, K_to)\n        selected_index = action[:,:self.k_max]\n        left = action[:,self.k_max:2*self.k_max]\n        right = action[:,2*self.k_max:]\n        \n        # prepare\n        rec_next = rec.clone()\n        right_nodes = rec.gather(1,selected_index)\n        argsort = rec.argsort()\n        \n        # new rec\n        rec_next.scatter_(1,left,right)\n        cur = left[:,:1].clone()\n        for i in range(self.size - 2): # self.size - 2 is already correct\n            next_cur = rec_next.gather(1,cur)\n            pre_next_wrt_old = argsort.gather(1, next_cur)\n            reverse_link_condition = ((cur!=pre_next_wrt_old) & ~((next_cur==right_nodes).any(-1,True)))\n            next_next_cur = rec_next.gather(1,next_cur)\n            rec_next.scatter_(1,next_cur,torch.where(reverse_link_condition, pre_next_wrt_old, next_next_cur))\n            # if i >= self.size - 2: assert (reverse_link_condition == False).all()\n            cur = next_cur\n            \n        return rec_next\n\n    def get_order(self, rec, return_solution = False):\n        \n        bs,p_size = rec.size()\n        visited_time = torch.zeros((bs,p_size),device = rec.device)\n        pre = torch.zeros((bs),device = rec.device).long()\n        for i in range(p_size - 1):\n            visited_time[torch.arange(bs),rec[torch.arange(bs),pre]] = i + 1\n            pre = rec[torch.arange(bs),pre]\n        if return_solution:\n            return visited_time.argsort() # return decoded solution in sequence\n        else:\n            return visited_time.long() # also return visited order\n\n    def check_feasibility(self, rec, partial_sum_wrt_route_plan, basic = False):\n        p_size = self.size\n        assert (\n            (torch.arange(p_size, out=rec.new())).view(1, -1).expand_as(rec)  == \n            rec.sort(1)[0]\n        ).all(), ((\n            (torch.arange(p_size, out=rec.new())).view(1, -1).expand_as(rec)  == \n            rec.sort(1)[0]\n        ).sum(-1),\"not visiting all nodes\")\n        \n        real_solution = self.get_order(rec, True)\n            \n        assert (\n            (torch.arange(p_size, out=rec.new())).view(1, -1).expand_as(rec)  == \n            real_solution.sort(1)[0]\n        ).all(), ((\n            (torch.arange(p_size, out=rec.new())).view(1, -1).expand_as(rec)  == \n            real_solution.sort(1)[0]\n        ).sum(-1),\"not valid tour\")\n            \n        if not basic:   \n            assert (partial_sum_wrt_route_plan <= 1 + 1e-5).all(), (\"not satisfying capacity constraint\", partial_sum_wrt_route_plan, partial_sum_wrt_route_plan.max())\n    \n    def get_costs(self, batch, rec, get_context = False, check_full_feasibility = False):\n        \n        coor = batch['coordinates']\n        coor_next = coor.gather(1, rec.long().unsqueeze(-1).expand(*rec.size(), 2))\n        cost = (coor - coor_next).norm(p=2, dim=2).sum(1)\n        \n        # check TSP feasibility if needed\n        if self.with_assert:\n            self.check_feasibility(rec, None, basic = True)\n        \n        # check full feasibility if needed\n        if check_full_feasibility or get_context:\n            context = self.preprocessing(rec, batch)\n            if check_full_feasibility:\n                self.check_feasibility(rec, context[-1], basic = False)          \n        \n        # get CVRP context\n        if get_context:\n            return cost, context\n        else:\n            return cost\n\n    def get_dynamic_feature(self, rec, batch, context):\n    \n        route_plan_0x, visited_time, cum_demand, partial_sum_wrt_route_plan = context\n        demand = batch['demand'].unsqueeze(-1)\n        cum_demand = cum_demand.unsqueeze(-1)\n        route_total_demand_per_node = partial_sum_wrt_route_plan.gather(-1, route_plan_0x).unsqueeze(-1)\n        \n        infeasibility_indicator_after_visit = torch.clamp_min(cum_demand - 1.00001, 0.0) > 0\n        infeasibility_indicator_before_visit = torch.clamp_min((cum_demand - demand) - 1.00001, 0.0) > 0\n        \n        to_actor = torch.cat((\n            cum_demand,  \n            demand, \n            route_total_demand_per_node - cum_demand,\n            (demand == 0).float(),\n            infeasibility_indicator_before_visit,\n            infeasibility_indicator_after_visit,\n            ), -1) # the node features\n        \n        return visited_time, to_actor\n        \n    def preprocessing(self, solutions, batch):\n        \n        batch_size, seq_length = solutions.size()\n        assert seq_length < 1000\n        arange = torch.arange(batch_size)\n        demand = batch['demand']\n        \n        pre = torch.zeros(batch_size, device = solutions.device).long()\n        route = torch.zeros(batch_size, device = solutions.device).long()\n        route_plan_visited_time = torch.zeros((batch_size,seq_length), device = solutions.device).long()\n        cum_demand = torch.zeros((batch_size,seq_length), device = solutions.device)\n        partial_sum_wrt_route_plan = torch.zeros((batch_size, self.dummy_size), device = solutions.device)\n        \n        for i in range(seq_length):\n            next_ = solutions[arange,pre]\n            next_is_dummy_node = next_ < self.dummy_size\n            route[next_is_dummy_node] += 1\n            route_plan_visited_time[arange,next_] = (route % self.dummy_size) * int(1e3) + (i+1) % self.size\n            new_cum_demand = partial_sum_wrt_route_plan[arange,route % self.dummy_size] + demand[arange, next_]\n            partial_sum_wrt_route_plan[arange,route % self.dummy_size] = new_cum_demand.clone()\n            cum_demand[arange,next_] = new_cum_demand * (~next_is_dummy_node)\n            \n            pre = next_.clone()\n    \n        route_plan_0x = (route_plan_visited_time // int(1e3))\n        \n        out =  (route_plan_0x, # route plan 0xxxxx\n                (route_plan_visited_time % int(1e3)), # visited time\n                cum_demand.clone(), # cum_demand (inclusive)\n                partial_sum_wrt_route_plan.clone()) # partial_sum_wrt_route_plan\n        \n        return out\n\n# ==========================================\n# File: problems/problem_tsp.py\n# Function/Context: TSP\n# ==========================================\nfrom torch.utils.data import Dataset\nimport torch\nimport pickle\nimport os\nfrom utils import augmentation\n\nclass TSP(object):\n\n    NAME = 'tsp'  # Travelling Salesman Problem\n    \n    def __init__(self, p_size, init_val_met = 'random', with_assert = False, DUMMY_RATE = 0, k = 4, with_bonus = False, with_regular = False):\n        self.size = p_size\n        self.do_assert = with_assert\n        self.init_val_met = init_val_met\n        self.k_max = k\n        self.state = 'eval'\n        print(f'TSP with {self.size} nodes.', \n              f'MAX {self.k_max}-opt.'\n              ' Do assert:', with_assert)\n    \n    def train(self):\n        self.state = 'train'\n        \n    def eval(self):\n        self.state = 'eval'\n    \n    def augment(self, batch, val_m, only_copy=False):\n        bs, gs, dim = batch['coordinates'].size()\n        if only_copy:\n            coordinates = batch['coordinates'].unsqueeze(1).expand(bs,val_m,gs,dim).clone().reshape(-1,gs,dim)\n        else:\n            coordinates = batch['coordinates'].unsqueeze(1).expand(bs,val_m,gs,dim).clone()\n            coordinates = augmentation(coordinates, val_m).reshape(-1,gs,dim)\n        return {'coordinates': coordinates}\n    \n    def input_feature_encoding(self, batch):\n        return batch['coordinates'].clone()\n    \n    def get_initial_solutions(self, batch):\n        \n        coordinates = batch['coordinates']\n        batch_size = coordinates.size(0)\n    \n        def get_solution(methods):\n            \n            if methods == 'random':\n                \n                set = torch.rand(batch_size,self.size).argsort().long()\n                rec = torch.zeros(batch_size, self.size).long()\n                index = torch.zeros(batch_size,1).long()\n                \n                for i in range(self.size - 1):\n                    rec.scatter_(1,set.gather(1, index + i), set.gather(1, index + i + 1))\n                \n                rec.scatter_(1,set[:,-1].view(-1,1), set.gather(1, index))\n                return rec\n\n            elif methods == 'greedy':\n               \n               candidates = torch.ones(batch_size,self.size).bool()\n               rec = torch.zeros(batch_size, self.size).long()\n               selected_node = torch.zeros(batch_size, 1).long()\n               candidates.scatter_(1, selected_node, 0)\n               \n               for i in range(self.size - 1):\n                   \n                   d1 = coordinates.cpu().gather(1, selected_node.unsqueeze(-1).expand(batch_size, self.size, 2))\n                   d2 = coordinates.cpu()\n                   \n                   dists = (d1 - d2).norm(p=2, dim=2)\n                   dists[~candidates] = 1e5\n                   \n                   next_selected_node = dists.min(-1)[1].view(-1,1)\n                   rec.scatter_(1,selected_node, next_selected_node)\n                   candidates.scatter_(1, next_selected_node, 0)\n                   selected_node = next_selected_node\n\n               return  rec\n            \n            else:\n                raise NotImplementedError()\n\n        return get_solution(self.init_val_met).expand(batch_size, self.size).clone()\n    \n    def step(self, batch, rec, action, obj, feasible_history, t, weights = 0):\n        \n        bs, gs = rec.size()\n        pre_bsf = obj[:,1:].clone() # batch_size, 3 (current, bsf, tsp_bsf)\n        \n        # k-opt step\n        next_state = self.k_opt(rec, action)\n        next_obj = self.get_costs(batch, next_state)\n        \n        # MDP step\n        now_obj = pre_bsf.clone()\n        now_obj[:,0] = next_obj.clone()\n        now_obj[:,1] = next_obj.clone()\n        now_bsf = torch.min(pre_bsf, now_obj)\n        rewards = (pre_bsf - now_bsf)\n        reward = torch.cat((rewards[:,:1], # reward\n                            rewards[:,:1] * 0., # regulation\n                            rewards[:,:1] * 0., # bonus\n                           ),-1)\n        \n        # return\n        out = (next_state, \n               reward,\n               torch.cat((next_obj[:,None], now_bsf),-1), \n               None, \n               None,\n               None,\n               None)\n        \n        return out\n    \n    def k_opt(self, rec, action):\n        \n        # action bs * (K_index, K_from, K_to)\n        selected_index = action[:,:self.k_max]\n        left = action[:,self.k_max:2*self.k_max]\n        right = action[:,2*self.k_max:]\n        \n        # prepare\n        rec_next = rec.clone()\n        right_nodes = rec.gather(1,selected_index)\n        argsort = rec.argsort()\n        \n        # new rec\n        rec_next.scatter_(1,left,right)\n        cur = left[:,:1].clone()\n        for i in range(self.size - 2): # self.size - 2 is already correct\n            next_cur = rec_next.gather(1,cur)\n            pre_next_wrt_old = argsort.gather(1, next_cur)\n            reverse_link_condition = ((cur!=pre_next_wrt_old) & ~((next_cur==right_nodes).any(-1,True)))\n            next_next_cur = rec_next.gather(1,next_cur)\n            rec_next.scatter_(1,next_cur,torch.where(reverse_link_condition, pre_next_wrt_old, next_next_cur))\n            # if i >= self.size - 2: assert (reverse_link_condition == False).all()\n            cur = next_cur\n            \n        return rec_next\n    \n    def get_order(self, rec, return_solution = False):\n        \n        bs,p_size = rec.size()\n        visited_time = torch.zeros((bs,p_size),device = rec.device)\n        pre = torch.zeros((bs),device = rec.device).long()\n        for i in range(p_size - 1):\n            visited_time[torch.arange(bs),rec[torch.arange(bs),pre]] = i + 1\n            pre = rec[torch.arange(bs),pre]\n        if return_solution:\n            return visited_time.argsort() # return decoded solution in sequence\n        else:\n            return visited_time.long() # also return visited order\n    \n    def check_feasibility(self, rec):\n        p_size = self.size\n        assert (\n            (torch.arange(p_size, out=rec.new())).view(1, -1).expand_as(rec)  == \n            rec.sort(1)[0]\n        ).all(), ((\n            (torch.arange(p_size, out=rec.new())).view(1, -1).expand_as(rec)  == \n            rec.sort(1)[0]\n        ).sum(-1),\"not visiting all nodes\")\n        \n        real_solution = self.get_order(rec, True)\n            \n        assert (\n            (torch.arange(p_size, out=rec.new())).view(1, -1).expand_as(rec)  == \n            real_solution.sort(1)[0]\n        ).all(), ((\n            (torch.arange(p_size, out=rec.new())).view(1, -1).expand_as(rec)  == \n            real_solution.sort(1)[0]\n        ).sum(-1),\"not valid tour\")\n        \n    def get_costs(self, batch, rec, get_context = False, check_full_feasibility = False):\n        \n        coor = batch['coordinates']\n        coor_next = coor.gather(1, rec.long().unsqueeze(-1).expand(*rec.size(), 2))\n        cost = (coor  - coor_next).norm(p=2, dim=2).sum(1)\n            \n        # check feasibility if needed\n        if self.do_assert or check_full_feasibility:\n            self.check_feasibility(rec)\n\n        if get_context:\n            return cost, None\n        else:\n            return cost\n        \n    @staticmethod\n    def make_dataset(*args, **kwargs):\n        return TSPDataset(*args, **kwargs)\n\n\nclass TSPDataset(Dataset):\n    def __init__(self, filename=None, size=20, num_samples=10000, offset=0, distribution=None, DUMMY_RATE=None):\n        \n        super(TSPDataset, self).__init__()\n        \n        self.data = []\n        self.size = size\n\n        if filename is not None:\n            assert os.path.splitext(filename)[1] == '.pkl', 'file name error'\n            \n            with open(filename, 'rb') as f:\n                data = pickle.load(f)\n            self.data = [self.make_instance(args) for args in data[offset:offset+num_samples]]\n\n        else:\n            self.data = [{'coordinates': torch.FloatTensor(self.size, 2).uniform_(0, 1)} for i in range(num_samples)]\n        \n        self.N = len(self.data)\n        \n        print(f'{self.N} instances initialized.')\n    \n    def make_instance(self, args):\n        return {'coordinates': torch.FloatTensor(args)}\n    \n    def __len__(self):\n        return self.N\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\n# ==========================================\n# File: utils/utils.py\n# Function/Context: augmentation\n# ==========================================\nimport torch\nimport math\nimport numpy as np\nimport random\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\ndef get_rotate_mat(theta_f: float) -> torch.Tensor:\n    theta = torch.tensor(theta_f)\n    return torch.tensor(\n        [[torch.cos(theta), -torch.sin(theta)], [torch.sin(theta), torch.cos(theta)]]\n    )\n\ndef rotate_tensor(x: torch.Tensor, d: float) -> torch.Tensor:\n    rot_mat = get_rotate_mat(d / 360 * 2 * np.pi).to(x.device)\n    return torch.matmul(x - 0.5, rot_mat) + 0.5\n\ndef augmentation(coordinates, val_m):\n    assert coordinates.size(1) == val_m\n    augments = ['Rotate', 'Flip_x-y', 'Flip_x_cor', 'Flip_y_cor']\n    for i in range(val_m):\n        random.shuffle(augments)\n        id_ = torch.rand(4)\n        for aug in augments:\n            if aug == 'Rotate':\n                coordinates[:,i] = rotate_tensor(coordinates[:,i], int(id_[0] * 4 + 1) * 90)\n            elif aug == 'Flip_x-y':\n                if int(id_[1] * 2 + 1) == 1:\n                     data = coordinates[:,i].clone()\n                     coordinates[:,i,:,0] = data[:,:,1]\n                     coordinates[:,i,:,1] = data[:,:,0]\n            elif aug == 'Flip_x_cor':\n                if int(id_[2] * 2 + 1) == 1:\n                     coordinates[:,i,:,0] = 1 - coordinates[:,i,:,0]\n            elif aug == 'Flip_y_cor':\n                if int(id_[3] * 2 + 1) == 1:\n                     coordinates[:,i,:,1] = 1 - coordinates[:,i,:,1]\n    return coordinates",
  "description": "Combined Analysis:\n- [GIRE_Example.ipynb]: This file implements the inference pipeline for NeuOpt-GIRE on CVRP-50, demonstrating key algorithm steps: 1) Configuration of GIRE parameters (wo_regular=False, wo_bonus=False, wo_MDP=True) enabling infeasible region exploration, 2) Loading of pre-trained neural network policy trained with reinforcement learning for k-opt exchanges, 3) Dynamic Data Augmentation (D2A) through random initial solution generation, 4) CVRP constraint handling via dummy depot nodes (DUMMY_RATE=0.4). The code directly corresponds to the paper's optimization model where constraints are handled through GIRE's reward shaping and feasibility features, though the exact mathematical formulation is abstracted in the imported modules.\n- [agent/ppo.py]: This file implements the core PPO training framework and search process for NeuOpt. The PPO class manages the actor (policy network for k-opt moves) and critic (value network). The rollout method executes the key search algorithm: 1) Initializes solutions, 2) Iteratively selects k-opt actions via the actor network, 3) Steps through the environment to update solutions and compute rewards, 4) Maintains feasibility history for GIRE constraint handling, 5) Implements Dynamic Data Augmentation (D2A) by augmenting coordinates when search stalls (stall_limit). This directly corresponds to the paper's flexible neural k-opt search with GIRE and D2A components.\n- [nets/actor_network.py]: This file implements the core neural network architecture (Actor) for the NeuOpt algorithm. The Actor network is responsible for selecting k-opt moves during the search process. Key components include: 1) Embedding network for node features, 2) Multi-head encoder for processing embeddings, 3) Position encoder for compatibility scores, and 4) k-opt decoder that outputs actions (S-move, I-move, E-move) for flexible k-opt exchanges. The forward method handles both TSP and CVRP problems with dynamic feature extraction (supporting Guided Infeasible Region Exploration via with_feature1 flag). The architecture implements the Recurrent Dual-Stream decoder and supports the flexible k-opt mechanism central to the paper's contribution.\n- [nets/graph_layers.py]: This file implements the core neural network components for NeuOpt's learning-to-search algorithm. The kopt_Decoder class directly corresponds to the Recurrent Dual-Stream (RDS) decoder described in the paper, which is central to performing flexible k-opt exchanges. It uses dual-stream attention mechanisms with optional GRU recurrence to capture correlations between removed and added edges during k-opt moves. The decoder operates iteratively for up to k_max steps, generating sequences of node selections that define k-opt exchanges. This aligns with the paper's key algorithmic step of neural-guided k-opt move generation.\n- [problems/problem_cvrp.py]: This file implements the core CVRP problem environment for the NeuOpt solver. It contains: 1) The CVRP class that defines the problem structure and constraints, 2) Flexible k-opt implementation (k_opt method) supporting kâ‰¥2 exchanges, 3) Guided Infeasible Region Exploration (GIRE) through the step method which handles both feasible and infeasible moves with reward shaping, 4) Dynamic Data Augmentation (D2A) via the augment method, 5) Capacity constraint checking and preprocessing for route planning. The implementation directly corresponds to the paper's mathematical model for CVRP with vehicle capacity constraints and the learning-to-search algorithm with flexible k-opt moves.\n- [problems/problem_tsp.py]: This file implements the core TSP environment for the NeuOpt learning-to-search algorithm. It provides:\n1. Problem definition (TSP class) with configurable k-opt operations (k_max)\n2. Dynamic Data Augmentation (D2A) via the augment() method\n3. Initial solution generation (random/greedy) for RL starting points\n4. Flexible k-opt implementation (k_opt()) that performs edge exchanges\n5. Cost computation (get_costs()) matching the TSP objective function\n6. Feasibility checking (check_feasibility()) for solution validation\n7. MDP step function (step()) that computes rewards based on solution improvement\n8. Dataset handling for training/evaluation instances\n\nKey algorithm components implemented:\n- Flexible k-opt exchanges (Tailored Action Factorization via action tensor structure)\n- Dynamic Data Augmentation (D2A) for escaping local optima\n- MDP formulation for reinforcement learning\n- Solution representation as successor arrays (rec)\n\nNote: The file focuses on the TSP problem environment; neural network components (RDS decoder) and GIRE constraint handling are implemented elsewhere in the codebase.\n- [utils/utils.py]: This file implements the Dynamic Data Augmentation (D2A) component of NeuOpt, which is a key algorithm step. The augmentation function applies random geometric transformations (rotations and flips) to problem instances during inference to help the solver escape local optima by generating diverse problem views. This directly corresponds to the D2A technique described in the paper that enhances search diversity when the solver stalls.",
  "dependencies": [
    "problems.problem_tsp.TSP",
    "torch.multiprocessing",
    "agent.ppo.PPO",
    "torch.distributed",
    "nets.graph_layers.kopt_Decoder",
    "agent.utils.gather_tensor_and_concat",
    "pickle",
    "utils.move_to",
    "torch.distributions.Categorical",
    "problems.problem_cvrp.total_history",
    "torch.utils.data.DataLoader",
    "utils.torch_load_cpu",
    "mySequential",
    "torch.nn.functional",
    "warnings",
    "nets.graph_layers.MultiHeadEncoder",
    "torch.nn.parallel.DistributedDataParallel",
    "nets.graph_layers.EmbeddingNet",
    "agent.utils.validate",
    "tqdm",
    "math",
    "nets.graph_layers.MultiHeadPosCompat",
    "json",
    "os",
    "problems.problem_cvrp.CVRPDataset",
    "nets.actor_network.Actor",
    "utils.augmentation",
    "utils.get_inner_model",
    "nets.critic_network.Critic",
    "random",
    "options.get_options",
    "numpy",
    "tqdm.notebook.tqdm",
    "pprint",
    "tensorboard_logger",
    "problems.problem_cvrp.CVRP",
    "utils.logger.log_to_tb_train",
    "utils.clip_grad_norms",
    "nn",
    "torch.nn",
    "torch"
  ]
}