{
  "paper_id": "CUTS_Neural_Causal_Discovery_from_Irregular_Time-Series_Data",
  "title": "CUTS: Neural Causal Discovery from Irregular Time-Series Data",
  "abstract": "Causal discovery from time-series data has been a central task in machine learning. Recently, Granger causality inference is gaining momentum due to its good explainability and high compatibility with emerging deep neural networks. However, most existing methods assume structured input data and degenerate greatly when encountering data with randomly missing entries or non-uniform sampling frequencies, which hampers their applications in real scenarios. To address this issue, here we present CUTS, a neural Granger causal discovery algorithm to jointly impute unobserved data points and build causal graphs, via plugging in two mutually boosting modules in an iterative framework: (i) Latent data prediction stage: designs a Delayed Supervision Graph Neural Network (DSGNN) to hallucinate and register irregular data which might be of high dimension and with complex distribution; (ii) Causal graph fitting stage: builds a causal adjacency matrix with imputed data under sparse penalty. Experiments show that CUTS effectively infers causal graphs from irregular time-series data, with significantly superior performance to existing methods. Our approach constitutes a promising step towards applying causal discovery to real applications with non-ideal observations.",
  "problem_description_natural": "The paper addresses the problem of discovering causal relationships (in the form of a directed acyclic graph) from irregularly sampled or partially observed time-series data. Traditional Granger causality methods assume complete, uniformly sampled data, but real-world data often suffer from random missingness or non-uniform sampling frequencies. The authors propose an iterative framework that alternates between (1) imputing missing data using a graph neural network conditioned on the current estimate of the causal graph, and (2) refining the causal graph using the imputed data under a sparsity constraint. The goal is to simultaneously recover missing observations and infer a sparse, accurate causal structure that reflects true Granger-causal dependencies among time-series variables.",
  "problem_type": "Causal discovery / structural learning (non-convex optimization with sparsity constraints)",
  "datasets": [
    "VAR",
    "Lorenz-96",
    "NetSim"
  ],
  "performance_metrics": [
    "AUROC"
  ],
  "lp_model": {
    "objective": "\\min_{\\boldsymbol{\\theta}} \\mathcal{L}_{graph}\\left(\\tilde{\\mathcal{X}}, \\hat{\\mathcal{X}}, \\mathcal{O}, \\boldsymbol{\\theta}\\right) = \\mathcal{L}_{pred}\\left(\\tilde{\\mathcal{X}}, \\hat{\\mathcal{X}}, \\mathcal{O}\\right) + \\lambda ||\\sigma(\\boldsymbol{\\theta})||_1",
    "constraints": [
      "\\hat{x}_{t,i} = f_{\\phi_i}(\\mathcal{X} \\odot \\mathcal{S})",
      "\\tilde{x}_{t,i}^{(m+1)} = \\begin{cases} (1 - \\alpha)\\tilde{x}_{t,i}^{(m)} + \\alpha \\hat{x}_{t,i}^{(m)} & o_{t,i} = 0 \\text{ and } m \\geq n_1 \\\\ \\tilde{x}_{t,i}^{0} & o_{t,i} = 1 \\text{ or } m < n_1 \\end{cases}",
      "\\mathcal{L}_{pred}\\left(\\tilde{\\mathcal{X}}, \\hat{\\mathcal{X}}, \\mathcal{O}\\right) = \\sum_{i=1}^{N} \\frac{\\langle \\mathcal{L}_2\\left(\\hat{\\boldsymbol{x}}_{1:L,i}, \\tilde{\\boldsymbol{x}}_{1:L,i}\\right), \\boldsymbol{o}_{1:L,i} \\rangle}{\\frac{1}{L} \\langle \\boldsymbol{o}_{1:L,i}, \\boldsymbol{o}_{1:L,i} \\rangle}"
    ],
    "variables": [
      "\\phi_i \\text{ for } i=1,...,N",
      "\\theta_{\\tau,ij} \\text{ for } \\tau=1,...,\\tau_{max}, i,j=1,...,N"
    ]
  },
  "raw_latex_model": "\\begin{align*}\nx_{t,i} &= f_i(\\boldsymbol{x}_{t-\\tau:t-1,1}, \\boldsymbol{x}_{t-\\tau:t-1,2}, ..., \\boldsymbol{x}_{t-\\tau:t-1,N}) + e_{t,i} \\\\\n\\hat{x}_{t,i} &= f_{\\phi_i}(\\mathcal{X} \\odot \\mathcal{S}) \\\\\n\\mathcal{L}_{pred}\\left(\\tilde{\\mathcal{X}}, \\hat{\\mathcal{X}}, \\mathcal{O}\\right) &= \\sum_{i=1}^{N} \\frac{\\langle \\mathcal{L}_2\\left(\\hat{\\boldsymbol{x}}_{1:L,i}, \\tilde{\\boldsymbol{x}}_{1:L,i}\\right), \\boldsymbol{o}_{1:L,i} \\rangle}{\\frac{1}{L} \\langle \\boldsymbol{o}_{1:L,i}, \\boldsymbol{o}_{1:L,i} \\rangle} \\\\\n\\mathcal{L}_{graph}\\left(\\tilde{\\mathcal{X}}, \\hat{\\mathcal{X}}, \\mathcal{O}, \\boldsymbol{\\theta}\\right) &= \\mathcal{L}_{pred}\\left(\\tilde{\\mathcal{X}}, \\hat{\\mathcal{X}}, \\mathcal{O}\\right) + \\lambda ||\\sigma(\\boldsymbol{\\theta})||_1\n\\end{align*}",
  "algorithm_description": "The CUTS algorithm performs causal discovery from irregular time-series data through an iterative three-phase process:\n1. Initial Training (n1 epochs): Optimize the Delayed Supervision Graph Neural Network (DSGNN) parameters \\(\\phi_i\\) and Causal Probability Graph (CPG) parameters \\(\\theta\\) using the observed data only, with missing entries filled initially (e.g., zero-order hold). Losses \\(\\mathcal{L}_{pred}\\) and \\(\\mathcal{L}_{graph}\\) are minimized without data imputation.\n2. Iterative Learning (n2 epochs): Alternate between two stages:\n   a. Latent Data Prediction Stage: Sample causal masks \\(\\mathcal{S}\\) based on current CPG, use DSGNN to predict values \\(\\hat{x}_{t,i}\\), and impute missing data points via Equation (6), but these imputed points are not used for supervision.\n   b. Causal Graph Fitting Stage: With the imputed data, optimize \\(\\theta\\) by minimizing \\(\\mathcal{L}_{graph}\\) to update the CPG.\n3. Fine-tuning (n3 epochs): Use all data points (including imputed) to fine-tune \\(\\theta\\) by minimizing \\(\\mathcal{L}_{ft}\\) from Equation (8), which includes supervision from imputed data and sparsity regularization.\nThe algorithm alternates between data imputation and causal graph fitting to mutually boost performance, with hyperparameters like learning rates scheduled over epochs."
}