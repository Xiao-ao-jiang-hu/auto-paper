{
  "file_path": "diffan/diffan.py, diffan/gaussian_diffusion.py",
  "function_name": "DiffAN.topological_ordering, GaussianDiffusion.p_mean_variance",
  "code_snippet": "\n\n# ==========================================\n# File: diffan/diffan.py\n# Function/Context: DiffAN.topological_ordering\n# ==========================================\nfrom logging import raiseExceptions\nimport torch\nimport numpy as np\nfrom functorch import vmap, jacrev, jacfwd\nfrom collections import Counter\nfrom copy import deepcopy\nfrom tqdm import tqdm\n\nfrom diffan.gaussian_diffusion import GaussianDiffusion, UniformSampler, get_named_beta_schedule, mean_flat, \\\n                                         LossType, ModelMeanType, ModelVarType\nfrom diffan.nn import DiffMLP\nfrom diffan.pruning import cam_pruning\nfrom diffan.utils import full_DAG\n\n\nclass DiffAN():\n    def __init__(self, n_nodes, masking = True, residue= True, \n                epochs: int = int(3e3), batch_size : int = 1024, learning_rate : float = 0.001):\n        self.n_nodes = n_nodes\n        assert self.n_nodes > 1, \"Not enough nodes, make sure the dataset contain at least 2 variables (columns).\"\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n        ## Diffusion parameters\n        self.n_steps = int(1e2)\n        betas = get_named_beta_schedule(schedule_name = \"linear\", num_diffusion_timesteps = self.n_steps, scale = 1, beta_start = 0.0001, beta_end = 0.02)\n        self.gaussian_diffusion = GaussianDiffusion(betas = betas, \n                                                    loss_type = LossType.MSE, \n                                                    model_mean_type= ModelMeanType.EPSILON,#START_X,EPSILON\n                                                    model_var_type=ModelVarType.FIXED_LARGE,\n                                                    rescale_timesteps = True,\n                                                    )\n        self.schedule_sampler = UniformSampler(self.gaussian_diffusion)\n\n        ## Diffusion training\n        self.epochs = epochs \n        self.batch_size = batch_size\n        self.model = DiffMLP(n_nodes).to(self.device)\n        self.model.float()\n        self.opt = torch.optim.Adam(self.model.parameters(), learning_rate)\n        self.val_diffusion_loss = []\n        self.best_loss = float(\"inf\")\n        self.early_stopping_wait = 300\n\n        ## Topological Ordering\n        self.n_votes = 3\n        self.masking = masking\n        self.residue = residue\n        self.sorting = (not masking) and (not residue)\n        ## Pruning\n        self.cutoff = 0.001\n    \n    def topological_ordering(self, X, step = None, eval_batch_size = None):\n        \n        if eval_batch_size is None:\n            eval_batch_size = self.batch_size\n        eval_batch_size = min(eval_batch_size, X.shape[0])\n\n        X = X[:self.batch_size]\n        \n        self.model.eval()\n        order = []\n        active_nodes = list(range(self.n_nodes))\n        \n        \n        steps_list = [step] if step is not None else range(0, self.n_steps+1, self.n_steps//self.n_votes)\n        if self.sorting:\n            steps_list = [self.n_steps//2]\n        pbar = tqdm(range(self.n_nodes-1), desc = \"Nodes ordered \")\n        leaf = None\n        for jac_step in pbar:        \n            leaves = []\n            for i, steps in enumerate(steps_list):\n                data_loader = torch.utils.data.DataLoader(X, eval_batch_size, drop_last = True)\n\n                model_fn_functorch = self.get_model_function_with_residue(steps, active_nodes, order)\n                leaf_ = self.compute_jacobian_and_get_leaf(data_loader, active_nodes, model_fn_functorch)\n                if self.sorting:\n                    order = leaf_.tolist()\n                    order.reverse()\n                    return order\n                leaves.append(leaf_)\n\n            leaf = Counter(leaves).most_common(1)[0][0]\n            leaf_global = active_nodes[leaf]\n            order.append(leaf_global)\n            active_nodes.pop(leaf)\n\n\n        order.append(active_nodes[0])\n        order.reverse()\n\n        return order\n\n    def get_model_function_with_residue(self, step, active_nodes, order):\n        t_functorch = (torch.ones(1)*step).long().to(self.device) # test if other ts or random ts are better, self.n_steps\n        get_score_active = lambda x: self.model(x, self.gaussian_diffusion._scale_timesteps(t_functorch))[:,active_nodes]\n        get_score_previous_leaves = lambda x: self.model(x, self.gaussian_diffusion._scale_timesteps(t_functorch))[:,order]\n        def model_fn_functorch(X):\n            score_active = get_score_active(X).squeeze()\n\n            if self.residue and len(order) > 0:\n                score_previous_leaves = get_score_previous_leaves(X).squeeze()\n                jacobian_ = jacfwd(get_score_previous_leaves)(X).squeeze()\n                if len(order) == 1:\n                    jacobian_, score_previous_leaves = jacobian_.unsqueeze(0), score_previous_leaves.unsqueeze(0)\n                score_active += torch.einsum(\"i,ij -> j\",score_previous_leaves/ jacobian_[:, order].diag(),jacobian_[:, active_nodes])#\n\n            return score_active\n        return model_fn_functorch\n\n    def get_masked(self, x, active_nodes):\n        dropout_mask = torch.zeros_like(x).to(self.device)\n        dropout_mask[:, active_nodes] = 1\n        return (x * dropout_mask).float()\n    \n    def compute_jacobian_and_get_leaf(self, data_loader, active_nodes, model_fn_functorch):\n        jacobian = []\n        for x_batch in data_loader:\n            x_batch_dropped = self.get_masked(x_batch, active_nodes) if self.masking else x_batch\n            jacobian_ = vmap(jacrev(model_fn_functorch))(x_batch_dropped.unsqueeze(1)).squeeze()\n            jacobian.append(jacobian_[...,active_nodes].detach().cpu().numpy())\n        jacobian = np.concatenate(jacobian, 0)\n        leaf = self.get_leaf(jacobian)\n        return leaf\n    \n    def get_leaf(self, jacobian_active):\n        jacobian_var = jacobian_active.var(0)\n        jacobian_var_diag = jacobian_var.diagonal()\n        var_sorted_nodes = np.argsort(jacobian_var_diag)\n        if self.sorting:\n            return var_sorted_nodes\n        leaf_current = var_sorted_nodes[0]\n        return leaf_current\n\n# ==========================================\n# File: diffan/gaussian_diffusion.py\n# Function/Context: GaussianDiffusion.p_mean_variance\n# ==========================================\nimport enum\nimport math\nimport torch\nimport torch as th\nimport torch.nn.functional as F\nimport random\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom abc import ABC, abstractmethod\n\ndef _extract_into_tensor(arr, timesteps, broadcast_shape):\n    \"\"\"\n    Extract values from a 1-D numpy array for a batch of indices.\n    :param arr: the 1-D numpy array.\n    :param timesteps: a tensor of indices into the array to extract.\n    :param broadcast_shape: a larger shape of K dimensions with the batch\n                            dimension equal to the length of timesteps.\n    :return: a tensor of shape [batch_size, 1, ...] where the shape has K dims.\n    \"\"\"\n    res = th.from_numpy(arr).to(device=timesteps.device)[timesteps].float()\n    while len(res.shape) < len(broadcast_shape):\n        res = res[..., None]\n    return res.expand(broadcast_shape)\n\nclass ModelMeanType(enum.Enum):\n    PREVIOUS_X = enum.auto()\n    START_X = enum.auto()\n    EPSILON = enum.auto()\n\nclass ModelVarType(enum.Enum):\n    LEARNED = enum.auto()\n    FIXED_SMALL = enum.auto()\n    FIXED_LARGE = enum.auto()\n    LEARNED_RANGE = enum.auto()\n\nclass LossType(enum.Enum):\n    MSE = enum.auto()\n    RESCALED_MSE = enum.auto()\n    KL = enum.auto()\n    RESCALED_KL = enum.auto()\n\nclass GaussianDiffusion:\n    def __init__(self, *, betas, model_mean_type, model_var_type, loss_type, rescale_timesteps=False, conditioning_noise=\"constant\"):\n        self.model_mean_type = model_mean_type\n        self.model_var_type = model_var_type\n        self.loss_type = loss_type\n        self.rescale_timesteps = rescale_timesteps\n        self.conditioning_noise = conditioning_noise\n        assert self.conditioning_noise in [\"reverse\", \"constant\"]\n        betas = np.array(betas, dtype=np.float64)\n        self.betas = betas\n        assert len(betas.shape) == 1, \"betas must be 1-D\"\n        assert (betas > 0).all() and (betas <= 1).all()\n        self.num_timesteps = int(betas.shape[0])\n        alphas = 1.0 - betas\n        self.alphas_cumprod = np.cumprod(alphas, axis=0)\n        self.alphas_cumprod_prev = np.append(1.0, self.alphas_cumprod[:-1])\n        self.alphas_cumprod_next = np.append(self.alphas_cumprod[1:], 0.0)\n        assert self.alphas_cumprod_prev.shape == (self.num_timesteps,)\n        self.sqrt_alphas_cumprod = np.sqrt(self.alphas_cumprod)\n        self.sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - self.alphas_cumprod)\n        self.log_one_minus_alphas_cumprod = np.log(1.0 - self.alphas_cumprod)\n        self.sqrt_recip_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod)\n        self.sqrt_recipm1_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod - 1)\n        self.posterior_variance = (betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod))\n        self.posterior_log_variance_clipped = np.log(np.append(self.posterior_variance[1], self.posterior_variance[1:]))\n        self.posterior_mean_coef1 = (betas * np.sqrt(self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod))\n        self.posterior_mean_coef2 = ((1.0 - self.alphas_cumprod_prev) * np.sqrt(alphas) / (1.0 - self.alphas_cumprod))\n\n    def q_mean_variance(self, x_start, t):\n        mean = (_extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = _extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = _extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def q_sample(self, x_start, t, noise=None):\n        if noise is None:\n            noise = th.randn_like(x_start)\n        assert noise.shape == x_start.shape\n        return (_extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start + _extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)\n\n    def q_posterior_mean_variance(self, x_start, x_t, t):\n        assert x_start.shape == x_t.shape\n        posterior_mean = (_extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start + _extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t)\n        posterior_variance = _extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = _extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        assert (posterior_mean.shape[0] == posterior_variance.shape[0] == posterior_log_variance_clipped.shape[0] == x_start.shape[0])\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def p_mean_variance(self, model, x, t, clip_denoised=True, denoised_fn=None, model_kwargs=None):\n        if model_kwargs is None:\n            model_kwargs = {}\n        B, C = x.shape[:2]\n        assert t.shape == (B,)\n        from functorch import vmap, jacrev\n        model_output = model(x, self._scale_timesteps(t), **model_kwargs)\n        jacobian = vmap(jacrev(model), randomness=\"same\")(x.unsqueeze(1), self._scale_timesteps(t).unsqueeze(1))\n        if self.model_var_type in [ModelVarType.LEARNED, ModelVarType.LEARNED_RANGE]:\n            assert model_output.shape == (B, C * 2, *x.shape[2:])\n            model_output, model_var_values = th.split(model_output, C, dim=1)\n            if self.model_var_type == ModelVarType.LEARNED:\n                model_log_variance = model_var_values\n                model_variance = th.exp(model_log_variance)\n            else:\n                min_log = _extract_into_tensor(self.posterior_log_variance_clipped, t, x.shape)\n                max_log = _extract_into_tensor(np.log(self.betas), t, x.shape)\n                frac = (model_var_values + 1) / 2\n                model_log_variance = frac * max_log + (1 - frac) * min_log\n                model_variance = th.exp(model_log_variance)\n        else:\n            model_variance, model_log_variance = {\n                ModelVarType.FIXED_LARGE: (np.append(self.posterior_variance[1], self.betas[1:]), np.log(np.append(self.posterior_variance[1], self.betas[1:]))),\n                ModelVarType.FIXED_SMALL: (self.posterior_variance, self.posterior_log_variance_clipped),\n            }[self.model_var_type]\n            model_variance = _extract_into_tensor(model_variance, t, x.shape)\n            model_log_variance = _extract_into_tensor(model_log_variance, t, x.shape)\n        def process_xstart(x):\n            if denoised_fn is not None:\n                x = denoised_fn(x, **model_kwargs)\n            if clip_denoised:\n                return x.clamp(-1, 1)\n            return x\n        if self.model_mean_type == ModelMeanType.PREVIOUS_X:\n            pred_xstart = process_xstart(self._predict_xstart_from_xprev(x_t=x, t=t, xprev=model_output))\n            model_mean = model_output\n        elif self.model_mean_type in [ModelMeanType.START_X, ModelMeanType.EPSILON]:\n            if self.model_mean_type == ModelMeanType.START_X:\n                pred_xstart = process_xstart(model_output)\n            else:\n                pred_xstart = process_xstart(self._predict_xstart_from_eps(x_t=x, t=t, eps=model_output))\n            model_mean, _, _ = self.q_posterior_mean_variance(x_start=pred_xstart, x_t=x, t=t)",
  "description": "Combined Analysis:\n- [diffan/diffan.py]: This file implements the core algorithm from the paper: it trains a diffusion model to learn the score function (gradient of log-density) and then uses the Jacobian (Hessian of log-density) to perform topological ordering via iterative leaf removal. The key steps are: 1) Training the diffusion model (train_score) to approximate the score function. 2) In topological_ordering, iteratively computing the Jacobian of the score for active nodes, selecting the leaf as the node with minimal variance of the diagonal Hessian entries (get_leaf). 3) Using residue (deciduous score) to update the score function without retraining when nodes are removed (get_model_function_with_residue). 4) Optionally using masking to restrict computations to active nodes. The implementation directly corresponds to the optimization model's greedy minimization of Hessian diagonal variance (Equation 9) and leverages Lemma 1 and Theorem 1 for leaf identification and score updates.\n- [diffan/gaussian_diffusion.py]: This file implements the core mathematical framework for diffusion models used in DiffAN's causal discovery algorithm. The key function `p_mean_variance` computes the model's output (score function) and its Jacobian (Hessian of log-density) via automatic differentiation using `functorch.vmap` and `functorch.jacrev`. This directly implements Equation 9 from the paper for leaf node identification by enabling computation of ∇ₓ(score(x,t)). The GaussianDiffusion class provides essential diffusion processes (q_sample, q_posterior_mean_variance) and parameterizations needed for training the score network. While the topological ordering algorithm itself is implemented elsewhere, this file provides the fundamental gradient/Hessian computation machinery required for the optimization model.",
  "dependencies": [
    "functorch.jacfwd",
    "tqdm.tqdm",
    "diffan.gaussian_diffusion.UniformSampler",
    "torch.utils.data.DataLoader",
    "copy.deepcopy",
    "functorch.jacrev",
    "diffan.nn.DiffMLP",
    "tqdm.auto",
    "math",
    "diffan.gaussian_diffusion.get_named_beta_schedule",
    "diffan.utils.full_DAG",
    "collections.Counter",
    "numpy",
    "enum",
    "abc",
    "torch",
    "functorch.vmap",
    "diffan.gaussian_diffusion.GaussianDiffusion",
    "functorch"
  ]
}