{
  "file_path": "actor/actor.py, agent.py, brain.py, evaluate.py",
  "function_name": "GNNPolicy, Agent.run, Brain",
  "code_snippet": "\n\n# ==========================================\n# File: actor/actor.py\n# Function/Context: GNNPolicy\n# ==========================================\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric\nimport numpy as np\n\n\nclass PreNormException(Exception):\n    pass\n\n\nclass PreNormLayer(torch.nn.Module):\n    def __init__(self, n_units, shift=True, scale=True, name=None):\n        super().__init__()\n        assert shift or scale\n        self.register_buffer('shift', torch.zeros(n_units) if shift else None)\n        self.register_buffer('scale', torch.ones(n_units) if scale else None)\n        self.n_units = n_units\n        self.waiting_updates = False\n        self.received_updates = False\n\n    def forward(self, input_):\n        if self.waiting_updates:\n            self.update_stats(input_)\n            self.received_updates = True\n            raise PreNormException\n\n        if self.shift is not None:\n            input_ = input_ + self.shift\n\n        if self.scale is not None:\n            input_ = input_ * self.scale\n\n        return input_\n\n    def start_updates(self):\n        self.avg = 0\n        self.var = 0\n        self.m2 = 0\n        self.count = 0\n        self.waiting_updates = True\n        self.received_updates = False\n\n    def update_stats(self, input_):\n        \"\"\"\n        Online mean and variance estimation. See: Chan et al. (1979) Updating\n        Formulae and a Pairwise Algorithm for Computing Sample Variances.\n        https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online_algorithm\n        \"\"\"\n        assert self.n_units == 1 or input_.shape[-1] == self.n_units, f\"Expected input dimension of size {self.n_units}, got {input_.shape[-1]}.\"\n\n        input_ = input_.reshape(-1, self.n_units)\n        sample_avg = input_.mean(dim=0)\n        sample_var = (input_ - sample_avg).pow(2).mean(dim=0)\n        sample_count = np.prod(input_.size())/self.n_units\n\n        delta = sample_avg - self.avg\n\n        self.m2 = self.var * self.count + sample_var * sample_count + delta ** 2 * self.count * sample_count / (\n                self.count + sample_count)\n\n        self.count += sample_count\n        self.avg += delta * sample_count / self.count\n        self.var = self.m2 / self.count if self.count > 0 else 1\n\n    def stop_updates(self):\n        \"\"\"\n        Ends pre-training for that layer, and fixes the layers's parameters.\n        \"\"\"\n        assert self.count > 0\n        if self.shift is not None:\n            self.shift = -self.avg\n\n        if self.scale is not None:\n            self.var[self.var < 1e-8] = 1\n            self.scale = 1 / torch.sqrt(self.var)\n\n        del self.avg, self.var, self.m2, self.count\n        self.waiting_updates = False\n        self.trainable = False\n        \n\n\nclass BipartiteGraphConvolution(torch_geometric.nn.MessagePassing):\n    def __init__(self):\n        super().__init__('add')\n        emb_size = 64\n        \n        self.feature_module_left = torch.nn.Sequential(\n            torch.nn.Linear(emb_size, emb_size)\n        )\n        self.feature_module_edge = torch.nn.Sequential(\n            torch.nn.Linear(1, emb_size, bias=False)\n        )\n        self.feature_module_right = torch.nn.Sequential(\n            torch.nn.Linear(emb_size, emb_size, bias=False)\n        )\n        self.feature_module_final = torch.nn.Sequential(\n            PreNormLayer(1, shift=False),\n            torch.nn.ReLU(),\n            torch.nn.Linear(emb_size, emb_size)\n        )\n        \n        self.post_conv_module = torch.nn.Sequential(\n            PreNormLayer(1, shift=False)\n        )\n\n        # output_layers\n        self.output_module = torch.nn.Sequential(\n            torch.nn.Linear(2*emb_size, emb_size),\n            torch.nn.ReLU(),\n            torch.nn.Linear(emb_size, emb_size),\n        )\n\n    def forward(self, left_features, edge_indices, edge_features, right_features):\n        output = self.propagate(edge_indices, size=(left_features.shape[0], right_features.shape[0]), \n                                node_features=(left_features, right_features), edge_features=edge_features)\n        return self.output_module(torch.cat([self.post_conv_module(output), right_features], dim=-1))\n\n    def message(self, node_features_i, node_features_j, edge_features):\n        output = self.feature_module_final(self.feature_module_left(node_features_i) \n                                           + self.feature_module_edge(edge_features) \n                                           + self.feature_module_right(node_features_j))\n        return output\n\n\nclass BaseModel(torch.nn.Module):\n    \"\"\"\n    Our base model class, which implements pre-training methods.\n    \"\"\"\n\n    def pre_train_init(self):\n        for module in self.modules():\n            if isinstance(module, PreNormLayer):\n                module.start_updates()\n\n    def pre_train_next(self):\n        for module in self.modules():\n            if isinstance(module, PreNormLayer) and module.waiting_updates and module.received_updates:\n                module.stop_updates()\n                return module\n        return None\n\n    def pre_train(self, *args, **kwargs):\n        try:\n            with torch.no_grad():\n                self.forward(*args, **kwargs)\n            return False\n        except PreNormException:\n            return True\n\n\nclass GNNPolicy(BaseModel):\n    def __init__(self):\n        super().__init__()\n        emb_size = 64\n        cons_nfeats = 5\n        edge_nfeats = 1\n        var_nfeats = 19\n\n        # CONSTRAINT EMBEDDING\n        self.cons_embedding = torch.nn.Sequential(\n            PreNormLayer(cons_nfeats),\n            torch.nn.Linear(cons_nfeats, emb_size),\n            torch.nn.ReLU(),\n            torch.nn.Linear(emb_size, emb_size),\n            torch.nn.ReLU(),\n        )\n\n        # EDGE EMBEDDING\n        self.edge_embedding = torch.nn.Sequential(\n            PreNormLayer(edge_nfeats),\n        )\n\n        # VARIABLE EMBEDDING\n        self.var_embedding = torch.nn.Sequential(\n            PreNormLayer(var_nfeats),\n            torch.nn.Linear(var_nfeats, emb_size),\n            torch.nn.ReLU(),\n            torch.nn.Linear(emb_size, emb_size),\n            torch.nn.ReLU(),\n        )\n\n        self.conv_v_to_c = BipartiteGraphConvolution()\n        self.conv_c_to_v = BipartiteGraphConvolution()\n\n        self.output_module = torch.nn.Sequential(\n            torch.nn.Linear(emb_size, emb_size),\n            torch.nn.ReLU(),\n            torch.nn.Linear(emb_size, 1, bias=False),\n        )\n\n    def forward(self, constraint_features, edge_indices, edge_features, variable_features):\n        reversed_edge_indices = torch.stack([edge_indices[1], edge_indices[0]], dim=0)\n        \n        constraint_features = self.cons_embedding(constraint_features)\n        edge_features = self.edge_embedding(edge_features)\n        variable_features = self.var_embedding(variable_features)\n\n        constraint_features = self.conv_v_to_c(variable_features, reversed_edge_indices, edge_features, constraint_features)\n        variable_features = self.conv_c_to_v(constraint_features, edge_indices, edge_features, variable_features)\n\n        output = self.output_module(variable_features).squeeze(-1)\n        return output\n\n# ==========================================\n# File: agent.py\n# Function/Context: Agent.run\n# ==========================================\nimport ecole\nimport threading\nimport queue\nimport utilities\nimport numpy as np\nfrom collections import namedtuple\n\nclass Agent(threading.Thread):\n    \"\"\"\n    Agent class. Receives tasks from the job sponsor, runs them and samples transitions if\n    requested.\n    \"\"\"\n    def __init__(self, name, time_limit, jobs_queue, policy_queries_queue, mode):\n        super().__init__(name=name)\n        self.jobs_queue = jobs_queue\n        self.policy_queries_queue = policy_queries_queue\n        self.policy_answers_queue = queue.Queue()\n        self.mode = mode\n\n        # Setup Ecole environment\n        scip_params={'separating/maxrounds': 0,\n                     'presolving/maxrestarts': 0,\n                     'limits/time': time_limit,\n                     'timing/clocktype': 2}\n        observation_function=(\n            ecole.observation.FocusNode(),\n            ecole.observation.NodeBipartite()\n            )\n        reward_function=ecole.reward.NNodes().cumsum()\n        information_function={\n            'nnodes': ecole.reward.NNodes().cumsum(),\n            'lpiters': ecole.reward.LpIterations().cumsum(),\n            'time': ecole.reward.SolvingTime().cumsum()\n        }\n\n        if mode == 'tmdp+ObjLim':\n            self.env = ObjLimBranchingEnv(scip_params=scip_params,\n                                          pseudo_candidates=False,\n                                          observation_function=observation_function,\n                                          reward_function=reward_function,\n                                          information_function=information_function)\n        elif mode == 'tmdp+DFS':\n            self.env = DFSBranchingEnv(scip_params=scip_params,\n                                       pseudo_candidates=False,\n                                       observation_function=observation_function,\n                                       reward_function=reward_function,\n                                       information_function=information_function)\n        elif mode == 'mdp':\n            self.env = MDPBranchingEnv(scip_params=scip_params,\n                                       pseudo_candidates=False,\n                                       observation_function=observation_function,\n                                       reward_function=reward_function,\n                                       information_function=information_function)\n        else:\n            raise NotImplementedError\n\n    def run(self):\n        while True:\n            job_sponsor = self.jobs_queue.get()\n\n            # check for a stopping order\n            if job_sponsor is None:\n                self.jobs_queue.task_done()\n                break\n\n            # Get task from job sponsor\n            task = job_sponsor.get()\n            instance = task['instance']\n            sample_rate = task['sample_rate']\n            greedy = task['greedy'] # should actions be chosen greedily w.r.t. the policy?\n            training = not greedy\n            samples = task['samples']\n            stats = task['stats']\n            policy_access = task['policy_access']\n            seed = instance['seed']\n\n            transitions = []\n            self.env.seed(seed)\n            rng = np.random.RandomState(seed)\n            if sample_rate > 0:\n                tree_recorder = TreeRecorder()\n\n            # Run episode\n            observation, action_set, cum_nnodes, done, info = self.env.reset(instance = instance['path'],\n                                                                             primal_bound=instance.get('sol', None),\n                                                                             training=training)\n            policy_access.wait()\n            iter_count = 0\n            while not done:\n                focus_node_obs, node_bipartite_obs = observation\n                state = utilities.extract_state(node_bipartite_obs, action_set, focus_node_obs.number)\n\n                # send out policy queries\n                self.policy_queries_queue.put({'state': state, 'greedy': greedy, 'receiver': self.policy_answers_queue})\n                action_idx = self.policy_answers_queue.get()\n\n                action = action_set[action_idx]\n\n                # collect transition samples if requested\n                if sample_rate > 0:\n                    tree_recorder.record_branching_decision(focus_node_obs)\n                    keep_sample = rng.rand() < sample_rate\n                    if keep_sample:\n                        transition = utilities.Transition(state, action_idx, cum_nnodes)\n                        transitions.append(transition)\n\n                observation, action_set, cum_nnodes, done, info = self.env.step(action)\n                iter_count += 1\n                if (iter_count>50000) and training: done=True # avoid too large trees during training for stability\n\n            if (iter_count>50000) and training: # avoid too large trees during training for stability\n                job_sponsor.task_done()\n                self.jobs_queue.task_done()\n                continue\n\n            # post-process the collected samples (credit assignment)\n            if sample_rate > 0:\n                if self.mode in ['tmdp+ObjLim', 'tmdp+DFS']:\n                    subtree_sizes = tree_recorder.calculate_subtree_sizes()\n                    for transition in transitions:\n                        transition.returns = -subtree_sizes[transition.node_id] - 1\n                else:\n                    assert self.mode == 'mdp'\n                    for transition in transitions:\n                        transition.returns = transition.cum_nnodes - cum_nnodes\n\n            # record episode samples and stats\n            samples.extend(transitions)\n            stats.append({'order': task, 'info': info})\n\n            # tell both the agent pool and the original task sponsor that the task is done\n            job_sponsor.task_done()\n            self.jobs_queue.task_done()\n\nclass TreeRecorder:\n    \"\"\"\n    Records the branch-and-bound tree from a custom brancher.\n\n    Every node in SCIP has a unique node ID. We identify nodes and their corresponding\n    attributes through the same ID system.\n    Depth groups keep track of groups of nodes at the same depth. This data structure\n    is used to speed up the computation of the subtree size.\n    \"\"\"\n    def __init__(self):\n        self.tree = {}\n        self.depth_groups = []\n\n    def record_branching_decision(self, focus_node, lp_cand=True):\n        id = focus_node.number\n        # Tree\n        self.tree[id] = {'parent': focus_node.parent_number,\n                         'lowerbound': focus_node.lowerbound,\n                         'num_children': 2 if lp_cand else 3  }\n        # Add to corresponding depth group\n        if len(self.depth_groups) > focus_node.depth:\n            self.depth_groups[focus_node.depth].append(id)\n        else:\n            self.depth_groups.append([id])\n\n    def calculate_subtree_sizes(self):\n        subtree_sizes = {id: 0 for id in self.tree.keys()}\n        for group in self.depth_groups[::-1]:\n            for id in group:\n                parent_id = self.tree[id]['parent']\n                subtree_sizes[id] += self.tree[id]['num_children']\n                if parent_id >= 0: subtree_sizes[parent_id] += subtree_sizes[id]\n        return subtree_sizes\n\n# ==========================================\n# File: brain.py\n# Function/Context: Brain\n# ==========================================\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric\nimport utilities\nimport itertools\nfrom pathlib import Path\nfrom actor.actor import GNNPolicy\nfrom datetime import datetime\n\nclass Brain:\n    \"\"\"\n    Brain class. Holds the policy, and receives requests from the agents to sample actions using\n    it, given a state. It also performs policy updates after receiving training samples from the\n    agents. \n    \"\"\"\n    def __init__(self, config, device, problem, mode):\n        self.config = config\n        self.device = device\n        self.problem = problem\n        self.mode = mode\n        self.actor = GNNPolicy().to(device)\n        self.optimizer = torch.optim.Adam(self.actor.parameters(), lr=self.config['lr'])\n        self.random = np.random.RandomState(seed=self.config['seed'])\n\n        # Create timestamp to save weights\n        current_date = datetime.now().strftime('%Y-%m-%d')\n        current_time = datetime.now().strftime('%H.%M.%S')\n        self.timestamp = f\"{current_date}--{current_time}\"\n\n    def sample_action_idx(self, states, greedy):\n\n        if isinstance(greedy, bool):\n            greedy = torch.tensor(np.repeat(greedy, len(states), dtype=torch.long))\n        elif not isinstance(greedy, torch.Tensor):\n            greedy = torch.tensor(greedy, dtype=torch.long)\n\n        states_loader = torch_geometric.data.DataLoader(states, batch_size=self.config['batch_size'])\n        greedy_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(greedy), batch_size=self.config['batch_size'])\n\n        action_idxs = []\n        for batch, (greedy,) in zip(states_loader, greedy_loader):\n            with torch.no_grad():\n                batch = batch.to(self.device)\n                logits = self.actor(batch.constraint_features, batch.edge_index, batch.edge_attr, batch.variable_features)\n                logits = logits[batch.action_set]\n\n                logits_end = batch.action_set_size.cumsum(-1)\n                logits_start = logits_end - batch.action_set_size\n                for start, end, greedy in zip(logits_start, logits_end, greedy):\n                    if greedy:\n                        action_idx = logits[start:end].argmax()\n                    else:\n                        action_idx = torch.distributions.categorical.Categorical(logits=logits[start:end]).sample()\n                    action_idxs.append(action_idx.item())\n\n        return action_idxs\n\n    def update(self, transitions):\n        n_samples = len(transitions)\n        if n_samples < 1:\n           stats = {'loss': 0.0, 'reinforce_loss': 0.0, 'entropy': 0.0}\n           return stats\n\n        transitions = torch_geometric.data.DataLoader(transitions, batch_size=16, shuffle=True)\n\n        stats = {}\n\n        self.optimizer.zero_grad()\n        for batch in transitions:\n            batch = batch.to(self.device)\n            loss = torch.tensor([0.0], device=self.device)\n            logits = self.actor(batch.constraint_features, batch.edge_index, batch.edge_attr, batch.variable_features)\n            logits = utilities.pad_tensor(logits[batch.action_set], batch.action_set_size)\n            dist = torch.distributions.categorical.Categorical(logits=logits)\n\n            # REINFORCE\n            returns = batch.returns.float()\n            reinforce_loss = - (returns * dist.log_prob(batch.action)).sum()\n            reinforce_loss /= n_samples\n            loss += reinforce_loss\n\n            # ENTROPY\n            entropy = dist.entropy().sum()\n            entropy /= n_samples\n            loss += - self.config['entropy_bonus']*entropy\n\n            loss.backward()\n\n            # Update stats\n            stats['loss'] = stats.get('loss', 0.0) + loss.item()\n            stats['reinforce_loss'] = stats.get('reinforce_loss', 0.0) + reinforce_loss.item()\n            stats['entropy'] = stats.get('entropy', 0.0) + entropy.item()\n\n        self.optimizer.step()\n\n        return stats\n\n    def save(self):\n        # Save in the same directory as the pretrained params\n        torch.save(self.actor.state_dict(),\n                   f\"actor/{self.problem}/0/{self.timestamp}--best_params--{self.mode}.pkl\")\n\n# ==========================================\n# File: evaluate.py\n# Function/Context: \n# ==========================================\nimport os\nimport sys\nimport importlib\nimport argparse\nimport csv\nimport numpy as np\nimport time\nimport pickle\n\nimport ecole\nimport pyscipopt\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        'problem',\n        help='MILP instance type to process.',\n        choices=['setcover', 'cauctions', 'ufacilities', 'indset', 'mknapsack'],\n    )\n    parser.add_argument(\n        '-g', '--gpu',\n        help='CUDA GPU id (-1 for CPU).',\n        type=int,\n        default=0,\n    )\n    args = parser.parse_args()\n\n    result_file = f\"{args.problem}_{time.strftime('%Y%m%d-%H%M%S')}.csv\"\n    instances = []\n    seeds = [0, 1, 2, 3, 4]\n    internal_branchers = ['relpscost']\n    gcnn_models = ['il', 'mdp', 'tmdp+DFS', 'tmdp+ObjLim']\n    time_limit = 3600\n\n    if args.problem == 'setcover':\n        instances += [{'type': 'test', 'path': f\"data/instances/setcover/test_400r_750c_0.05d/instance_{i+1}.lp\"} for i in range(40)]\n        instances += [{'type': 'transfer', 'path': f\"data/instances/setcover/transfer_500r_1000c_0.05d/instance_{i+1}.lp\"} for i in range(40)]\n\n    elif args.problem == 'cauctions':\n        instances += [{'type': 'test', 'path': f\"data/instances/cauctions/test_100_500/instance_{i+1}.lp\"} for i in range(40)]\n        instances += [{'type': 'transfer', 'path': f\"data/instances/cauctions/transfer_200_1000/instance_{i+1}.lp\"} for i in range(40)]\n\n    elif args.problem == 'ufacilities':\n        instances += [{'type': 'test', 'path': f\"data/instances/ufacilities/test_35_35_5/instance_{i+1}.lp\"} for i in range(40)]\n        instances += [{'type': 'transfer', 'path': f\"data/instances/ufacilities/transfer_60_35_5/instance_{i+1}.lp\"} for i in range(40)]\n\n    elif args.problem == 'indset':\n        instances += [{'type': 'test', 'path': f\"data/instances/indset/test_500_4/instance_{i+1}.lp\"} for i in range(40)]\n        instances += [{'type': 'transfer', 'path': f\"data/instances/indset/transfer_1000_4/instance_{i+1}.lp\"} for i in range(40)]\n\n    elif args.problem == 'mknapsack':\n        instances += [{'type': 'test', 'path': f\"data/instances/mknapsack/test_100_6/instance_{i+1}.lp\"} for i in range(40)]\n        instances += [{'type': 'transfer', 'path': f\"data/instances/mknapsack/transfer_100_12/instance_{i+1}.lp\"} for i in range(40)]\n\n    else:\n        raise NotImplementedError\n\n    branching_policies = []\n\n    # SCIP internal brancher baselines\n    for brancher in internal_branchers:\n        for seed in seeds:\n            branching_policies.append({\n                    'type': 'internal',\n                    'name': brancher,\n                    'seed': seed,\n             })\n    # GCNN models\n    for model in gcnn_models:\n        for seed in seeds:\n            branching_policies.append({\n                'type': 'gcnn',\n                'name': model,\n                'seed': seed,\n            })\n\n    print(f\"problem: {args.problem}\")\n    print(f\"gpu: {args.gpu}\")\n    print(f\"time limit: {time_limit} s\")\n\n    ### PYTORCH SETUP ###\n    if args.gpu == -1:\n        os.environ['CUDA_VISIBLE_DEVICES'] = ''\n        device = 'cpu'\n    else:\n        os.environ['CUDA_VISIBLE_DEVICES'] = f'{args.gpu}'\n        device = f\"cuda:0\"\n\n    import torch\n    from actor.actor import GNNPolicy\n\n    # load and assign tensorflow models to policies (share models and update parameters)\n    loaded_models = {}\n    loaded_calls = {}\n    for policy in branching_policies:\n        if policy['type'] == 'gcnn':\n            if policy['name'] not in loaded_models:\n                ### MODEL LOADING ###\n                model = GNNPolicy().to(device)\n                if policy['name'] == 'il':\n                    model.load_state_dict(torch.load(f'actor/{args.problem}/0/il.pkl'))\n                elif policy['name'] == 'mdp':\n                    model.load_state_dict(torch.load(f'actor/{args.problem}/0/mdp.pkl'))\n                elif policy['name'] == 'tmdp+DFS':\n                    model.load_state_dict(torch.load(f'actor/{args.problem}/0/tmdp+DFS.pkl'))\n                elif policy['name'] == 'tmdp+ObjLim':\n                    model.load_state_dict(torch.load(f'actor/{args.problem}/0/tmdp+ObjLim.pkl'))\n                else:\n                    raise Exception(f\"Unrecognized GNN policy {policy[name]}\")\n                loaded_models[policy['name']] = model\n\n            policy['model'] = loaded_models[policy['name']]\n\n    print(\"running SCIP...\")\n\n    fieldnames = [\n        'policy',\n        'seed',\n        'type',\n        'instance',\n        'nnodes',\n        'nlps',\n        'stime',\n        'gap',\n        'status',\n        'walltime',\n        'proctime',\n    ]\n    os.makedirs('results', exist_ok=True)\n    scip_parameters = {'separating/maxrounds': 0, 'presolving/maxrestarts': 0,\n                       'limits/time': time_limit, 'timing/clocktype': 1}\n\n    with open(f\"results/{result_file}\", 'w', newline='') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n\n        for instance in instances:\n            print(f\"{instance['type']}: {instance['path']}...\")\n\n            for policy in branching_policies:\n                if policy['type'] == 'internal':\n                    # Run SCIP's default brancher\n                    env = ecole.environment.Configuring(scip_params={**scip_parameters,\n                                                        f\"branching/{policy['name']}/priority\": 9999999})\n                    env.seed(policy['seed'])\n\n                    walltime = time.perf_counter()\n                    proctime = time.process_time()\n\n                    env.reset(instance['path'])\n                    _, _, _, _, _ = env.step({})\n\n                    walltime = time.perf_counter() - walltime\n                    proctime = time.process_time() - proctime\n\n                elif policy['type'] == 'gcnn':\n                    # Run the GNN policy\n                    env = ecole.environment.Branching(observation_function=ecole.observation.NodeBipartite(),\n                                                      scip_params=scip_parameters)\n                    env.seed(policy['seed'])\n                    torch.manual_seed(policy['seed'])\n\n                    walltime = time.perf_counter()\n                    proctime = time.process_time()\n\n                    observation, action_set, _, done, _ = env.reset(instance['path'])\n                    while not done:\n                        with torch.no_grad():\n                            observation = (torch.from_numpy(observation.row_features.astype(np.float32)).to(device),\n                                           torch.from_numpy(observation.edge_features.indices.astype(np.int64)).to(device),\n                                           torch.from_numpy(observation.edge_features.values.astype(np.float32)).view(-1, 1).to(device),\n                                           torch.from_numpy(observation.column_features.astype(np.float32)).to(device))\n\n                            logits = policy['model'](*observation)\n                            action = action_set[logits[action_set.astype(np.int64)].argmax()]\n                            observation, action_set, _, done, _ = env.step(action)\n\n                    walltime = time.perf_counter() - walltime\n                    proctime = time.process_time() - proctime\n\n                scip_model = env.model.as_pyscipopt()\n                stime = scip_model.getSolvingTime()\n                nnodes = scip_model.getNNodes()\n                nlps = scip_model.getNLPs()\n                gap = scip_model.getGap()\n                status = scip_model.getStatus()\n\n                writer.writerow({\n                    'policy': f\"{policy['type']}:{policy['name']}\",\n                    'seed': policy['seed'],\n                    'type': instance['type'],\n                    'instance': instance['path'],\n                    'nnodes': nnodes,\n                    'nlps': nlps,\n                    'stime': stime,\n                    'gap': gap,\n                    'status': status,\n                    'walltime': walltime,\n                    'proctime': proctime,\n                })\n                csvfile.flush()\n\n                print(f\"  {policy['type']}:{policy['name']} {policy['seed']} - {nnodes} nodes {nlps} lps {stime:.2f} ({walltime:.2f} wall {proctime:.2f} proc) s. {status}\")",
  "description": "Combined Analysis:\n- [actor/actor.py]: This file implements the core neural network architecture (GNNPolicy) for learning branching policies in MILP Branch-and-Bound. The GNN processes the bipartite graph representation of MILP constraints and variables through two bipartite graph convolutions (variable-to-constraint and constraint-to-variable) to compute variable scores for branching decisions. This directly corresponds to the policy network in the RL framework described in the paper, which learns to select branching variables without imitating expensive expert heuristics.\n- [agent.py]: This code implements the core Reinforcement Learning (RL) agent logic for learning branching policies in Branch-and-Bound using tree Markov Decision Processes (tree MDPs) as described in the paper. The Agent class runs episodes by interacting with custom Ecole environments (supporting modes 'mdp', 'tmdp+DFS', 'tmdp+ObjLim'), samples actions from a policy network via PolicySampler, collects transitions, and computes returns based on the tree structure for credit assignment in tree MDP modes. The TreeRecorder class is used to record the branch-and-bound tree and calculate subtree sizes, which are essential for defining rewards in tree MDPs. This aligns with the paper's algorithm steps of RL training with tree MDPs to learn effective branching decisions.\n- [brain.py]: This file implements the core reinforcement learning policy component for the Tree MDP branching algorithm. The Brain class encapsulates the GNN-based policy network (actor) that selects branching variables during Branch-and-Bound. Key implementations include: 1) Action sampling via policy network inference with greedy/exploratory selection (sample_action_idx), 2) Policy gradient updates using REINFORCE with entropy regularization (update method), which aligns with the paper's RL formulation for learning branching policies. The code directly implements the policy optimization step of the Tree MDP framework, where the GNN processes MILP bipartite graph representations (constraint/variable features and edge attributes) to produce action logits.\n- [evaluate.py]: This file implements the evaluation component of the Tree MDPs paper's core algorithm. It evaluates learned branching policies (GCNN models trained with different RL approaches: MDP, tree MDP+DFS, tree MDP+ObjLim) against SCIP's default branching rule. The key algorithm steps implemented are: 1) Loading trained GNN models that implement the learned branching policies, 2) Setting up SCIP solver environments with specific parameters, 3) Executing the Branch-and-Bound algorithm with different branching policies, 4) Collecting performance metrics (nodes, solving time, gap, etc.). The code specifically shows how the learned GNN policy interacts with the B&B tree during solving - at each node, the GNN processes the bipartite graph observation and selects the branching variable. This directly implements the application of the RL-learned branching policies within the B&B algorithm framework described in the paper.",
  "dependencies": [
    "DFSBranchingEnv",
    "utilities",
    "PreNormException",
    "pickle",
    "MDPBranchingEnv",
    "pathlib.Path",
    "torch.distributions.categorical.Categorical",
    "time",
    "queue",
    "pyscipopt",
    "threading",
    "csv",
    "torch.nn.functional",
    "BipartiteGraphConvolution",
    "ObjLimBranchingEnv",
    "PreNormLayer",
    "ecole",
    "torch.optim.Adam",
    "actor.actor.GNNPolicy",
    "os",
    "argparse",
    "utilities.pad_tensor",
    "TreeRecorder",
    "numpy",
    "collections.namedtuple",
    "torch_geometric",
    "BaseModel",
    "torch",
    "GNNPolicy from actor.actor",
    "datetime.datetime",
    "itertools"
  ]
}