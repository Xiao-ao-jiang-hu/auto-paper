{
  "file_path": "1_generate_data.py, 2_training.py, 3_testing.py, models.py",
  "function_name": "generateMILPunfoldable, generateMILPfoldable, process, process, GCNPolicy",
  "code_snippet": "\n\n# ==========================================\n# File: 1_generate_data.py\n# Function/Context: generateMILPunfoldable, generateMILPfoldable\n# ==========================================\nimport numpy as np\nimport random as rd\nimport os\nimport argparse\nfrom pandas import read_csv\nimport pyscipopt as scip\n\ndef generateMILPunfoldable(k_data, configs, folder):\n    '''\n    This function generates and saves unfoldable MILP instances.\n    - k_data: the number of instances you want to generate\n    - configs: (m,n,nnz), configurations of each MILP instance\n    - folder: the folder you want to save those generated MILPs\n    '''\n    m,n,nnz,env = configs\n    \n    count_feas = 0\n\n    for k in range(k_data):\n        path = folder + \"MIP_env\" + str(env) + \"_unfoldable_\" + str(k)\n        if not os.path.exists(path):\n            os.makedirs(path)\n\n        c = np.array([ rd.normalvariate(0, 1) for j in range(n) ]) * 0.01\n        b = np.array([ rd.normalvariate(0, 1) for i in range(m) ])\n        circ = np.array([ rd.randint(0, 2) for i in range(m) ])\n        NI = np.array([ rd.randint(0, 1) for j in range(n) ])\n        lb = np.array([ rd.normalvariate(0, 10) for j in range(n) ])\n        ub = np.array([ rd.normalvariate(0, 10) for j in range(n) ])\n\n        for j in range(n):\n            if lb[j] > ub[j]:\n                temp = lb[j]\n                lb[j] = ub[j]\n                ub[j] = temp\n\n        A = np.zeros((m, n))\n        EdgeIndex = np.zeros((nnz, 2))\n        EdgeIndex1D = rd.sample(range(m * n), nnz)\n        EdgeFeature = np.array([ rd.normalvariate(0, 1) for l in range(nnz) ])\n        \n        for l in range(nnz):\n            i = int(EdgeIndex1D[l] / n)\n            j = EdgeIndex1D[l] - i * n\n            EdgeIndex[l, 0] = i\n            EdgeIndex[l, 1] = j\n            A[i, j] = EdgeFeature[l]\n\n        opt_model = scip.Model(\"Unfoldable MIP Model \" + str(k))\n        opt_model.hideOutput()\n\n        x_vars = []\n\n        for j in range(n):\n            if NI[j] == 1:\n                x_vars.append( opt_model.addVar(vtype = \"INTEGER\", lb = lb[j], ub = ub[j], name=\"x_{0}\".format(j)) )\n            else:\n                x_vars.append( opt_model.addVar(vtype = \"CONTINUOUS\", lb = lb[j], ub = ub[j], name=\"x_{0}\".format(j)) )\n\n        for i in range(m):\n            if circ[i] == 0:\n                opt_model.addCons(scip.quicksum(A[i,j] * x_vars[j] for j in range(n)) <= b[i], name=\"constraint_{0}\".format(i))\n            elif circ[i] == 1:\n                opt_model.addCons(scip.quicksum(A[i,j] * x_vars[j] for j in range(n)) == b[i], name=\"constraint_{0}\".format(i))\n            else:\n                opt_model.addCons(scip.quicksum(A[i,j] * x_vars[j] for j in range(n)) >= b[i], name=\"constraint_{0}\".format(i))\n\n        opt_model.setObjective(scip.quicksum(x_vars[j] * c[j] for j in range(n)), \"minimize\")\n        \n        ## save the MILP instance\n        # opt_model.writeProblem(filename = path + \".mps\")\n        np.savetxt(path + '/ConFeatures.csv', np.hstack((b.reshape(m, 1), circ.reshape(m, 1))), delimiter = ',', fmt = '%10.5f')\n        np.savetxt(path + '/EdgeFeatures.csv', EdgeFeature, fmt = '%10.5f')\n        np.savetxt(path + '/EdgeIndices.csv', EdgeIndex, delimiter = ',', fmt = '%d')\n        np.savetxt(path + '/VarFeatures.csv', np.hstack((c.reshape(n, 1), NI.reshape(n, 1), lb.reshape(n, 1), ub.reshape(n, 1))), delimiter = ',', fmt = '%10.5f')\n        \n        ## call SCIP solver to solve the MILP\n        opt_model.optimize()\n        \n        ## save the results\n        if opt_model.getStatus() == \"optimal\":\n            count_feas += 1\n            sol = np.array([opt_model.getVal(x) for x in x_vars]).T\n            obj = opt_model.getObjVal()\n            np.savetxt(path + '/Labels_feas.csv', [1], fmt = '%d')\n            np.savetxt(path + '/Labels_obj.csv', [obj], fmt = '%10.5f')\n            np.savetxt(path + '/Labels_solu.csv', sol, fmt = '%10.5f')\n            print(\"MILP generated:\", k, '/', k_data, \"Status: optimal.\")\n        elif opt_model.getStatus() == \"infeasible\":\n            np.savetxt(path + '/Labels_feas.csv', [0], fmt = '%d')\n            print(\"MILP generated:\", k, '/', k_data, \"Status: infeasible.\")\n        else:\n            print(\"Unexpected model status. Quit.\")\n            quit()\n\n    print(\"Ratio of feasible instances:\", count_feas, '/', k_data)\n\n\ndef generateMILPfoldable(k_data, configs, folder):\n    '''\n    This function generates and saves foldable MILP instances.\n    - k_data: the number of instances you want to generate\n    - configs: (m,n), configurations of each LP instance\n    - folder: the folder you want to save those generated LPs\n    '''\n    m,n,nnz,env = configs\n    \n    count_feas = 0\n\n    for k in range(int(k_data / 2)):\n        k1 = 2 * k\n        path1 = folder + \"MIP_env\" + str(env) + \"_foldable_\" + str(k1)\n        if not os.path.exists(path1):\n            os.makedirs(path1)\n\n        k2 = 2 * k + 1\n        path2 = folder + \"MIP_env\" + str(env) + \"_foldable_\" + str(k2)\n        if not os.path.exists(path2):\n            os.makedirs(path2)\n\n        if env == 2:\n            c = np.ones((n,)) * 0.01\n        else:\n            c = np.zeros((n,))\n        b = np.array([ 1 for i in range(m) ])\n        \n        lb = np.array([ rd.normalvariate(0, 10) for j in range(n) ])\n        ub = np.array([ rd.normalvariate(0, 10) for j in range(n) ])\n        NI = np.array([ 0 for j in range(n) ])\n\n        for j in range(n):\n            if lb[j] > ub[j]:\n                temp = lb[j]\n                lb[j] = ub[j]\n                ub[j] = temp\n\n        j_list = rd.sample(range(n), 6)\n        for j in j_list:\n            lb[j] = 0\n            ub[j] = 1\n            NI[j] = 1\n\n        A1 = np.zeros((m, n))\n        EdgeIndex1 = np.zeros((12, 2))\n        EdgeFeature1 = np.ones((12, 1))\n        for i in range(6):\n            j1 = j_list[i]\n            if i == 5:\n                j2 = j_list[0]\n            else:\n                j2 = j_list[i + 1]\n            \n            EdgeIndex1[2 * i, 0] = i\n            EdgeIndex1[2 * i + 1, 0] = i\n            EdgeIndex1[2 * i, 1] = j1\n            EdgeIndex1[2 * i + 1, 1] = j2\n        \n        A2 = np.zeros((m, n))\n        EdgeIndex2 = np.zeros((12, 2))\n        EdgeFeature2 = np.ones((12, 1))\n        for i in range(6):\n            j1 = j_list[i]\n            if i == 2:\n                j2 = j_list[0]\n            elif i == 5:\n                j2 = j_list[3]\n            else:\n                j2 = j_list[i + 1]\n            \n            EdgeIndex2[2 * i, 0] = i\n            EdgeIndex2[2 * i + 1, 0] = i\n            EdgeIndex2[2 * i, 1] = j1\n            EdgeIndex2[2 * i + 1, 1] = j2\n        \n        for l in range(12):\n            i = int(EdgeIndex1[l, 0])\n            j = int(EdgeIndex1[l, 1])\n            A1[i, j] = EdgeFeature1[l, 0]\n            i = int(EdgeIndex2[l, 0])\n            j = int(EdgeIndex2[l, 1])\n            A2[i, j] = EdgeFeature2[l, 0]\n\n        opt_model1 = scip.Model(\"Foldable MIP Model\" + str(k1))\n        opt_model2 = scip.Model(\"Foldable MIP Model\" + str(k2))\n        opt_model1.hideOutput()\n        opt_model2.hideOutput()\n\n        x_vars1 = []\n        x_vars2 = []\n\n        for j in range(n):\n            if NI[j] == 1:\n                x_vars1.append( opt_model1.addVar(vtype = \"INTEGER\", lb = lb[j], ub = ub[j], name=\"x_{0}\".format(j)) )\n                x_vars2.append( opt_model2.addVar(vtype = \"INTEGER\", lb = lb[j], ub = ub[j], name=\"x_{0}\".format(j)) )\n            else:\n                x_vars1.append( opt_model1.addVar(vtype = \"CONTINUOUS\", lb = lb[j], ub = ub[j], name=\"x_{0}\".format(j)) )\n                x_vars2.append( opt_model2.addVar(vtype = \"CONTINUOUS\", lb = lb[j], ub = ub[j], name=\"x_{0}\".format(j)) )\n\n        for i in range(m):\n            opt_model1.addCons(scip.quicksum(A1[i,j] * x_vars1[j] for j in range(n)) == b[i], name=\"constraint_{0}\".format(i))\n            opt_model2.addCons(scip.quicksum(A2[i,j] * x_vars2[j] for j in range(n)) == b[i], name=\"constraint_{0}\".format(i))\n\n        opt_model1.setObjective(scip.quicksum(x_vars1[j] * c[j] for j in range(n)), \"minimize\")\n        opt_model2.setObjective(scip.quicksum(x_vars2[j] * c[j] for j in range(n)), \"minimize\")\n        \n        ## save the MILP instance\n        # opt_model1.writeProblem(filename = path1 + \".mps\")\n        # opt_model2.writeProblem(filename = path2 + \".mps\")\n        np.savetxt(path1 + '/ConFeatures.csv', np.hstack((b.reshape(m, 1), np.ones((m, 1)))), delimiter = ',', fmt = '%10.5f')\n        np.savetxt(path1 + '/EdgeFeatures.csv', EdgeFeature1, fmt = '%10.5f')\n        np.savetxt(path1 + '/EdgeIndices.csv', EdgeIndex1, delimiter = ',', fmt = '%d')\n        np.savetxt(path1 + '/VarFeatures.csv', np.hstack((c.reshape(n, 1), NI.reshape(n, 1), lb.reshape(n, 1), ub.reshape(n, 1))), delimiter = ',', fmt = '%10.5f')\n        np.savetxt(path2 + '/ConFeatures.csv', np.hstack((b.reshape(m, 1), np.ones((m, 1)))), delimiter = ',', fmt = '%10.5f')\n        np.savetxt(path2 + '/EdgeFeatures.csv', EdgeFeature2, fmt = '%10.5f')\n        np.savetxt(path2 + '/EdgeIndices.csv', EdgeIndex2, delimiter = ',', fmt = '%d')\n        np.savetxt(path2 + '/VarFeatures.csv', np.hstack((c.reshape(n, 1), NI.reshape(n, 1), lb.reshape(n, 1), ub.reshape(n, 1))), delimiter = ',', fmt = '%10.5f')\n\n        ## solve MILPs\n        opt_model1.optimize()\n        opt_model2.optimize()\n\n        if opt_model1.getStatus() == \"optimal\":\n            count_feas += 1\n            sol1 = np.array([opt_model1.getVal(x) for x in x_vars1]).T\n            np.savetxt(path1 + '/Labels_feas.csv', [1], fmt = '%d')\n            np.savetxt(path1 + '/Labels_obj.csv', [opt_model1.getObjVal()], fmt = '%10.5f')\n            np.savetxt(path1 + '/Labels_solu.csv', sol1, fmt = '%10.5f')\n            print(\"MILP generated:\", k1, '/', k_data, \"Status: optimal.\")\n        elif opt_model1.getStatus() == \"infeasible\":\n            np.savetxt(path1 + '/Labels_feas.csv', [0], fmt = '%d')\n            print(\"MILP generated:\", k1, '/', k_data, \"Status: infeasible.\")\n        else:\n            print(\"Unexpected model status. Quit.\")\n            quit()\n        \n        if opt_model2.getStatus() == \"optimal\":\n            count_feas += 1\n            sol2 = np.array([opt_model2.getVal(x) for x in x_vars2]).T\n            np.savetxt(path2 + '/Labels_feas.csv', [1], fmt = '%d')\n            np.savetxt(path2 + '/Labels_obj.csv', [opt_model2.getObjVal()], fmt = '%10.5f')\n            np.savetxt(path2 + '/Labels_solu.csv', sol2, fmt = '%10.5f')\n            print(\"MILP generated:\", k2, '/', k_data, \"Status: optimal.\")\n        elif opt_model2.getStatus() == \"infeasible\":\n            np.savetxt(path2 + '/Labels_feas.csv', [0], fmt = '%d')\n            print(\"MILP generated:\", k2, '/', k_data, \"Status: infeasible.\")\n        else:\n            print(\"Unexpected model status. Quit.\")\n            quit()\n\n    print(\"Ratio of feasible instances:\", count_feas, '/', k_data)\n\n# ==========================================\n# File: 2_training.py\n# Function/Context: process\n# ==========================================\nimport numpy as np\nfrom pandas import read_csv\nimport tensorflow as tf\nimport argparse\nimport os\nfrom models import GCNPolicy\n\n## ARGUMENTS OF THE SCRIPT\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--data\", \t\thelp=\"number of training data\", \tdefault=1000)\nparser.add_argument(\"--gpu\", \t\thelp=\"gpu index\", \t\t\t\t\tdefault=\"0\")\nparser.add_argument(\"--embSize\", \thelp=\"embedding size of GNN\", \t\tdefault=\"6\")\nparser.add_argument(\"--epoch\", \t\thelp=\"num of epoch\", \t\t\t\tdefault=\"10000\")\nparser.add_argument(\"--type\", \t\thelp=\"what's the type of the model\",default=\"fea\", \tchoices = ['fea','obj','sol'])\nparser.add_argument(\"--data_path\", \thelp=\"path of data\", \t\t\t\tdefault=None)\nargs = parser.parse_args()\n\n## FUNCTION OF TRAINING PER EPOCH\ndef process(model, dataloader, optimizer, type = 'fea'):\n\n\tc, ei, ev, v, n_cs, n_vs, n_csm, n_vsm, cand_scores = dataloader\n\tbatched_states = (c, ei, ev, v, n_cs, n_vs, n_csm, n_vsm)  \n\n\twith tf.GradientTape() as tape:\n\t\tlogits = model(batched_states, tf.convert_to_tensor(True)) \n\t\tloss = tf.keras.metrics.mean_squared_error(cand_scores, logits)\n\t\tloss = tf.reduce_mean(loss)\n\tgrads = tape.gradient(target=loss, sources=model.variables)\n\toptimizer.apply_gradients(zip(grads, model.variables))\n\t\n\tlogits = model(batched_states, tf.convert_to_tensor(False)) \n\tloss = tf.keras.metrics.mean_squared_error(cand_scores, logits)\n\tloss = tf.reduce_mean(loss)\n\n\treturn_loss = loss.numpy()\n\terrs = None\n\terr_rate = None\n\t\n\tif type == \"fea\":\n\t\terrs_fp = np.sum((logits.numpy() > 0.5) & (cand_scores.numpy() < 0.5))\n\t\terrs_fn = np.sum((logits.numpy() < 0.5) & (cand_scores.numpy() > 0.5))\n\t\terrs = errs_fp + errs_fn\n\t\terr_rate = errs / cand_scores.shape[0]\n\n\treturn return_loss, errs, err_rate\n\n## SET-UP HYPER PARAMETERS\nmax_epochs = int(args.epoch)\nlr = 0.0003\nseed = 0\n\n## SET-UP DATASET\ntrainfolder = args.data_path\nn_Samples = int(args.data)\nn_Cons_small = 6 # Each MILP has 6 constraints\nn_Vars_small = 20 # Each MILP has 20 variables\nif trainfolder == \"data-env1/unfoldable\":\n\tn_Eles_small = 60 # Each MILP has 60 nonzeros in matrix A\nelse:\n\tn_Eles_small = 12\n\n## SET-UP MODEL\nembSize = int(args.embSize)\nif not os.path.exists('./saved-models/'):\n\tos.mkdir('./saved-models/')\nmodel_setting = trainfolder.replace('/','-')\nmodel_path = './saved-models/' + model_setting + '-' + args.type + '-d' + str(n_Samples) + '-s' + str(embSize) + '.pkl'\n\n## LOAD DATASET INTO MEMORY\nif args.type == \"fea\":\n\tvarFeatures = read_csv(trainfolder + \"/VarFeatures_all.csv\", header=None).values[:n_Vars_small * n_Samples,:]\n\tconFeatures = read_csv(trainfolder + \"/ConFeatures_all.csv\", header=None).values[:n_Cons_small * n_Samples,:]\n\tedgFeatures = read_csv(trainfolder + \"/EdgeFeatures_all.csv\", header=None).values[:n_Eles_small * n_Samples,:]\n\tedgIndices = read_csv(trainfolder + \"/EdgeIndices_all.csv\", header=None).values[:n_Eles_small * n_Samples,:]\n\tlabels = read_csv(trainfolder + \"/Labels_feas.csv\", header=None).values[:n_Samples,:]\nif args.type == \"obj\":\n\tvarFeatures = read_csv(trainfolder + \"/VarFeatures_feas.csv\", header=None).values[:n_Vars_small * n_Samples,:]\n\tconFeatures = read_csv(trainfolder + \"/ConFeatures_feas.csv\", header=None).values[:n_Cons_small * n_Samples,:]\n\tedgFeatures = read_csv(trainfolder + \"/EdgeFeatures_feas.csv\", header=None).values[:n_Eles_small * n_Samples,:]\n\tedgIndices = read_csv(trainfolder + \"/EdgeIndices_feas.csv\", header=None).values[:n_Eles_small * n_Samples,:]\n\tlabels = read_csv(trainfolder + \"/Labels_obj.csv\", header=None).values[:n_Samples,:]\nif args.type == \"sol\":\n\tvarFeatures = read_csv(trainfolder + \"/VarFeatures_feas.csv\", header=None).values[:n_Vars_small * n_Samples,:]\n\tconFeatures = read_csv(trainfolder + \"/ConFeatures_feas.csv\", header=None).values[:n_Cons_small * n_Samples,:]\n\tedgFeatures = read_csv(trainfolder + \"/EdgeFeatures_feas.csv\", header=None).values[:n_Eles_small * n_Samples,:]\n\tedgIndices = read_csv(trainfolder + \"/EdgeIndices_feas.csv\", header=None).values[:n_Eles_small * n_Samples,:]\n\tlabels = read_csv(trainfolder + \"/Labels_solu.csv\", header=None).values[:n_Vars_small * n_Samples,:]\n\nnConsF = conFeatures.shape[1]\nnVarF = varFeatures.shape[1]\nnEdgeF = edgFeatures.shape[1]\nn_Cons = conFeatures.shape[0]\nn_Vars = varFeatures.shape[0]\n\n## SET-UP TENSORFLOW\ntf.random.set_seed(seed)\ngpu_index = int(args.gpu)\ntf.config.set_soft_device_placement(True)\ngpus = tf.config.list_physical_devices('GPU')\ntf.config.set_visible_devices(gpus[gpu_index], 'GPU')\ntf.config.experimental.set_memory_growth(gpus[gpu_index], True)\n\nwith tf.device(\"GPU:\"+str(gpu_index)):\n\n\t### LOAD DATASET INTO GPU ###\n\tvarFeatures = tf.constant(varFeatures, dtype=tf.float32)\n\tconFeatures = tf.constant(conFeatures, dtype=tf.float32)\n\tedgFeatures = tf.constant(edgFeatures, dtype=tf.float32)\n\tedgIndices = tf.constant(edgIndices, dtype=tf.int32)\n\tedgIndices = tf.transpose(edgIndices)\n\tlabels = tf.constant(labels, dtype=tf.float32)\n\ttrain_data = (conFeatures, edgIndices, edgFeatures, varFeatures, n_Cons, n_Vars, n_Cons_small, n_Vars_small, labels)\n\n\t### INITIALIZATION ###\n\tif args.type == \"sol\":\n\t\tmodel = GCNPolicy(embSize, nConsF, nEdgeF, nVarF, isGraphLevel = False)\n\telse:\n\t\tmodel = GCNPolicy(embSize, nConsF, nEdgeF, nVarF)\n\toptimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n\tloss_init,_,_ = process(model, train_data, optimizer, type = args.type)\n\tepoch = 0\n\tcount_restart = 0\n\terr_best = 2\n\tloss_best = 1e10\n\t\n\t### MAIN LOOP ###\n\twhile epoch <= max_epochs:\n\t\ttrain_loss,errs,err_rate = process(model, train_data, optimizer, type = args.type)\n\t\t\t\n\t\tif args.type == \"fea\":\n\t\t\tprint(f\"EPOCH: {epoch}, TRAIN LOSS: {train_loss}, ERRS: {errs}, ERRATE: {err_rate}\")\n\t\t\tif err_rate < err_best:\n\t\t\t\tmodel.save_state(model_path)\n\t\t\t\tprint(\"model saved to:\", model_path)\n\t\t\t\terr_best = err_rate\n\t\telse:\n\t\t\tprint(f\"EPOCH: {epoch}, TRAIN LOSS: {train_loss}\")\n\t\t\tif train_loss < loss_best:\n\t\t\t\tmodel.save_state(model_path)\n\t\t\t\tprint(\"model saved to:\", model_path)\n\t\t\t\tloss_best = train_loss\n\t\t\n\t\t## If the loss does not go down, we restart the training to re-try another initialization.\n\t\tif epoch == 200 and count_restart < 3 and (train_loss > loss_init * 0.8 or (err_rate != None and err_rate > 0.5)):\n\t\t\tprint(\"Fail to reduce loss, restart...\")\n\t\t\tmodel = GCNPolicy(embSize, nConsF, nEdgeF, nVarF)\n\t\t\toptimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n\t\t\tloss_init,_,_ = process(model, train_data, optimizer, type = args.type)\n\t\t\tepoch = 0\n\t\t\tcount_restart += 1\n\t\t\t\n\t\tepoch += 1\n\t\n\tprint(\"Count of restart:\", count_restart)\n\tmodel.summary()\n\n# ==========================================\n# File: 3_testing.py\n# Function/Context: process\n# ==========================================\nimport numpy as np\nfrom pandas import read_csv\nimport tensorflow as tf\nimport argparse\nfrom models import GCNPolicy\n\n## ARGUMENTS OF THE SCRIPT\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--exp_env\",    default='2',\t\t\t\t\tchoices = ['1','2'])\nparser.add_argument(\"--data\", \t\thelp=\"number of testing data\", \tdefault=1000)\nparser.add_argument(\"--gpu\", \t\thelp=\"gpu index\", \t\t\t\tdefault=\"0\")\nparser.add_argument(\"--data_path\", \tdefault=None)\nparser.add_argument(\"--model_path\", default=None)\nargs = parser.parse_args()\n\n## FUNCTION OF TRAINING PER EPOCH\ndef process(model, dataloader, type = 'fea', n_Vars_small = 20):\n\n\tc, ei, ev, v, n_cs, n_vs, n_csm, n_vsm, cand_scores = dataloader\n\tbatched_states = (c, ei, ev, v, n_cs, n_vs, n_csm, n_vsm)  \n\tlogits = model(batched_states, tf.convert_to_tensor(False)) \n\t\n\treturn_err = None\n\t\n\tif type == \"fea\":\n\t\terrs_fp = np.sum((logits.numpy() > 0.5) & (cand_scores.numpy() < 0.5))\n\t\terrs_fn = np.sum((logits.numpy() < 0.5) & (cand_scores.numpy() > 0.5))\n\t\terrs = errs_fp + errs_fn\n\t\treturn_err = errs / cand_scores.shape[0]\n\t\n\telse:\n\t\tloss = tf.keras.metrics.mean_squared_error(cand_scores, logits)\n\t\treturn_err = tf.reduce_mean(loss).numpy()\n\n\treturn return_err\n\n## SET-UP MODEL\nmodel_path = args.model_path\nembSize = int(model_path[:-4].split('-')[-1][1:])\ntype = model_path[:-4].split('-')[-3]\n\n## SET-UP DATASET\ndatafolder = args.data_path\nn_Samples_test = int(args.data)\nn_Cons_small = 6 # Each MILP has 6 constraints\nn_Vars_small = 20 # Each MILP has 20 variables\nif \"data-env1-unfoldable\" in model_path:\n\tn_Eles_small = 60 # Each MILP has 60 nonzeros in matrix A\nelse:\n\tn_Eles_small = 12\n\n## LOAD DATASET INTO MEMORY\nif type == \"fea\":\n\tvarFeatures = read_csv(datafolder + \"/VarFeatures_all.csv\", header=None).values[:n_Vars_small * n_Samples_test,:]\n\tconFeatures = read_csv(datafolder + \"/ConFeatures_all.csv\", header=None).values[:n_Cons_small * n_Samples_test,:]\n\tedgFeatures = read_csv(datafolder + \"/EdgeFeatures_all.csv\", header=None).values[:n_Eles_small * n_Samples_test,:]\n\tedgIndices = read_csv(datafolder + \"/EdgeIndices_all.csv\", header=None).values[:n_Eles_small * n_Samples_test,:]\n\tlabels = read_csv(datafolder + \"/Labels_feas.csv\", header=None).values[:n_Samples_test,:]\nif type == \"obj\":\n\tvarFeatures = read_csv(datafolder + \"/VarFeatures_feas.csv\", header=None).values[:n_Vars_small * n_Samples_test,:]\n\tconFeatures = read_csv(datafolder + \"/ConFeatures_feas.csv\", header=None).values[:n_Cons_small * n_Samples_test,:]\n\tedgFeatures = read_csv(datafolder + \"/EdgeFeatures_feas.csv\", header=None).values[:n_Eles_small * n_Samples_test,:]\n\tedgIndices = read_csv(datafolder + \"/EdgeIndices_feas.csv\", header=None).values[:n_Eles_small * n_Samples_test,:]\n\tlabels = read_csv(datafolder + \"/Labels_obj.csv\", header=None).values[:n_Samples_test,:]\nif type == \"sol\":\n\tvarFeatures = read_csv(datafolder + \"/VarFeatures_feas.csv\", header=None).values[:n_Vars_small * n_Samples_test,:]\n\tconFeatures = read_csv(datafolder + \"/ConFeatures_feas.csv\", header=None).values[:n_Cons_small * n_Samples_test,:]\n\tedgFeatures = read_csv(datafolder + \"/EdgeFeatures_feas.csv\", header=None).values[:n_Eles_small * n_Samples_test,:]\n\tedgIndices = read_csv(datafolder + \"/EdgeIndices_feas.csv\", header=None).values[:n_Eles_small * n_Samples_test,:]\n\tlabels = read_csv(datafolder + \"/Labels_solu.csv\", header=None).values[:n_Vars_small * n_Samples_test,:]\n\nnConsF = conFeatures.shape[1]\nnVarF = varFeatures.shape[1]\nnEdgeF = edgFeatures.shape[1]\nn_Cons = conFeatures.shape[0]\nn_Vars = varFeatures.shape[0]\n\n## SET-UP TENSORFLOW\ngpu_index = int(args.gpu)\ntf.config.set_soft_device_placement(True)\ngpus = tf.config.list_physical_devices('GPU')\ntf.config.set_visible_devices(gpus[gpu_index], 'GPU')\ntf.config.experimental.set_memory_growth(gpus[gpu_index], True)\n\nwith tf.device(\"GPU:\"+str(gpu_index)):\n\n\t### LOAD DATASET INTO GPU ###\n\tvarFeatures = tf.constant(varFeatures, dtype=tf.float32)\n\tconFeatures = tf.constant(conFeatures, dtype=tf.float32)\n\tedgFeatures = tf.constant(edgFeatures, dtype=tf.float32)\n\tedgIndices = tf.constant(edgIndices, dtype=tf.int32)\n\tedgIndices = tf.transpose(edgIndices)\n\tlabels = tf.constant(labels, dtype=tf.float32)\n\tdata = (conFeatures, edgIndices, edgFeatures, varFeatures, n_Cons, n_Vars, n_Cons_small, n_Vars_small, labels)\n\n\t### LOAD MODEL ###\n\tif type == \"sol\":\n\t\tmodel = GCNPolicy(embSize, nConsF, nEdgeF, nVarF, isGraphLevel = False)\n\telse:\n\t\tmodel = GCNPolicy(embSize, nConsF, nEdgeF, nVarF)\n\tmodel.restore_state(\"./saved-models/\" + model_path)\n\n\t### TEST MODEL ###\n\terr = process(model, data, type = type, n_Vars_small = n_Vars_small)\n\tmodel.summary()\n\tprint(f\"MODEL: {model_path}, DATA-SET: {datafolder}, NUM-DATA: {n_Samples_test}, EXP: {args.exp_env}, ERR: {err}\")\n\n# ==========================================\n# File: models.py\n# Function/Context: GCNPolicy\n# ==========================================\nimport tensorflow as tf\nimport tensorflow.keras as K\nimport numpy as np\nimport pickle\n\n\nclass BipartiteGraphConvolution(K.Model):\n    \"\"\"\n    Partial bipartite graph convolution (either left-to-right or right-to-left).\n    \"\"\"\n\n    def __init__(self, emb_size, activation, initializer, right_to_left=False):\n        super().__init__()\n        \n        self.emb_size = emb_size\n        self.activation = activation\n        self.initializer = initializer\n        self.right_to_left = right_to_left\n\n        # feature layers\n        self.feature_module_left = K.Sequential([\n            K.layers.Dense(units=self.emb_size, activation=None, use_bias=True, kernel_initializer=self.initializer),\n            K.layers.Activation(self.activation),\n            K.layers.Dense(units=self.emb_size, activation=None, kernel_initializer=self.initializer),\n        ])\n        self.feature_module_edge = K.Sequential([\n        ])\n        self.feature_module_right = K.Sequential([\n            K.layers.Dense(units=self.emb_size, activation=None, use_bias=False, kernel_initializer=self.initializer),\n            K.layers.Activation(self.activation),\n            K.layers.Dense(units=self.emb_size, activation=None, kernel_initializer=self.initializer),\n        ])\n\n        # output_layers\n        self.output_module = K.Sequential([\n            K.layers.Dense(units=self.emb_size, activation=None, kernel_initializer=self.initializer),\n            K.layers.Activation(self.activation),\n            K.layers.Dense(units=self.emb_size, activation=None, kernel_initializer=self.initializer),\n        ])\n    \n    def build(self, input_shapes):\n        \n        l_shape, ei_shape, ev_shape, r_shape = input_shapes\n\n        self.feature_module_left.build(l_shape)\n        self.feature_module_edge.build(ev_shape)\n        self.feature_module_right.build(r_shape)\n        self.output_module.build([None, self.emb_size + (l_shape[1] if self.right_to_left else r_shape[1])])\n        self.built = True\n    \n    def call(self, inputs):\n    \n        left_features, edge_indices, edge_features, right_features, scatter_out_size = inputs\n\n        if self.right_to_left:\n            scatter_dim = 0\n            prev_features = self.feature_module_left(left_features)\n        else:\n            scatter_dim = 1\n            prev_features = self.feature_module_right(right_features)\n\n        # compute joint features\n        if scatter_dim == 0:\n            joint_features = self.feature_module_edge(edge_features) * tf.gather(\n                    self.feature_module_right(right_features),\n                    axis=0,\n                    indices=edge_indices[1]\n                )\n        else:\n            joint_features = self.feature_module_edge(edge_features) * tf.gather(\n                    self.feature_module_left(left_features),\n                    axis=0,\n                    indices=edge_indices[0]\n                )\n\n        # perform convolution\n        conv_output = tf.scatter_nd(\n            updates=joint_features,\n            indices=tf.expand_dims(edge_indices[scatter_dim], axis=1),\n            shape=[scatter_out_size, self.emb_size]\n        )\n\n        # mean convolution\n        neighbour_count = tf.scatter_nd(\n            updates=tf.ones(shape=[tf.shape(edge_indices)[1], 1], dtype=tf.float32),\n            indices=tf.expand_dims(edge_indices[scatter_dim], axis=1),\n            shape=[scatter_out_size, 1])\n        output = self.output_module(tf.concat([\n            conv_output,\n            prev_features,\n        ], axis=1))\n\n        return output\n\nclass GCNPolicy(K.Model):\n    \"\"\"\n    Our bipartite Graph Convolutional neural Network (GCN) model.\n    \"\"\"\n\n    def __init__(self, embSize, nConsF, nEdgeF, nVarF, isGraphLevel = True):\n        super().__init__()\n\n        self.emb_size = embSize\n        self.cons_nfeats = nConsF\n        self.edge_nfeats = nEdgeF\n        self.var_nfeats = nVarF\n        self.is_graph_level = isGraphLevel \n        # \"isGraphLevel == True\" means the output is graph-level, each graph has an output value;\n        # Otherwise, each variable has an output value.\n\n        self.activation = K.activations.relu\n        self.initializer = K.initializers.Orthogonal()\n\n        # CONSTRAINT EMBEDDING\n        self.cons_embedding = K.Sequential([\n            K.layers.Dense(units=self.emb_size, activation=self.activation, kernel_initializer=self.initializer),\n        ])\n\n        # EDGE EMBEDDING\n        self.edge_embedding = K.Sequential([\n        ])\n\n        # VARIABLE EMBEDDING\n        self.var_embedding = K.Sequential([\n            K.layers.Dense(units=self.emb_size, activation=self.activation, kernel_initializer=self.initializer),\n        ])\n\n        # GRAPH CONVOLUTIONS\n        self.conv_v_to_c = BipartiteGraphConvolution(self.emb_size, self.activation, self.initializer, right_to_left=True)\n        self.conv_c_to_v = BipartiteGraphConvolution(self.emb_size, self.activation, self.initializer)\n\n        # GRAPH CONVOLUTIONS\n        self.conv_v_to_c2 = BipartiteGraphConvolution(self.emb_size, self.activation, self.initializer, right_to_left=True)\n        self.conv_c_to_v2 = BipartiteGraphConvolution(self.emb_size, self.activation, self.initializer)\n\n        # OUTPUT\n        self.output_module = K.Sequential([\n            K.layers.Dense(units=self.emb_size, activation=self.activation, kernel_initializer=self.initializer),\n            K.layers.Dense(units=1, activation=None, kernel_initializer=self.initializer, use_bias=False),\n        ])\n        \n        # build model right-away\n        self.build([\n            (None, self.cons_nfeats),\n            (2, None),\n            (None, self.edge_nfeats),\n            (None, self.var_nfeats),\n            (None, ),\n            (None, ),\n        ])\n        \n        # save / restore fix\n        self.variables_topological_order = [v.name for v in self.variables]\n        \n        # save input signature for compilation\n        self.input_signature = [\n            (\n                tf.TensorSpec(shape=[None, self.cons_nfeats], dtype=tf.float32),\n                tf.TensorSpec(shape=[2, None], dtype=tf.int32),\n                tf.TensorSpec(shape=[None, self.edge_nfeats], dtype=tf.float32),\n                tf.TensorSpec(shape=[None, self.var_nfeats], dtype=tf.float32),\n                tf.TensorSpec(shape=[None], dtype=tf.int32),\n                tf.TensorSpec(shape=[None], dtype=tf.int32),\n            ),\n            tf.TensorSpec(shape=[], dtype=tf.bool),\n        ]\n        \n    \n    def build(self, input_shapes):\n        \n        c_shape, ei_shape, ev_shape, v_shape, nc_shape, nv_shape = input_shapes\n        emb_shape = [None, self.emb_size]\n\n        if not self.built:\n            self.cons_embedding.build(c_shape)\n            self.edge_embedding.build(ev_shape)\n            self.var_embedding.build(v_shape)\n            self.conv_v_to_c.build((emb_shape, ei_shape, emb_shape, emb_shape))\n            self.conv_c_to_v.build((emb_shape, ei_shape, emb_shape, emb_shape))\n            self.conv_v_to_c2.build((emb_shape, ei_shape, emb_shape, emb_shape))\n            self.conv_c_to_v2.build((emb_shape, ei_shape, emb_shape, emb_shape))\n            if self.is_graph_level:\n                self.output_module.build([None, 2 * self.emb_size])\n            else:\n                self.output_module.build(emb_shape)\n            self.built = True\n\n    def call(self, inputs, training):\n        \n        constraint_features, edge_indices, edge_features, variable_features, n_cons_total, n_vars_total, n_cons_small, n_vars_small = inputs\n\n        # EMBEDDINGS\n        constraint_features = self.cons_embedding(constraint_features)\n        edge_features = self.edge_embedding(edge_features)\n        variable_features = self.var_embedding(variable_features)\n\n        # GRAPH CONVOLUTIONS\n        constraint_features = self.conv_v_to_c((\n            constraint_features, edge_indices, edge_features, variable_features, n_cons_total))\n        constraint_features = self.activation(constraint_features)\n\n        variable_features = self.conv_c_to_v((\n            constraint_features, edge_indices, edge_features, variable_features, n_vars_total))\n        variable_features = self.activation(variable_features)\n\n        constraint_features = self.conv_v_to_c2((\n            constraint_features, edge_indices, edge_features, variable_features, n_cons_total))\n        constraint_features = self.activation(constraint_features)\n\n        variable_features = self.conv_c_to_v2((\n            constraint_features, edge_indices, edge_features, variable_features, n_vars_total))\n        variable_features = self.activation(variable_features)\n        \n        if self.is_graph_level:\n            variable_features = tf.reshape(variable_features, [int(n_vars_total / n_vars_small), n_vars_small, self.emb_size])\n            variable_features_mean = tf.reduce_mean(variable_features, axis = 1)\n            constraint_features = tf.reshape(constraint_features, [int(n_cons_total / n_cons_small), n_cons_small, self.emb_size])\n            constraint_features_mean = tf.reduce_mean(constraint_features, axis = 1)\n            final_features = tf.concat([variable_features_mean, constraint_features_mean], 1)\n        else:\n            final_features = variable_features\n\n        # OUTPUT\n        output = self.output_module(final_features)\n        return output\n        \n    def save_state(self, path):\n        with open(path, 'wb') as f:\n            for v_name in self.variables_topological_order:\n                v = [v for v in self.variables if v.name == v_name][0]\n                pickle.dump(v.numpy(), f)\n\n    def restore_state(self, path):\n        with open(path, 'rb') as f:\n            for v_name in self.variables_topological_order:\n                v = [v for v in self.variables if v.name == v_name][0]\n                v.assign(pickle.load(f))",
  "description": "Combined Analysis:\n- [1_generate_data.py]: This file directly implements the core MILP optimization model from the paper by generating both foldable and unfoldable MILP instances. The functions generateMILPunfoldable and generateMILPfoldable create MILP instances with the exact mathematical structure described: objective c^T x, constraints Ax ∘ b (with ∘ ∈ {≤, =, ≥}), bounds l ≤ x ≤ u, and integer constraints x_j ∈ ℤ for j ∈ I. The code uses PySCIPOpt to construct and solve these MILPs, then saves the graph representation (node/edge features) and labels (feasibility, objective value, solution) for GNN training/testing. This aligns with the paper's experimental methodology of generating data to test GNN representation capabilities.\n- [2_training.py]: This file implements the training pipeline for GNNs to predict MILP properties (feasibility, objective value, optimal solutions) as described in the paper. The core logic includes: 1) Loading MILP graph representations (variable/constraint features, edge indices/features, labels), 2) Training a GCNPolicy model with gradient descent using MSE loss, 3) Task-specific evaluation (binary classification error for feasibility, regression loss for objective/solution), 4) Training loop with restart mechanism for optimization stability. The code directly corresponds to the paper's Algorithm Steps of using GNNs to approximate mappings from MILP instances to their properties.\n- [3_testing.py]: This file implements the testing/evaluation phase of the GNN-based MILP prediction framework described in the paper. It loads pre-trained GNN models and evaluates their performance on predicting MILP properties (feasibility, optimal objective value, optimal solutions). The core logic includes: 1) Loading MILP instances represented as bipartite graphs (constraint/variable nodes, edge features for coefficients), 2) Loading trained GCNPolicy models, 3) Running forward passes through the GNN, 4) Computing evaluation metrics (classification error for feasibility, MSE for objective/solution predictions). This directly implements the experimental validation of the paper's theoretical findings about GNN representation power for MILPs.\n- [models.py]: This file implements the core GNN architecture described in the paper for representing MILPs. The GCNPolicy class constructs a bipartite graph neural network where constraints and variables form two node types, connected by edges representing constraint coefficients. The model performs alternating graph convolutions (variable-to-constraint and constraint-to-variable) to learn representations, then outputs either graph-level predictions (feasibility/objective value) or variable-level predictions (optimal solutions). This directly implements the paper's key algorithm of applying GNNs to MILP-induced bipartite graphs for property prediction.",
  "dependencies": [
    "numpy",
    "pyscipopt.scip",
    "pandas.read_csv",
    "BipartiteGraphConvolution",
    "os",
    "pickle",
    "models.GCNPolicy",
    "pandas",
    "argparse",
    "tensorflow.keras",
    "random",
    "tensorflow"
  ]
}