{
  "file_path": "ecord/environment/environment.py, ecord/models/ecodqn.py, ecord/models/gnn_embedder.py, ecord/models/rnn_decoder.py, ecord/solvers/actors/ecord.py",
  "function_name": "Environment, GNN_ECODQN, GNNEmbedder, _RNNDecoderBase, GRUDecoder, LSTMDecoder, Actor_Q.action_select, Actor_Q.get_Q_values, Actor_DoubleQ.action_select, Actor_DoubleQ.get_Q_values",
  "code_snippet": "\n\n# ==========================================\n# File: ecord/environment/environment.py\n# Function/Context: Environment\n# ==========================================\nfrom collections import namedtuple\n\nimport torch\nfrom torch_scatter import segment_csr\nfrom collections import deque\nimport time\n\nfrom dataclasses import dataclass\n\nfrom ecord.environment import Tradjectories, NodeObservation, GlobalObservation\n\n@dataclass\nclass EnvObservation:\n    node_features: torch.tensor  # [num_nodes, num_tradj, dim]\n    glob_features: torch.tensor # [num_graph, num_tradj, dim]\n    edge_index: torch.tensor\n    edge_attr: torch.tensor\n    batch_idx: torch.tensor  # [num_nodes]\n    batch_ptr: torch.tensor  # [num_graph+1]\n    degree: torch.tensor\n    num_tradj: torch.tensor\n\n    def to(self, device):\n        for attr_name in [\n            'node_features',  # [num_nodes, num_tradj, dim]\n            'glob_features',  # [num_graph, num_tradj, dim]\n            'edge_index',\n            'edge_attr',\n            'batch_idx',  # [num_nodes]\n            'batch_ptr',  # [num_graph+1]\n            'degree']:\n            attr = getattr(self, attr_name)\n            if attr is not None:\n                setattr(self, attr_name, attr.to(device))\n\nclass Environment:\n\n    def __init__(self,\n                 batch_nx,\n                 num_tradj,\n                 batch_tg,\n                 node_features=[\n                     NodeObservation.STATE,\n                     NodeObservation.PEEK_NORM,\n                     NodeObservation.STEPS_STATIC_CLIP,\n                 ],\n                 glob_features=[\n                     GlobalObservation.SCORE_FROM_BEST_NORM,\n                     GlobalObservation.PEEK_MAX_NORM,\n                 ],\n                 init_state=None,\n                 intermediate_reward=0,\n                 revisit_reward=0,\n                 add_edges_to_observations=False,\n                 device='cpu'):\n        self.device = device\n        self.tradj = Tradjectories(\n            batch_nx,\n            num_tradj,\n            batch_tg,\n            node_features,\n            glob_features,\n            init_state\n        )\n\n        self._batch_idx = batch_tg.batch.to(self.device)\n        self._batch_ptr = batch_tg.ptr.to(self.device)\n        self._batch_ptr_cpu = batch_tg.ptr.cpu()\n        self._num_tradj = num_tradj\n\n        if add_edges_to_observations:\n            # If we need the graph structure, store it here.\n            self._edge_index = batch_tg.edge_index.to(self.device)\n            self._edge_attr = batch_tg.edge_attr.to(self.device)\n            self._degree = batch_tg.degree.to(self.device)\n        else:\n            # If we don't need the graph structure, use None's.\n            self._edge_index = None\n            self._edge_attr = None\n            self._degree = None\n\n        self.scores = self.tradj.get_scores().clone()\n        self.best_scores = self.tradj.get_best_scores().clone()\n        self.num_nodes = torch.from_numpy(self.tradj._num_nodes_list)[:, None]\n\n        self._intermediate_reward = intermediate_reward\n        self._revisit_reward = revisit_reward\n\n        if self._intermediate_reward != 0:\n            self.state_last_min = None\n        if self._revisit_reward != 0:\n            self.state_history = None\n\n    def observe(self):\n        # node_features : [nodes, num_tradj, dim]\n        node_features = self.tradj.get_node_features(flat=True).to(self.device)\n        # glob_features : [graphs, num_tradj, dim]\n        glob_features = self.tradj.get_glob_features().to(self.device)\n\n        obs = EnvObservation(\n            node_features = node_features,\n            glob_features = glob_features,\n            edge_index = self._edge_index,\n            edge_attr = self._edge_attr,\n            batch_idx = self._batch_idx,\n            batch_ptr = self._batch_ptr,\n            degree = self._degree,\n            num_tradj = self._num_tradj\n        )\n\n        return obs\n\n    def _calc_reward(self, new_scores):\n        reward = (new_scores - self.best_scores).clamp(min=0) / self.num_nodes\n\n        if self._intermediate_reward != 0:\n            peeks = self.tradj.get_peeks(flat=True)\n            is_local_minima = (segment_csr((peeks > 0).int(), self._batch_ptr_cpu, reduce='sum') == 0)\n            is_new_local_minima = is_local_minima.clone()\n            state = self.tradj.get_states(flat=True)\n\n            if self.state_last_min is not None:\n                for g_idx, t_idx in zip(*torch.where(is_local_minima)):\n                    s = state[self._batch_ptr[g_idx]:self._batch_ptr[g_idx + 1], t_idx]\n                    if self.state_last_min is not None:\n                        s_last = self.state_last_min[self._batch_ptr[g_idx]:self._batch_ptr[g_idx + 1], t_idx]\n                        if (s == s_last).all():\n                            is_new_local_minima[g_idx, t_idx] = False\n                        else:\n                            self.state_last_min[self._batch_ptr[g_idx]:self._batch_ptr[g_idx + 1], t_idx] = s\n\n            else:\n                self.state_last_min = -1*torch.ones_like(state)\n\n            reward[is_new_local_minima & (reward > 0)] += self._intermediate_reward\n\n        if self._revisit_reward != 0:\n            is_revisited_local_minima = is_local_minima & ~is_new_local_minima\n            reward[is_revisited_local_minima] += self._revisit_reward\n\n        return reward\n\n    def step(self, actions, ret_reward=True):\n        self.tradj.step(actions.detach().cpu(), apply_batch_offset=True)\n\n        new_scores = self.tradj.get_scores().clone()\n        if ret_reward:\n            reward = self._calc_reward(new_scores)\n        else:\n            reward = None\n\n        self.scores = new_scores\n        self.best_scores = self.tradj.get_best_scores().clone()\n\n        return self.observe(), reward\n\n# ==========================================\n# File: ecord/models/ecodqn.py\n# Function/Context: GNN_ECODQN\n# ==========================================\nimport torch\nfrom torch import nn\nfrom torch_scatter import scatter\n\nfrom ecord.models._base import _DevicedModule\n\n\nclass ECODQN_layer(nn.Module):\n\n    def __init__(self, dim_embedding):\n        super().__init__()\n\n        # eq (6)\n        self.message = nn.Sequential(\n            nn.Linear(2 * dim_embedding, dim_embedding),\n            nn.ReLU()\n        )\n\n        # eq (7)\n        self.update = nn.Sequential(\n            nn.Linear(2 * dim_embedding, dim_embedding),\n            nn.ReLU()\n        )\n\n    def forward(self, x, edge_index, edge_attr, x_agg_emb, *args, **kwargs):\n        col, row = edge_index\n\n        x_agg = scatter(\n            edge_attr.unsqueeze(-1) * x[col],\n            row,\n            reduce='mean',\n            dim=0,\n            dim_size=len(x)\n        )\n        m = self.message(\n            torch.cat([x_agg, x_agg_emb], dim=-1)\n        )\n        x = self.update(torch.cat([x, m], dim=-1))\n        return x\n\n\nclass GNN_ECODQN(nn.Module, _DevicedModule):\n\n    def __init__(\n            self,\n            dim_in,\n            dim_embedding,\n            num_layers,\n            device=None,\n            out_device='cpu'\n    ):\n        super().__init__()\n\n        # eq (4)\n        self.embed_node = nn.Sequential(\n            nn.Linear(dim_in, dim_embedding),\n            nn.ReLU()\n        )\n\n        # eq (5) inner\n        self.embed_node_and_edge = nn.Sequential(\n            nn.Linear(dim_in + 1, dim_embedding - 1),\n            nn.ReLU()\n        )\n\n        # eq (5) outer\n        self.embed_agg_nodes_and_degree = nn.Sequential(\n            nn.Linear(dim_embedding, dim_embedding),\n            nn.ReLU()\n        )\n\n        # eq (6-7)\n        self.layers = nn.ModuleList([\n            ECODQN_layer(dim_embedding) for _ in range(num_layers)\n        ])\n\n        # eq (8) inner\n        self.agg_nodes = nn.Sequential(\n            nn.Linear(dim_embedding, dim_embedding),\n            # nn.ReLU()\n        )\n\n        # eq (8) outer\n        self.readout = nn.Sequential(\n            nn.ReLU(),\n            nn.Linear(2*dim_embedding, 1),\n        )\n\n\n\n        self.set_devices(device, out_device)\n\n    def _expand_as_over_tradj(self, trg, src):\n        assert src.dim() == 3, \"Source should have shape [nodes, tradj, -1]\"\n        _, t, _ = src.size()\n        while trg.dim() < src.dim():\n            trg = trg.unsqueeze(-1)\n        trg = trg.repeat(1,t,1)\n        return trg\n\n    def forward(self, x, edge_index, edge_attr, batch, degree, *args, **kwargs):\n\n        x, edge_index, edge_attr, batch, degree = [\n            t.to(self.device) for t in [x, edge_index, edge_attr, batch, degree]\n        ]\n        col, row = edge_index\n\n\n        x_emb = self.embed_node(x)\n\n        node_edge = scatter(\n            torch.cat([x[col], self._expand_as_over_tradj(edge_attr, x)], dim=-1),\n            row,\n            reduce='mean',\n            dim=0,\n            dim_size = len(batch)\n        )\n        node_edge = self.embed_node_and_edge(node_edge)\n\n        degree_norm = scatter(degree, batch, reduce='max', dim=0)[batch]\n        degree_norm = self._expand_as_over_tradj(degree_norm, node_edge)\n        x_agg_emb = self.embed_agg_nodes_and_degree(\n            torch.cat([node_edge, degree_norm], dim=-1)\n        )\n\n        for layer in self.layers:\n            x_emb = layer(x_emb, edge_index, edge_attr, x_agg_emb)\n\n        g_agg = scatter(\n            x_emb, batch, reduce='mean', dim=0,\n        )\n        g_agg = self.agg_nodes(g_agg)\n\n        inp = torch.cat([g_agg[batch], x_emb], dim=-1)\n        q_vals = self.readout(inp)\n\n        q_vals = q_vals.to(self.out_device)\n\n        # [nodes, tradj, 1] --> [nodes, tradj]\n        return q_vals.squeeze(-1)\n\n# ==========================================\n# File: ecord/models/gnn_embedder.py\n# Function/Context: GNNEmbedder\n# ==========================================\nimport math\nimport time\n\nimport torch\nimport torch_geometric.nn as gnn\nfrom torch import Tensor\nfrom torch import nn\nfrom torch.nn import Parameter as Param\nfrom torch_geometric.nn.conv import MessagePassing\nfrom torch_geometric.typing import Adj, OptTensor\nfrom torch_scatter import scatter_mean\nfrom torch_sparse import SparseTensor, matmul\n\nfrom ecord.models._base import _DevicedModule\n\n\ndef uniform(size, tensor):\n    if tensor is not None:\n        bound = 1.0 / math.sqrt(size)\n        tensor.data.uniform_(-bound, bound)\n\n\nclass GatedGraphConv(MessagePassing):\n    r\"\"\"The gated graph convolution operator from the `\"Gated Graph Sequence\n    Neural Networks\" <https://arxiv.org/abs/1511.05493>`_ paper\n\n    .. math::\n        \\mathbf{h}_i^{(0)} &= \\mathbf{x}_i \\, \\Vert \\, \\mathbf{0}\n\n        \\mathbf{m}_i^{(l+1)} &= \\sum_{j \\in \\mathcal{N}(i)} e_{j,i} \\cdot\n        \\mathbf{\\Theta} \\cdot \\mathbf{h}_j^{(l)}\n\n        \\mathbf{h}_i^{(l+1)} &= \\textrm{GRU} (\\mathbf{m}_i^{(l+1)},\n        \\mathbf{h}_i^{(l)})\n\n    up to representation :math:`\\mathbf{h}_i^{(L)}`.\n    The number of input channels of :math:`\\mathbf{x}_i` needs to be less or\n    equal than :obj:`out_channels`.\n    :math:`e_{j,i}` denotes the edge weight from source node :obj:`j` to target\n    node :obj:`i` (default: :obj:`1`)\n\n    Args:\n        out_channels (int): Size of each input sample.\n        num_layers (int): The sequence length :math:`L`.\n        aggr (string, optional): The aggregation scheme to use\n            (:obj:`\"add\"`, :obj:`\"mean\"`, :obj:`\"max\"`).\n            (default: :obj:`\"add\"`)\n        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n            an additive bias. (default: :obj:`True`)\n        **kwargs (optional): Additional arguments of\n            :class:`torch_geometric.nn.conv.MessagePassing`.\n    \"\"\"\n    def __init__(self, out_channels: int, num_layers: int, aggr: str = 'add',\n                 bias: bool = True, **kwargs):\n        super().__init__(aggr=aggr, **kwargs)\n\n        self.out_channels = out_channels\n        self.num_layers = num_layers\n\n        self.weight = Param(Tensor(num_layers, out_channels, out_channels))\n        self.rnn = torch.nn.GRUCell(out_channels, out_channels, bias=bias)\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        uniform(self.out_channels, self.weight)\n        self.rnn.reset_parameters()\n\n    def _prepare_input(self, x):\n        if x.size(-1) > self.out_channels:\n            raise ValueError('The number of input channels is not allowed to '\n                             'be larger than the number of output channels')\n\n        if x.size(-1) < self.out_channels:\n            zero = x.new_zeros(x.size(0), self.out_channels - x.size(-1))\n            x = torch.cat([x, zero], dim=1)\n\n        return x\n\n    def forward(self, x: Tensor, edge_index: Adj,\n                edge_weight: OptTensor = None, batch=None) -> Tensor:\n        \"\"\"\"\"\"\n        x = self._prepare_input(x)\n\n        for i in range(self.num_layers):\n            m = torch.matmul(x, self.weight[i])\n            # propagate_type: (x: Tensor, edge_weight: OptTensor)\n            m = self.propagate(edge_index, x=m, edge_weight=edge_weight,\n                               size=None)\n            x = self.rnn(m, x)\n\n        return x\n\n    def message(self, x_j: Tensor, edge_weight: OptTensor):\n        return x_j if edge_weight is None else edge_weight.view(-1, 1) * x_j\n\n    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n        return matmul(adj_t, x, reduce=self.aggr)\n\n    def __repr__(self):\n        return '{}({}, num_layers={})'.format(self.__class__.__name__,\n                                              self.out_channels,\n                                              self.num_layers)\n\n\nclass NodeModel(nn.Module):\n    def __init__(self, out_channels: int, **kwargs):\n        super().__init__()\n\n        self.out_channels = out_channels\n\n        self.msg_snd = torch.nn.Sequential(\n            nn.Linear(out_channels, out_channels),\n            nn.LeakyReLU(),\n        )\n        self.msg_rec = torch.nn.Sequential(\n            nn.Linear(2 * out_channels, 2 * out_channels),\n            nn.LeakyReLU(),\n        )\n\n        self.rnn = torch.nn.GRUCell(out_channels*2, out_channels, bias=True)\n\n    def _prepare_input(self, x):\n        if x.size(-1) > self.out_channels:\n            raise ValueError('The number of input channels is not allowed to '\n                             'be larger than the number of output channels')\n\n        if x.size(-1) < self.out_channels:\n            zero = x.new_zeros(x.size(0), self.out_channels - x.size(-1))\n            x = torch.cat([x, zero], dim=1)\n\n        return x\n\n    def forward(self, x, edge_index, edge_attr, batch):\n        \"\"\"\"\"\"\n        x = self._prepare_input(x)\n\n        row, col = edge_index  # row/col <--> send/recieve\n\n        msg = scatter_mean(\n            self.msg_snd(x)[row] *  edge_attr.view(-1, 1),\n            col, dim=0, dim_size=x.size(0)\n        )\n        x = self.rnn(\n            self.msg_rec(\n                torch.cat([msg, x], dim=-1)\n            ),\n            x)\n\n        return x\n\n\nclass GNNEmbedder(nn.Module, _DevicedModule):\n\n    def __init__(\n            self,\n            dim_in,\n            dim_embedding,\n            num_layers,\n            use_layer_norm,\n            return_dummy_graph_embeddings = False,\n            device=None,\n            out_device='cpu'\n    ):\n        super().__init__()\n\n        self.embed = nn.Linear(dim_in, dim_embedding)\n        self.return_dummy_graph_embeddings = return_dummy_graph_embeddings\n\n        self.num_layers = num_layers\n        self.node_model = NodeModel(\n                out_channels=dim_embedding,\n                num_layers=1,\n                aggr=\"mean\",\n            )\n\n        self.use_layer_norm = use_layer_norm\n        if self.use_layer_norm:\n            self.norm = gnn.LayerNorm(dim_embedding, affine=False)\n\n        self.set_devices(device, out_device)\n\n    def forward(self, x, edge_index, edge_attr, batch, *args, **kwargs):\n\n        x, edge_index, edge_attr, batch = (x.to(self.device),\n                                   edge_index.to(self.device),\n                                   edge_attr.to(self.device),\n                                   batch.to(self.device))\n\n        x_in = x.clone()\n\n\n        x = self.embed(x)\n        if self.use_layer_norm:\n            x = self.norm(x)\n\n        for _ in range(self.num_layers):\n            x = self.node_model(x, edge_index, edge_attr, batch)\n            if self.use_layer_norm:\n                x = self.norm(x)\n\n        res = []\n        for _ in range(10):\n\n            x = x_in.clone()\n\n            t = time.time()\n\n            x = self.embed(x_in)\n            if self.use_layer_norm:\n                x = self.norm(x)\n\n            for _ in range(self.num_layers):\n                x = self.node_model(x, edge_index, edge_attr, batch)\n                if self.use_layer_norm:\n                    x = self.norm(x)\n\n            res.append(x[0])\n\n        x = x.to(self.out_device)\n\n        if self.return_dummy_graph_embeddings:\n            x = torch.zeros_like(x)\n\n        return x\n\n# ==========================================\n# File: ecord/models/rnn_decoder.py\n# Function/Context: _RNNDecoderBase, GRUDecoder, LSTMDecoder\n# ==========================================\nimport math\nfrom abc import ABC, abstractmethod\nfrom enum import Enum\n\nimport torch\nimport torch_geometric.nn as gnn\nfrom torch import nn\nfrom torch_scatter import scatter_mean\n\nfrom ecord.models._base import _DevicedModule\n\n\nclass RNNOutput(Enum):\n    DOT = 0\n    MLP_Q_VALUE = 1\n    MLP_ACTION_STATE = 2\n    MLP_ADVANTAGE_STATE = 3\n    MLP_V_VALUE = 4\n\nclass _RNNDecoderBase(nn.Module, _DevicedModule, ABC):\n\n    def __init__(self,\n                 dim_node_in,\n                 dim_msg_in,\n                 dim_node_embedding,\n                 dim_internal_embedding,\n                 num_layers=2,\n                 learn_graph_embedding=False,\n                 learn_scale=False,\n                 project_rnn_to_gnn=False,\n                 output_type = RNNOutput.DOT,\n                 output_activation = torch.tanh,\n                 return_dummy_hidden_state = False, # For ablations only.\n                 device=None,\n                 out_device=\"cpu\"):\n\n        super().__init__()\n\n        self.dim_node_in = dim_node_in\n        self.dim_msg_in = dim_msg_in\n        self.dim_msg_out = 64\n        self.dim_node_embedding = dim_node_embedding\n        self.dim_internal_embedding = dim_internal_embedding\n        self.learn_graph_embedding = learn_graph_embedding\n        self.learn_scale = learn_scale\n        self.num_layers = num_layers\n        self.project_rnn_to_gnn = project_rnn_to_gnn\n        self.return_dummy_hidden_state = return_dummy_hidden_state\n\n        if self.learn_graph_embedding:\n            d_pool_in = self.dim_node_in\n            gate_nn = nn.Sequential(\n                nn.Linear(d_pool_in, 1)\n            )\n            feat_nn = nn.Sequential(\n                nn.Linear(d_pool_in, 2 * d_pool_in),\n                nn.LeakyReLU(),\n                nn.Linear(2 * d_pool_in, dim_internal_embedding)\n            )\n            self.graph_pool = gnn.GlobalAttention(gate_nn, feat_nn)\n\n            dim_graph = 2 * dim_internal_embedding\n\n        else:\n            dim_graph = dim_internal_embedding\n\n        self.output_type = output_type\n\n        if self.output_type is RNNOutput.DOT:\n            self._dim_dot = 2 * self.dim_node_embedding\n            self.lin_node = nn.Sequential(\n                nn.Linear(self.dim_node_embedding, self._dim_dot, bias=False),\n            )\n            self.lin_graph = nn.Sequential(\n                nn.Linear(dim_graph, self._dim_dot, bias=False),\n            )\n\n        else:\n            # self.output type in [RNNOutput.MLP_V_VALUE, RNNOutput.MLP_Q_VALUE,\n            # RNNOutput.MLP_ACTION_STATE, RNNOutput.MLP_ADVANTAGE_STATE]\n            d = self.dim_node_embedding\n            if self.project_rnn_to_gnn:\n                d_state = self.dim_node_embedding\n                self.proj_rnn_to_gnn = nn.Sequential(\n                    nn.Tanh(),\n                    nn.Linear(self.dim_internal_embedding, self.dim_internal_embedding//2),\n                    nn.LayerNorm(self.dim_internal_embedding//2), nn.LeakyReLU(),\n                    nn.Linear(self.dim_internal_embedding//2, self.dim_node_embedding),\n                    nn.LayerNorm(self.dim_node_embedding), nn.LeakyReLU(),\n                )\n            else:\n                # d = self.dim_internal_embedding + self.dim_node_embedding\n                d_state = self.dim_internal_embedding\n                self.proj_rnn_to_gnn = nn.LayerNorm(self.dim_internal_embedding)\n            d += d_state\n\n            if self.learn_graph_embedding:\n                # [hidden_state, dim_node, graph_embedding]\n                d += self.dim_internal_embedding\n            if self.output_type is RNNOutput.MLP_V_VALUE:\n                self.val_out = nn.Sequential(\n                    nn.Linear(self.dim_internal_embedding, self.dim_internal_embedding),\n                    nn.LayerNorm(self.dim_internal_embedding // 2), nn.LeakyReLU(),\n                    nn.Linear(self.dim_internal_embedding, 1),\n                )\n\n            elif self.output_type in [RNNOutput.MLP_ACTION_STATE, RNNOutput.MLP_ADVANTAGE_STATE]:\n                self.val_out = nn.Sequential(\n                    nn.Tanh(),\n                    nn.Linear(d_state, d_state),\n                    nn.LeakyReLU(),\n                    nn.Linear(d_state, 1)\n                )\n\n            self.mlp_out = nn.Sequential(\n                nn.Linear(d, d),\n                nn.LayerNorm(d), nn.LeakyReLU(),\n                nn.Linear(d, 1)\n            )\n\n        self.msg_in = nn.Sequential(\n            nn.Linear(self.dim_msg_in, self.dim_msg_out),\n            nn.LeakyReLU(),\n        )\n\n        self.graph_rnn = self._init_graph_rnn(self.num_layers)\n\n        self.output_activation = output_activation\n\n        if self.learn_scale:\n            # self.beta = torch.nn.Parameter(data=torch.FloatTensor([1]), requires_grad=True)\n            self.scale_net = nn.Sequential(\n                nn.Linear(self.dim_internal_embedding, 1)\n            )\n\n        self.set_devices(device, out_device)\n\n    @abstractmethod\n    def _init_graph_rnn(self):\n        raise NotImplementedError\n\n    @abstractmethod\n    def init_embeddings(self, node_features, batch, num_tradj):\n        '''\n        node_features : [graph * num_nodes_max, dim_embedding]\n        '''\n        raise NotImplementedError\n\n    @abstractmethod\n    def update_embeddings(self, update_embeddings):\n        '''\n        update_embeddings : [num_graphs, num_tradj, dim_msg_in]\n        '''\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_internal_state(self):\n        raise NotImplementedError\n\n    @abstractmethod\n    def set_internal_state(self):\n        raise NotImplementedError\n\n    def _get_current_embeddings(self, *args, **kwargs):\n        '''\n        Returns graph_embeddings of shape:\n            graph_embeddings : [num_graphs, num_tradj, dim]\n        '''\n        if self.hidden_embeddings is None:\n            raise Exception(\"Embeddings must be set by init_embeddings(...) before forward call.\")\n        else:\n            if type(self.hidden_embeddings) is list:\n                embeddings = self.hidden_embeddings[-1]\n            else:\n                embeddings = self.hidden_embeddings\n\n        if self.learn_graph_embedding:\n            embeddings = torch.cat([embeddings, self.graph_embeddings], dim=-1)\n\n        if self.return_dummy_hidden_state:\n            # Return zeros, essentially ablating the RNN.\n            embeddings = torch.zeros_like(embeddings)\n\n        return embeddings\n\n    def _get_scale(self):\n        if not self.learn_scale:\n            return 1\n        else:\n            # return self.beta\n            # self._get_current_embeddings() --> [num_graphs, num_tradj, dim]\n            # self.scale_net(...) --> [num_graphs, num_tradj, 1]\n            return self.scale_net(self._get_current_embeddings())\n\n    def forward(self, embeddings, batch=None, out_device=None):\n        '''\n        Takes embeddings with shape:\n            embeddings : [num_graphs, num_nodes_max, num_tradj, dim_embeddings] if batch is None\n            embeddings : [num_nodes_in_batch, num_tradj, dim_embeddings] otherwise\n\n        and returns attention weights to every node:\n            out :  [num_graphs, num_tradj, num_nodes_max]  if batch is None\n            out:   [num_nodes_in_batch, num_tradj] otherwise\n\n        It is assumed that masking of this attention is handled outside of the\n        decoder class.\n        '''\n        if batch is not None:\n            batch = batch.to(self.device)\n\n        if self.output_type is RNNOutput.DOT:\n            out = self._forward_dot(embeddings, batch)\n        elif self.output_type is RNNOutput.MLP_V_VALUE:\n            out = self._forward_value_with_agg(embeddings, batch)\n        elif self.output_type is RNNOutput.MLP_Q_VALUE:\n            out = self._forward_value(embeddings, batch)\n        else:\n            out = self._forward_action_value(embeddings, batch)\n        if self.output_activation is not None:\n            out = self.output_activation(out)\n\n        if out.dim() > 2 and out.size(-1)==1:\n            # [num nodes, num_tradj, 1] --> [num nodes, num_tradj]\n            out.squeeze_(-1)\n\n        if self.learn_scale:\n            # scale: [num_nodes_in_batch, num_tradj]\n            scale = self._get_scale()\n            if out.dim() > 2:\n                # scale: [num_nodes_in_batch, num_tradj, 1]\n                scale.unsqueeze_(-1)\n            out = scale * out\n\n        if out_device is None:\n            out_device = self.out_device\n        return out.to(out_device)\n\n    def _forward_dot(self, embeddings, batch=None):\n        internal_embeddings = self._get_current_embeddings()\n        if batch is None:\n            n = embeddings.size(1)\n            attn = torch.einsum('gtni, gtni -> gtn',\n                                self.lin_graph(internal_embeddings).unsqueeze(2).repeat(1, 1, n, 1),\n                                self.lin_node(embeddings.permute(0, 2, 1, 3))\n                                )\n        else:\n            g = self.lin_graph(internal_embeddings)\n            n = self.lin_node(embeddings)\n            # attn : [num_nodes, num_tradj]\n            attn = (n * g[batch]).sum(-1)\n\n        return attn / math.sqrt(self._dim_dot)\n\n    def _forward_value_with_agg(self, embeddings, batch=None):\n        if batch is None:\n            raise NotImplementedError()\n\n        internal_embeddings = self._get_current_embeddings()\n\n        val = self.val_out(internal_embeddings)\n\n        inp = torch.cat([\n            internal_embeddings[batch],\n            embeddings\n        ], dim=-1)\n\n        action_val = self.mlp_out(inp)\n\n        out = action_val + val[batch]\n\n        return out\n\n    def _forward_value(self, embeddings, batch=None):\n        internal_embeddings = self._get_current_embeddings()\n\n        inp = torch.cat([\n            self.proj_rnn_to_gnn(internal_embeddings)[batch],\n            embeddings\n        ], dim=-1)\n\n        out = self.mlp_out(inp)\n\n        return out\n\n    def _forward_action_value(self, embeddings, batch=None):\n        if batch is None:\n            raise NotImplementedError()\n\n        internal_embeddings = self.proj_rnn_to_gnn(self._get_current_embeddings())\n\n        inp = torch.cat([\n            internal_embeddings[batch],\n            embeddings\n        ], dim=-1)\n\n        action_val = self.mlp_out(inp)\n\n        if self.output_type in [RNNOutput.MLP_ACTION_STATE, RNNOutput.MLP_ADVANTAGE_STATE]:\n            # Calculate state value from graph embeddings.\n            state_val = self.val_out(internal_embeddings)\n            if self.output_type is RNNOutput.MLP_ADVANTAGE_STATE:\n                # Convert action value to advantage value.\n                action_val = action_val - scatter_mean(action_val, batch, dim=0)[batch]\n            out = action_val + state_val[batch]\n        else:\n            # Use action values as output.\n            out = action_val\n\n            out.squeeze_(-1)\n\n        return out\n\n# ==========================================\n# File: ecord/solvers/actors/ecord.py\n# Function/Context: Actor_Q.action_select, Actor_Q.get_Q_values, Actor_DoubleQ.action_select, Actor_DoubleQ.get_Q_values\n# ==========================================\nimport itertools\nimport time\nfrom copy import deepcopy\nfrom dataclasses import asdict\n\nimport torch\nfrom torch_scatter import scatter_max, scatter_min\n\nfrom ecord.solvers.actors._base import ActorState, _ActorBase\n\n\ndef scatter_softmax_sample(x, batch, dim=-1):\n    '''\n    https://stackoverflow.com/questions/4463561/weighted-random-selection-from-array\n    '''\n    x_corr = x - scatter_max(x, batch, dim=0)[0][batch]\n    # x_corr = x - segment_coo(x, batch, reduce='max')[batch]\n    z = -(torch.rand_like(x)).log() / x_corr.exp()\n    return scatter_min(z, batch, dim=dim)[1]\n\ndef maybe_cuda_synchronise():\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n\nclass Actor_Q(_ActorBase):\n    mask_val = -1e3\n\n    def __init__(\n            self,\n            gnn,\n            rnn,\n            gnn2rnn=None,\n            node_encoder=None,\n            node_classifier=None,\n            add_glob_to_nodes=False,\n            add_glob_to_obs=False,\n            default_actor_idx=None,\n            detach_gnn_from_rnn=False,\n            device=None,\n    ):\n        self.gnn_target = deepcopy(gnn)\n        self.rnn_target = deepcopy(rnn)\n        if node_encoder is not None:\n            self.node_encoder_target = deepcopy(node_encoder)\n        else:\n            self.node_encoder_target = node_encoder\n\n        assert (default_actor_idx in [None, 0]), \"Invalid default_actor_idx.\"\n\n        super().__init__(\n            gnn,\n            rnn,\n            gnn2rnn=gnn2rnn,\n            node_encoder=node_encoder,\n            node_classifier=node_classifier,\n            add_glob_to_nodes=add_glob_to_nodes,\n            add_glob_to_obs=add_glob_to_obs,\n            detach_gnn_from_rnn=detach_gnn_from_rnn,\n            device=device\n        )\n\n    def update_target_network(self, polyak=1):\n        self.gnn_target = self.__update_network_params(self.gnn, self.gnn_target, polyak)\n        self.rnn_target = self.__update_network_params(self.rnn, self.rnn_target, polyak)\n        if self.node_encoder_target is not None:\n            self.node_encoder_target = self.__update_network_params(self.node_encoder, self.node_encoder_target, polyak)\n\n    def __update_network_params(self, src, trg, polyak=1):\n        if polyak == 1:\n            trg.load_state_dict(src.state_dict())\n            for param in trg.parameters():\n                param.requires_grad = False\n        else:\n            for target_param, param in zip(trg.parameters(), src.parameters()):\n                target_param.data.copy_((1 - polyak) * target_param.data + polyak * param.data)\n        return trg\n\n    def get_Q_values(self, obs, use_target_network=False, actor_idx=None, mask=None):\n        '''Calculate Q-values based on an observation of the environment\n        and the current internal state of the actor.\n\n        Given an observation, this requires:\n            (i) Update the RNN internal state according to the node-embeddings\n                of the last actions taken (at t-1) and the newly observe state of\n                the graph (at t).\n            (ii) Generate new node embeddings from the GNN embeddings (t=0) and newly\n                 observed node features (at t).\n            (iii) Calculate Q-values from RNN per-graph state and the per-node embeddings.\n        '''\n        if use_target_network:\n            rnn = self.rnn_target\n        else:\n            rnn = self.rnn\n\n        assert (actor_idx in [None, 0]), \"Invalid default_actor_idx.\"\n\n        self._update_internal_state(obs, use_target_network)  # (i)\n\n        embeddings = self._get_node_embeddings(obs)  # (ii)\n\n        preds = rnn(embeddings, obs.batch_idx, out_device=self.device)  # (iii)\n\n        if mask is not None:\n            preds.masked_fill_(mask, self.mask_val)\n\n        return preds\n\n    def action_select(\n            self,\n            obs,\n            epsilon=0,\n            tau=0,\n            use_target_network=False,\n            cache_selected_embeddings=True,\n            return_q_values=False,\n            return_timings=False,\n            actor_idx=None,\n            mask=None\n    ):\n        '''Select the next action.\n\n        First calculates the Q-values, then selects with epsilon-greedy policy.\n\n        Additionally, stores the embeddings of the selected actions internally,\n        for use at the next time-step.\n        '''\n        assert (actor_idx in [None, 0]), \"Invalid default_actor_idx.\"\n        times = {}\n\n        obs.to(self.device)\n\n        if return_timings:\n            t = time.time()\n            maybe_cuda_synchronise()\n        q_vals = self.get_Q_values(obs, use_target_network, mask)\n        if return_timings:\n            maybe_cuda_synchronise()\n            times['calc_q_vals'] = time.time() - t\n\n        if tau==0:\n            if return_timings:\n                t = time.time()\n                maybe_cuda_synchronise()\n            action_vals, actions = scatter_max(q_vals, obs.batch_idx, dim=0)\n            if return_timings:\n                maybe_cuda_synchronise()\n                times['action_select'] = time.time() - t\n        else:\n            actions = scatter_softmax_sample(q_vals.detach() / tau, obs.batch_idx, dim=0)\n\n        # take away batch offset.\n        actions -= obs.batch_ptr[:-1][:, None].to(actions.device)\n        # times['action_select'] = time.time() - t\n        actions = actions.to(self.device)\n\n        if epsilon > 0:\n            act_randomly = (torch.rand(*actions.shape) < epsilon)\n            if act_randomly.any():\n                if mask is None:\n                    num_nodes = (obs.batch_ptr[1:] - obs.batch_ptr[:-1]).to(self.device)\n                    rand_acts = (torch.rand(actions.shape, device=self.device) * num_nodes[:, None]).long()\n                    actions[act_randomly] = rand_acts[act_randomly].to(actions)\n                else:\n                    # compute random choice from allowed actions as per-mask\n                    raise NotImplementedError()\n\n        if cache_selected_embeddings:\n            self.get_and_cache_action_embeddings(obs, actions, stop_grad=True)\n\n        ret = [actions]\n        if return_q_values:\n            ret += [q_vals]\n        if return_timings:\n            ret += [times]\n        if len(ret)==0:\n            ret = ret[0]\n\n        return ret\n\nclass Actor_DoubleQ(_ActorBase):\n    mask_val = -1e3\n\n    def __init__(\n            self,\n            gnn,\n            rnn,\n            node_encoder=None,\n            node_classifier=None,\n            add_glob_to_nodes=False,\n            add_glob_to_obs=False,\n            default_actor_idx=None,\n            detach_gnn_from_rnn=False,\n            device=None\n    ):\n        self._actor1 = Actor_Q(\n            gnn = gnn,\n            rnn = rnn,\n            node_encoder = node_encoder,\n            node_classifier = node_classifier,\n            add_glob_to_nodes = add_glob_to_nodes,\n            add_glob_to_obs = add_glob_to_obs,\n            detach_gnn_from_rnn = detach_gnn_from_rnn,\n            device = device,\n        )\n\n        def reset(m):\n            try:\n                m.reset_parameters()\n            except:\n                pass\n\n        gnn2 = deepcopy(gnn).apply(reset)\n        rnn2 = deepcopy(rnn).apply(reset)\n        if node_encoder is not None:\n            node_encoder2 = deepcopy(node_encoder).apply(reset)\n        else:\n            node_encoder2 = node_encoder\n\n        self._actor2 = Actor_Q(\n            gnn = gnn2,\n            rnn = rnn2,\n            node_encoder = node_encoder2,\n            add_glob_to_nodes = add_glob_to_nodes,\n            add_glob_to_obs = add_glob_to_obs,\n            detach_gnn_from_rnn = detach_gnn_from_rnn,\n            device = device,\n        )\n\n        self.default_actor_idx = default_actor_idx\n\n        self.set_device(device)\n\n    def parameters(self):\n        return itertools.chain(self._actor1.parameters(), self._actor2.parameters())\n\n    def update_target_network(self, polyak=1):\n        self._actor1.update_target_network(polyak)\n        self._actor2.update_target_network(polyak)\n\n    def get_Q_values(self, obs, use_target_network=False, mask=None, actor_idx=-1):\n        '''Calculate Q-values based on an observation of the environment\n        and the current internal state of the actor.\n\n        Given an observation, this requires:\n            (i) Update the RNN internal state according to the node-embeddings\n                of the last actions taken (at t-1) and the newly observe state of\n                the graph (at t).\n            (ii) Generate new node embeddings from the GNN embeddings (t=0) and newly\n                 observed node features (at t).\n            (iii) Calculate Q-values from RNN per-graph state and the per-node embeddings.\n        '''\n        if actor_idx == -1:\n            actor_idx = self.default_actor_idx\n        actors = [self._actor1, self._actor2]\n        if actor_idx is None:\n            preds = [\n                actor.get_Q_values(\n                    obs=obs,\n                    use_target_network=use_target_network,\n                    mask=mask,\n                )\n                for actor in actors\n            ]\n        else:\n            preds = actors[actor_idx].get_Q_values(\n                obs=obs,\n                use_target_network=use_target_network,\n                mask=mask,\n            )\n        return preds\n\n    def action_select(\n            self,\n            obs,\n            epsilon=0,\n            tau=0,\n            use_target_network=False,\n            cache_selected_embeddings=True,\n            return_q_values=False,\n            mask=None,\n            actor_idx=-1,\n    ):\n        '''Select the next action.\n\n        First calculates the Q-values, then selects with epsilon-greedy policy.\n\n        Additionally, stores the embeddings of the selected actions internally,\n        for use at the next time-step.\n        '''\n        if actor_idx == -1:\n            actor_idx = self.default_actor_idx\n        q_vals = self.get_Q_values(obs, use_target_network, mask, actor_idx=actor_idx)\n        if actor_idx is None:\n            preds = torch.stack(q_vals, dim=-1).min(-1).values\n        else:\n            preds = q_vals\n\n        if tau is None or tau==0:\n            action_vals, actions = scatter_max(preds, obs.batch_idx.to(preds.device), dim=0)\n        else:\n            actions = scatter_softmax_sample(preds / tau, obs.batch_idx.to(preds.device), dim=0)",
  "description": "Combined Analysis:\n- [ecord/environment/environment.py]: This file implements the core environment for the ECORD algorithm, which directly corresponds to the optimization model for the Maximum Cut problem. The Environment class manages the state of the binary vertex assignments (x_i âˆˆ {0,1}) and computes the objective function (cut value) through the Tradjectories module. The step() method implements the action selection (node flipping) that changes the binary assignments, and the reward calculation in _calc_reward() is based on improvement in the cut value relative to the best score found, normalized by the number of nodes. This directly implements the Max-Cut optimization objective where the agent learns to maximize the cut value through sequential node flips.\n- [ecord/models/ecodqn.py]: This file implements the GNN component of the ECORD architecture described in the paper. Specifically, it corresponds to part (a) of the architecture diagram where a GNN generates embeddings for each vertex at t=0. The implementation follows equations (4)-(8) from the paper: Equation (4) for node embedding, Equation (5) for node-edge aggregation and degree normalization, Equations (6-7) for message passing layers, and Equation (8) for readout and Q-value computation. The GNN processes graph structure (edge_index), edge weights (edge_attr), node features (x), and graph batching information to produce Q-values for each node, which are used by the downstream recurrent unit and Q-network for action selection in the Max-Cut problem.\n- [ecord/models/gnn_embedder.py]: This file implements the Graph Neural Network (GNN) embedding component of the ECORD architecture, which corresponds to step (a) in the paper's algorithm: 'A GNN generates embeddings for each vertex at t = 0.' The GNNEmbedder class processes graph-structured data (node features, edge indices, edge attributes) through multiple message-passing layers to produce node embeddings that capture structural information. These embeddings are then used as input to the recurrent unit in subsequent timesteps. The implementation includes custom message-passing layers (GatedGraphConv and NodeModel) with GRU-based updates, layer normalization, and device management. While this file doesn't directly implement the Max-Cut optimization objective or the RL action selection, it provides the foundational graph representation learning that enables the downstream optimization process.\n- [ecord/models/rnn_decoder.py]: This file implements the recurrent decoder component of ECORD that corresponds to algorithm step (b) in the paper's architecture diagram. It maintains a hidden state representation of the optimization trajectory and produces Q-values for node selection actions. The base class provides different output mechanisms (dot product, MLP Q-values, action-state decomposition) for computing action values, while GRUDecoder and LSTMDecoder implement specific RNN variants for state maintenance. This directly implements the 'fast action decoding with a recurrent unit' described in the algorithm steps.\n- [ecord/solvers/actors/ecord.py]: This file implements the core action selection and Q-value computation logic of the ECORD algorithm. The Actor_Q and Actor_DoubleQ classes encapsulate the RL agent's decision-making process for the Max-Cut problem. Key steps include: (1) Updating RNN internal state using previous node embeddings and current graph observations, (2) Generating node embeddings from GNN and current features, (3) Computing Q-values via RNN, and (4) Selecting actions using epsilon-greedy or softmax policies with scatter operations for graph-structured data. This directly corresponds to the algorithm's fast action decoding component described in the paper.",
  "dependencies": [
    "torch_scatter.scatter_min",
    "torch_scatter.scatter",
    "time",
    "torch_sparse",
    "uniform",
    "torch_geometric.nn",
    "ecord.environment.Tradjectories",
    "torch_scatter.scatter_mean",
    "math",
    "_expand_as_over_tradj",
    "ecord.solvers.actors._base.ActorState",
    "scatter_softmax_sample",
    "ecord.solvers.actors._base._ActorBase",
    "maybe_cuda_synchronise",
    "ecord.environment.GlobalObservation",
    "ecord.environment.NodeObservation",
    "enum",
    "NodeModel",
    "ecord.models._base._DevicedModule",
    "_DevicedModule",
    "collections",
    "ecord.models._base",
    "torch.nn",
    "torch_geometric",
    "abc",
    "torch_scatter.scatter_max",
    "torch_scatter",
    "ECODQN_layer",
    "torch",
    "GatedGraphConv"
  ]
}