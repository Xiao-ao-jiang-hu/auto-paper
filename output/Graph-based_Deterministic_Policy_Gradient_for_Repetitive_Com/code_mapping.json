{
  "file_path": "agent_gdpg_util.py, heuristics_mwcds.py, mwds_gcn_call_twin.py, mwds_gcn_train_twin.py, mwis_gcn_call_twin.py, mwis_gcn_train_twin.py, solver_base_tf2.py, steiner_gcn_call_twin.py, steiner_gcn_train_twin.py",
  "function_name": "GDPGAgent, DPGAgent, DQNAgent, Solver, DPGAgent",
  "code_snippet": "\n\n# ==========================================\n# File: agent_gdpg_util.py\n# Function/Context: GDPGAgent\n# ==========================================\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nimport os\nimport shutil\nsys.path.append( '%s/gcn' % os.path.dirname(os.path.realpath(__file__)) )\nimport time\nimport random\nimport scipy.io as sio\nimport numpy as np\nimport scipy.sparse as sp\nfrom multiprocessing import Queue\nfrom copy import deepcopy\nimport networkx as nx\n\nimport tensorflow as tf\nfrom collections import deque\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom gcn.utils import *\nfrom runtime_config import flags, FLAGS\n# Settings (FLAGS)\nfrom heuristics import *\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Dropout, Input\nfrom tensorflow.keras.losses import MeanSquaredError\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam, schedules\nfrom tensorflow.keras.regularizers import l2\n\nfrom spektral.data.loaders import SingleLoader\nfrom spektral.datasets.citation import Citation\nfrom spektral.layers import ChebConv\nfrom spektral.transforms import LayerPreprocess\nfrom solver_base_tf2 import Solver\ntf.config.run_functions_eagerly(True)\n\n# Some preprocessing\nnsr = np.power(10.0, -FLAGS.snr_db/20.0)\n\n\nclass GDPGAgent(Solver):\n    def __init__(self, input_flags, memory_size=5000):\n        super(GDPGAgent, self).__init__(input_flags, memory_size)\n        self.flags = input_flags\n        self.num_supports = 1 + self.flags.max_degree\n        self.l2_reg = 5e-4\n        self.model = self._build_model()\n        self.memory_crt = deque(maxlen=memory_size)\n        self.memory_act = deque(maxlen=memory_size)\n        self.mse = MeanSquaredError()\n        self.critic = self._build_critic(num_layer=3)\n        self.critic.trainable = False\n\n    def _build_model(self):\n        # Neural Net for Actor Model\n        x_in = Input(shape=(self.feature_size,), dtype=tf.float64, name=\"x_in\")\n        a_in = Input((None, ), sparse=True, dtype=tf.float64, name=\"a_in\")\n\n        gc_l = x_in\n        for l in range(self.flags.num_layer):\n            if l < self.flags.num_layer - 1:\n                act = \"leaky_relu\"\n                output_dim = self.flags.hidden1\n            else:\n                act = \"relu\"\n                output_dim = self.flags.diver_num\n            do_l = Dropout(self.flags.dropout, dtype='float64')(gc_l)\n            gc_l = ChebConv(\n                output_dim, K=self.num_supports, activation=act,\n                kernel_regularizer=l2(self.l2_reg),\n                use_bias=False,\n                dtype='float64'\n            )([do_l, a_in])\n\n        # gc_l = (gc_l - tf.reduce_mean(gc_l))/(6*tf.math.reduce_std(gc_l)) + 0.5\n        # gc_l = (gc_l - tf.reduce_mean(gc_l)) + 0.5\n        # Build model\n        model = Model(inputs=[x_in, a_in], outputs=gc_l)\n        if self.flags.learning_decay == 1.0:\n            self.optimizer = Adam(learning_rate=self.learning_rate)\n        else:\n            lr_schedule = schedules.ExponentialDecay(\n                initial_learning_rate=self.learning_rate,\n                decay_steps=200,\n                decay_rate=self.flags.learning_decay)\n            self.optimizer = Adam(learning_rate=lr_schedule)\n        model.summary()\n        return model\n\n    def _build_critic(self, num_layer=3, dropout=0.0):\n        # Neural Net for Critic Model\n        x_in = Input(shape=(self.flags.diver_num,), dtype=tf.float64, name=\"x_in\")\n        a_in = Input((None, ), sparse=True, dtype=tf.float64, name=\"a_in\")\n\n        # gc_l = (x_in - tf.reduce_mean(x_in))/(6*tf.math.reduce_std(x_in)) + 0.5\n        gc_l = x_in\n        for l in range(num_layer):\n            if l < num_layer - 1:\n                act = \"leaky_relu\"\n                output_dim = 32\n            else:\n                act = \"linear\"\n                output_dim = 1\n            do_l = Dropout(dropout, dtype='float64')(gc_l)\n            gc_l = ChebConv(\n                output_dim, K=self.num_supports, activation=act,\n                kernel_regularizer=l2(self.l2_reg),\n                use_bias=False,\n                dtype='float64'\n            )([do_l, a_in])\n\n        # Build model\n        model = Model(inputs=[x_in, a_in], outputs=gc_l)\n        if self.flags.learning_decay == 1.0:\n            self.opt_crt = Adam(learning_rate=0.001)\n        else:\n            lr_schedule = schedules.ExponentialDecay(\n                initial_learning_rate=0.001,\n                decay_steps=200,\n                decay_rate=self.flags.learning_decay)\n            self.opt_crt = Adam(learning_rate=lr_schedule)\n        model.summary()\n        return model\n\n    def load_critic(self, name):\n        ckpt = tf.train.latest_checkpoint(name)\n        if ckpt:\n            self.critic.load_weights(ckpt)\n            print('Critic loaded ' + ckpt)\n\n    def save_critic(self, checkpoint_path):\n        self.critic.save_weights(checkpoint_path)\n\n    def load(self, name):\n        ckpt = tf.train.latest_checkpoint(name)\n        if ckpt:\n            self.model.load_weights(ckpt)\n            print('Actor loaded ' + ckpt)\n\n    def save(self, checkpoint_path):\n        self.model.save_weights(checkpoint_path)\n\n    def makestate(self, adj, wts_nn):\n        reduced_nn = wts_nn.shape[0]\n        # features = np.ones([reduced_nn, self.feature_size])\n        features = np.multiply(np.ones([reduced_nn, self.feature_size]), wts_nn)\n        support = simple_polynomials(adj, self.flags.max_degree)\n        state = {\"features\": features, \"support\": support[1]}\n        return state\n\n    def memorize_crt(self, grad, loss, reward):\n        self.memory_crt.append((grad.copy(), loss, reward))\n\n    def memorize_act(self, grad, loss, reward):\n        self.memory_act.append((grad.copy(), loss, reward))\n\n    def predict(self, state):\n        x_in = tf.convert_to_tensor(state[\"features\"], dtype=tf.float64)\n        coord, values, shape = state[\"support\"]\n        a_in = tf.sparse.SparseTensor(coord, values, shape)\n        act_values = self.model([x_in, a_in])\n        return act_values, np.argmax(act_values.numpy())\n\n    def act(self, state, train):\n        act_values, action = self.predict(state)\n        return act_values, action  # returns action\n\n    def predict_critic(self, z_out, state):\n        coord, values, shape = state[\"support\"]\n        a_in = tf.sparse.SparseTensor(coord, values, shape)\n        sch_pred = self.critic([z_out, a_in])\n        return sch_pred\n\n    def replay(self, batch_size):\n        if len(self.memory) < batch_size:\n            return float('NaN'), float('NaN')\n        self.reward_mem.clear()\n        minibatch = random.sample(self.memory, batch_size)\n        self.critic.trainable = True\n        self.model.trainable = True\n        for state, act_vals, solu, next_state, reward in minibatch:\n            if next_state:\n                act_vals_next, _ = self.predict(next_state)\n                val_next = self.predict_critic(act_vals_next, next_state)\n            else:\n                val_next = np.zeros_like(act_vals)\n            reward_t = tf.convert_to_tensor(reward, dtype=tf.float64) / 100.0\n            reward_t = tf.reshape(reward_t, (-1, 1))\n            val_vec = reward_t + self.gamma * val_next\n            act_vals_t = tf.convert_to_tensor(act_vals, dtype=tf.float64)\n            val_this = self.predict_critic(act_vals_t, state)\n            with tf.GradientTape() as g:\n                g.watch(self.critic.trainable_weights)\n                regularization_loss = tf.reduce_sum(self.critic.losses)\n                loss_value = tf.sqrt(self.mse(val_vec, val_this)) + regularization_loss\n                gradients = g.gradient(loss_value, self.critic.trainable_weights)\n                self.memorize_crt(gradients, loss_value.numpy(), reward)\n\n            with tf.GradientTape() as g:\n                g.watch(self.model.trainable_weights)\n                act_val, _ = self.act(state, False)\n                val_crt = self.predict_critic(act_val, state)\n                regularization_loss = tf.reduce_sum(self.model.losses)\n                obj_fn = -tf.reduce_mean(val_crt[:, 0])\n                obj_fn = obj_fn + regularization_loss\n                gradients = g.gradient(obj_fn, self.model.trainable_weights)\n                self.memorize_act(gradients, obj_fn.numpy(), 0)\n\n        loss_c = self.replay_crt(batch_size)\n        loss_a = self.replay_act(batch_size*10)\n        # self.memory.clear()\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n        return loss_a, loss_c\n\n    def replay_act(self, batch_size):\n        if len(self.memory_act) < batch_size:\n            return float('NaN')\n        self.reward_mem.clear()\n        minibatch = random.sample(self.memory_act, batch_size)\n        losses = []\n        for grad, loss, _ in minibatch:\n            self.optimizer.apply_gradients(zip(grad, self.model.trainable_weights))\n            losses.append(loss)\n\n        self.memory_act.clear()\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n        return np.nanmean(losses)\n\n    def replay_crt(self, batch_size):\n        if len(self.memory_crt) < batch_size:\n            return float('NaN')\n        minibatch = random.sample(self.memory_crt, batch_size)\n        losses = []\n        for grad, loss, _ in minibatch:\n            self.opt_crt.apply_gradients(zip(grad, self.critic.trainable_weights))\n            losses.append(loss)\n\n        self.memory_crt.clear()\n        return np.nanmean(losses)\n\n    def utility(self, adj_0, wts_0, train=False):\n        \"\"\"\n        GCN for per utility function\n        \"\"\"\n        adj = adj_0.copy()\n        wts_nn = np.reshape(wts_0, (wts_0.shape[0], self.flags.feature_size))\n\n        # GCN\n        state = self.makestate(adj, wts_nn)\n        act_vals = self.act(state, train)\n\n        gcn_wts = act_vals\n\n        return gcn_wts, state\n\n    def schedule(self, adj_0, wts_0, train=False):\n        \"\"\"\n        GCN followed by LGS\n        \"\"\"\n        adj = adj_0.copy()\n        wts_nn = np.reshape(wts_0, (wts_0.shape[0], self.flags.feature_size))\n\n        # GCN\n        state = self.makestate(adj, wts_nn)\n        act_vals_t, act = self.act(state, train)\n        act_vals = act_vals_t.numpy()\n\n        if self.flags.predict == 'mwis':\n            gcn_wts = np.multiply(act_vals.flatten(), wts_nn.flatten())\n        else:\n            gcn_wts = act_vals.flatten()\n\n        mwis, _ = local_greedy_search(adj, gcn_wts)\n        solu = list(mwis)\n        mwis_rt = mwis\n        total_wt = np.sum(wts_nn[solu, 0])\n        return mwis_rt, total_wt, state, act_vals\n\n    def actor_train(self, adj_0, wts_0, train=False):\n        adj = adj_0.copy()\n        nn  = wts_0.shape[0]\n        wts_nn = np.reshape(wts_0, (nn, FLAGS.feature_size))\n        ones = np.ones_like(wts_nn)\n\n        # GCN\n        with tf.GradientTape() as g:\n            g.watch(self.model.trainable_weights)\n            state = self.makestate(adj, ones)\n            act_val, act = self.act(state, train)\n            act_val_norm = act_val\n            # act_val_norm = 0.5 + (act_val - tf.reduce_mean(act_val))\n            sch_pred = self.predict_critic(act_val_norm, state)\n\n            if train:\n                regularization_loss = tf.reduce_sum(self.model.losses)\n                obj_fn = -tf.reduce_mean(sch_pred[:, 0])\n                obj_fn = obj_fn + regularization_loss\n                gradients = g.gradient(obj_fn, self.model.trainable_weights)\n                self.memorize_act(gradients, obj_fn.numpy(), 0)\n        return state, act_val\n\n    def critic_train(self, adj_0, zs_0, state, n_samples=1, z_std=0.15):\n        \"\"\"\n        GCN followed by LGS\n        wts_0: topology weighted utility\n        \"\"\"\n        adj = adj_0.copy()\n        zs_nn = zs_0.numpy()\n        nn = zs_nn.shape[0]\n\n        # GCN\n        with tf.GradientTape() as g:\n            g.watch(self.critic.trainable_weights)\n            sch_pred = self.predict_critic(zs_0, state)\n\n            ind_vec = np.zeros_like(sch_pred.numpy())\n            apu_avg = 0.0\n            for i in range(n_samples):\n                wts = np.random.uniform(0, 1, size=(nn, 1))\n                # zs_i = zs_nn + np.random.normal(0, z_std, size=(nn, 1))\n                zs_i = zs_nn + np.random.uniform(-0.5*z_std, 0.5*z_std, size=(nn, 1))\n                if FLAGS.predict == 'mwis':\n                    gcn_wts = np.multiply(zs_i.flatten(), wts.flatten())\n                else:\n                    gcn_wts = zs_i.flatten()\n\n                _, total_ref = local_greedy_search(adj, wts)\n                mwis, _ = local_greedy_search(adj, gcn_wts)\n                solu = list(mwis)\n                total_wt = np.sum(wts[solu, 0])\n                ind_vec[solu, 0] += 1.0/float(n_samples)\n                apu_avg += total_wt/(total_ref*float(n_samples)+1e-6)\n\n            reward = apu_avg\n            ind_vec[:, 1] = 1.0 - ind_vec[:, 0]\n            y_target = tf.convert_to_tensor(ind_vec, dtype=tf.float64)\n            regularization_loss = tf.reduce_sum(self.critic.losses)\n            loss_value = tf.sqrt(self.mse(y_target, sch_pred)) + regularization_loss\n            gradients = g.gradient(loss_value, self.critic.trainable_weights)\n            self.memorize_crt(gradients, loss_value.numpy(), reward)\n        return ind_vec, apu_avg\n\n# ==========================================\n# File: heuristics_mwcds.py\n# Function/Context: \n# ==========================================\nimport networkx as nx\nimport dwave_networkx as dnx\nimport igraph as ig\nimport pulp as plp\nfrom pulp import GLPK\nimport numpy as np\nimport pandas as pd\nimport scipy.sparse as sp\nimport copy\nimport time\nprint(nx.__version__)\n\n\ndef zero_rows(M, rows):\n    diag = sp.eye(M.shape[0]).tolil()\n    for r in rows:\n        diag[r, r] = 0\n    return diag.dot(M)\n\n\ndef zero_columns(M, columns):\n    diag = sp.eye(M.shape[1]).tolil()\n    for c in columns:\n        diag[c, c] = 0\n    return M.dot(diag)\n\n\n# From @wim's post\ndef nunique(a):\n    df = pd.DataFrame(a.T)\n    return df.nunique().to_numpy(dtype=np.float)\n\n\ndef steiner_terminal_2hop(adj, wts):\n    adj_0 = adj.copy()\n    wts_0 = np.array(wts).flatten()\n    # verts = np.array(range(wts_0.size))\n    terminals = set()\n    adj_2 = adj_0.dot(adj_0)\n    verts = wts_0.argsort()\n    verts = verts.tolist()\n    while len(verts) > 0:\n        i = verts.pop(0)\n        _, nb1_set = np.nonzero(adj_0[i])\n        _, nb2_set = np.nonzero(adj_2[i])\n        terminals.add(i)\n        verts = list(set(verts) - set(nb1_set))\n        verts = list(set(verts) - set(nb2_set))\n    return terminals\n\n\ndef mwds_greedy_mis(adj, wts):\n    '''\n    Ref: Ant Colony Optimization Applied to Minimum Weighted Dominating Set Problem\n    Color: White: Uncovered, Black: Dominating, Gray: Covered\n    Return MWDS set and the total weights of MWDS\n    :param adj: adjacency matrix (sparse)\n    :param wts: weights of vertices\n    :return: mwds, total_wt\n    '''\n    adj_0 = adj.copy()\n    wts_0 = np.array(wts).flatten()\n    verts = np.array(range(wts_0.size))\n    mwds = set()\n    gray_set = set()\n    white_set = set(verts)\n    wts_1 = wts_0.copy()\n    while len(white_set) > 0:\n        covers = adj_0.dot(wts_1)\n        # covers = adj_0.dot(np.diag(np.eye(wts_0.size)))\n        weights = np.divide(wts_0, 1 + covers)\n        # weights = wts_0 - covers\n        # weights = np.divide(1 + covers, wts_0)\n        weights[list(gray_set)] = np.Inf\n        weights[list(mwds)] = np.Inf\n        # weights[covers == 0] = -1\n        i = np.argmin(weights)\n        # if covers[i] == 0:\n        #     continue\n        _, nb_set = np.nonzero(adj_0[i])\n        mwds.add(i)\n        nb_set = set(nb_set).intersection(white_set)\n        gray_set = gray_set.union(nb_set)\n        nb_set.add(i)\n        white_set = white_set - nb_set - mwds\n        rm_set = list(nb_set)\n        wts_1[rm_set] = 0\n        # adj_0 = zero_rows(adj_0, rm_set)\n        # adj_0 = zero_columns(adj_0, rm_set)\n    total_ws = np.sum(wts[list(mwds)])\n    return mwds, gray_set, total_ws\n\n\ndef mwds_greedy(adj, wts):\n    '''\n    Ref: Ant Colony Optimization Applied to Minimum Weighted Dominating Set Problem\n    Color: White: Uncovered, Black: Dominating, Gray: Covered\n    Return MWDS set and the total weights of MWDS\n    :param adj: adjacency matrix (sparse)\n    :param wts: weights of vertices\n    :return: mwds, total_wt\n    '''\n    adj_0 = adj.copy()\n    wts_0 = np.array(wts).flatten()\n    verts = np.array(range(wts_0.size))\n    mwds = set()\n    gray_set = set()\n    white_set = set(verts)\n    wts_1 = wts_0.copy() + 1e-6\n    degrees = adj_0.sum(axis=1)\n    degrees = np.asarray(degrees).flatten()\n    while len(white_set) > 0:\n        covers = adj_0.dot(wts_1)\n        # covers = adj_0.dot(np.diag(np.eye(wts_0.size)))\n        weights = np.divide(wts_0, 1 + covers)\n        # weights = wts_0 - covers\n        weights[list(mwds)] = np.Inf\n        weights[np.logical_and(covers == 0, degrees > 0)] = np.Inf\n        i = np.argmin(weights)\n        _, nb_set = np.nonzero(adj_0[i])\n        mwds.add(i)\n        nb_set = set(nb_set).intersection(white_set)\n        gray_set = gray_set.union(nb_set)\n        nb_set.add(i)\n        white_set = white_set - nb_set - mwds\n        rm_set = list(nb_set)\n        wts_1[rm_set] = 0\n    total_ws = np.sum(wts[list(mwds)])\n    return mwds, gray_set, total_ws\n\n\ndef node_weight_steiner_set(adj, wts, mis):\n    # Shortest Path Heuristic\n    adj_0 = adj.copy()\n    wts_0 = np.array(wts).flatten()\n    nodes = np.array(range(wts_0.size))\n    mwcds = copy.deepcopy(mis)\n    mwcds_list = list(mwcds)\n    gray_set = set(nodes) - mwcds\n    nodes_forest = np.full_like(nodes, np.nan, dtype=np.double)\n    nodes_forest[mwcds_list] = mwcds_list\n    nodes_forest += 1.0 # forest id initialized as node id\n    cover_forest = np.full_like(nodes, np.nan, dtype=np.double)\n    # Merge any connected trees\n    for i in mwcds_list:\n        _, nb_set = np.nonzero(adj_0[i])\n        nb_tid = nodes_forest[nb_set]\n        nb_tids = nb_tid[~np.isnan(nb_tid)]\n        cover_forest[i] = len(set(nb_set).intersection(gray_set))\n        # nb_trees = nb_set[~np.isnan(nb_tid)]\n        if len(nb_tids) > 0:\n            fid_max = np.nanmax(nb_tid)\n            nodes_forest[i] = fid_max\n            for tid in nb_tids:\n                nodes_forest[nodes_forest == tid] = fid_max\n    # Creates a quotient graph\n    tids = np.unique(nodes_forest[mwcds_list])\n    trees = [set(np.argwhere(nodes_forest==tid).flatten()) for tid in tids]\n    forest = []\n    for tree in trees:\n        tree_dict = {'head': max(tree), 'member': tree, 'cover': np.sum(cover_forest[list(tree)])}\n        forest.append(tree_dict)\n\n    wts_st = copy.deepcopy(wts_0)\n    wts_st[mwcds_list] = 0\n\n    def edge_weight(s, d, attr):\n        return wts_st[s]+wts_st[d]\n    g = nx.from_scipy_sparse_matrix(adj_0)\n    start = max(forest, key=lambda x: x['cover'])\n    forest.remove(start)\n    while len(forest) > 0:\n        dists = []\n        paths = []\n        for tree in forest:\n            src = start['head']\n            dst = tree['head']\n            path = nx.shortest_path(g, src, dst, weight=edge_weight)\n            dists.append(np.sum(wts_st[path]))\n            paths.append(path)\n        pid = np.argmin(dists)\n        path = paths[pid]\n        tree = forest[pid]\n        wts_st[path] = 0\n        newtree = start['member'].union(tree['member']).union(set(path))\n        newhead = max(newtree)\n        start['head'] = newhead\n        start['member'] = newtree\n        forest.remove(tree)\n    mwcds = start['member']\n    total_ws = np.sum(wts_0[list(mwcds)])\n    return mwcds, total_ws\n\n\ndef nwst_sph(adj, wts, mis):\n    # Shortest Path Heuristic, no sorting\n    adj_0 = adj.copy()\n    wts_0 = np.array(wts).flatten()\n    nodes = np.array(range(wts_0.size))\n    mwcds = copy.deepcopy(mis)\n    mwcds_list = list(mwcds)\n    gray_set = set(nodes) - mwcds\n    nodes_forest = np.full_like(nodes, np.nan, dtype=np.double)\n    nodes_forest[mwcds_list] = mwcds_list\n    nodes_forest += 1.0 # forest id initialized as node id\n    cover_forest = np.full_like(nodes, np.nan, dtype=np.double)\n    # Merge any connected trees\n    for i in mwcds_list:\n        _, nb_set = np.nonzero(adj_0[i])\n        nb_tid = nodes_forest[nb_set]\n        nb_tids = nb_tid[~np.isnan(nb_tid)]\n        cover_forest[i] = len(set(nb_set).intersection(gray_set))\n        # nb_trees = nb_set[~np.isnan(nb_tid)]\n        if len(nb_tids) > 0:\n            fid_max = np.nanmax(nb_tid)\n            nodes_forest[i] = fid_max\n            for tid in nb_tids:\n                nodes_forest[nodes_forest == tid] = fid_max\n    # Creates a quotient graph\n    tids = np.unique(nodes_forest[mwcds_list])\n    trees = [set(np.argwhere(nodes_forest==tid).flatten()) for tid in tids]\n    forest = []\n    for tree in trees:\n        tree_dict = {'head': max(tree), 'member': tree, 'cover': np.sum(cover_forest[list(tree)])}\n        forest.append(tree_dict)\n\n    wts_st = copy.deepcopy(wts_0)\n    wts_st[mwcds_list] = 0\n\n    def edge_weight(s, d, attr):\n        return wts_st[s]+wts_st[d]\n    g = nx.from_scipy_sparse_matrix(adj_0)\n    # start = max(forest, key=lambda x: x['cover'])\n    start = forest[0]\n    forest.remove(start)\n    while len(forest) > 0:\n        dists = []\n        paths = []\n        for tree in forest:\n            src = start['head']\n            dst = tree['head']\n            path = nx.shortest_path(g, src, dst, weight=edge_weight)\n            dists.append(np.sum(wts_st[path]))\n            paths.append(path)\n        pid = np.argmin(dists)\n        path = paths[pid]\n        tree = forest[pid]\n        wts_st[path] = 0\n        newtree = start['member'].union(tree['member']).union(set(path))\n        newhead = max(newtree)\n        start['head'] = newhead\n        start['member'] = newtree\n        forest.remove(tree)\n    mwcds = start['member']\n    total_ws = np.sum(wts_0[list(mwcds)])\n    return mwcds, total_ws\n\n\ndef steiner_tree_mst(adj, wts, terminals):\n    adj_0 = adj.copy()\n    wts_0 = np.array(wts).flatten()\n    nodes = np.array(range(wts_0.size))\n    graph = nx.from_scipy_sparse_matrix(adj_0)\n    for u in graph:\n        graph.nodes[u]['weight'] = wts_0[u]\n    mwcds = copy.deepcopy(terminals)\n    mwcds_list = list(mwcds)\n    wts_st = copy.deepcopy(wts_0)\n    wts_st[mwcds_list] = 0\n\n    def edge_weight(s, d, attr):\n        return wts_st[s]+wts_st[d]\n\n    total_ws = 0.0\n    sg = nx.algorithms.approximation.steinertree.steiner_tree(graph, terminals, weight=edge_weight)\n    st = list(sg.nodes)\n    mwcds = set(st)\n    total_ws = np.sum(wts_0[st])\n    return mwcds, total_ws\n\n\ndef greedy_mwcds(adj, wts):\n    adj_0 = adj.copy()\n    wts_0 = np.array(wts).flatten()\n    # step 1: find MWDS, which is an independent set\n    mwds, gray_set, _ = mwds_greedy_mis(adj_0, wts_0)\n    # step 2: find node weighted steiner tree, with MWDS as the set of terminals\n    nodes = np.array(range(wts_0.size))\n    mwcds = copy.deepcopy(mwds)\n    mwds_list = list(mwds)\n    nodes_forest = np.full_like(nodes, np.nan, dtype=np.double)\n    nodes_forest[mwds_list] = mwds_list\n    nodes_forest += 1\n    gray_list = np.array(list(gray_set))\n    gray_wts = wts_0[gray_list]\n    idx = np.argsort(gray_wts)\n    gray_list = gray_list[idx]\n    degrees = adj_0.sum(axis=1)\n    degrees = np.asarray(degrees).flatten()\n    for i in gray_list:\n        if degrees[i] <= 1:\n            continue\n        _, nb_set = np.nonzero(adj_0[i])\n        nb_fid = nodes_forest[nb_set]\n        nb_fid_set = set(nb_fid[~np.isnan(nb_fid)])\n        if len(nb_fid_set) > 1:\n            mwcds.add(i)\n            fid_max = np.nanmax(nb_fid)\n            nodes_forest[i] = fid_max\n            for fid in nb_fid_set:\n                nodes_forest[nodes_forest==fid] = fid_max\n\n    total_ws = np.sum(wts_0[list(mwcds)])\n    return mwcds, mwds, total_ws\n\n\ndef greedy_mwcds2(adj, wts):\n    adj_0 = adj.copy()\n    wts_0 = np.array(wts).flatten()\n    # step 1: find MWDS, which is an independent set\n    # mwds, gray_set, _ = mwds_greedy_mis(adj_0, wts_0)\n    mwds, gray_set, _ = mwds_greedy(adj_0, wts_0)\n    # step 2: find node weighted steiner tree, with MWDS as the set of terminals\n    mwcds, total_ws = node_weight_steiner_set(adj_0, wts_0, mwds)\n    return mwcds, mwds, total_ws\n\n\ndef mwcds_vvv(adj, wts):\n    adj_0 = adj.copy()\n    wts_0 = np.array(wts).flatten()\n    # step 1: find MWDS, which is an independent set\n    graph = nx.from_scipy_sparse_matrix(adj_0)\n    for u in graph:\n        graph.nodes[u]['weight'] = wts_0[u]\n    mwds = nx.algorithms.approximation.min_weighted_dominating_set(graph, 'weight')\n    # step 2: find node weighted steiner tree, with MWDS as the set of terminals\n    mwcds, total_ws = node_weight_steiner_set(adj_0, wts_0, mwds)\n    return mwcds, mwds, total_ws\n\n\ndef mwds_vvv(adj, wts):\n    adj_0 = adj.copy()\n    wts_0 = np.array(wts).flatten()\n    # step 1: find MWDS, which is an independent set\n    graph = nx.from_scipy_sparse_matrix(adj_0)\n    for u in graph:\n        graph.nodes[u]['weight'] = wts_0[u]\n    mwds = nx.algorithms.approximation.min_weighted_dominating_set(graph, 'weight')\n    total_ws = np.sum(wts_0[list(mwds)])\n    gray_set = set(range(wts_0.size)) - mwds\n    return mwds, gray_set, total_ws\n\n\ndef dist_greedy_mwds(adj, wts):\n    '''\n    Ref: Ant Colony Optimization Applied to Minimum Weighted Dominating Set Problem (weight heuristic)\n    Color: White: Uncovered, Black: Dominating, Gray: Covered\n    Return MWDS set and the total weights of MWDS\n    :param adj: adjacency matrix (sparse)\n    :param wts: weights of vertices\n    :return: mwds, total_wt\n    '''\n    adj_0 = adj.copy()\n    wts_0 = np.array(wts).flatten()\n    verts = np.array(range(wts_0.size))\n    mwds = set()\n    gray_set = set()\n    white_set = set(verts)\n    while len(white_set) > 0:\n        # weights = np.divide(1 + adj_0.dot(np.diag(np.eye(wts_0.size))), wts_0)\n        weights = np.divide(wts_0, 1 + adj_0.dot(wts_0))\n        weights[list(gray_set)] = 0\n        weights[list(mwds)] = 0\n        for i in white_set:\n            proc = False\n            _, nb_set = np.nonzero(adj_0[i])\n            if len(nb_set) > 0:\n                nb_min = np.amin(weights[nb_set])\n                nb_min_set = nb_set[weights[nb_set] == nb_min]\n                if weights[i] < nb_min or (weights[i] == nb_min and i < np.amax(nb_min_set)):\n                    proc = True\n            else:\n                proc = True\n            if proc:\n                mwds.add(i)\n                nb_set = set(nb_set).intersection(white_set)\n                gray_set = gray_set.union(nb_set)\n                nb_set.add(i)\n        rm_set = gray_set.union(mwds)\n        white_set = white_set - rm_set\n        adj_0 = zero_rows(adj_0, list(rm_set))\n        adj_0 = zero_columns(adj_0, list(rm_set))\n    total_ws = np.sum(wts_0[list(mwds)])\n    return mwds, gray_set, total_ws\n\n\ndef dist_node_weight_steiner_set(adj, wts, mis):\n    adj_0 = adj.copy()\n    wts_0 = np.array(wts).flatten()\n    nodes = np.array(range(wts_0.size))\n    mwcds = copy.deepcopy(mis)\n    mwcds_list = list(mwcds)\n    gray_set = set(nodes) - mwcds\n    nodes_forest = np.full_like(nodes, np.nan, dtype=np.double)\n    nodes_forest[mwcds_list] = mwcds_list\n    nodes_forest += 1.0 # forest id initialized as node id\n    cover_forest = np.full_like(nodes, np.nan, dtype=np.double)\n    # Merge any connected trees\n    for i in mwcds_list:\n        _, nb_set = np.nonzero(adj_0[i])\n        nb_tid = nodes_forest[nb_set]\n        nb_tids = nb_tid[~np.isnan(nb_tid)]\n        cover_forest[i] = len(set(nb_set).intersection(gray_set))\n        # nb_trees = nb_set[~np.isnan(nb_tid)]\n        if len(nb_tids) > 0:\n            fid_max = np.nanmax(nb_tid)\n            nodes_forest[i] = fid_max\n            for tid in nb_tids:\n                nodes_forest[nodes_forest == tid] = fid_max\n    # Creates a quotient graph\n    tids = np.unique(nodes_forest[mwcds_list])\n    trees = [set(np.argwhere(nodes_forest==tid).flatten()) for tid in tids]\n    forest = []\n    for tree in trees:\n        tree_dict = {'head': max(tree), 'member': tree}\n        forest.append(tree_dict)\n\n    wts_st = copy.deepcopy(wts_0)\n    wts_st[mwcds_list] = 0\n\n    def edge_weight(s, d, attr):\n        return wts_st[s]+wts_st[d]\n    g = nx.from_scipy_sparse_matrix(adj_0)\n    while len(forest) > 1:\n        dst_paths = []\n        dst_trees = []\n        for start in forest:\n            dists = []\n            paths = []\n            for tree in forest:\n                if tree['head'] == start['\n\n# ==========================================\n# File: mwds_gcn_call_twin.py\n# Function/Context: DPGAgent\n# ==========================================\nimport sys\nimport os\nimport shutil\nsys.path.append( '%s/gcn' % os.path.dirname(os.path.realpath(__file__)) )\nimport time\nimport random\nimport scipy.io as sio\nimport numpy as np\nimport scipy.sparse as sp\nfrom multiprocessing import Queue\nfrom copy import deepcopy\nimport networkx as nx\nfrom scipy.stats.stats import pearsonr\n\nimport tensorflow as tf\nfrom collections import deque\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom gcn.utils import *\nfrom runtime_config import flags, FLAGS\n# Settings (FLAGS)\nfrom heuristics_mwcds import *\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Dropout, Input\nfrom tensorflow.keras.losses import MeanSquaredError\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam, SGD, schedules\nfrom tensorflow.keras.regularizers import l2\n\nfrom spektral.data.loaders import SingleLoader\nfrom spektral.datasets.citation import Citation\nfrom spektral.layers import ChebConv\nfrom spektral.transforms import LayerPreprocess\nfrom solver_base_tf2 import Solver\ntf.config.run_functions_eagerly(True)\n\n# Some preprocessing\nnsr = np.power(10.0, -FLAGS.snr_db/20.0)\n\n# heuristic_func = mwds_greedy_mis\nheuristic_func = mwds_greedy\n# heuristic_func = mwds_vvv\n\n\nclass DPGAgent(Solver):\n    def __init__(self, input_flags, memory_size=5000):\n        super(DPGAgent, self).__init__(input_flags, memory_size)\n        self.flags = input_flags\n        self.num_supports = 1 + self.flags.max_degree\n        self.l2_reg = 1e-4\n        self.model = self._build_model()\n        self.memory_crt = deque(maxlen=memory_size)\n        self.mse = MeanSquaredError()\n        self.critic = self._build_critic(num_layer=5)\n        self.critic.trainable = True\n        self.model.trainable = True\n\n    def _build_model(self):\n        # Neural Net for Actor Model\n        x_in = Input(shape=(self.feature_size,), dtype=tf.float64, name=\"x_in\")\n        a_in = Input((None, ), sparse=True, dtype=tf.float64, name=\"a_in\")\n\n        gc_l = x_in\n        for l in range(self.flags.num_layer):\n            if l < self.flags.num_layer - 1:\n                act = \"leaky_relu\"\n                output_dim = self.flags.hidden1\n            else:\n                output_dim = self.flags.diver_num\n                if output_dim == 1:\n                    act = \"relu\"\n                elif output_dim == 2:\n                    act = \"linear\"\n                else:\n                    raise ValueError(\"unsupported diver_num {}\".format(output_dim))\n            do_l = Dropout(self.flags.dropout, dtype='float64')(gc_l)\n            gc_l = ChebConv(\n                output_dim, K=self.num_supports, activation=act,\n                kernel_regularizer=l2(self.l2_reg),\n                use_bias=True,\n                dtype='float64'\n            )([do_l, a_in])\n\n        # Build model\n        model = Model(inputs=[x_in, a_in], outputs=gc_l)\n        if self.flags.learning_decay == 1.0:\n            self.optimizer = Adam(learning_rate=self.learning_rate)\n        else:\n            lr_schedule = schedules.ExponentialDecay(\n                initial_learning_rate=self.learning_rate,\n                decay_steps=200,\n                decay_rate=self.flags.learning_decay)\n            self.optimizer = Adam(learning_rate=lr_schedule)\n        model.summary()\n        return model\n\n    def _build_critic(self, num_layer=3, dropout=0.0):\n        # Neural Net for Critic Model\n        x_in = Input(shape=(self.flags.diver_num+1,), dtype=tf.float64, name=\"x_in\")\n        a_in = Input((None, ), sparse=True, dtype=tf.float64, name=\"a_in\")\n\n        gc_l = x_in\n        for l in range(num_layer):\n            if l < num_layer - 1:\n                act = \"leaky_relu\"\n                output_dim = 32\n            else:\n                act = \"softmax\"\n                output_dim = 2\n            do_l = Dropout(dropout, dtype='float64')(gc_l)\n            gc_l = ChebConv(\n                output_dim, K=self.num_supports, activation=act,\n                kernel_regularizer=l2(self.l2_reg),\n                use_bias=True,\n                dtype='float64'\n            )([do_l, a_in])\n\n        # Build model\n        model = Model(inputs=[x_in, a_in], outputs=gc_l)\n        if self.flags.learning_decay == 1.0:\n            self.opt_crt = Adam(learning_rate=0.0001)\n        else:\n            lr_schedule = schedules.ExponentialDecay(\n                initial_learning_rate=0.0001,\n                decay_steps=200,\n                decay_rate=self.flags.learning_decay)\n            self.opt_crt = Adam(learning_rate=lr_schedule)\n        model.summary()\n        return model\n\n    def load_critic(self, name):\n        ckpt = tf.train.latest_checkpoint(name)\n        if ckpt:\n            self.critic.load_weights(ckpt)\n            print('Critic loaded ' + ckpt)\n\n    def load(self, name):\n        ckpt = tf.train.latest_checkpoint(name)\n        if ckpt:\n            self.model.load_weights(ckpt)\n            print('Actor loaded ' + ckpt)\n\n    def save(self, checkpoint_path):\n        self.model.save_weights(checkpoint_path)\n\n    def save_critic(self, checkpoint_path):\n        self.critic.save_weights(checkpoint_path)\n\n    def makestate(self, adj, wts_nn):\n        reduced_nn = wts_nn.shape[0]\n        features = np.multiply(np.ones([reduced_nn, self.feature_size]), wts_nn)\n        support = simple_polynomials(adj, self.flags.max_degree)\n        state = {\"features\": features, \"support\": support[1]}\n        return state\n\n    def memorize_crt(self, grad, loss, reward):\n        self.memory_crt.append((grad.copy(), loss, reward))\n\n    def predict(self, state):\n        x_in = tf.convert_to_tensor(state[\"features\"], dtype=tf.float64)\n        coord, values, shape = state[\"support\"]\n        a_in = tf.sparse.SparseTensor(coord, values, shape)\n        act_values = self.model([x_in, a_in])\n        return act_values, np.argmax(act_values.numpy())\n\n    def act(self, state, train, explore=0.0):\n        act_values, action = self.predict(state)\n        if explore > 0.001:\n            noise = tf.random.uniform(act_values.shape, -explore, explore, dtype=act_values.dtype)\n            act_values += noise\n        return act_values, action  # returns action\n\n    def predict_critic(self, z_out, state):\n        x_in = tf.convert_to_tensor(state[\"features\"], dtype=tf.float64)\n        coord, values, shape = state[\"support\"]\n        a_in = tf.sparse.SparseTensor(coord, values, shape)\n        x_in = tf.concat([x_in, z_out], axis=1)\n        sch_pred = self.critic([x_in, a_in])\n        return sch_pred\n\n    def replay(self, batch_size):\n        if len(self.memory) < batch_size:\n            return float('NaN')\n        self.reward_mem.clear()\n        minibatch = random.sample(self.memory, batch_size)\n        losses = []\n        for grad, _, _, loss, _ in minibatch:\n            self.optimizer.apply_gradients(zip(grad, self.model.trainable_weights))\n            losses.append(loss)\n\n        self.memory.clear()\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n        return np.nanmean(losses)\n\n    def replay_crt(self, batch_size):\n        if len(self.memory_crt) < batch_size:\n            return float('NaN')\n        minibatch = random.sample(self.memory_crt, batch_size)\n        losses = []\n        for grad, loss, _ in minibatch:\n            self.opt_crt.apply_gradients(zip(grad, self.critic.trainable_weights))\n            losses.append(loss)\n\n        self.memory_crt.clear()\n        return np.nanmean(losses)\n\n    def foo_train(self, adj_0, wts_0, train=False):\n        adj = adj_0.copy()\n        nn  = wts_0.shape[0]\n        wts_nn = np.reshape(wts_0, (nn, FLAGS.feature_size))\n        ones = np.ones_like(wts_nn)\n\n        # GCN\n        with tf.GradientTape() as g:\n            g.watch(self.model.trainable_weights)\n            state = self.makestate(adj, ones)\n            act_val, act = self.act(state, train, explore=0.03)\n            act_val_norm = act_val\n            sch_pred = self.predict_critic(act_val_norm, state)\n            regularization_loss = tf.reduce_sum(self.model.losses)\n            obj_fn = tf.reduce_sum(sch_pred[:, 0])\n            obj_fn = obj_fn + regularization_loss\n        if train: # Place gradient computing outside GradientTape\n            gradients = g.gradient(obj_fn, self.model.trainable_weights)\n            self.memorize(gradients, [], [], obj_fn.numpy(), 0)\n        return state, act_val\n\n    def predict_train(self, adj, zs_0, state, n_samples=1, z_std=0.15):\n        \"\"\"\n        GCN followed by LGS\n        wts_0: topology weighted utility\n        \"\"\"\n        zs_nn = zs_0.numpy()\n        nn = zs_nn.shape[0]\n        adj_0 = adj.copy()\n\n        # GCN\n        with tf.GradientTape() as g:\n            g.watch(self.critic.trainable_weights)\n            sch_pred = self.predict_critic(zs_0, state)\n\n            ind_vec = np.zeros((nn, 2))\n            for i in range(n_samples):\n                wts = np.random.uniform(0.0, 1.0, size=(nn, 1))\n                zs_i = zs_nn + np.random.uniform(-0.5*z_std, 0.5*z_std, size=(nn, self.flags.diver_num))\n                if FLAGS.predict == 'mpy':\n                    if self.flags.diver_num == 2:\n                        gcn_wts = zs_i[:, 0].flatten() * wts.flatten() + zs_i[:, 1].flatten()\n                    else:\n                        gcn_wts = np.multiply(zs_i.flatten(), wts.flatten())\n                    gcn_wts = np.clip(gcn_wts, a_min=0.0, a_max=None)\n                else:\n                    gcn_wts = zs_i.flatten()\n                mwds_i, gray_set_i, total_wt_i = heuristic_func(adj_0, gcn_wts)\n                solu = list(mwds_i)\n                ind_vec[solu, 0] += wts[solu, 0]/(float(n_samples))\n\n            ind_vec[:, 1] = 1.0 - ind_vec[:, 0]\n            corr, pval = pearsonr(ind_vec[:, 0], sch_pred[:, 0].numpy())\n            reward = corr\n            y_target = tf.convert_to_tensor(ind_vec, dtype=tf.float64)\n            regularization_loss = tf.reduce_sum(self.critic.losses)\n            loss_value = tf.sqrt(self.mse(y_target, sch_pred)) + regularization_loss\n        gradients = g.gradient(loss_value, self.critic.trainable_weights)\n        self.memorize_crt(gradients, loss_value.numpy(), reward)\n        return ind_vec, reward\n\n# ==========================================\n# File: mwds_gcn_train_twin.py\n# Function/Context: \n# ==========================================\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nimport os\nimport shutil\nsys.path.append( '%s/gcn' % os.path.dirname(os.path.realpath(__file__)) )\n\nimport time\nimport random\nimport scipy.io as sio\nimport numpy as np\nimport scipy.sparse as sp\nfrom multiprocessing import Queue\nfrom copy import deepcopy\nfrom scipy.stats.stats import pearsonr, linregress\nfrom scipy.spatial import distance_matrix\n\nimport tensorflow as tf\nimport networkx as nx\nfrom collections import deque\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom gcn.utils import *\n# Settings (FLAGS)\nfrom runtime_config import flags, FLAGS\nfrom heuristics_mwcds import *\n\nflags.DEFINE_string('gtype', 'er', 'training graph type: er, grp, ws, ba')\nflags.DEFINE_string('test_datapath', './data/ER_Graph_Uniform_NP20_test', 'test dataset')\nflags.DEFINE_integer('ntrain', 1, 'Number of units in hidden layer 1.')\nflags.DEFINE_integer('nvalid', 100, 'Number of outputs.')\n\nfrom mwds_gcn_call_twin import DPGAgent, heuristic_func\n\n# Get a list of available GPUs\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)\n# Set the number of GPUs to use\nnum_gpus = len(gpus)\n# Set up a MirroredStrategy to use all available GPUs\nif num_gpus > 1:\n    strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:%d\" % i for i in range(num_gpus)])\nelse:\n    strategy = tf.distribute.get_strategy() # default strategy\n# Define and compile your model within the strategy scope\nwith strategy.scope():\n    dqn_agent = DPGAgent(FLAGS, 5000)\n\n# test data path\ndata_path = FLAGS.datapath\ntest_datapath = FLAGS.test_datapath\n\n# Some preprocessing\nnoout = min(FLAGS.diver_num, FLAGS.diver_out) # number of outputs\ntime_limit = FLAGS.timeout  # time limit for searching\nbackoff_thresh = 1 - FLAGS.backoff_prob\n\nnum_supports = 1 + FLAGS.max_degree\nnsr = np.power(10.0, -FLAGS.snr_db/20.0)\n\nfrom directory import create_result_folder, find_model_folder\nmodel_origin = find_model_folder(FLAGS, 'dpg_policy')\ncritic_origin = find_model_folder(FLAGS, 'critic')\n\ndef random_graph(size, k=20, p=0.25, gtype='grp', gseed=None):\n    if gtype == 'grp':\n        graph = nx.gaussian_random_partition_graph(size, k, min(7, k), p, max(0.1, p/3.0), seed=gseed)\n    elif gtype == 'ws':\n        graph = nx.connected_watts_strogatz_graph(size, k, p, tries=1000, seed=gseed)\n    elif gtype == 'er':\n        graph = nx.generators.random_graphs.fast_gnp_random_graph(size, float(k) / float(size))\n    elif gtype == 'ba':\n        graph = nx.generators.random_graphs.barabasi_albert_graph(size, int(np.round(k * p)))\n    else:\n        raise ValueError('Unsupported graph type')\n    wts = np.random.uniform(0.0, 1.0, (size,))\n    for u in graph:\n        graph.nodes[u]['weight'] = wts[u]\n    adj = nx.adjacency_matrix(graph, nodelist=list(range(size)), weight=None)\n    return graph, adj, wts\n\n# Main training loop\nfor epoch in range(FLAGS.epochs):\n    losses = []\n    losses_crt = []\n    cnt = 0\n    f_ct = 0\n    p_corrs = []\n    p_ratios = []\n    z_means = []\n    z_stds = []\n    newtime = time.time()\n\n    for id in np.random.permutation(2000):\n        size = np.random.choice(gsizes)\n        k = np.random.randint(10, 30)\n        p = np.random.uniform(0.15, 0.35)\n        seed = id+epoch*100\n        graph, adj, wts = random_graph(size=size, k=k, p=p, gtype=FLAGS.gtype)\n        adj_0 = adj.copy()\n        nn = adj_0.shape[0]\n        mwds_0, gray_0, total_wt_0 = heuristic_func(adj, wts)\n\n        state, zs_t = dqn_agent.foo_train(adj_0, wts, train=True)\n        zs_np = zs_t.numpy()\n        if dqn_agent.flags.diver_num == 2:\n            gcn_wts = zs_np[:, 0].flatten() * wts.flatten() + zs_np[:, 1].flatten()\n        else:\n            gcn_wts = np.multiply(zs_np.flatten(), wts.flatten())\n        top_wts = np.clip(gcn_wts, a_min=0.0, a_max=None)\n        mwds_i, gray_i, _ = heuristic_func(adj_0, top_wts)\n        total_wt_i = np.sum(wts[list(mwds_i)])\n        ind_vec, apu_avg = dqn_agent.predict_train(adj_0, zs_t, state, n_samples=n_samples)\n        p_ratio = total_wt_i/total_wt_0\n        p_ratios.append(p_ratio)\n        p_corrs.append(apu_avg)\n        z_means.append(np.mean(zs_t.numpy()))\n        z_stds.append(np.std(zs_t.numpy()))\n        f_ct += 1\n        if cnt < batch_size - 1:\n            cnt += 1\n            continue\n        else:\n            cnt = 0\n            runtime = time.time() - newtime\n            newtime = time.time()\n\n        test_ratio = []\n        test_ratio2 = []\n        for j in range(test_len):\n            graph, adj_1, wts_1, nn, k, p = test_instances[j]\n            state, zs_t = dqn_agent.foo_train(adj_1, wts_1, train=False)\n            zs_np = zs_t.numpy()\n            if dqn_agent.flags.diver_num == 2:\n                gcn_wts = zs_np[:, 0].flatten() * wts_1.flatten() + zs_np[:, 1].flatten()\n            else:\n                gcn_wts = np.multiply(zs_np.flatten(), wts_1.flatten())\n            top_wts = np.clip(gcn_wts, a_min=0.0, a_max=None)\n            mwds_0, gray_0, total_wt_0 = heuristic_func(adj_1, wts_1)\n            mwds_i, gray_i, _ = heuristic_func(adj_1, top_wts)\n            total_wt_i = np.sum(wts_1[list(mwds_i)])\n            test_ratio.append(total_wt_i / total_wt_0)\n\n        if np.mean(test_ratio) < best_ratio:\n            dqn_agent.save(os.path.join(model_origin, 'cp-{epoch:04d}.ckpt'.format(epoch=epoch)))\n            dqn_agent.save_critic(os.path.join(critic_origin, 'cp-{epoch:04d}.ckpt'.format(epoch=epoch)))\n            best_ratio = np.mean(test_ratio)\n\n        loss = dqn_agent.replay(batch_size)\n        losses.append(loss)\n        loss_crt = dqn_agent.replay_crt(batch_size)\n        losses_crt.append(loss_crt)\n\n        tr_factor = -np.nanmean(test_ratio)/loss\n        if tr_factor > tr_best:\n            tr_best = tr_factor\n        tr_dive = (tr_factor - tr_best)/tr_best\n\n        print(\"Epoch: {}\".format(epoch),\n              \"ID: %03d\" % f_ct,\n              \"Model: Actor\",\n              \"Train_Ratio: {:.4f}\".format(np.mean(p_ratios)),\n              \"Test_Ratio: {:.4f}\".format(np.mean(test_ratio)),\n              \"Loss: {:.4f}\".format(loss),\n              \"Corr: {:.4f}\".format(np.mean(p_corrs)),\n              \"L_crt: {:.4f}\".format(loss_crt),\n              \"Track: {:.4f}\".format(tr_factor),\n              \"runtime: {:.2f}\".format(runtime),\n              \"z_std: {:.3f}\".format(np.nanmean(z_stds)),\n              \"z_avg: {:.3f}\".format(np.nanmean(z_means)))\n        p_ratios = []\n        z_means = []\n        z_stds = []\n        p_corrs = []\n\n    loss_vec.append(np.mean(losses))\nprint(loss_vec)\n\n# ==========================================\n# File: mwis_gcn_call_twin.py\n# Function/Context: DQNAgent\n# ==========================================\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nimport os\nimport shutil\nsys.path.append( '%s/gcn' % os.path.dirname(os.path.realpath(__file__)) )\nimport time\nimport random\nimport scipy.io as sio\nimport numpy as np\nimport scipy.sparse as sp\nfrom multiprocessing import Queue\nfrom copy import deepcopy\nimport networkx as nx\nfrom scipy.stats.stats import pearsonr\n\nimport tensorflow as tf\nfrom collections import deque\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom gcn.utils import *\nfrom runtime_config import flags, FLAGS\n# Settings (FLAGS)\nfrom heuristics import *\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Dropout, Input\nfrom tensorflow.keras.losses import MeanSquaredError\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam, SGD, schedules\nfrom tensorflow.keras.regularizers import l2\n\nfrom spektral.data.loaders import SingleLoader\nfrom spektral.datasets.citation import Citation\nfrom spektral.layers import ChebConv\nfrom spektral.transforms import LayerPreprocess\nfrom solver_base_tf2 import Solver\ntf.config.run_functions_eagerly(True)\n\n\nclass DQNAgent(Solver):\n    def __init__(self, input_flags, memory_size=5000):\n        super(DQNAgent, self).__init__(input_flags, memory_size)\n        self.flags = input_flags\n        self.num_supports = 1 + self.flags.max_degree\n        self.l2_reg = 5e-4\n        self.model = self._build_model()\n        self.memory_crt = deque(maxlen=memory_size)\n        self.mse = MeanSquaredError()\n        self.critic = self._build_critic(num_layer=5)\n\n    def _build_model(self):\n        # Neural Net for Actor Model\n        x_in = Input(shape=(self.feature_size,), dtype=tf.float64, name=\"x_in\")\n        a_in = Input((None, ), sparse=True, dtype=tf.float64, name=\"a_in\")\n\n        gc_l = x_in\n        for l in range(self.flags.num_layer):\n            if l < self.flags.num_layer - 1:\n                act = \"leaky_relu\"\n                output_dim = self.flags.hidden1\n            else:\n                act = \"leaky_relu\"\n                output_dim = self.flags.diver_num\n            do_l = Dropout(self.flags.dropout, dtype='float64')(gc_l)\n            gc_l = ChebConv(\n                output_dim, K=self.num_supports, activation=act,\n                kernel_regularizer=l2(self.l2_reg),\n                use_bias=False,\n                dtype='float64'\n            )([do_l, a_in])\n\n        # Build model\n        model = Model(inputs=[x_in, a_in], outputs=gc_l)\n        if self.flags.learning_decay == 1.0:\n            self.optimizer = Adam(learning_rate=self.learning_rate)\n        else:\n            lr_schedule = schedules.ExponentialDecay(\n                initial_learning_rate=self.learning_rate,\n                decay_steps=200,\n                decay_rate=self.flags.learning_decay)\n            self.optimizer = Adam(learning_rate=lr_schedule)\n        model.summary()\n        return model\n\n    def _build_critic(self, num_layer=3, dropout=0.0):\n        # Neural Net for Critic Model\n        x_in = Input(shape=(self.flags.diver_num+1,), dtype=tf.float64, name=\"x_in\")\n        a_in = Input((None, ), sparse=True, dtype=tf.float64, name=\"a_in\")\n\n        gc_l = (x_in - tf.reduce_mean(x_in))/(6*tf.math.reduce_std(x_in)) + 0.5\n        for l in range(num_layer):\n            if l < num_layer - 1:\n                act = \"leaky_relu\"\n                output_dim = 64\n            else:\n                act = \"softmax\"\n                output_dim = 2\n            do_l = Dropout(dropout, dtype='float64')(gc_l)\n            gc_l = ChebConv(\n                output_dim, K=self.num_supports, activation=act,\n                kernel_regularizer=l2(self.l2_reg),\n                use_bias=True,\n                dtype='float64'\n            )([do_l, a_in])\n\n        # Build model\n        model = Model(inputs=[x_in, a_in], outputs=gc_l)\n        if self.flags.learning_decay == 1.0:\n            self.opt_crt = Adam(learning_rate=0.0001)\n        else:\n            lr_schedule = schedules.ExponentialDecay(\n                initial_learning_rate=0.0001,\n                decay_steps=200,\n                decay_rate=self.flags.learning_decay)\n            self.opt_crt = Adam(learning_rate=lr_schedule)\n        model.summary()\n        return model\n\n    def load_critic(self, name):\n        ckpt = tf.train.latest_checkpoint(name)\n        if ckpt:\n            self.critic.load_weights(ckpt)\n            print('Critic loaded ' + ckpt)\n\n    def load(self, name):\n        ckpt = tf.train.latest_checkpoint(name)\n        if ckpt:\n            self.model.load_weights(ckpt)\n            print('Actor loaded ' + ckpt)\n\n    def save(self, checkpoint_path):\n        self.model.save_weights(checkpoint_path)\n\n    def save_critic(self, checkpoint_path):\n        self.critic.save_weights(checkpoint_path)\n\n    def makestate(self, adj, wts_nn):\n        reduced_nn = wts_nn.shape[0]\n        features = np.ones([reduced_nn, self.feature_size])\n        support = simple_polynomials(adj, self.flags.max_degree)\n        state = {\"features\": features, \"support\": support[1]}\n        return state\n\n    def memorize_crt(self, grad, loss, reward):\n        self.memory_crt.append((grad.copy(), loss, reward))\n\n    def predict(self, state):\n        x_in = tf.convert_to_tensor(state[\"features\"], dtype=tf.float64)\n        coord, values, shape = state[\"support\"]\n        a_in = tf.sparse.SparseTensor(coord, values, shape)\n        act_values = self.model([x_in, a_in])\n        return act_values, np.argmax(act_values.numpy())\n\n    def act(self, state, train, explore=0.0):\n        act_values, action = self.predict(state)\n        if explore > 0.001:\n            noise = tf.random.uniform(act_values.shape, -explore, explore, dtype=act_values.dtype)\n            act_values += noise\n        return act_values, action  # returns action\n\n    def predict_critic(self, z_out, state):\n        x_in = tf.convert_to_tensor(state[\"features\"], dtype=tf.float64)\n        coord, values, shape = state[\"support\"]\n        a_in = tf.sparse.SparseTensor(coord, values, shape)\n        x_in = tf.concat([x_in, z_out], axis=1)\n        sch_pred = self.critic([x_in, a_in])\n        return sch_pred\n\n    def replay(self, batch_size):\n        if len(self.memory) < batch_size:\n            return float('NaN')\n        self.reward_mem.clear()\n        minibatch = random.sample(self.memory, batch_size)\n        losses = []\n        for grad, _, _, loss, _ in minibatch:\n            self.optimizer.apply_gradients(zip(grad, self.model.trainable_weights))\n            losses.append(loss)\n\n        self.memory.clear()\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n        return np.nanmean(losses)\n\n    def replay_crt(self, batch_size):\n        if len(self.memory_crt) < batch_size:\n            return float('NaN')\n        minibatch = random.sample(self.memory_crt, batch_size)\n        losses = []\n        for grad, loss, _ in minibatch:\n            self.opt_crt.apply_gradients(zip(grad, self.critic.trainable_weights))\n            losses.append(loss)\n\n        self.memory_crt.clear()\n        return np.nanmean(losses)\n\n    def solve_mwis(self, adj_0, wts_0, train=False, grd=1.0):\n        \"\"\"\n        GCN followed by LGS\n        \"\"\"\n        adj = adj_0.copy()\n        wts_nn = np.reshape(wts_0, (wts_0.shape[0], self.flags.feature_size))\n\n        # GCN\n        state = self.makestate(adj, wts_nn)\n        act_vals_t, act = self.act(state, train)\n        act_vals = act_vals_t.numpy()\n\n        if self.flags.predict == 'mwis':\n            gcn_wts = np.multiply(act_vals.flatten(), wts_nn.flatten())\n        else:\n            gcn_wts = act_vals.flatten()\n\n        mwis, _ = local_greedy_search(adj, gcn_wts)\n        solu = list(mwis)\n        mwis_rt = mwis\n        total_wt = np.sum(wts_nn[solu, 0])\n        return mwis_rt, total_wt\n\n    def foo_train(self, adj_0, wts_0, train=False):\n        adj = adj_0.copy()\n        nn  = wts_0.shape[0]\n        wts_nn = np.reshape(wts_0, (nn, FLAGS.feature_size))\n        ones = np.ones_like(wts_nn)\n\n        # GCN\n        with tf.GradientTape() as g:\n            g.watch(self.model.trainable_weights)\n            state = self.makestate(adj, ones)\n            act_val, act = self.act(state, train, explore=0.03)\n            act_val_norm = act_val\n            sch_pred = self.predict_critic(act_val_norm, state)\n            regularization_loss = tf.reduce_sum(self.model.losses)\n            obj_fn = -tf.reduce_mean(sch_pred[:, 0])\n            obj_fn = obj_fn + regularization_loss\n        if train: # place gradient computing outside GradientTape\n            gradients = g.gradient(obj_fn, self.model.trainable_weights)\n            self.memorize(gradients, [], [], obj_fn.numpy(), 0)\n        return state, act_val\n\n    def predict_train(self, adj_0, zs_0, state, n_samples=1, z_std=0.15):\n        \"\"\"\n        GCN followed by LGS\n        wts_0: topology weighted utility\n        \"\"\"\n        adj = adj_0.copy()\n        zs_nn = zs_0.numpy()\n        nn = zs_nn.shape[0]\n\n        # GCN\n        with tf.GradientTape() as g:\n            g.watch(self.critic.trainable_weights)\n            sch_pred = self.predict_critic(zs_0, state)\n\n            ind_vec = np.zeros_like(sch_pred.numpy())\n            apu_avg = 0.0\n            for i in range(n_samples):\n                wts = np.random.uniform(0, 1, size=(nn, 1))\n                zs_i = zs_nn + np.random.uniform(-0.5*z_std, 0.5*z_std, size=(nn, 1))\n                if FLAGS.predict == 'mwis':\n                    gcn_wts = np.multiply(zs_i.flatten(), wts.flatten())\n                else:\n                    gcn_wts = zs_i.flatten()\n\n                mwis, _ = local_greedy_search(adj, gcn_wts)\n                solu = list(mwis)\n                total_wt = np.sum(wts[solu, 0])\n                ind_vec[solu, 0] += wts[solu, 0]/float(n_samples)\n\n            corr, pval = pearsonr(ind_vec[:, 0], sch_pred[:, 0].numpy())\n            reward = corr\n            y_target = tf.convert_to_tensor(ind_vec, dtype=tf.float64)\n            regularization_loss = tf.reduce_sum(self.critic.losses)\n            loss_value = tf.sqrt(self.mse(y_target, sch_pred)) + regularization_loss\n        gradients = g.gradient(loss_value, self.critic.trainable_weights)\n        self.memorize_crt(gradients, loss_value.numpy(), reward)\n        return ind_vec, reward\n\n# ==========================================\n# File: mwis_gcn_train_twin.py\n# Function/Context: \n# ==========================================\nimport sys\nimport os\nimport shutil\nsys.path.append('%s/gcn' % os.path.dirname(os.path.realpath(__file__)))\n\nimport time\nimport random\nimport scipy.io as sio\nimport numpy as np\nimport scipy.sparse as sp\nfrom multiprocessing import Queue\nfrom copy import deepcopy\nfrom scipy.stats.stats import pearsonr, linregress\n\nimport tensorflow as tf\nfrom collections import deque\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom gcn.utils import *\nfrom runtime_config import flags, FLAGS\nfrom heuristics import *\n\nflags.DEFINE_string('test_datapath', './data/ER_Graph_Uniform_NP20_test', 'test dataset')\nflags.DEFINE_integer('ntrain', 1, 'Number of units in hidden layer 1.')\nflags.DEFINE_integer('nvalid', 100, 'Number of outputs.')\nfrom mwis_gcn_call_twin import DQNAgent\n\n# Get a list of available GPUs\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)\n# Set the number of GPUs to use\nnum_gpus = len(gpus)\n# Set up a MirroredStrategy to use all available GPUs\nif num_gpus > 1:\n    strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:%d\" % i for i in range(num_gpus)])\nelse:\n    strategy = tf.distribute.get_strategy() # default strategy\n# Define and compile your model within the strategy scope\nwith strategy.scope():\n    dqn_agent = DQNAgent(FLAGS, 5000)\n\n# test data path\ndata_path = FLAGS.datapath\ntest_datapath = FLAGS.test_datapath\nval_mat_names = sorted(os.listdir(data_path))\ntest_mat_names = sorted(os.listdir(test_datapath))\n\n# Some preprocessing\nnoout = min(FLAGS.diver_num, FLAGS.diver_out) # number of outputs\ntime_limit = FLAGS.timeout  # time limit for searching\nbackoff_thresh = 1 - FLAGS.backoff_prob\n\nnum_supports = 1 + FLAGS.max_degree\nnsr = np.power(10.0, -FLAGS.snr_db/20.0)\n\nfrom directory import create_result_folder, find_model_folder\nmodel_origin = find_model_folder(FLAGS, 'dqn')\ncritic_origin = find_model_folder(FLAGS, 'critic')\n\ntry:\n    dqn_agent.load_critic(critic_origin)\nexcept:\n    print(\"Unable to load {}\".format(critic_origin))\n\ntry:\n    dqn_agent.load(model_origin)\nexcept:\n    print(\"Unable to load {}\".format(model_origin))\n\nbest_IS_vec = []\nloss_vec = []\nresults = pd.DataFrame([], columns=[\"data\", \"p\"])\ncsvname = \"./output/{}_{}_train_foo.csv\".format(model_origin.split('/')[-1], test_datapath.split('/')[-1])\n\nepislon_reset = [5, 10, 15, 20]\nepislon_val = 1.0\neval_size = FLAGS.nvalid\nn_samples = FLAGS.ntrain\nbest_ratio = 1.0\nlast_ap = 1.0\nbatch_size = 100\ntr_best = 0\nfor epoch in range(FLAGS.epochs):\n    losses = []\n    losses_crt = []\n    cnt = 0\n    f_ct = 0\n    q_totals = []\n    p_ratios = []\n    z_means = []\n    p_corrs = []\n    newtime = time.time()\n    for id in np.random.permutation(len(val_mat_names)):\n        best_IS_num = -1\n        mat_contents = sio.loadmat(data_path + '/' + val_mat_names[id])\n        adj_0 = mat_contents['adj']\n        nn = adj_0.shape[0]\n        wts = np.random.uniform(0, 1, size=(nn, 1))\n        start_time = time.time()\n        _, greedy_util = greedy_search(adj_0, wts)\n        state, zs_t = dqn_agent.foo_train(adj_0, wts, train=True)\n        mwis, ss_util = dqn_agent.solve_mwis(adj_0, wts, train=False, grd=greedy_util)\n        zn_t = 0.5 + (zs_t - tf.reduce_mean(zs_t))\n        ind_vec, apu_avg = dqn_agent.predict_train(adj_0, zs_t, state, n_samples=n_samples)\n        p_ratio = ss_util.flatten()/greedy_util.flatten()\n        solu = list(mwis)\n        q_totals.append(len(solu))\n        p_ratios.append(p_ratio[0])\n        z_means.append(np.mean(zs_t.numpy()))\n        p_corrs.append(apu_avg)\n        f_ct += 1\n        if cnt < batch_size - 1:\n            cnt += 1\n            continue\n        else:\n            cnt = 0\n            runtime = time.time() - newtime\n            newtime = time.time()\n\n        test_ratio = []\n        test_ratio2 = []\n        test_len = len(test_mat_names)\n        for j in range(test_len):\n            mat_contents = sio.loadmat(test_datapath + '/' + test_mat_names[j % test_len])\n            adj_0 = mat_contents['adj']\n            wts = mat_contents['weights'].transpose()\n            nn = adj_0.shape[0]\n            _, greedy_util = greedy_search(adj_0, wts)\n            bsf_q = []\n            q_ct = 0\n            res_ct = 0\n            out_id = -1\n            _, best_IS_util = dqn_agent.solve_mwis(adj_0, wts, train=False)\n            test_ratio.append(best_IS_util / greedy_util)\n\n        if np.mean(test_ratio) > best_ratio:\n            dqn_agent.save(os.path.join(model_origin, 'cp-{epoch:04d}.ckpt'.format(epoch=epoch)))\n            dqn_agent.save_critic(os.path.join(critic_origin, 'cp-{epoch:04d}.ckpt'.format(epoch=epoch)))\n            best_ratio = np.mean(test_ratio)\n\n        loss = dqn_agent.replay(batch_size)\n        loss_crt = dqn_agent.replay_crt(batch_size)\n        if loss is None:\n            loss = float('NaN')\n        losses.append(loss)\n\n        tr_factor = -np.nanmean(test_ratio)/loss\n        if tr_factor > tr_best:\n            tr_best = tr_factor\n        tr_dive = (tr_factor - tr_best)/tr_best\n\n        print(\"Epoch: {}\".format(epoch),\n              \"ID: %03d\" % f_ct,\n              \"Model: Actor\",\n              \"Train_Ratio: {:.4f}\".format(np.mean(p_ratios)),\n              \"Test_Ratio: {:.4f}\".format(np.mean(test_ratio)),\n              \"Loss: {:.4f}\".format(loss),\n              \"Corr: {:.4f}\".format(np.mean(p_corrs)),\n              \"L_Avg: {:.4f}\".format(np.mean(loss_crt)),\n              \"Track: {:.4f}\".format(tr_factor),\n              \"runtime: {:.2f}\".format(runtime),\n              \"z_avg: {:.3f}\".format(np.nanmean(z_means)))\n        p_ratios = []\n        z_means = []\n        p_corrs = []\n\n    loss_vec.append(np.mean(losses))\nprint(loss_vec)\n\n# ==========================================\n# File: solver_base_tf2.py\n# Function/Context: Solver\n# ==========================================\nimport sys\nimport os\nimport shutil\n\nsys.path.append('%s/gcn' % os.path.dirname(os.path.realpath(__file__)))\nimport time\nimport random\nimport scipy.io as sio\nimport numpy as np\nimport scipy.sparse as sp\nfrom multiprocessing import Queue\nfrom copy import deepcopy\nimport networkx as nx\n\nimport tensorflow as tf\nfrom collections import deque\nfrom gcn.utils import *\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nfrom runtime_config import flags, FLAGS\n# Settings (FLAGS)\nfrom heuristics_mwcds import *\n\nif not hasattr(flags.FLAGS, 'epsilon'):\n    flags.DEFINE_float('epsilon', 1.0, 'initial exploration rate')\nif not hasattr(flags.FLAGS, 'epsilon_min'):\n    flags.DEFINE_float('epsilon_min', 0.001, 'minimal exploration rate')\nif not hasattr(flags.FLAGS, 'epsilon_decay'):\n    flags.DEFINE_float('epsilon_decay', 0.985, 'exploration rate decay per replay')\nif not hasattr(flags.FLAGS, 'gamma'):\n    flags.DEFINE_float('gamma', 1.0, 'gamma')\n\n# Some preprocessing\nnum_supports = 1 + FLAGS.max_degree\nnsr = np.power(10.0, -FLAGS.snr_db / 20.0)\n\n\nclass Solver(object):\n    def __init__(self, input_flags, memory_size):\n        self.feature_size = input_flags.feature_size\n        self.memory = deque(maxlen=memory_size)\n        self.reward_mem = deque(maxlen=memory_size)\n        self.flags = input_flags\n        self.delta = 0.000001  # prevent empty solution\n        self.gamma = self.flags.gamma  # discount rate\n        self.epsilon = self.flags.epsilon  # exploration rate\n        self.epsilon_min = self.flags.epsilon_min\n        self.epsilon_decay = self.flags.epsilon_decay\n        self.learning_rate = self.flags.learning_rate\n        self.sess = None\n        self.saver = None\n\n    def _build_model(self):\n        raise NotImplementedError\n\n    def makestate(self, adj, wts_nn):\n        reduced_nn = wts_nn.shape[0]\n        norm_wts = np.amax(wts_nn) + 1e-9\n        if self.flags.predict == 'mwis':\n            features = np.ones([reduced_nn, self.flags.feature_size])\n        else:\n            features = np.multiply(np.ones([reduced_nn, self.flags.feature_size]), wts_nn / norm_wts)\n        features_raw = features.copy()\n        features = sp.lil_matrix(features)\n        if self.flags.predict == 'mwis':\n            features = preprocess_features(features)\n        else:\n            features = sparse_to_tuple(features)\n        support = simple_polynomials(adj, self.flags.max_degree)\n        state = {\"features\": features, \"support\": support, \"features_raw\": features_raw}\n        return state\n\n    def act(self, state, train):\n        raise NotImplementedError\n\n    def predict(self, state):\n        raise NotImplementedError\n\n    def memorize(self, state, act_vals, solu, next_state, reward):\n        self.memory.append((state.copy(), act_vals.copy(), solu.copy(), next_state.copy(), reward))\n        self.reward_mem.append(reward)\n\n    def load(self, name):\n        ckpt = tf.train.get_checkpoint_state(name)\n        if ckpt:\n            with self.sess.as_default():\n                self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n            print('loaded ' + ckpt.model_checkpoint_path)\n\n    def save(self, name):\n        with self.sess.as_default():\n            self.saver.save(self.sess, os.path.join(name, \"model.ckpt\"))\n\n    def mellowmax(self, q_vec, omega, beta):\n        c = np.max(q_vec)\n        a_size = np.size(q_vec)\n        mellow = c + np.log(np.sum(np.exp(omega * (q_vec - c))) / a_size) / omega\n        return mellow\n\n    def utility(self, adj_0, wts_0, train=False):\n        \"\"\"\n        GCN for per utility function\n        \"\"\"\n        adj = adj_0.copy()\n        wts_nn = np.reshape(wts_0, (wts_0.shape[0], self.flags.feature_size))\n\n        # GCN\n        state = self.makestate(adj, wts_nn)\n        act_vals, _ = self.act(state, train)\n\n        gcn_wts = act_vals.numpy()\n\n        return gcn_wts, state\n\n\n# use gpu 0\nos.environ['CUDA_VISIBLE_DEVICES'] = str(0)\n\n# Initialize session\nconfig = tf.compat.v1.ConfigProto()\nconfig.gpu_options.allow_growth = True\n\n# ==========================================\n# File: steiner_gcn_call_twin.py\n# Function/Context: DPGAgent\n# ==========================================\nimport sys\nimport os\nimport shutil\nsys.path.append( '%s/gcn' % os.path.dirname(os.path.realpath(__file__)) )\nimport time\nimport random\nimport scipy.io as sio\nimport numpy as np\nimport scipy.sparse as sp\nfrom multiprocessing import Queue\nfrom copy import deepcopy\nimport networkx as nx\nfrom scipy.stats.stats import pearsonr\n\nimport tensorflow as tf\nfrom collections import deque\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom gcn.utils import *\nfrom runtime_config import flags, FLAGS\nfrom heuristics_mwcds import *\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Dropout, Input\nfrom tensorflow.keras.losses import MeanSquaredError\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam, SGD, schedules\nfrom tensorflow.keras.regularizers import l2\n\nfrom spektral.data.loaders import SingleLoader\nfrom spektral.datasets.citation import Citation\nfrom spektral.layers import ChebConv\nfrom spektral.transforms import LayerPreprocess\nfrom solver_base_tf2 import Solver\ntf.config.run_functions_eagerly(True)\n\n# Some preprocessing\nnsr = np.power(10.0, -FLAGS.snr_db/20.0)\n\n# heuristic_func = node_weight_steiner_set\nheuristic_func = nwst_sph\n\n\nclass DPGAgent(Solver):\n    def __init__(self, input_flags, memory_size=5000):\n        super(DPGAgent, self).__init__(input_flags, memory_size)\n        self.flags = input_flags\n        self.num_supports = 1 + self.flags.max_degree\n        self.l2_reg = 5e-4\n        self.model = self._build_model()\n        self.memory_crt = deque(maxlen=memory_size)\n        self.mse = MeanSquaredError()\n        self.critic = self._build_critic(num_layer=5)\n        self.critic.trainable = True\n        self.model.trainable = True\n\n    def _build_model(self):\n        # Neural Net for Actor Model\n        x_in = Input(shape=(self.feature_size+2,), dtype=tf.float64, name=\"x_in\")\n        a_in = Input((None, ), sparse=True, dtype=tf.float64, name=\"a_in\")\n\n        gc_l = x_in\n        for l in range(self.flags.num_layer):\n            if l < self.flags.num_layer - 1:\n                act = \"leaky_relu\"\n                output_dim = self.flags.hidden1\n            else:\n                output_dim = self.flags.diver_num\n                if output_dim == 1:\n                    act = \"sigmoid\"\n                elif output_dim == 2:\n                    act = \"tanh\"\n                else:\n                    raise ValueError(\"unsupported diver_num {}\".format(output_dim))\n\n            do_l = Dropout(self.flags.dropout, dtype='float64')(gc_l)\n            gc_l = ChebConv(\n                output_dim, K=self.num_supports, activation=act,\n                kernel_regularizer=l2(self.l2_reg),\n                use_bias=True,\n                dtype='float64'\n            )([do_l, a_in])\n\n        # Build model\n        model = Model(inputs=[x_in, a_in], outputs=gc_l)\n        if self.flags.learning_decay == 1.0:\n            self.optimizer = Adam(learning_rate=self.learning_rate)\n        else:\n            lr_schedule = schedules.ExponentialDecay(\n                initial_learning_rate=self.learning_rate,\n                decay_steps=200,\n                decay_rate=self.flags.learning_decay)\n            self.optimizer = Adam(learning_rate=lr_schedule)\n        model.summary()\n        return model\n\n    def _build_critic(self, num_layer=3, dropout=0.0):\n        # Neural Net for Critic Model\n        x_in = Input(shape=(self.flags.diver_num+3,), dtype=tf.float64, name=\"x_in\")\n        a_in = Input((None, ), sparse=True, dtype=tf.float64, name=\"a_in\")\n\n        gc_l = x_in\n        for l in range(num_layer):\n            if l < num_layer - 1:\n                act = \"leaky_relu\"\n                output_dim = 32\n            else:\n                act = \"softmax\"\n                output_dim = 2\n            do_l = Dropout(dropout, dtype='float64')(gc_l)\n            gc_l = ChebConv(\n                output_dim, K=self.num_supports, activation=act,\n                kernel_regularizer=l2(self.l2_reg),\n                use_bias=True,\n                dtype='float64'\n            )([do_l, a_in])\n\n        # Build model\n        model = Model(inputs=[x_in, a_in], outputs=gc_l)\n        if self.flags.learning_decay == 1.0:\n            self.opt_crt = Adam(learning_rate=0.0001)\n        else:\n            lr_schedule = schedules.ExponentialDecay(\n                initial_learning_rate=0.0001,\n                decay_steps=200,\n                decay_rate=self.flags.learning_decay)\n            self.opt_crt = Adam(learning_rate=lr_schedule)\n        model.summary()\n        return model\n\n    def foo_train(self, adj_0, wts_0, terminals, train=False):\n        adj = adj_0.copy()\n        nn  = wts_0.shape[0]\n        wts_nn = np.reshape(wts_0, (nn, FLAGS.feature_size))\n        gsignal = np.ones(shape=(nn, FLAGS.feature_size + 2), dtype=np.float)\n        # one hot encoding of the terminals\n        term_list = list(terminals)\n        gsignal[:, 1] = 0\n        gsignal[term_list, 1] = 1\n        gsignal[term_list, 2] = 0\n\n        # GCN\n        with tf.GradientTape() as g:\n            g.watch(self.model.trainable_weights)\n            state = self.makestate(adj, gsignal)\n            act_val, act = self.act(state, train, explore=0.0)\n            act_val_norm = act_val\n            sch_pred = self.predict_critic(act_val_norm, state)\n            regularization_loss = tf.reduce_sum(self.model.losses)\n            obj_fn = tf.reduce_sum(sch_pred[:, 0])\n            obj_fn = obj_fn + regularization_loss\n        if train:\n            gradients = g.gradient(obj_fn, self.model.trainable_weights)\n            self.memorize(gradients, [], [], obj_fn.numpy(), 0)\n        return state, act_val\n\n    def predict_train(self, adj, terminals, zs_0, state, n_samples=1, z_std=0.15):\n        \"\"\"\n        GCN followed by LGS\n        wts_0: topology weighted utility\n        \"\"\"\n        zs_nn = zs_0.numpy()\n        nn = zs_nn.shape[0]\n        adj_0 = adj.copy()\n\n        # GCN\n        with tf.GradientTape() as g:\n            g.watch(self.critic.trainable_weights)\n            sch_pred = self.predict_critic(zs_0, state)\n\n            ind_vec = np.zeros((nn, 2))\n            apu_avg = 0.0\n            for i in range(n_samples):\n                wts = np.random.uniform(0.00, 1.0, size=(nn, 1))\n                zs_i = zs_nn + np.random.uniform(-0.5*z_std, 0.5*z_std, size=(nn, self.flags.diver_num))\n                if FLAGS.predict == 'mpy':\n                    if self.flags.diver_num == 2:\n                        gcn_wts = zs_i[:, 0].flatten() * wts.flatten() + zs_i[:, 1].flatten()\n                    else:\n                        gcn_wts = np.multiply(zs_i.flatten(), wts.flatten())\n                    gcn_wts = np.clip(gcn_wts, a_min=0.0, a_max=None)\n                else:\n                    gcn_wts = zs_i.flatten()\n                mwcds_i, total_wt_i = heuristic_func(adj_0, gcn_wts, terminals)\n                solu = list(mwcds_i)\n                ind_vec[solu, 0] += wts[solu, 0]/(float(n_samples))\n\n            corr, pval = pearsonr(ind_vec[:, 0], sch_pred[:, 0].numpy())\n            reward = corr\n            y_target = tf.convert_to_tensor(ind_vec, dtype=tf.float64)\n            regularization_loss = tf.reduce_sum(self.critic.losses)\n            loss_value = tf.sqrt(self.mse(y_target, sch_pred)) + regularization_loss\n        gradients = g.gradient(loss_value, self.critic.trainable_weights)\n        self.memorize_crt(gradients, loss_value.numpy(), reward)\n        return ind_vec, reward\n\n# ==========================================\n# File: steiner_gcn_train_twin.py\n# Function/Context: \n# ==========================================\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nimport os\nimport shutil\nsys.path.append( '%s/gcn' % os.path.dirname(os.path.realpath(__file__)) )\n\nimport time\nimport random\nimport scipy.io as sio\nimport numpy as np\nimport scipy.sparse as sp\nfrom multiprocessing import Queue\nfrom copy import deepcopy\nfrom scipy.stats.stats import pearsonr, linregress\nfrom scipy.spatial import distance_matrix\n\nimport tensorflow as tf\nimport networkx as nx\nfrom collections import deque\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom gcn.utils import *\n# Settings (FLAGS)\nfrom runtime_config import flags, FLAGS\nfrom heuristics_mwcds import *\n\nflags.DEFINE_string('gtype', 'er', 'training graph type: er, grp, ws, ba')\nflags.DEFINE_string('test_datapath', './data/ER_Graph_Uniform_NP20_test', 'test dataset')\nflags.DEFINE_integer('ntrain', 1, 'Number of units in hidden layer 1.')\nflags.DEFINE_integer('nvalid', 100, 'Number of outputs.')\n\nfrom steiner_gcn_call_twin import DPGAgent, heuristic_func\n\n# Get a list of available GPUs\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)\n# Set the number of GPUs to use\nnum_gpus = len(gpus)\n# Set up a MirroredStrategy to use all available GPUs\nif num_gpus > 1:\n    strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:%d\" % i for i in range(num_gpus)])\nelse:\n    strategy = tf.distribute.get_strategy() # default strategy\n# Define and compile your model within the strategy scope\nwith strategy.scope():\n    dqn_agent = DPGAgent(FLAGS, 5000)\n\n# test data path\ndata_path = FLAGS.datapath\ntest_datapath = FLAGS.test_datapath\n\n# Some preprocessing\nnoout = min(FLAGS.diver_num, FLAGS.diver_out) # number of outputs\ntime_limit = FLAGS.timeout  # time limit for searching\nbackoff_thresh = 1 - FLAGS.backoff_prob\n\nnum_supports = 1 + FLAGS.max_degree\nnsr = np.power(10.0, -FLAGS.snr_db/20.0)\n\nfrom directory import create_result_folder, find_model_folder\nmodel_origin = find_model_folder(FLAGS, 'dpg_policy')\ncritic_origin = find_model_folder(FLAGS, 'critic')\n\n# # use gpu 0\n# os.environ['CUDA_VISIBLE_DEVICES'] = str(0)\n#\n# # Initialize session\n# config = tf.compat.v1.ConfigProto()\n# config.gpu_options.allow_growth = True\n\n\ndef random_graph(size, k=20, p=0.25, gtype='grp', gseed=None):\n    if gtype == 'grp':\n        graph = nx.gaussian_random_partition_graph(size, k, min(7, k), p, max(0.1, p/3.0), seed=gseed)\n    elif gtype == 'ws':\n        graph = nx.connected_watts_strogatz_graph(size, k, p, tries=1000, seed=gseed)\n    elif gtype == 'er':\n        graph = nx.generators.random_graphs.fast_gnp_random_graph(size, float(k) / float(size))\n    elif gtype == 'ba':\n        graph = nx.generators.random_graphs.barabasi_albert_graph(size, int(np.round(k * p)))\n    else:\n        raise ValueError('Unsupported graph type')\n    wts = np.random.uniform(0.00, 1.0, (size,))\n    for u in graph:\n        graph.nodes[u]['weight'] = wts[u]\n    adj = nx.adjacency_matrix(graph, nodelist=list(range(size)), weight=None)\n    return graph, adj, wts\n\n\ntry:\n    dqn_agent.load(model_origin)\nexcept:\n    print(\"Unable to load {}\".format(model_origin))\n\ntry:\n    dqn_agent.load_critic(critic_origin)\nexcept:\n    print(\"Unable to load {}\".format(model_origin))\n\ntest_len = 20\ntest_instances = []\n\ngsizes = list(range(100, 350, 50))\nsizes = np.random.choice(gsizes, size=(test_len,))\nks = np.random.randint(10, 30, size=(test_len,))\nps = np.random.uniform(0.15, 0.35, size=(test_len,))\nj = 0\nwhile len(test_instances) < test_len:\n    graph, adj_0, wts_0 = random_graph(size=sizes[j], k=ks[j], p=ps[j], gtype=FLAGS.gtype)\n    p_t = np.random.uniform(0.5, 0.9)\n    terminals = nx.maximal_independent_set(graph)\n    terminals = np.array(terminals)\n    terminals = terminals[np.random.uniform(0.0, 1.0, size=(len(terminals, ))) <= p_t]\n    terminals = set(terminals.tolist())\n    test_instances.append((graph, adj_0, wts_0, terminals, sizes[j], ks[j], ps[j]))\n    j += 1\n\n\nbest_IS_vec = []\nloss_vec = []\nresults = pd.DataFrame([], columns=[\"data\", \"p\"])\ncsvname = \"./output/{}_{}_train_foo.csv\".format(model_origin.split('/')[-1], test_datapath.split('/')[-1])\n\nepislon_reset = [5, 10, 15, 20]\nepislon_val = 1.0\neval_size = FLAGS.nvalid\nn_samples = FLAGS.ntrain\nbest_ratio = 3.0\nlast_ap = 1.0\nbatch_size = 100\ntr_best = 0\nfor epoch in range(FLAGS.epochs):\n    losses = []\n    losses_crt = []\n    cnt = 0\n    f_ct = 0\n    p_corrs = []\n    p_ratios = []\n    z_means = []\n    z_stds = []\n    newtime = time.time()\n    for id in np.random.permutation(2000):\n        # size = np.random.choice([100, 150, 200, 250])\n        # size = np.random.choice([100, 150, 200, 250, 300])\n        size = np.random.choice(gsizes)\n        k = np.random.randint(10, 30)\n        p = np.random.uniform(0.15, 0.35)\n        seed = id+epoch*100\n        graph, adj, wts = random_graph(size=size, k=k, p=p, gtype=FLAGS.gtype)\n        if not nx.is_connected(graph):\n            print(\"unconnected\")\n            continue\n        adj_0 = adj.copy()\n        nn = adj_0.shape[0]\n        p_t = np.random.uniform(0.5, 0.9)\n        terminals = nx.maximal_independent_set(graph)\n        terminals = np.array(terminals)\n        terminals = terminals[np.random.uniform(0.0, 1.0, size=(len(terminals,))) <= p_t]\n        terminals = set(terminals.tolist())\n\n        mwcds_0, _ = heuristic_func(adj, wts, terminals)\n        total_wt_0 = np.sum(wts[list(mwcds_0-terminals)])\n\n        state, zs_t = dqn_agent.foo_train(adj_0, wts, terminals, train=True)\n        zs_np = zs_t.numpy()\n        if dqn_agent.flags.diver_num == 2:\n            gcn_wts = zs_np[:, 0].flatten() * wts.flatten() + zs_np[:, 1].flatten()\n        else:\n            gcn_wts = np.multiply(zs_np.flatten(), wts.flatten())\n        top_wts = np.clip(gcn_wts, a_min=0.0, a_max=None)\n        mwcds_i, _ = heuristic_func(adj_0, top_wts, terminals)\n        total_wt_i = np.sum(wts[list(mwcds_i-terminals)])\n        ind_vec, apu_avg = dqn_agent.predict_train(adj_0, terminals, zs_t, state, n_samples=n_samples)\n        p_ratio = total_wt_i/total_wt_0\n        p_ratios.append(p_ratio)\n        p_corrs.append(apu_avg)\n        z_means.append(np.mean(zs_t.numpy()))\n        z_stds.append(np.std(zs_t.numpy()))\n        f_ct += 1\n        if cnt < batch_size - 1:\n            cnt += 1\n            continue\n        else:\n            cnt = 0\n            runtime = time.time() - newtime\n            newtime = time.time()\n\n        test_ratio = []\n        test_ratio2 = []\n        for j in range(test_len):\n            graph, adj_1, wts_1, terminals, nn, k, p = test_instances[j]\n            state, zs_t = dqn_agent.foo_train(adj_1, wts_1, terminals, train=False)\n            zs_np = zs_t.numpy()\n            if dqn_agent.flags.diver_num == 2:\n                gcn_wts = zs_np[:, 0].flatten() * wts_1.flatten() + zs_np[:, 1].flatten()\n            else:\n                gcn_wts = np.multiply(zs_np.flatten(), wts_1.flatten())\n            top_wts = np.clip(gcn_wts, a_min=0.0, a_max=None)\n            mwcds_0, _ = heuristic_func(adj_1, wts_1, terminals)\n            total_wt_0 = np.sum(wts_1[list(mwcds_0 - terminals)])\n            mwcds_i, _ = heuristic_func(adj_1, top_wts, terminals)\n            total_wt_i = np.sum(wts_1[list(mwcds_i - terminals)])\n            test_ratio.append(total_wt_i / total_wt_0)\n\n        if np.mean(test_ratio) < best_ratio:\n            dqn_agent.save(os.path.join(model_origin, 'cp-{epoch:04d}.ckpt'.format(epoch=epoch)))\n            dqn_agent.save_critic(os.path.join(critic_origin, 'cp-{epoch:04d}.ckpt'.format(epoch=epoch)))\n            best_ratio = np.mean(test_ratio)\n\n        loss = dqn_agent.replay(batch_size)\n        losses.append(loss)\n        loss_crt = dqn_agent.replay_crt(batch_size)\n        losses_crt.append(loss_crt)\n\n        tr_factor = -np.nanmean(test_ratio)/loss\n        if tr_factor > tr_best:\n            tr_best = tr_factor\n        tr_dive = (tr_factor - tr_best)/tr_best\n\n        print(\"Epoch: {}\".format(epoch),\n              \"ID: %03d\" % f_ct,\n              \"Model: Actor\",\n              \"Train_Ratio: {:.4f}\".format(np.mean(p_ratios)),\n              \"Test_Ratio: {:.4f}\".format(np.mean(test_ratio)),\n              \"Loss: {:.4f}\".format(loss),\n              \"Corr: {:.4f}\".format(np.mean(p_corrs)),\n              \"L_crt: {:.4f}\".format(loss_crt),\n              \"Track: {:.4f}\".format(tr_factor),\n              \"runtime: {:.2f}\".format(runtime),\n              \"z_std: {:.3f}\".format(np.nanmean(z_stds)),\n              \"z_avg: {:.3f}\".format(np.nanmean(z_means)),\n              flush=True\n              )\n        p_ratios = []\n        z_means = []\n        z_stds = []\n        p_corrs = []\n\n    loss_vec.append(np.mean(losses))\nprint(loss_vec)",
  "description": "Combined Analysis:\n- [agent_gdpg_util.py]: This file implements the core GDPG (Graph-based Deterministic Policy Gradient) algorithm from the paper. The GDPGAgent class contains the actor and critic graph neural networks (ChebConv layers) that learn node representations for repetitive combinatorial optimization. Key components: 1) Actor model (_build_model) outputs node scores, 2) Critic model (_build_critic) evaluates action quality, 3) Schedule method combines GCN with local greedy search (LGS) to produce feasible solutions, 4) Replay methods implement off-policy training with twin networks, 5) Actor_train and critic_train methods compute gradients via the non-differentiable heuristic (LGS) using the twin network approximation. The code directly maps to the paper's GDPG-Twin algorithm for learning reusable policies under hard constraints.\n- [heuristics_mwcds.py]: This file implements heuristic algorithms for Minimum Weighted Connected Dominating Set (MWCDS) and related problems (MWDS, Steiner Tree). The core logic aligns with the paper's focus on repetitive combinatorial optimization problems under hard constraints, specifically for graph-based problems with node weights. The heuristics (e.g., greedy_mwcds, mwds_greedy) serve as fast, non-differentiable blocks that the GDPG-Twin algorithm aims to improve by learning reusable node representations. The code includes both centralized and distributed heuristics, matching the paper's application to independent R-COPs and graph-based MDPs with distributed constraints.\n- [mwds_gcn_call_twin.py]: This file implements the GDPG-Twin algorithm for the Minimum Weight Dominating Set (MWDS) problem as described in the paper. The DPGAgent class uses an actor-critic framework where the actor (model) learns node representations via ChebConv GCN layers, and the critic (critic) approximates the outcomes of the non-differentiable heuristic (mwds_greedy). The key methods foo_train (actor update) and predict_train (critic update) implement the twin network approach: the critic predicts node selection probabilities, which are compared against heuristic solutions via Pearson correlation as reward. The code handles graph inputs, uses polynomial graph convolutions, and includes exploration noise and experience replay, aligning with the paper's GDPG-Twin methodology for repetitive COPs.\n- [mwds_gcn_train_twin.py]: This file implements the core training loop of the GDPG-Twin algorithm described in the paper. It trains an actor-critic agent (DPGAgent) for the Minimum Weight Dominating Set (MWDS) problem. The key components match the paper's description: 1) Uses actor-critic RL framework with twin networks (DPGAgent from mwds_gcn_call_twin), 2) Handles repetitive COPs with changing node weights on fixed graph topologies, 3) Integrates with non-differentiable heuristic (heuristic_func) for constraint satisfaction, 4) Implements the twin network approach where the agent learns to transform node weights (via zs_t) to improve heuristic performance, 5) Uses batch training with experience replay (replay and replay_crt methods), 6) Evaluates on test instances for validation. The code specifically addresses the independent R-COP setting where each instance is statistically independent.\n- [mwis_gcn_call_twin.py]: This file implements the GDPG-Twin algorithm for the Maximum Weight Independent Set (MWIS) problem. The DQNAgent class contains the actor-critic twin network architecture described in the paper. Key implementations include: 1) Actor network (_build_model) using ChebConv layers to learn node representations, 2) Critic network (_build_critic) that predicts heuristic outcomes, 3) solve_mwis method that combines GCN outputs with local greedy search (LGS) heuristic, 4) Training methods (foo_train, predict_train) that use the twin network to approximate non-differentiable heuristic outcomes and compute gradients via correlation with actual LGS results. The code directly implements the GDPG-Twin approach of learning reusable node representations to improve fast heuristics for repetitive COPs.\n- [mwis_gcn_train_twin.py]: This file implements the training loop for the GDPG-Twin algorithm applied to the Maximum Weight Independent Set (MWIS) problem. It demonstrates the core actor-critic training framework with twin networks for repetitive combinatorial optimization. The code:\n1. Initializes the DQNAgent (actor-critic with twin network) using TensorFlow distributed strategy\n2. Loads pre-trained models for actor and critic networks\n3. Implements the training loop that:\n   - Samples random node weights on fixed graph topologies (simulating repetitive COPs)\n   - Uses greedy heuristic as baseline\n   - Calls the agent's training methods (foo_train, solve_mwis, predict_train)\n   - Computes performance ratios between learned policy and greedy heuristic\n   - Performs periodic validation on test datasets\n   - Updates actor and critic networks via replay buffers\n   - Saves models when test performance improves\n4. Implements the key GDPG-Twin components: twin network for approximating non-differentiable heuristic outcomes, actor-critic updates, and constraint satisfaction through the MWIS heuristic wrapper.\nThe implementation directly corresponds to the paper's Algorithm 1 (GDPG-Twin) for independent R-COPs, specifically for the MWIS problem with binary variables and edge constraints.\n- [solver_base_tf2.py]: This file implements the base infrastructure for the GDPG-Twin algorithm described in the paper. The Solver class provides: 1) Graph state construction via makestate() that processes adjacency matrices and node weights into GCN-compatible features, 2) Experience replay memory for actor-critic training, 3) Core utility() method that applies GCN to graph inputs to produce node-level action values (policy outputs), 4) Exploration mechanisms (epsilon-greedy parameters), and 5) Model persistence. While the specific actor-critic networks are implemented in derived classes (via _build_model(), act(), predict()), this base class encapsulates the graph processing pipeline and reinforcement learning framework essential for the paper's approach to repetitive COPs.\n- [steiner_gcn_call_twin.py]: This file implements the GDPG-Twin algorithm from the paper. The DPGAgent class contains the actor-critic architecture with twin networks: 1) Actor model (_build_model) uses ChebConv GCN layers to learn node representations and output actions. 2) Critic model (_build_critic) predicts heuristic outcomes. The key methods foo_train() trains the actor using deterministic policy gradient, while predict_train() trains the critic by comparing predictions with actual heuristic outcomes (nwst_sph). This matches the paper's GDPG-Twin approach of learning reusable node representations to improve fast heuristics for repetitive COPs.\n- [steiner_gcn_train_twin.py]: This file implements the core training logic of the GDPG-Twin algorithm for repetitive combinatorial optimization problems. It trains an actor-critic agent (DPGAgent) that learns to modify node weights to improve the performance of a non-differentiable heuristic (heuristic_func) for the Minimum Weight Connected Dominating Set (MWCDS) problem with Steiner constraints. The training loop follows the GDPG-Twin approach: 1) Generate random graphs with terminals, 2) Use the actor network to produce weight adjustments (zs_t), 3) Apply heuristic to both original and adjusted weights, 4) Compute improvement ratio as reward signal, 5) Update actor and critic networks via replay buffers. The code implements the twin network concept through the DPGAgent's methods (foo_train, predict_train, replay, replay_crt) which approximate heuristic outcomes for gradient-based learning.",
  "dependencies": [
    "directory.find_model_folder",
    "networkx",
    "copy",
    "mwis_gcn_call_twin.DQNAgent",
    "gcn.utils.simple_polynomials",
    "directory",
    "tensorflow",
    "time",
    "igraph",
    "runtime_config",
    "steiner_gcn_call_twin.heuristic_func",
    "dwave_networkx",
    "scipy",
    "spektral",
    "heuristics_mwcds",
    "solver_base_tf2",
    "heuristics.local_greedy_search",
    "solver_base_tf2.Solver",
    "steiner_gcn_call_twin.DPGAgent",
    "directory.create_result_folder",
    "numpy",
    "heuristics",
    "pulp",
    "gcn.utils",
    "mwds_gcn_call_twin",
    "pandas",
    "tensorflow.keras"
  ]
}