{
  "file_path": "notears/linear.py, rescore_main.py",
  "function_name": "notears_linear, notears_nonlinear, golem_linear, daggnn_nonlinear, grandag_nonlinear",
  "code_snippet": "\n\n# ==========================================\n# File: notears/linear.py\n# Function/Context: notears_linear\n# ==========================================\nimport numpy as np\nimport scipy.linalg as slin\nimport scipy.optimize as sopt\nfrom scipy.special import expit as sigmoid\nimport matplotlib.pyplot as plt\nimport os\nimport tqdm as tqdm\nbeta = 0.1\nreweight_list = []\nepoch = 6\ndef notears_linear(X, lambda1, loss_type, max_iter=100, h_tol=1e-8, rho_max=1e+16, w_threshold=0.3, B_true=None):\n    \"\"\"Solve min_W L(W; X) + lambda1 ‖W‖_1 s.t. h(W) = 0 using augmented Lagrangian.\n\n    Args:\n        X (np.ndarray): [n, d] sample matrix\n        lambda1 (float): l1 penalty parameter\n        loss_type (str): l2, logistic, poisson\n        max_iter (int): max num of dual ascent steps #\n        h_tol (float): exit if |h(w_est)| <= htol\n        rho_max (float): exit if rho >= rho_max\n        w_threshold (float): drop edge if |weight| < threshold\n\n    Returns:\n        W_est (np.ndarray): [d, d] estimated DAG\n    \"\"\"\n    def _loss(W):\n        \"\"\"Evaluate value and gradient of loss.\"\"\"\n        M = X @ W  \n        if loss_type == 'l2':\n            R = X - M\n            loss = 0.5 / X.shape[0] * (R ** 2).sum()\n            G_loss = - 1.0 / X.shape[0] * X.T @ R \n\n        elif loss_type == 'logistic':\n            loss = 1.0 / X.shape[0] * (np.logaddexp(0, M) - X * M).sum()\n            G_loss = 1.0 / X.shape[0] * X.T @ (sigmoid(M) - X)\n        elif loss_type == 'poisson':\n            S = np.exp(M)\n            loss = 1.0 / X.shape[0] * (S - X * M).sum()\n            G_loss = 1.0 / X.shape[0] * X.T @ (S - X)\n        else:\n            raise ValueError('unknown loss type')\n        return loss, G_loss\n\n    def _reweighloss(W):\n        M = X @ W\n        R = X - M\n        assert loss_type in ['l2', 'logistic', 'poisson']\n        rewei_num = len(reweight_list)\n        re_matrix = np.eye(100,100)\n        for idx in reweight_list:\n            re_matrix[idx][idx] = beta\n        loss = 0.5 / X.shape[0] * ((re_matrix @ R) ** 2).sum()\n        G_loss = - 1.0 / X.shape[0] * X.T @ ((re_matrix**2) @ R)\n        # return loss, G_loss\n        return loss,G_loss\n\n    \n    def _single_loss(W):\n        \n        M = X @ W\n        assert loss_type == 'l2'\n        R = X - M\n        \n        loss = 0.5 / X.shape[0] * (R ** 2)\n        return loss\n\n    def _h(W):\n        \"\"\"Evaluate value and gradient of acyclicity constraint.\"\"\"\n        \n        E = slin.expm(W * W)  # (Zheng et al. 2018)\n        \n        if np.any(np.isnan(E)) or np.any(np.isinf(E)):\n            print('nan in expm')\n            return 1\n        h = np.trace(E) - d\n        if np.any( np.isnan(h)) or np.any(np.isinf(h)):\n            print('nan in h')\n            return 1\n        #     # A different formulation, slightly faster at the cost of numerical stability\n        #     M = np.eye(d) + W * W / d  # (Yu et al. 2019)\n        #     E = np.linalg.matrix_power(M, d - 1)\n        #     h = (E.T * M).sum() - d\n        G_h = E.T * W * 2\n        return h, G_h\n\n    def _adj(w):\n        \"\"\"Convert doubled variables ([2 d^2] array) back to original variables ([d, d] matrix).\"\"\"\n        return (w[:d * d] - w[d * d:]).reshape([d, d])\n\n    def _func(w):\n        \"\"\"Evaluate value and gradient of augmented Lagrangian for doubled variables ([2 d^2] array).\"\"\"\n        W = _adj(w)\n        loss, G_loss = _loss(W)\n        h, G_h = _h(W)\n        obj = loss + 0.5 * rho * h * h + alpha * h + lambda1 * w.sum()\n        G_smooth = G_loss + (rho * h + alpha) * G_h\n        g_obj = np.concatenate(\n            (G_smooth + lambda1, - G_smooth + lambda1), axis=None)\n        return obj, g_obj\n\n    def _refunc(w):\n        W = _adj(w)\n        loss, G_loss = _reweighloss(W)\n        h, G_h = _h(W)\n        obj = loss + 0.5 * rho * h * h + alpha * h + lambda1 * w.sum()\n        G_smooth = G_loss + (rho * h + alpha) * G_h\n        g_obj = np.concatenate(\n            (G_smooth + lambda1, - G_smooth + lambda1), axis=None)\n        return obj, g_obj\n\n    n, d = X.shape  \n    \n    w_est, rho, alpha, h = np.zeros(2 * d * d), 1.0, 0.0, np.inf\n\n    bnds = [(0, 0) if i == j else (0, None) for _ in range(2)\n            for i in range(d) for j in range(d)]  \n    if loss_type == 'l2':\n        X = X - np.mean(X, axis=0, keepdims=True)\n\n    ob_loss = []\n    total_loss = []\n    xepoch = 0\n    for iter in tqdm.tqdm(range(max_iter)):\n        w_new, h_new = None, None\n        while rho < rho_max:\n            xepoch+=1\n            if xepoch<epoch:\n                sol = sopt.minimize(\n                    _func, w_est, method='L-BFGS-B', jac=True, bounds=bnds)\n            elif xepoch==epoch:\n                w_new = sol.x\n                loss_record = _single_loss(_adj(w_new))\n                each_loss = loss_record.sum(axis=1)\n                each_loss_idx = each_loss.argsort()\n                reweight_list = each_loss_idx[:int(len(each_loss_idx)*0.1)]\n                sol = sopt.minimize(\n                    _refunc, w_est, method='L-BFGS-B', jac=True, bounds=bnds)\n                # sol = sopt.minimize(\n                #     _func, w_est, method='L-BFGS-B', jac=True, bounds=bnds)\n            elif xepoch>epoch:\n                sol = sopt.minimize(\n                    _refunc, w_est, method='L-BFGS-B', jac=True, bounds=bnds)\n\n            w_new = sol.x  \n            loss_record = _single_loss(_adj(w_new))\n            \n            each_loss = loss_record.sum(axis=1)\n            ob_loss.append(each_loss)\n            total_loss.append(_loss(_adj(w_new))[0])\n            h_new, _ = _h(_adj(w_new))\n            if h_new > 0.25 * h:       \n                rho *= 10\n            else:\n                break\n        w_est, h = w_new, h_new     \n        alpha += rho * h            \n        if h <= h_tol or rho >= rho_max:  \n            break\n    observed_loss = ob_loss[0].reshape(100, 1)\n    for i in range(1, len(ob_loss)):\n        observed_loss = np.concatenate(\n            (observed_loss, ob_loss[i].reshape(100, 1)), axis=1)\n    plt.figure(figsize=(50, 50))\n    for i in range(100):\n        plt.subplot(10, 10, i + 1)\n        plt.plot(observed_loss[i])\n        plt.title(i, fontsize=10)\n        plt.title('node {}'.format(i))\n        plt.axis('on')\n        plt.box(True)\n\n    if not os.path.exists('linear_notears'):\n        os.makedirs('linear_notears')\n    plt.savefig('linear_notears/observation.png')    \n\n    plt.figure(figsize=(20, 10))\n    plt.plot(total_loss)\n    plt.title('total loss(30nodes+noise*10)')\n    plt.savefig('linear_notears/total_loss.png')\n\n\n    W_est = _adj(w_est)\n    W_est[np.abs(W_est) < w_threshold] = 0\n    return W_est, iter\n\n# ==========================================\n# File: rescore_main.py\n# Function/Context: notears_nonlinear, golem_linear, daggnn_nonlinear, grandag_nonlinear\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport tqdm as tqdm\nfrom adaptive_model.adapModel import *\nfrom utils import *\n\nclass TensorDatasetIndexed(data.Dataset):\n    def __init__(self,tensor):\n        self.tensor=tensor\n    \n    def __getitem__(self,index):\n        return (self.tensor[index],index)\n    \n    def __len__(self):\n        return self.tensor.size(0)\n\ndef notears_nonlinear(model: nn.Module,\n                      adaptive_model: nn.Module,\n                      X: np.ndarray,\n                      train_loader: data.DataLoader,\n                      lambda1: float = 0.,\n                      lambda2: float = 0.,\n                      max_iter: int = 50,\n                      h_tol: float = 1e-8,\n                      rho_max: float = 1e+16,\n                      w_threshold: float = 0.3\n                      ):\n    \n    rho, alpha, h = 1.0, 0.0, np.inf\n    adp_flag = False\n    for j in tqdm.tqdm(range(max_iter)):\n        if j > args.reweight_epoch:\n            adp_flag = True\n            if not IF_baseline:\n                print(\"Re-weighting\")\n                reweight_idx_tmp = adap_reweight_step(args,adaptive_model, train_loader, args.adaptive_lambda , model, args.adaptive_epoch, args.adaptive_lr)\n             \n                if IF_figure:\n                    record_distribution(reweight_idx_tmp,j)\n                \n            rho, alpha, h = dual_ascent_step(args, model, X, train_loader, lambda1, lambda2,\n                                         rho, alpha, h, rho_max, adp_flag, adaptive_model)\n\n        else:\n            rho, alpha, h = dual_ascent_step(args, model, X, train_loader, lambda1, lambda2,\n                                         rho, alpha, h, rho_max, adp_flag, adaptive_model)\n        \n       \n        if h <= h_tol or rho >= rho_max:\n            break\n    W_est = model.fc1_to_adj()\n    W_est[np.abs(W_est) < w_threshold] = 0\n\n    hard_index, easy_index = hard_mining(args, X, model, single_loss, ratio=0.01)\n   \n    return W_est, hard_index, easy_index\n\ndef golem_linear(model: nn.Module,\n                      adaptive_model: nn.Module,\n                      X: np.ndarray,\n                      train_loader: data.DataLoader,\n                      lambda1: float = 0.,\n                      lambda2: float = 0.,\n                      max_iter: int = 5,\n                      h_tol: float = 1e-8,\n                      rho_max: float = 1e+16,\n                      w_threshold: float = 0.3,\n                      ):\n    adp_flag = False\n    for j in tqdm.tqdm(range(max_iter)):\n        if j > args.reweight_epoch:\n            adp_flag = True\n            if not IF_baseline:\n                print(\"Re-weighting\")\n                reweight_idx_tmp,R_tmp,idx= adap_reweight_step(args,adaptive_model, train_loader, args.adaptive_lambda , model, args.adaptive_epoch, args.adaptive_lr)\n              \n            h=dual_ascent_step_golem(args, model, X, train_loader, adp_flag, adaptive_model)\n        else:\n            h=dual_ascent_step_golem(args, model, X, train_loader, adp_flag, adaptive_model)\n        \n        if h <= h_tol:\n            break\n    \n\n    W_est = model.W_to_adj()\n \n    W_est[np.abs(W_est) < w_threshold] = 0\n\n    while not ut.is_dag(W_est):\n        w_threshold+=0.01\n        W_est[np.abs(W_est) < w_threshold] = 0\n\n    hard_index, easy_index = hard_mining(args, X, model, single_loss, ratio=0.01)\n   \n    return W_est, hard_index, easy_index\n\ndef daggnn_nonlinear(model: nn.Module,\n                      adaptive_model: nn.Module,\n                      X: np.ndarray,\n                      train_loader: data.DataLoader,\n                      max_iter: int = 20,\n                      h_tol: float = 1e-8,\n                      rho_max: float = 1e+16,\n                      w_threshold: float = 0.3,\n                      true_graph=None\n                      ):\n    rho, alpha, h = 1.0, 0.0, np.inf\n    adp_flag = False\n    for j in tqdm.tqdm(range(max_iter)):\n        if j > args.reweight_epoch:\n            \n            adp_flag = True\n            if not IF_baseline:\n                print(\"Re-weighting\")\n                reweight_idx_tmp = adap_reweight_step(args,adaptive_model, train_loader, args.adaptive_lambda , model, args.adaptive_epoch, args.adaptive_lr)\n                \n                if IF_figure:\n                    record_distribution(reweight_idx_tmp,j)\n                \n            rho, alpha, h = dual_ascent_step_daggnn(args, model, X, train_loader,\n                                         rho, alpha, h, rho_max, adp_flag, adaptive_model,true_graph)\n\n        else:\n            rho, alpha, h = dual_ascent_step_daggnn(args, model, X, train_loader,\n                                         rho, alpha, h, rho_max, adp_flag, adaptive_model,true_graph)\n        \n        print(rho,\" \",alpha,\" \",h)\n\n        W_est = model.get_adj()\n        W_est[np.abs(W_est) < w_threshold] = 0\n\n        acc = ut.count_accuracy(true_graph, W_est != 0)\n        print(acc)\n        \n        if h <= h_tol or rho >= rho_max:\n            break\n  \n    hard_index, easy_index = hard_mining(args, X, model, single_loss, ratio=0.01)\n    return W_est, hard_index, easy_index\n\ndef grandag_nonlinear(model: nn.Module,\n                      adaptive_model: nn.Module,\n                      X: np.ndarray,\n                      train_loader: data.DataLoader,\n                      max_iter: int = 1000,\n                      h_tol: float = 1e-8,\n                      rho_max: float = 1e+16,\n                      w_threshold: float = 0.3,\n                      true_graph=None,\n                      exp_path=None\n                      ):\n    \n    if os.path.exists(os.path.join(exp_path, \"model.pkl\")):\n        print(\"Train already computed. Loading result from disk.\")\n        model=load(exp_path, \"model.pkl\")\n    else:\n        rho, alpha, h = 1e-3, 0.0, np.inf\n        mus, lambdas, w_adjs= [],[],[]\n        iter_cnt=0\n        adp_flag = False\n        for j in tqdm.tqdm(range(max_iter)):\n            if j > args.reweight_epoch:\n             \n                adp_flag = True\n                if not IF_baseline:\n                    print(\"Re-weighting\")\n                    reweight_idx_tmp = adap_reweight_step(args,adaptive_model, train_loader, args.adaptive_lambda , model, args.adaptive_epoch, args.adaptive_lr)\n                    if IF_figure:\n                        record_distribution(reweight_idx_tmp,j)\n                    \n                rho, alpha, h, mus, lambdas,w_adjs,iter_cnt= dual_ascent_step_grandag(args, model, X, train_loader,\n                                            rho, alpha, h, rho_max, adp_flag, adaptive_model,true_graph,mus,lambdas,w_adjs,iter_cnt)\n\n            else:\n                rho, alpha, h, mus, lambdas,w_adjs,iter_cnt = dual_ascent_step_grandag(args, model, X, train_loader,\n                                            rho, alpha, h, rho_max, adp_flag, adaptive_model,true_graph,mus,lambdas,w_adjs,iter_cnt)\n            \n            if h <= h_tol or rho >= rho_max:\n                break\n\n        W_est_pre = model.get_w_adj().detach().cpu().numpy().astype(np.float32)\n\n        thresholds = np.unique(W_est_pre)\n        for step, t in enumerate(thresholds):\n            to_keep = torch.Tensor(W_est_pre > t + 1e-8).to(model.device)\n            new_adj = model.adjacency * to_keep\n            if is_acyclic(new_adj.cpu().detach()):\n                model.adjacency.copy_(new_adj)\n                break\n\n    W_est = model.adjacency.cpu().detach().numpy().astype(np.float32)\n    print(W_est)\n    acc = ut.count_accuracy(true_graph, W_est != 0)\n    print(\"STAGE:train\")\n    dump(model, exp_path, 'model')\n    print(acc)\n    \n    return W_est, hard_index, easy_index",
  "description": "Combined Analysis:\n- [notears/linear.py]: This file implements the core ReScore algorithm for linear models. It modifies the standard NOTEARS optimization by introducing adaptive sample reweighting through a bilevel structure:\n\n1. **Inner Loop (Weight Update)**: At epoch 6 (controlled by global variable `epoch`), the code identifies the top 10% of samples with smallest individual losses (easy samples) via `_single_loss()`, stores their indices in `reweight_list`, and creates a diagonal reweighting matrix where these samples are downweighted by factor `beta=0.1`. This implements the constraint-compatible weight update maximizing the reweighted score.\n\n2. **Outer Loop (DAG Update)**: The main optimization alternates between standard loss (`_func`) and reweighted loss (`_refunc`). Before epoch 6, it uses standard loss; from epoch 6 onward, it uses the reweighted loss that emphasizes harder samples. The DAG constraint is enforced via augmented Lagrangian with acyclicity function `_h()`.\n\n3. **Mathematical Correspondence**: The reweighted loss `_reweighloss()` implements S_w(G; X) = Σ_i w_i l(x_i, f(x_i)) with diagonal weight matrix. The weight constraints are enforced heuristically through sample selection and fixed downweighting. The outer objective combines reweighted loss with DAG penalty via augmented Lagrangian.\n\n4. **Algorithm Flow**: The code follows the paper's alternating steps: (a) compute per-sample losses, (b) update weights by selecting/downweighting easy samples, (c) optimize DAG with reweighted objective. This matches the bilevel optimization described, though with simplified weight constraints rather than the full continuous optimization.\n- [rescore_main.py]: This file implements the core ReScore bilevel optimization framework across multiple causal discovery methods (NOTEARS, GOLEM, DAG-GNN, GraNDAG). The key algorithm steps are: 1) Outer loop iterations for DAG learning, 2) After args.reweight_epoch, the inner loop activates via adap_reweight_step() to compute adaptive sample weights, 3) Dual ascent steps (dual_ascent_step variants) optimize the reweighted objective with DAG constraints. The implementation matches the paper's mathematical model: outer minimization of S_w*(G;X) + P_DAG(G) with inner maximization of S_w(G;X) subject to weight constraints. The adp_flag controls when reweighting is active, and IF_baseline allows ablation studies.",
  "dependencies": [
    "torch.nn",
    "utils",
    "notears.utils",
    "tqdm",
    "numpy",
    "utils (external module for simulation and evaluation)",
    "scipy.special.expit",
    "os",
    "torch.utils.data",
    "torch",
    "adaptive_model.adapModel",
    "scipy.linalg",
    "runhelps.runhelper",
    "matplotlib.pyplot",
    "adaptive_model.baseModel",
    "scipy.optimize"
  ]
}