{
  "file_path": "constraint_layers/lml.py, constraint_layers/sinkhorn.py, facility_location_experiment.py, max_covering_experiment.py, src/facility_location_methods.py, src/gumbel_sinkhorn_topk.py, src/max_covering_methods.py",
  "function_name": "LML (Logistic Maximum Likelihood) layer, Sinkhorn, cardnn_facility_location, gumbel_sinkhorn_topk, cardnn_max_covering",
  "code_snippet": "\n\n# ==========================================\n# File: constraint_layers/lml.py\n# Function/Context: LML (Logistic Maximum Likelihood) layer\n# ==========================================\nimport torch\nfrom torch.autograd import Function, Variable, grad\nfrom torch.nn import Module\nfrom torch.nn.parameter import Parameter\n\nimport numpy as np\nimport numpy.random as npr\n\n#from semantic_version import Version\n#version = Version('.'.join(torch.__version__.split('.')[:3]))\n#old_torch = version < Version('0.4.0')\nold_torch = False\n\ndef bdot(x, y):\n    return torch.bmm(x.unsqueeze(1), y.unsqueeze(2)).squeeze()\n\nclass LML(Module):\n    def __init__(self, N, eps=1e-4, n_iter=100, branch=None, verbose=0):\n        super().__init__()\n        self.N = N\n        self.eps = eps\n        self.n_iter = n_iter\n        self.branch = branch\n        self.verbose = verbose\n\n    def forward(self, x):\n        return LML_Function.apply(\n            x, self.N, self.eps, self.n_iter, self.branch, self.verbose\n        )\n\n\nclass LML_Function(Function):\n    @staticmethod\n    def forward(ctx, x, N, eps, n_iter, branch, verbose):\n        ctx.N = N\n        ctx.eps = eps\n        ctx.n_iter = n_iter\n        ctx.branch = branch\n        ctx.verbose = verbose\n\n        branch = ctx.branch\n        if branch is None:\n            if not x.is_cuda:\n                branch = 10\n            else:\n                branch = 100\n\n        single = x.ndimension() == 1\n        orig_x = x\n        if single:\n            x = x.unsqueeze(0)\n        assert x.ndimension() == 2\n\n        n_batch, nx = x.shape\n        if nx <= ctx.N:\n            y = (1.-1e-5)*torch.ones(n_batch, nx).type_as(x)\n            if single:\n                y = y.squeeze(0)\n            if old_torch:\n                ctx.save_for_backward(orig_x)\n                ctx.y = y\n                ctx.nu = torch.Tensor()\n            else:\n                ctx.save_for_backward(orig_x, y, torch.Tensor())\n            return y\n\n        x_sorted, _ = torch.sort(x, dim=1, descending=True)\n\n        # The sigmoid saturates the interval [-7, 7]\n        nu_lower = -x_sorted[:,ctx.N-1] - 7.\n        nu_upper = -x_sorted[:,ctx.N] + 7.\n\n        ls = torch.linspace(0,1,branch).type_as(x)\n\n        for i in range(ctx.n_iter):\n            r = nu_upper-nu_lower\n            I = r > ctx.eps\n            n_update = I.sum()\n            if n_update == 0:\n                break\n\n            Ix = I.unsqueeze(1).expand_as(x) if old_torch else I\n\n            nus = r[I].unsqueeze(1)*ls + nu_lower[I].unsqueeze(1)\n            _xs = x[Ix].view(n_update, 1, nx) + nus.unsqueeze(2)\n            fs = torch.sigmoid(_xs).sum(dim=2) - ctx.N\n            # assert torch.all(fs[:,0] < 0) and torch.all(fs[:,-1] > 0)\n\n            i_lower = ((fs < 0).sum(dim=1) - 1).long()\n            J = i_lower < 0\n            if J.sum() > 0:\n                print('LML Warning: An example has all positive iterates.')\n                i_lower[J] = 0\n\n            i_upper = i_lower + 1\n\n            nu_lower[I] = nus.gather(1, i_lower.unsqueeze(1)).squeeze()\n            nu_upper[I] = nus.gather(1, i_upper.unsqueeze(1)).squeeze()\n\n            if J.sum() > 0:\n                nu_lower[J] -= 7.\n\n        if ctx.verbose >= 0 and np.any(I.cpu().numpy()):\n            print('LML Warning: Did not converge.')\n            # import ipdb; ipdb.set_trace()\n\n        nu = nu_lower + r/2.\n        y = torch.sigmoid(x+nu.unsqueeze(1))\n        if single:\n            y = y.squeeze(0)\n\n        if old_torch:\n            # Storing these in the object may cause memory leaks.\n            ctx.save_for_backward(orig_x)\n            ctx.y = y\n            ctx.nu = nu\n        else:\n            ctx.save_for_backward(orig_x, y, nu)\n        return y\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        if old_torch:\n            x, = ctx.saved_tensors\n            y = ctx.y\n            nu = ctx.nu\n        else:\n            x, y, nu = ctx.saved_tensors\n\n        single = x.ndimension() == 1\n        if single:\n            x = x.unsqueeze(0)\n            y = y.unsqueeze(0)\n            grad_output = grad_output.unsqueeze(0)\n\n        assert x.ndimension() == 2\n        assert y.ndimension() == 2\n        assert grad_output.ndimension() == 2\n\n        n_batch, nx = x.shape\n        if nx <= ctx.N:\n            dx = torch.zeros_like(x)\n            if single:\n                dx = dx.squeeze()\n            grads = tuple([dx] + [None]*5)\n            return grads\n\n        Hinv = 1./(1./y + 1./(1.-y))\n        dnu = bdot(Hinv, grad_output)/Hinv.sum(dim=1)\n        dx = -Hinv*(-grad_output+dnu.unsqueeze(1))\n\n        if single:\n            dx = dx.squeeze()\n\n        grads = tuple([dx] + [None]*5)\n        return grads\n\n# ==========================================\n# File: constraint_layers/sinkhorn.py\n# Function/Context: Sinkhorn\n# ==========================================\nimport torch\nimport torch.nn as nn\nfrom torch import Tensor\n\n\nclass Sinkhorn(nn.Module):\n    r\"\"\"\n    Sinkhorn algorithm projects the input matrix into a matrix that satisfies all row distributions and column\n    distributions.\n\n    :param max_iter: maximum iterations (default: ``10``)\n    :param tau: the hyper parameter :math:`\\tau` controlling the temperature (default: ``1``)\n    :param epsilon: a small number for numerical stability (default: ``1e-4``)\n    :param log_forward: apply log-scale computation for better numerical stability (must be ``True``)\n    :param batched_operation: apply batched_operation for better efficiency (but may cause issues for back-propagation,\n     default: ``False``)\n    \"\"\"\n    def __init__(self, max_iter: int=10, tau: float=1., epsilon: float=1e-4,\n                 log_forward: bool=True, batched_operation: bool=False):\n        super(Sinkhorn, self).__init__()\n        self.max_iter = max_iter\n        self.tau = tau\n        self.epsilon = epsilon\n        self.log_forward = log_forward\n        if not log_forward:\n            print('Warning: Sinkhorn algorithm without log forward is deprecated because log_forward is more stable.')\n        self.batched_operation = batched_operation # batched operation may cause instability in backward computation,\n                                                   # but will boost computation.\n\n    def forward(self, s: Tensor, row_prob: Tensor, col_prob: Tensor, dummy_row: bool=False) -> Tensor:\n        r\"\"\"\n        :param s: :math:`(b\\times n_1 \\times n_2)` input 3d tensor. :math:`b`: batch size\n        :param row_prob: :math:`(b \\times n_1)` marginal probabilities in dim1 (rows)\n        :param col_prob: :math:`(b \\times n_2)` marginal probabilities in dim2 (columns)\n        :param dummy_row: whether to add dummy rows (rows whose elements are all 0) to pad the matrix to square matrix.\n         default: ``False``\n        :return: :math:`(b\\times n_1 \\times n_2)` the computed doubly-stochastic matrix\n\n        .. note::\n            We support batched instances with different number of nodes, therefore ``nrows`` and ``ncols`` are\n            required to specify the exact number of objects of each dimension in the batch. If not specified, we assume\n            the batched matrices are not padded.\n\n        .. note::\n            The original Sinkhorn algorithm only works for square matrices. To handle cases where the graphs to be\n            matched have different number of nodes, it is a common practice to add dummy rows to construct a square\n            matrix. After the row and column normalizations, the padded rows are discarded.\n\n        .. note::\n            We assume row number <= column number. If not, the input matrix will be transposed.\n        \"\"\"\n        if self.log_forward:\n            return self.forward_log(s, row_prob, col_prob, dummy_row)\n        else:\n            raise NotImplementedError\n\n    def forward_log(self, s, row_prob, col_prob, dummy_row=False):\n        \"\"\"Compute sinkhorn with row/column normalization in the log space.\"\"\"\n        if len(s.shape) == 2:\n            s = s.unsqueeze(0)\n            matrix_input = True\n        elif len(s.shape) == 3:\n            matrix_input = False\n        else:\n            raise ValueError('input data shape not understood.')\n\n        batch_size = s.shape[0]\n\n        # operations are performed on log_s\n        s = s / self.tau\n\n        log_row_prob = torch.log(row_prob).unsqueeze(2)\n        log_col_prob = torch.log(col_prob).unsqueeze(1)\n\n        if self.batched_operation:\n            log_s = s\n            last_log_s = log_s\n\n            for i in range(self.max_iter):\n                if i % 2 == 0:\n                    log_sum = torch.logsumexp(log_s, 2, keepdim=True)\n                    log_s = log_s - log_sum + log_row_prob\n                    if torch.max(torch.norm((log_s - last_log_s).view(batch_size, -1), dim=-1)) <= 1e-2:\n                        break\n                    last_log_s = log_s\n                else:\n                    log_sum = torch.logsumexp(log_s, 1, keepdim=True)\n                    log_s = log_s - log_sum + log_col_prob\n\n            if matrix_input:\n                log_s.squeeze_(0)\n\n            return torch.exp(log_s)\n        else:\n            ret_log_s = torch.full((batch_size, s.shape[1], s.shape[2]), -float('inf'), device=s.device, dtype=s.dtype)\n\n            for b in range(batch_size):\n                row_slice = slice(0, nrows[b])\n                col_slice = slice(0, ncols[b])\n                log_s = s[b, row_slice, col_slice]\n\n                for i in range(self.max_iter):\n                    if i % 2 == 0:\n                        log_sum = torch.logsumexp(log_s, 1, keepdim=True)\n                        log_s = log_s - log_sum\n                    else:\n                        log_sum = torch.logsumexp(log_s, 0, keepdim=True)\n                        log_s = log_s - log_sum\n\n                ret_log_s[b, row_slice, col_slice] = log_s\n\n            if dummy_row:\n                if dummy_shape[1] > 0:\n                    ret_log_s = ret_log_s[:, :-dummy_shape[1]]\n                for b in range(batch_size):\n                    ret_log_s[b, ori_nrows[b]:nrows[b], :ncols[b]] = -float('inf')\n\n            if transposed:\n                ret_log_s = ret_log_s.transpose(1, 2)\n            if matrix_input:\n                ret_log_s.squeeze_(0)\n\n            return torch.exp(ret_log_s)\n\n# ==========================================\n# File: facility_location_experiment.py\n# Function/Context: \n# ==========================================\nfrom src.facility_location_methods import *\nimport time\nimport xlwt\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport os\nfrom src.facility_location_data import get_random_data, get_starbucks_data\nfrom src.config import load_config\n\n####################################\n#             config               #\n####################################\n\ncfg = load_config()\ndevice = torch.device('cuda:0')\nfor k, v in cfg.items():\n    print(f'{k}: {v}')\n\n\n####################################\n#            training              #\n####################################\n\nif cfg.train_data_type == 'random':\n    cfg_dict = {}\n    if 'with_demand' in cfg:\n        cfg_dict['demand'] = cfg.with_demand\n    if 'max_demand' in cfg:\n        cfg_dict['max_demand'] = cfg.max_demand\n    train_dataset = get_random_data(cfg.num_data, cfg.dim, 0, device, **cfg_dict)\nelif cfg.train_data_type == 'starbucks':\n    train_dataset = get_starbucks_data(device)\nelse:\n    raise ValueError(f'Unknown dataset name {cfg.train_dataset_type}!')\n\nmodel = GNNModel().to(device)\n\nfor method_name in cfg.methods:\n    model_path = f'facility_location_{cfg.train_data_type}_{cfg.train_num_facilities}-{cfg.num_data}_{method_name}.pt'\n    if not os.path.exists(model_path) and method_name in ['cardnn-gs', 'cardnn-s', 'linsatnet', 'egn']:\n        print(f'Training the model weights for {method_name}...')\n        model = GNNModel().to(device)\n        if method_name in ['cardnn-gs', 'cardnn-s']:\n            train_optimizer = torch.optim.Adam(model.parameters(), lr=cfg.train_lr)\n            for epoch in range(cfg.train_iter):\n                # training loop\n                obj_sum = 0\n                for index, (_, points, __) in enumerate(train_dataset):\n                    graph, dist = build_graph_from_points(points, None, True, cfg.distance_metric)\n                    latent_vars = model(graph)\n                    if method_name == 'cardnn-gs':\n                        sample_num = cfg.train_gumbel_sample_num\n                        noise_fact = cfg.gumbel_sigma\n                    else:\n                        sample_num = 1\n                        noise_fact = 0\n                    top_k_indices, probs = gumbel_sinkhorn_topk(\n                        latent_vars, cfg.train_num_facilities, max_iter=100, tau=.05,\n                        sample_num=sample_num, noise_fact=noise_fact, return_prob=True\n                    )\n                    # compute objective by softmax\n                    obj = compute_objective_differentiable(dist, probs, temp=50) # set smaller temp during training\n                    obj.mean().backward()\n                    obj_sum += obj.mean()\n                    train_optimizer.step()\n                    train_optimizer.zero_grad()\n                print(f'epoch {epoch}/{cfg.train_iter}, obj={obj_sum / len(train_dataset)}')\n\n        if method_name in ['linsatnet']:\n            train_optimizer = torch.optim.Adam(model.parameters(), lr=cfg.train_lr)\n            for epoch in range(cfg.train_iter):\n                # training loop\n                obj_sum = 0\n                for index, (_, points, demands) in enumerate(train_dataset):\n                    graph, dist = build_graph_from_points(points, None, True, cfg.distance_metric)\n                    latent_vars = model(graph)\n\n                    A = torch.ones(1, len(points), device=latent_vars.device)\n                    b = torch.tensor([cfg.train_num_facilities], dtype=A.dtype, device=latent_vars.device)\n                    if demands is None:\n                        C = d = None\n                    else:\n                        C = torch.ones(1, len(points), device=latent_vars.device)\n                        d = torch.tensor([torch.sum(demands)], dtype=C.dtype, device=latent_vars.device)\n                    probs = gumbel_linsat_layer(torch.sigmoid(latent_vars), A=A, b=b, C=C, d=d,\n                                                max_iter=cfg.linsat_sk_iter, tau=cfg.linsat_tau,\n                                                sample_num=cfg.train_gumbel_sample_num, noise_fact=cfg.linsat_sigma)\n\n                    # compute objective by softmax\n                    obj = compute_objective_differentiable(dist, probs, demands, temp=cfg.linsat_softmax_temp)\n                    obj.mean().backward()\n                    obj_sum += obj.mean()\n\n                    train_optimizer.step()\n                    train_optimizer.zero_grad()\n\n                print(f'epoch {epoch}/{cfg.train_iter}, obj={obj_sum / len(train_dataset)}')\n\n        if method_name in ['egn']:\n            train_optimizer = torch.optim.Adam(model.parameters(), lr=cfg.train_lr_egn)\n            # training loop\n            for epoch in range(cfg.train_iter_egn):\n                obj_sum = 0\n                for index, (_, points, demands) in enumerate(train_dataset):\n                    graph, dist = build_graph_from_points(points, None, True, cfg.distance_metric)\n                    probs = model(graph)\n                    cardinality_cv = torch.relu(probs.sum() - cfg.train_num_facilities)\n                    if demands is None:\n                        total_cv = cardinality_cv\n                    else:\n                        total_cv = cardinality_cv + torch.relu(torch.sum(demands) - probs.sum())\n                    obj = compute_objective_differentiable(dist, probs, demands, temp=50) + cfg.egn_beta * total_cv\n                    obj.mean().backward()\n                    obj_sum += obj.mean()\n                    train_optimizer.step()\n                    train_optimizer.zero_grad()\n\n                print(f'epoch {epoch}/{cfg.train_iter_egn}, obj={obj_sum / len(train_dataset)}')\n        torch.save(model.state_dict(), model_path)\n        print(f'Model saved to {model_path}.')\n\n\n####################################\n#            testing               #\n####################################\n\nwb = xlwt.Workbook()\nws = wb.add_sheet(f'flp_{cfg.test_data_type}_{cfg.test_num_facilities}-{cfg.num_data}')\nws.write(0, 0, 'name')\ntimestamp = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n\nif cfg.test_data_type == 'random':\n    cfg_dict = {}\n    if 'with_demand' in cfg:\n        cfg_dict['demand'] = cfg.with_demand\n    if 'max_demand' in cfg:\n        cfg_dict['max_demand'] = cfg.max_demand\n    dataset = get_random_data(cfg.num_data, cfg.dim, 1, device, **cfg_dict)\nelif cfg.test_data_type == 'starbucks':\n    dataset = get_starbucks_data(device)\nelse:\n    raise ValueError(f'Unknown dataset name {cfg.train_dataset_type}!')\n\nfor index, (prob_name, points, demands) in enumerate(dataset):\n    fig = plt.figure(figsize=(10, 10))\n    plt.plot(points[:, 0].cpu(), points[:, 1].cpu(), 'b.')\n    method_idx = 0\n    print('-' * 20)\n    print(f'{prob_name} points={len(points)} select={cfg.test_num_facilities}')\n    ws.write(index + 1, 0, prob_name)\n\n    if 'greedy' in cfg.methods:\n        method_idx += 1\n        prev_time = time.time()\n        selected_points, selected_indices = greedy_facility_location(points, cfg.test_num_facilities, demands,\n                                                                     distance=cfg.distance_metric, device=points.device)\n        objective = compute_objective(points, selected_points, cfg.distance_metric, demands).item()\n        print(f'{prob_name} greedy objective={objective:.4f} '\n              f'selected={sorted(selected_indices.cpu().numpy().tolist())} '\n              f'time={time.time() - prev_time}')\n        if index == 0:\n            ws.write(0, method_idx * 2 - 1, 'greedy-objective')\n            ws.write(0, method_idx * 2, 'greedy-time')\n        ws.write(index + 1, method_idx * 2 - 1, objective)\n        ws.write(index + 1, method_idx * 2, time.time() - prev_time)\n\n    if 'gurobi' in cfg.methods:\n        # Gurobi - integer programming\n        method_idx += 1\n        prev_time = time.time()\n        ip_obj, ip_scores = gurobi_facility_location(\n            points, cfg.test_num_facilities, demands, distance=cfg.distance_metric, linear_relaxation=False,\n            timeout_sec=cfg.solver_timeout, verbose=cfg.verbose)\n        ip_scores = torch.tensor(ip_scores)\n        top_k_indices = torch.nonzero(ip_scores, as_tuple=False).view(-1)\n        print(f'{prob_name} Gurobi objective={ip_obj:.4f} '\n              f'selected={sorted(top_k_indices.cpu().numpy().tolist())} '\n              f'time={time.time() - prev_time}')\n        if index == 0:\n            ws.write(0, method_idx * 2 - 1, 'Gurobi-objective')\n            ws.write(0, method_idx * 2, 'Gurobi-time')\n        ws.write(index + 1, method_idx * 2 - 1, ip_obj)\n        ws.write(index + 1, method_idx * 2, time.time() - prev_time)\n\n    if 'scip' in cfg.methods:\n        # SCIP - integer programming\n        method_idx += 1\n        prev_time = time.time()\n        ip_obj, ip_scores = ortools_facility_location(\n            points, cfg.test_num_facilities, distance=cfg.distance_metric, linear_relaxation=False,\n            timeout_sec=cfg.solver_timeout, solver_name='SCIP')\n        ip_scores = torch.tensor(ip_scores)\n        top_k_indices = torch.nonzero(ip_scores, as_tuple=False).view(-1)\n        print(f'{prob_name} SCIP objective={ip_obj:.4f} '\n              f'selected={sorted(top_k_indices.cpu().numpy().tolist())} '\n              f'time={time.time() - prev_time}')\n        if index == 0:\n            ws.write(0, method_idx * 2 - 1, 'SCIP-objective')\n            ws.write(0, method_idx * 2, 'SCIP-time')\n        ws.write(index + 1, method_idx * 2 - 1, ip_obj)\n        ws.write(index + 1, method_idx * 2, time.time() - prev_time)\n\n    if 'egn' in cfg.methods:\n        # Erdos Goes Neural\n        method_idx += 1\n        model.load_state_dict(torch.load(f'facility_location_{cfg.train_data_type}_{cfg.train_num_facilities}-{cfg.num_data}_egn.pt'))\n        objective, selected_indices, finish_time = egn_facility_location(\n            points, cfg.test_num_facilities, model, cfg.softmax_temp, cfg.egn_beta,\n            time_limit=-1, distance_metric=cfg.distance_metric)\n        print(f'{prob_name} EGN objective={objective:.4f} selected={sorted(selected_indices.cpu().numpy().tolist())} time={finish_time}')\n        if index == 0:\n            ws.write(0, method_idx * 2 - 1, 'EGN-objective')\n            ws.write(0, method_idx * 2, 'EGN-time')\n        ws.write(index + 1, method_idx * 2 - 1, objective)\n        ws.write(index + 1, method_idx * 2, finish_time)\n\n        method_idx += 1\n        objective, selected_indices, finish_time = egn_facility_location(\n            points, cfg.test_num_facilities, model, cfg.softmax_temp, cfg.egn_beta, cfg.egn_trials,\n            time_limit=-1, distance_metric=cfg.distance_metric)\n        print(f'{prob_name} EGN-accu objective={objective:.4f} selected={sorted(selected_indices.cpu().numpy().tolist())} time={finish_time}')\n        if index == 0:\n            ws.write(0, method_idx * 2 - 1, 'EGN-accu-objective')\n            ws.write(0, method_idx * 2, 'EGN-accu-time')\n        ws.write(index + 1, method_idx * 2 - 1, objective)\n        ws.write(index + 1, method_idx * 2, finish_time)\n\n    if 'cardnn-s' in cfg.methods:\n        # CardNN-S\n        method_idx += 1\n        model.load_state_dict(torch.load(f'facility_location_{cfg.train_data_type}_{cfg.train_num_facilities}-{cfg.num_data}_cardnn-s.pt'))\n        objective, selected_indices, finish_time = cardnn_facility_location(points, cfg.test_num_facilities, model,\n                                                                            cfg.softmax_temp, 1, 0, cfg.sinkhorn_tau,\n                                                                            cfg.sinkhorn_iter, cfg.soft_opt_iter,\n                                                                            time_limit=-1,\n                                                                            distance_metric=cfg.distance_metric,\n                                                                            verbose=cfg.verbose)\n        print(f'{prob_name} CardNN-S objective={objective:.4f} selected={sorted(selected_indices.cpu().numpy().tolist())} time={finish_time}')\n        if index == 0:\n            ws.write(0, method_idx * 2 - 1, 'CardNN-S-objective')\n            ws.write(0, method_idx * 2, 'CardNN-S-time')\n        ws.write(index + 1, method_idx * 2 - 1, objective)\n        ws.write(index + 1, method_idx * 2, finish_time)\n\n    if 'cardnn-gs' in cfg.methods:\n        # CardNN-GS\n        method_idx += 1\n        model.load_state_dict(torch.load(f'facility_location_{cfg.train_data_type}_{cfg.train_num_facilities}-{cfg.num_data}_cardnn-gs.pt'))\n        objective, selected_indices, finish_time = cardnn_facility_location(points, cfg.test_num_facilities, model,\n                                                                            cfg.softmax_temp, cfg.gumbel_sample_num,\n                                                                            cfg.gumbel_sigma, cfg.sinkhorn_tau,\n                                                                            cfg.sinkhorn_iter, cfg.gs_opt_iter,\n                                                                            time_limit=-1,\n                                                                            distance_metric=cfg.distance_metric,\n                                                                            verbose=cfg.verbose)\n\n        print(f'{prob_name} CardNN-GS objective={objective:.4f} selected={sorted(selected_indices.cpu().numpy().tolist())} time={finish_time}')\n        if index == 0:\n            ws.write(0, method_idx * 2 - 1, 'CardNN-GS-objective')\n            ws.write(0, method_idx * 2, 'CardNN-GS-time')\n        ws.write(index + 1, method_idx * 2 - 1, objective)\n        ws.write(index + 1, method_idx * 2, finish_time)\n\n    if 'cardnn-hgs' in cfg.methods:\n        # CardNN-HGS\n        method_idx += 1\n        model.load_state_dict(torch.load(f'facility_location_{cfg.train_data_type}_{cfg.train_num_facilities}-{cfg.num_data}_cardnn-gs.pt'))\n        objective, selected_indices, finish_time = cardnn_facility_location(points, cfg.test_num_facilities, model,\n                                                                            cfg.softmax_temp, cfg.gumbel_sample_num,\n                                                                            cfg.homotophy_sigma, cfg.homotophy_tau,\n                                                                            cfg.homotophy_sk_iter,\n                                                                            cfg.homotophy_opt_iter, time_limit=-1,\n                                                                            distance_metric=cfg.distance_metric,\n                                                                            verbose=cfg.verbose)\n        print(f'{prob_name} CardNN-HGS objective={objective:.4f} selected={sorted(selected_indices.cpu().numpy().tolist())} time={finish_time}')\n        if index == 0:\n            ws.write(0, method_idx * 2 - 1, 'CardNN-HGS-objective')\n            ws.write(0, method_idx * 2, 'CardNN-HGS-time')\n        ws.write(index + 1, method_idx * 2 - 1, objective)\n        ws.write(index + 1, method_idx * 2, finish_time)\n\n# ==========================================\n# File: max_covering_experiment.py\n# Function/Context: \n# ==========================================\nfrom src.max_covering_methods import *\nimport time\nimport xlwt\nfrom datetime import datetime\nimport os\nfrom src.config import load_config\nfrom src.max_covering_data import get_random_dataset, get_twitch_dataset\n\n####################################\n#             config               #\n####################################\n\ncfg = load_config()\ndevice = torch.device('cuda:0')\nfor k, v in cfg.items():\n    print(f'{k}: {v}')\n\n\n####################################\n#            training              #\n####################################\n\nif cfg.train_data_type == 'random':\n    train_dataset = get_random_dataset(cfg.num_items, cfg.num_sets, 1)\nelif cfg.train_data_type == 'twitch':\n    train_dataset = get_twitch_dataset()\nelse:\n    raise ValueError(f'Unknown training dataset {cfg.train_data_type}!')\n\nmodel = GNNModel().to(device)\n\nfor method_name in cfg.methods:\n    model_path = f'max_covering_{cfg.train_data_type}_{cfg.train_max_covering_items}-{cfg.num_sets}-{cfg.num_items}_{method_name}.pt'\n    if not os.path.exists(model_path) and method_name in ['cardnn-gs', 'cardnn-s', 'linsatnet', 'egn']:\n        print(f'Training the model weights for {method_name}...')\n        model = GNNModel().to(device)\n        if method_name in ['cardnn-gs', 'cardnn-s']:\n            train_optimizer = torch.optim.Adam(model.parameters(), lr=cfg.train_lr)\n            for epoch in range(cfg.train_iter):\n                # training loop\n                obj_sum = 0\n                for name, weights, sets in train_dataset:\n                    bipartite_adj = None\n                    graph = build_graph_from_weights_sets(weights, sets, device)\n                    latent_vars = model(graph)\n                    if method_name == 'cardnn-gs':\n                        sample_num = cfg.train_gumbel_sample_num\n                        noise_fact = cfg.gumbel_sigma\n                    else:\n                        sample_num = 1\n                        noise_fact = 0\n                    top_k_indices, probs = gumbel_sinkhorn_topk(\n                        latent_vars, cfg.train_max_covering_items, max_iter=cfg.sinkhorn_iter, tau=cfg.sinkhorn_tau,\n                        sample_num=sample_num, noise_fact=noise_fact, return_prob=True\n                    )\n                    # compute objective by softmax\n                    obj, _ = compute_obj_differentiable(weights, sets, probs, bipartite_adj, device=probs.device)\n                    (-obj).mean().backward()\n                    obj_sum += obj.mean()\n\n                    train_optimizer.step()\n                    train_optimizer.zero_grad()\n\n                print(f'epoch {epoch}/{cfg.train_iter}, obj={obj_sum / len(train_dataset)}')\n\n        if method_name in ['linsatnet']:\n            train_optimizer = torch.optim.Adam(model.parameters(), lr=cfg.train_lr)\n            for epoch in range(cfg.train_iter):\n                # training loop\n                obj_sum = 0\n                for name, weights, sets in train_dataset:\n                    bipartite_adj = None\n                    graph = build_graph_from_weights_sets(weights, sets, device)\n                    latent_vars = model(graph)\n\n                    A = torch.ones(1, len(sets), device=latent_vars.device)\n                    b = torch.tensor([cfg.train_max_covering_items], dtype=A.dtype, device=latent_vars.device)\n                    probs = gumbel_linsat_layer(torch.sigmoid(latent_vars), A=A, b=b,\n                                                max_iter=cfg.linsat_sk_iter, tau=cfg.linsat_tau,\n                                                sample_num=cfg.train_gumbel_sample_num, noise_fact=cfg.linsat_sigma)\n\n                    # compute objective by softmax\n                    obj, _ = compute_obj_differentiable(weights, sets, probs, bipartite_adj, device=probs.device)\n                    (-obj).mean().backward()\n                    obj_sum += obj.mean()\n\n                    train_optimizer.step()\n                    train_optimizer.zero_grad()\n\n                print(f'epoch {epoch}/{cfg.train_iter}, obj={obj_sum / len(train_dataset)}')\n\n        if method_name in ['egn']:\n            train_optimizer = torch.optim.Adam(model.parameters(), lr=cfg.train_lr)\n            # training loop\n            for epoch in range(cfg.train_iter):\n                obj_sum = 0\n                for name, weights, sets in train_dataset:\n                    bipartite_adj = None\n                    graph = build_graph_from_weights_sets(weights, sets, device)\n                    probs = model(graph)\n                    constraint_conflict = torch.relu(probs.sum() - cfg.train_max_covering_items)\n                    obj, _ = compute_obj_differentiable(weights, sets, probs, bipartite_adj, device=probs.device)\n                    obj = obj - cfg.egn_beta * constraint_conflict\n                    (-obj).mean().backward()\n                    obj_sum += obj.mean()\n\n                    train_optimizer.step()\n                    train_optimizer.zero_grad()\n                print(f'epoch {epoch}/{cfg.train_iter}, obj={obj_sum / len(train_dataset)}')\n        torch.save(model.state_dict(), model_path)\n        print(f'Model saved to {model_path}.')\n\n\n####################################\n#            testing               #\n####################################\n\nif cfg.test_data_type == 'random':\n    dataset = get_random_dataset(cfg.num_items, cfg.num_sets, 0)\nelif cfg.test_data_type == 'twitch':\n    dataset = get_twitch_dataset()\nelse:\n    raise ValueError(f'Unknown testing dataset {cfg.test_data_type}!')\n\nwb = xlwt.Workbook()\nws = wb.add_sheet(f'max_covering_{cfg.test_max_covering_items}-{cfg.num_sets}-{cfg.num_items}')\nws.write(0, 0, 'name')\ntimestamp = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n\ntorch.random.manual_seed(1)\nfor index, (name, weights, sets) in enumerate(dataset):\n    method_idx = 0\n    print('-' * 20)\n    print(f'{name} items={len(weights)} sets={len(sets)}')\n    ws.write(index + 1, 0, name)\n\n    # greedy\n    if 'greedy' in cfg.methods:\n        method_idx += 1\n        prev_time = time.time()\n        objective, selected = greedy_max_covering(weights, sets, cfg.test_max_covering_items)\n        print(f'{name} greedy objective={objective} selected={sorted(selected)} time={time.time() - prev_time}')\n        if index == 0:\n            ws.write(0, method_idx * 2 - 1, 'greedy-objective')\n            ws.write(0, method_idx * 2, 'greedy-time')\n        ws.write(index + 1, method_idx * 2 - 1, objective)\n        ws.write(index + 1, method_idx * 2, time.time() - prev_time)\n\n    # SCIP - integer programming\n    if 'scip' in cfg.methods:\n        method_idx += 1\n        prev_time = time.time()\n        ip_obj, ip_scores = ortools_max_covering(weights, sets, cfg.test_max_covering_items, solver_name='SCIP', linear_relaxation=False, timeout_sec=cfg.solver_timeout)\n        ip_scores = torch.tensor(ip_scores)\n        top_k_indices = torch.nonzero(ip_scores, as_tuple=False).view(-1)\n        top_k_indices = sorted(top_k_indices.cpu().numpy().tolist())\n        print(f'{name} SCIP objective={ip_obj:.0f} selected={top_k_indices} time={time.time() - prev_time}')\n        if index == 0:\n            ws.write(0, method_idx * 2 - 1, 'SCIP-objective')\n            ws.write(0, method_idx * 2, 'SCIP-time')\n        ws.write(index + 1, method_idx * 2 - 1, ip_obj)\n        ws.write(index + 1, method_idx * 2, time.time() - prev_time)\n\n    # Gurobi - integer programming\n    if 'gurobi' in cfg.methods:\n        method_idx += 1\n        prev_time = time.time()\n        ip_obj, ip_scores = gurobi_max_covering(weights, sets, cfg.test_max_covering_items, linear_relaxation=False, timeout_sec=cfg.solver_timeout, verbose=cfg.verbose)\n        ip_scores = torch.tensor(ip_scores)\n        top_k_indices = torch.nonzero(ip_scores, as_tuple=False).view(-1)\n        top_k_indices = sorted(top_k_indices.cpu().numpy().tolist())\n        print(f'{name} Gurobi objective={ip_obj:.0f} selected={top_k_indices} time={time.time() - prev_time}')\n        if index == 0:\n            ws.write(0, method_idx * 2 - 1, 'Gurobi-objective')\n            ws.write(0, method_idx * 2, 'Gurobi-time')\n        ws.write(index + 1, method_idx * 2 - 1, ip_obj)\n        ws.write(index + 1, method_idx * 2, time.time() - prev_time)\n\n    weights = torch.tensor(weights, dtype=torch.float, device=device)\n\n    # Erdos Goes Neural\n    if 'egn' in cfg.methods:\n        method_idx += 1\n        model.load_state_dict(torch.load(f'max_covering_{cfg.train_data_type}_{cfg.train_max_covering_items}-{cfg.num_sets}-{cfg.num_items}_egn.pt'))\n        objective, best_top_k_indices, finish_time = egn_max_covering(weights, sets, cfg.test_max_covering_items, model, cfg.egn_beta)\n        print(f'{index} EGN objective={objective:.4f} selected={sorted(best_top_k_indices.cpu().numpy().tolist())} time={finish_time}')\n        if index == 0:\n            ws.write(0, method_idx * 2 - 1, 'EGN-objective')\n            ws.write(0, method_idx * 2, 'EGN-time')\n        ws.write(index + 1, method_idx * 2 - 1, objective)\n        ws.write(index + 1, method_idx * 2, finish_time)\n\n        method_idx += 1\n        objective, best_top_k_indices, finish_time = egn_max_covering(weights, sets, cfg.test_max_covering_items, model, cfg.egn_beta, cfg.egn_trials)\n        print(f'{index} EGN-accu objective={objective:.4f} selected={sorted(best_top_k_indices.cpu().numpy().tolist())} time={finish_time}')\n        if index == 0:\n            ws.write(0, method_idx * 2 - 1, 'EGN-accu-objective')\n            ws.write(0, method_idx * 2, 'EGN-accu-time')\n        ws.write(index + 1, method_idx * 2 - 1, objective)\n        ws.write(index + 1, method_idx * 2, finish_time)\n\n    # CardNN-S\n    if 'cardnn-s' in cfg.methods:\n        method_idx += 1\n        prev_time = time.time()\n        model_path = f'max_covering_{cfg.train_data_type}_{cfg.train_max_covering_items}-{cfg.num_sets}-{cfg.num_items}_cardnn-s.pt'\n        model.load_state_dict(torch.load(model_path))\n        best_obj, best_top_k_indices = cardnn_max_covering(weights, sets, cfg.test_max_covering_items, model, 1, 0,\n                                                           cfg.sinkhorn_tau, cfg.sinkhorn_iter, cfg.soft_opt_iter,\n                                                           verbose=cfg.verbose)\n        print(f'{name} CardNN-S objective={best_obj:.0f} selected={sorted(best_top_k_indices.cpu().numpy().tolist())} time={time.time() - prev_time}')\n        if index == 0:\n            ws.write(0, method_idx * 2 - 1, 'CardNN-S-objective')\n            ws.write(0, method_idx * 2, 'CardNN-S-time')\n        ws.write(index + 1, method_idx * 2 - 1, best_obj.item())\n        ws.write(index + 1, method_idx * 2, time.time() - prev_time)\n\n    # CardNN-GS\n    if 'cardnn-gs' in cfg.methods:\n        method_idx += 1\n        prev_time = time.time()\n        model_path = f'max_covering_{cfg.train_data_type}_{cfg.train_max_covering_items}-{cfg.num_sets}-{cfg.num_items}_cardnn-gs.pt'\n        model.load_state_dict(torch.load(model_path))\n        best_obj, best_top_k_indices = cardnn_max_covering(weights, sets, cfg.test_max_covering_items, model,\n                                                           cfg.gumbel_sample_num, cfg.gumbel_sigma, cfg.sinkhorn_tau,\n                                                           cfg.sinkhorn_iter, cfg.gs_opt_iter, verbose=cfg.verbose)\n        print(f'{name} CardNN-GS objective={best_obj:.0f} selected={sorted(best_top_k_indices.cpu().numpy().tolist())} time={time.time() - prev_time}')\n        if index == 0:\n            ws.write(0, method_idx * 2 - 1, 'CardNN-GS-objective')\n            ws.write(0, method_idx * 2, 'CardNN-GS-time')\n        ws.write(index + 1, method_idx * 2 - 1, best_obj.item())\n        ws.write(index + 1, method_idx * 2, time.time() - prev_time)\n\n    # CardNN-HGS\n    if 'cardnn-hgs' in cfg.methods:\n        method_idx += 1\n        prev_time = time.time()\n        model_path = f'max_covering_{cfg.train_data_type}_{cfg.train_max_covering_items}-{cfg.num_sets}-{cfg.num_items}_cardnn-gs.pt'\n        model.load_state_dict(torch.load(model_path))\n        best_obj, best_top_k_indices = cardnn_max_covering(weights, sets, cfg.test_max_covering_items, model,\n                                                           cfg.gumbel_sample_num, cfg.homotophy_sigma,\n                                                           cfg.homotophy_tau, cfg.homotophy_sk_iter,\n                                                           cfg.homotophy_opt_iter, verbose=cfg.verbose)\n        print(f'{name} CardNN-HGS objective={best_obj:.0f} selected={sorted(best_top_k_indices.cpu().numpy().tolist())} time={time.time() - prev_time}')\n        if index == 0:\n            ws.write(0, method_idx * 2 - 1, 'CardNN-HGS-objective')\n            ws.write(0, method_idx * 2, 'CardNN-HGS-time')\n        ws.write(index + 1, method_idx * 2 - 1, best_obj.item())\n        ws.write(index + 1, method_idx * 2, time.time() - prev_time)\n\n    # LinSATNet\n    if 'linsatnet' in cfg.methods:\n        method_idx += 1\n        prev_time = time.time()\n        model_path = f'max_covering_{cfg.train_data_type}_{cfg.train_max_covering_items}-{cfg.num_sets}-{cfg.num_items}_linsatnet.pt'\n        model.load_state_dict(torch.load(model_path))\n        best_obj, best_top_k_indices = linsat_max_covering(weights, sets, cfg.test_max_covering_items, model,\n                                                           cfg.gumbel_sample_num, cfg.linsat_sigma,\n                                                           cfg.linsat_tau, cfg.linsat_sk_iter,\n                                                           cfg.linsat_opt_iter, verbose=cfg.verbose)\n        print(f'{name} LinSATNet objective={best_obj:.0f} selected={sorted(best_top_k_indices.cpu().numpy().tolist())} time={time.time() - prev_time}')\n        if index == 0:\n            ws.write(0, method_idx * 2 - 1, 'LinSATNet-objective')\n            ws.write(0, method_idx * 2, 'LinSATNet-time')\n        ws.write(index + 1, method_idx * 2 - 1, best_obj.item())\n        ws.write(index + 1, method_idx * 2, time.time() - prev_time)\n\n    # perturb-TopK\n    if 'perturb-topk' in cfg.methods:\n        method_idx += 1\n        prev_time = time.time()\n        model_path = f'max_covering_{cfg.train_data_type}_{cfg.train_max_covering_items}-{cfg.num_sets}-{cfg.num_items}_cardnn-gs.pt'\n        model.load_state_dict(torch.load(model_path))\n        best_obj, best_top_k_indices = perturb_max_covering(weights, sets, cfg.test_max_covering_items, model,\n                                                            cfg.gumbel_sample_num * 10, cfg.perturb_sigma,\n                                                            cfg.perturb_opt_iter, verbose=cfg.verbose)\n        print(f'{name} perturb-TopK objective={best_obj:.0f} selected={sorted(best_top_k_indices.cpu().numpy().tolist())} time={time.time() - prev_time}')\n        if index == 0:\n            ws.write(0, method_idx * 2 - 1, 'perturb-TopK-objective')\n            ws.write(0, method_idx * 2, 'perturb-TopK-time')\n        ws.write(index + 1, method_idx * 2 - 1, best_obj.item())\n        ws.write(index + 1, method_idx * 2, time.time() - prev_time)\n\n# ==========================================\n# File: src/facility_location_methods.py\n# Function/Context: cardnn_facility_location\n# ==========================================\nimport numpy as np\nimport torch\nfrom torch import Tensor\nfrom typing import Union, Tuple\nfrom ortools.linear_solver import pywraplp\nimport torch_geometric as pyg\nimport time\nfrom src.gumbel_sinkhorn_topk import gumbel_sinkhorn_topk\nfrom src.gumbel_linsat_layer import gumbel_linsat_layer, init_linsat_constraints\nfrom constraint_layers.sinkhorn import Sinkhorn\nfrom src.optimal_transport import ortools_ot, OptimalTransportLayer\n\n\ndef compute_objective(points, selected_points, distance_metric='euclidean', demands=None):\n    if len(points.shape) == 2:\n        points = points.unsqueeze(0)\n    dist_func = _get_distance_func(distance_metric)\n    dist = dist_func(points, selected_points, points.device)\n    if demands is None: # FLP w/o demand\n        return torch.sum(torch.gather(dist, -1, torch.argmin(dist, dim=-1).unsqueeze(-1)).squeeze(-1), dim=-1)\n    else: # FLP w/ demand\n        if len(selected_points.shape) == 3:\n            batch_size = selected_points.shape[0]\n        else:\n            selected_points = selected_points.unsqueeze(0)\n            batch_size = 1\n        dummy = selected_points.shape[1] - torch.sum(demands)\n        row_dummy = torch.cat((demands, dummy.unsqueeze(-1)), dim=-1).unsqueeze(0).repeat(batch_size, 1)\n        col = torch.ones(selected_points.shape[1]).unsqueeze(0).repeat(batch_size, 1)\n        dist_dummy = torch.cat((dist, torch.zeros(batch_size, 1, dist.shape[2], device=dist.device)), dim=1)\n        obj_scores = ortools_ot(dist_dummy, row_dummy, col)\n        return obj_scores\n\n\ndef compute_objective_differentiable(dist, probs, demands=None, temp=30, sk_iter=40):\n    if demands is None: # FLP w/o demand\n        exp_dist = torch.exp(-temp / dist.mean() * dist)\n        exp_dist_probs = exp_dist.unsqueeze(0) * probs.unsqueeze(-1)\n        probs_per_dist = exp_dist_probs / exp_dist_probs.sum(1, keepdim=True)\n        obj = (probs_per_dist * dist).sum(dim=(1, 2))\n    else: # FLP w/ demand\n        # the objective estimation is an OT problem: move probs to demands\n        batch_size = probs.shape[0]\n        # sk = Sinkhorn(max_iter=sk_iter, tau=1 / temp, batched_operation=True)\n        sk2 = OptimalTransportLayer(gamma=temp, maxiters=sk_iter) # this implementation requires less GPU mem\n        dummy = torch.sum(probs, dim=-1) - torch.sum(demands)\n        col_dummy = torch.cat((demands.unsqueeze(0).repeat(batch_size, 1), dummy.unsqueeze(-1)), dim=-1)\n        if len(dist.shape) == 2:\n            dist_dummy = torch.cat((dist, torch.zeros(dist.shape[0], 1, device=dist.device)), dim=-1).unsqueeze(0).repeat(batch_size, 1, 1)\n        else:\n            assert len(dist.shape) == 3\n            dist_dummy = torch.cat((dist, torch.zeros(dist.shape[0], dist.shape[1], 1, device=dist.device)), dim=-1)\n        # transp_mat = sk(-dist_dummy, probs, col_dummy)\n        transp_mat = sk2(dist_dummy, probs, col_dummy) * torch.sum(probs, dim=1).reshape(batch_size, 1, 1)\n        obj = (transp_mat * dist_dummy).sum(dim=(1, 2))\n    return obj\n\n\ndef build_graph_from_points(points, dist=None, return_dist=False, distance_metric='euclidean'):\n    if dist is None:\n        dist_func = _get_distance_func(distance_metric)\n        dist = dist_func(points, points, points.device)\n    norm_dist = dist * 1.414 / dist.max()\n    edge_indices = torch.nonzero(norm_dist <= 0.02, as_tuple=False).transpose(0, 1)\n    edge_attrs = (points.unsqueeze(0) - points.unsqueeze(1))[torch.nonzero(norm_dist <= 0.02, as_tuple=True)] + 0.5\n    g = pyg.data.Data(x=points, edge_index=edge_indices, edge_attr=edge_attrs)\n    if return_dist:\n        return g, dist\n    else:\n        return g\n\n\ndef cardnn_facility_location(points, num_facilities, model,\n                             softmax_temp, sample_num, noise, tau, sk_iters, opt_iters,\n                             grad_step=0.1, time_limit=-1, distance_metric='euclidean', verbose=True):\n    \"\"\"\n    The Cardinality neural network (CardNN) solver for facility location. This implementation supports 3 variants:\n    CardNN-S (Sinkhorn), CardNN-GS (Gumbel-Sinkhorn) and CardNN-HGS (Homotopy-Gumbel-Sinkhorn).\n\n    Args:\n        points: the set of points\n        num_facilities: max number of facilities (i.e. the cardinality)\n        model: the GNN model\n        softmax_temp: temperature of softmax (actually softmin) when estimating the objective\n        sample_num: sampling number of Gumbel\n        noise: sigma of Gumbel noise\n        tau: annealing parameter of Sinkhorn\n        sk_iters: number of max iterations of Sinkhorn\n        opt_iters: number of optimizaiton operations in testing\n        grad_step: the gradient step (i.e. \"learning rate\") in testing-time optimization\n        time_limit: upper limit of solving time\n        distance_metric: euclidean or manhattan\n        verbose: show more information in solving\n\n    If noise=0, it is CardNN-S;\n    If noise>0, it is CardNN-GS;\n    If multiple values of \"noise, tau, sk_iters, opt_iters\" are given, it is CardNN-HGS.\n\n    Returns: best objective score, best solution\n    \"\"\"\n    prev_time = time.time()\n\n    # Graph modeling of the original problem\n    graph, dist = build_graph_from_points(points, None, True, distance_metric)\n\n    # Predict the initial latent variables by NN\n    latent_vars = model(graph).detach()\n\n    # Optimize over the latent variables in test\n    latent_vars.requires_grad_(True)\n    optimizer = torch.optim.Adam([latent_vars], lr=grad_step)\n    best_obj = float('inf')\n    best_top_k_indices = []\n    best_found_at_idx = -1\n\n    # Optimization steps (by gradient)\n    if type(noise) == list and type(tau) == list and type(sk_iters) == list and type(opt_iters) == list:\n        iterable = zip(noise, tau, sk_iters, opt_iters)\n    else:\n        iterable = zip([noise], [tau], [sk_iters], [opt_iters])\n    opt_iter_offset = 0\n    for noise, tau, sk_iters, opt_iters in iterable:\n        for opt_idx in range(opt_iter_offset, opt_iter_offset + opt_iters):\n            # time limit control\n            if time_limit > 0 and time.time() - prev_time > time_limit:\n                break\n\n            gumbel_weights_float = torch.sigmoid(latent_vars)\n            top_k_indices, probs = gumbel_sinkhorn_topk(\n                gumbel_weights_float, num_facilities,\n                max_iter=sk_iters, tau=tau, sample_num=sample_num, noise_fact=noise, return_prob=True\n            )\n\n            # estimate objective by softmax (in the computational graph)\n            obj = compute_objective_differentiable(dist, probs, temp=softmax_temp)\n\n            obj.mean().backward()\n            if opt_idx % 10 == 0 and verbose:\n                print(f'idx:{opt_idx} estimated {obj.min():.4f}, {obj.mean():.4f}, best {best_obj:.4f} found at {best_found_at_idx}')\n\n            # compute the real objective (detached from the computational graph)\n            cluster_centers = torch.gather(\n                torch.repeat_interleave(points.unsqueeze(0), top_k_indices.shape[0], 0),\n                1,\n                torch.repeat_interleave(top_k_indices.unsqueeze(-1), points.shape[-1], -1)\n            )\n            obj = compute_objective(points.unsqueeze(0), cluster_centers, distance_metric)\n\n            # find the best solution till now\n            best_idx = torch.argmin(obj)\n            min_obj, top_k_indices = obj[best_idx], top_k_indices[best_idx]\n            if min_obj < best_obj:\n                best_obj = min_obj\n                best_top_k_indices = top_k_indices\n                best_found_at_idx = opt_idx\n            if opt_idx % 10 == 0 and verbose:\n                print(f'idx:{opt_idx} real {obj.min():.4f}, {obj.mean():.4f}, best {best_obj:.4f} found at {best_found_at_idx}, now time:{time.time()-prev_time:.2f}')\n            optimizer.step()\n            optimizer.zero_grad()\n        opt_iter_offset += opt_iters\n    cluster_centers = torch.stack([torch.gather(points[:, _], 0, best_top_k_indices) for _ in range(points.shape[1])], dim=-1)\n\n    # fast neighbor search by k-means (as post-processing)\n    choice_cluster, cluster_centers, selected_indices = discrete_kmeans(points, num_facilities, init_x=cluster_centers, distance=distance_metric, device=points.device)\n    objective = compute_objective(points, cluster_centers, distance_metric).item()\n\n    return objective, selected_indices, time.time() - prev_time\n\n# ==========================================\n# File: src/gumbel_sinkhorn_topk.py\n# Function/Context: gumbel_sinkhorn_topk\n# ==========================================\nfrom constraint_layers.sinkhorn import Sinkhorn\nimport torch\n\n\ndef soft_topk(scores, k, max_iter=100, tau=1., return_prob=False):\n    return gumbel_sinkhorn_topk(scores, k, max_iter, tau, 0, 1, return_prob)\n\n\ndef gumbel_sinkhorn_topk(scores, k, max_iter=100, tau=1., noise_fact=1., sample_num=1000, return_prob=False):\n    anchors = torch.tensor([scores.min(), scores.max()], device=scores.device)\n    dist_mat = torch.abs(scores.unsqueeze(-1) - anchors.view(1, 2))\n\n    row_prob = torch.ones(1, scores.shape[0], device=scores.device)\n    col_prob = torch.stack(\n        (torch.full((1,), scores.shape[0] - k, dtype=torch.float, device=scores.device),\n         torch.full((1,), k, dtype=torch.float, device=scores.device),),\n        dim=1\n    )\n\n    sk = Sinkhorn(max_iter=max_iter, tau=tau, batched_operation=True)\n\n    def sample_gumbel(t_like, eps=1e-20):\n        \"\"\"\n        randomly sample standard gumbel variables\n        \"\"\"\n        u = torch.empty_like(t_like).uniform_()\n        return -torch.log(-torch.log(u + eps) + eps)\n\n    s_rep = torch.repeat_interleave(-dist_mat.unsqueeze(0), sample_num, dim=0)\n    gumbel_noise = sample_gumbel(s_rep[:, :, 0]) * noise_fact\n    gumbel_noise = torch.stack((gumbel_noise, -gumbel_noise), dim=-1)\n    s_rep = s_rep + gumbel_noise\n    rows_rep = torch.repeat_interleave(row_prob, sample_num, dim=0)\n    cols_rep = torch.repeat_interleave(col_prob, sample_num, dim=0)\n\n    output = sk(s_rep, rows_rep, cols_rep)\n\n    top_k_indices = torch.topk(output[:, :, 1], k, dim=-1).indices\n\n    if return_prob:\n        return top_k_indices, output[:, :, 1]\n    else:\n        return top_k_indices\n\n# ==========================================\n# File: src/max_covering_methods.py\n# Function/Context: cardnn_max_covering\n# ==========================================\nfrom copy import deepcopy\nfrom ortools.linear_solver import pywraplp\nimport numpy as np\nimport torch\nimport torch_geometric as pyg\nimport time\nfrom src.gumbel_sinkhorn_topk import gumbel_sinkhorn_topk\nfrom src.gumbel_linsat_layer import gumbel_linsat_layer, init_linsat_constraints\nimport src.perturbations as perturbations\nimport constraint_layers.blackbox_diff as blackbox_diff\nfrom constraint_layers.lml import LML\n\n\n####################################\n#        helper functions          #\n####################################\n\ndef compute_objective(weights, sets, selected_sets, bipartite_adj=None, device=torch.device('cpu')):\n    if not isinstance(weights, torch.Tensor):\n        weights = torch.tensor(weights, dtype=torch.float, device=device)\n\n    if not isinstance(selected_sets, torch.Tensor):\n        selected_sets = torch.tensor(selected_sets, device=device)\n\n    if bipartite_adj is None:\n        bipartite_adj = torch.zeros((len(sets), len(weights)), dtype=torch.float, device=device)\n        for _i, _set in enumerate(sets):\n            bipartite_adj[_i, _set] = 1\n\n    selected_items = bipartite_adj[selected_sets, :].sum(dim=-2).clamp_(0, 1)\n    return torch.matmul(selected_items, weights)\n\n\ndef compute_obj_differentiable(weights, sets, latent_probs, bipartite_adj=None, device=torch.device('cpu')):\n    if not isinstance(weights, torch.Tensor):\n        weights = torch.tensor(weights, dtype=torch.float, device=device)\n    if not isinstance(latent_probs, torch.Tensor):\n        latent_probs = torch.tensor(latent_probs, device=device)\n    if bipartite_adj is None:\n        bipartite_adj = torch.zeros((len(sets), len(weights)), dtype=torch.float, device=device)\n        for _i, _set in enumerate(sets):\n            bipartite_adj[_i, _set] = 1\n    selected_items = torch.clamp_max(torch.matmul(latent_probs, bipartite_adj), 1)\n    return torch.matmul(selected_items, weights), bipartite_adj\n\n\nclass BipartiteData(pyg.data.Data):\n    def __init__(self, edge_index, x_src, x_dst):\n        super(BipartiteData, self).__init__()\n        self.edge_index = edge_index\n        self.x1 = x_src\n        self.x2 = x_dst\n\n    def __inc__(self, key, value):\n        if key == 'edge_index':\n            return torch.tensor([[self.x1.size(0)], [self.x2.size(0)]])\n        else:\n            return super(BipartiteData, self).__inc__(key, value)\n\n\ndef build_graph_from_weights_sets(weights, sets, device=torch.device('cpu')):\n    x_src = torch.ones(len(sets), 1, device=device)\n    x_tgt = torch.tensor(weights, dtype=torch.float, device=device).unsqueeze(-1)\n    index_1, index_2 = [], []\n    for set_idx, set_items in enumerate(sets):\n        for set_item in set_items:\n            index_1.append(set_idx)\n            index_2.append(set_item)\n    edge_index = torch.tensor([index_1, index_2], device=device)\n    return BipartiteData(edge_index, x_src, x_tgt)\n\n\n#################################################\n#         Learning Max-kVC Methods              #\n#################################################\n\nclass GNNModel(torch.nn.Module):\n    # Max covering model (3-layer Bipartite SageConv)\n    def __init__(self):\n        super(GNNModel, self).__init__()\n        self.gconv1_w2s = pyg.nn.SAGEConv((1, 1), 16)\n        self.gconv1_s2w = pyg.nn.SAGEConv((1, 1), 16)\n        self.gconv2_w2s = pyg.nn.SAGEConv((16, 16), 16)\n        self.gconv2_s2w = pyg.nn.SAGEConv((16, 16), 16)\n        self.gconv3_w2s = pyg.nn.SAGEConv((16, 16), 16)\n        self.gconv3_s2w = pyg.nn.SAGEConv((16, 16), 16)\n        self.fc = torch.nn.Linear(16, 1)\n\n    def forward(self, g):\n        assert type(g) == BipartiteData\n        reverse_edge_index = torch.stack((g.edge_index[1], g.edge_index[0]), dim=0)\n        new_x1 = torch.relu(self.gconv1_w2s((g.x2, g.x1), reverse_edge_index))\n        new_x2 = torch.relu(self.gconv1_s2w((g.x1, g.x2), g.edge_index))\n        x1, x2 = new_x1, new_x2\n        new_x1 = torch.relu(self.gconv2_w2s((x2, x1), reverse_edge_index))\n        new_x2 = torch.relu(self.gconv2_s2w((x1, x2), g.edge_index))\n        x1, x2 = new_x1, new_x2\n        new_x1 = torch.relu(self.gconv3_w2s((x2, x1), reverse_edge_index))\n        new_x2 = torch.relu(self.gconv3_s2w((x1, x2), g.edge_index))\n        x = self.fc(new_x1).squeeze(-1)\n        return torch.sigmoid(x)\n\n\ndef cardnn_max_covering(weights, sets, max_covering_items, model, sample_num, noise, tau, sk_iters, opt_iters, verbose=True):\n    \"\"\"\n    The Cardinality neural network (CardNN) solver for max covering. This implementation supports 3 variants:\n    CardNN-S (Sinkhorn), CardNN-GS (Gumbel-Sinkhorn) and CardNN-HGS (Homotopy-Gumbel-Sinkhorn).\n\n    Args:\n        weights: the weights of each element\n        sets: indicate which elements are covered by each set\n        max_covering_items: max number of sets (i.e. the cardinality)\n        model: the GNN model\n        sample_num: sampling number of Gumbel\n        noise: sigma of Gumbel noise\n        tau: annealing parameter of Sinkhorn\n        sk_iters: number of max iterations of Sinkhorn\n        opt_iters: number of optimizaiton operations in testing\n        verbose: show more information in solving\n\n    If noise=0, it is CardNN-S;\n    If noise>0, it is CardNN-GS;\n    If multiple values of \"noise, tau, sk_iters, opt_iters\" are given, it is CardNN-HGS.\n\n    Returns: best objective score, best solution\n\n    \"\"\"\n    # Graph modeling of the original problem\n    graph = build_graph_from_weights_sets(weights, sets, weights.device)\n\n    # Predict the initial latent variables by NN\n    latent_vars = model(graph).detach()\n\n    # Optimize over the latent variables in test\n    latent_vars.requires_grad_(True)\n    optimizer = torch.optim.Adam([latent_vars], lr=.1)\n    bipartite_adj = None\n    best_obj = 0\n    best_top_k_indices = []\n    best_found_at_idx = -1\n\n    # Optimization steps (by gradient)\n    if type(noise) == list and type(tau) == list and type(sk_iters) == list and type(opt_iters) == list:\n        iterable = zip(noise, tau, sk_iters, opt_iters)\n    else:\n        iterable = zip([noise], [tau], [sk_iters], [opt_iters])\n    for noise, tau, sk_iters, opt_iters in iterable:\n        for train_idx in range(opt_iters):\n            gumbel_weights_float = torch.sigmoid(latent_vars)\n            # noise = 1 - 0.75 * train_idx / 1000\n            top_k_indices, probs = gumbel_sinkhorn_topk(gumbel_weights_float, max_covering_items,\n                max_iter=sk_iters, tau=tau, sample_num=sample_num, noise_fact=noise, return_prob=True)\n\n            # estimate the objective score (in the computational graph)\n            obj, bipartite_adj = compute_obj_differentiable(weights, sets, probs, bipartite_adj, probs.device)\n            (-obj).mean().backward()\n            if train_idx % 10 == 0 and verbose:\n                print(f'idx:{train_idx} {obj.max():.1f}, {obj.mean():.1f}, best {best_obj:.0f} found at {best_found_at_idx}')\n\n            # compute the real objective (detached from the computational graph)\n            obj = compute_objective(weights, sets, top_k_indices, bipartite_adj, device=probs.device)\n\n            # find the best solution till now\n            best_idx = torch.argmax(obj)\n            max_obj, top_k_indices = obj[best_idx], top_k_indices[best_idx]\n            if max_obj > best_obj:\n                best_obj = max_obj\n                best_top_k_indices = top_k_indices\n                best_found_at_idx = train_idx\n            if train_idx % 10 == 0 and verbose:\n                print(f'idx:{train_idx} {obj.max():.1f}, {obj.mean():.1f}, best {best_obj:.0f} found at {best_found_at_idx}')\n\n            optimizer.step()\n            optimizer.zero_grad()\n    return best_obj, best_top_k_indices",
  "description": "Combined Analysis:\n- [constraint_layers/lml.py]: This file implements the LML (Logistic Maximum Likelihood) layer, which is a differentiable projection layer that enforces cardinality constraints. The forward pass solves the convex optimization problem: minimize -x*y - entropy(y) subject to sum(y) = N and y  [0,1], where N is the cardinality constraint. This corresponds to a continuous relaxation of the cardinality-constrained combinatorial optimization problem. The implementation uses a bisection method (branching search) to find the Lagrange multiplier nu that satisfies the sum constraint, then computes y = sigmoid(x + nu). The backward pass provides gradients through this projection. This matches the core optimization logic from the paper, which uses differentiable layers to enforce cardinality constraints in neural combinatorial solvers.\n- [constraint_layers/sinkhorn.py]: This file implements the Sinkhorn algorithm, a key component of the differentiable optimal transport layers used in the paper's CardNN-S and CardNN-GS models. The Sinkhorn layer projects input matrices to doubly-stochastic matrices with specified row and column marginal distributions, enabling soft constraint satisfaction. In the context of cardinality-constrained optimization, this allows the neural network to enforce constraints (like selecting exactly k items) in a differentiable manner by setting appropriate marginals (e.g., row_prob = [1/k, ..., 1/k] for k selections). The forward_log method performs iterative row/column normalization in log-space for numerical stability, aligning with the paper's use of Sinkhorn for constraint integration in non-autoregressive solvers.\n- [facility_location_experiment.py]: This file implements the complete training and evaluation pipeline for the facility location problem using neural combinatorial solvers. It directly implements the CardNN-S and CardNN-GS algorithms from the paper through: 1) Training loops that use Gumbel-Sinkhorn layers to enforce cardinality constraints (x_i  k) while minimizing the facility location objective via differentiable relaxation. 2) Evaluation procedures that test the trained models on both synthetic and real-world data. The code matches the paper's mathematical model where the objective is computed via softmax approximation of min distances, and constraints are enforced through differentiable optimal transport layers. Key algorithm components include: gumbel_sinkhorn_topk for constraint satisfaction, compute_objective_differentiable for objective computation, and end-to-end training via backpropagation.\n- [max_covering_experiment.py]: This file implements the complete training and evaluation pipeline for the Max Covering Problem (MCP) with cardinality constraints, directly corresponding to the paper's focus. The core logic includes: 1) Training neural solvers (CardNN-S, CardNN-GS, LinSATNet, EGN) using differentiable constraint layers (Sinkhorn, Gumbel-Sinkhorn, LinSAT) to enforce the cardinality constraint x_i  k, 2) Self-supervised learning via direct optimization of the MCP objective, 3) Evaluation against traditional solvers (Greedy, SCIP, Gurobi). The code explicitly implements the paper's key algorithm steps: using GNN to generate latent variables, applying differentiable top-k layers for constraint satisfaction, and optimizing the objective via gradient descent. The mathematical model is realized through the objective computation (compute_obj_differentiable) and constraint enforcement layers.\n- [src/facility_location_methods.py]: This file implements the core CardNN algorithm for cardinality-constrained facility location as described in the paper. The cardnn_facility_location function directly implements the one-shot neural combinatorial solver using differentiable optimal transport layers (Sinkhorn/Gumbel-Sinkhorn) to enforce cardinality constraints. It matches the optimization model: minimizing sum of distances to nearest facility (compute_objective) subject to selecting at most k facilities (enforced via gumbel_sinkhorn_topk). The algorithm uses a GNN to predict initial scores, then performs test-time optimization via gradient descent on a differentiable objective approximation (compute_objective_differentiable). The implementation supports all three variants: CardNN-S (noise=0), CardNN-GS (noise>0), and CardNN-HGS (multiple parameter schedules).\n- [src/gumbel_sinkhorn_topk.py]: This file directly implements the CardNN-GS algorithm from the paper, which uses Gumbel-Sinkhorn optimal transport to enforce cardinality constraints. The core logic maps to the paper's Algorithm 2: 1) Creates a 2-column transport problem (select vs not-select) with column marginals fixed to [n-k, k] to enforce exactly k selections. 2) Uses Sinkhorn iterations with temperature  for differentiable relaxation. 3) Adds Gumbel noise for stochastic sampling during training. 4) Outputs top-k indices from the transport probabilities. This provides the differentiable cardinality-constrained selection mechanism central to the non-autoregressive solver architecture.\n- [src/max_covering_methods.py]: This file implements the core optimization logic for the Max Covering Problem (MCP) with cardinality constraints as described in the paper. Specifically, the function `cardnn_max_covering` implements the CardNN solver (including variants S, GS, and HGS) that uses differentiable optimal transport layers (Sinkhorn and Gumbel-Sinkhorn) to enforce the cardinality constraint (selecting at most k sets). The algorithm follows the paper's one-shot neural combinatorial solver approach: (1) models the problem as a bipartite graph, (2) uses a GNN to predict initial latent variables, (3) optimizes the latent variables via gradient descent with a differentiable relaxation of the cardinality constraint, and (4) computes the objective via a differentiable surrogate. The helper functions compute the objective and build the graph representation. The code matches the paper's mathematical model (maximizing coverage under cardinality constraint) and algorithm steps (non-autoregressive, differentiable constraint handling via optimal transport).",
  "dependencies": [
    "src.facility_location_data",
    "GNNModel",
    "constraint_layers.sinkhorn.Sinkhorn",
    "gurobi_facility_location",
    "constraint_layers.lml",
    "egn_facility_location",
    "greedy_max_covering",
    "src.config",
    "compute_objective",
    "matplotlib",
    "time",
    "perturb_max_covering",
    "cardnn_max_covering",
    "bdot (helper function)",
    "gurobi_max_covering",
    "constraint_layers.sinkhorn",
    "src.gumbel_linsat_layer",
    "ortools_max_covering",
    "build_graph_from_points",
    "egn_max_covering",
    "compute_objective_differentiable",
    "src.max_covering_data",
    "src.max_covering_methods",
    "src.gumbel_sinkhorn_topk",
    "constraint_layers.blackbox_diff",
    "os",
    "gumbel_linsat_layer",
    "datetime",
    "src.optimal_transport",
    "linsat_max_covering",
    "sample_gumbel (helper function)",
    "xlwt",
    "numpy",
    "compute_obj_differentiable",
    "build_graph_from_weights_sets",
    "torch.nn",
    "greedy_facility_location",
    "ortools_facility_location",
    "cardnn_facility_location",
    "src.facility_location_methods",
    "torch_geometric",
    "gumbel_sinkhorn_topk",
    "torch",
    "src.perturbations",
    "ortools.linear_solver"
  ]
}