{
  "file_path": "JSSP_Env.py, PPO_jssp_multiInstances.py, models/actor_critic.py, models/graphcnn_congForSJSSP.py, permissibleLS.py, test_learned.py, updateEntTimeLB.py, validation.py",
  "function_name": "SJSSP, main, ActorCritic.forward, GraphCNN, permissibleLeftShift, test, calEndTimeLB, validate",
  "code_snippet": "\n\n# ==========================================\n# File: JSSP_Env.py\n# Function/Context: SJSSP\n# ==========================================\nimport gym\nimport numpy as np\nfrom gym.utils import EzPickle\nfrom uniform_instance_gen import override\nfrom updateEntTimeLB import calEndTimeLB\nfrom Params import configs\nfrom permissibleLS import permissibleLeftShift\nfrom updateAdjMat import getActionNbghs\n\n\nclass SJSSP(gym.Env, EzPickle):\n    def __init__(self,\n                 n_j,\n                 n_m):\n        EzPickle.__init__(self)\n\n        self.step_count = 0\n        self.number_of_jobs = n_j\n        self.number_of_machines = n_m\n        self.number_of_tasks = self.number_of_jobs * self.number_of_machines\n        # the task id for first column\n        self.first_col = np.arange(start=0, stop=self.number_of_tasks, step=1).reshape(self.number_of_jobs, -1)[:, 0]\n        # the task id for last column\n        self.last_col = np.arange(start=0, stop=self.number_of_tasks, step=1).reshape(self.number_of_jobs, -1)[:, -1]\n        self.getEndTimeLB = calEndTimeLB\n        self.getNghbs = getActionNbghs\n\n    def done(self):\n        if len(self.partial_sol_sequeence) == self.number_of_tasks:\n            return True\n        return False\n\n    @override\n    def step(self, action):\n        # action is a int 0 - 224 for 15x15 for example\n        # redundant action makes no effect\n        if action not in self.partial_sol_sequeence:\n\n            # UPDATE BASIC INFO:\n            row = action // self.number_of_machines\n            col = action % self.number_of_machines\n            self.step_count += 1\n            self.finished_mark[row, col] = 1\n            dur_a = self.dur[row, col]\n            self.partial_sol_sequeence.append(action)\n\n            # UPDATE STATE:\n            # permissible left shift\n            startTime_a, flag = permissibleLeftShift(a=action, durMat=self.dur, mchMat=self.m, mchsStartTimes=self.mchsStartTimes, opIDsOnMchs=self.opIDsOnMchs)\n            self.flags.append(flag)\n            # update omega or mask\n            if action not in self.last_col:\n                self.omega[action // self.number_of_machines] += 1\n            else:\n                self.mask[action // self.number_of_jobs] = 1\n\n            self.temp1[row, col] = startTime_a + dur_a\n\n            self.LBs = calEndTimeLB(self.temp1, self.dur_cp)\n\n            # adj matrix\n            precd, succd = self.getNghbs(action, self.opIDsOnMchs)\n            self.adj[action] = 0\n            self.adj[action, action] = 1\n            if action not in self.first_col:\n                self.adj[action, action - 1] = 1\n            self.adj[action, precd] = 1\n            self.adj[succd, action] = 1\n            if flag and precd != action and succd != action:  # Remove the old arc when a new operation inserts between two operations\n                self.adj[succd, precd] = 0\n\n        # prepare for return\n        fea = np.concatenate((self.LBs.reshape(-1, 1)/configs.et_normalize_coef,\n                              self.finished_mark.reshape(-1, 1)), axis=1)\n        reward = - (self.LBs.max() - self.max_endTime)\n        if reward == 0:\n            reward = configs.rewardscale\n            self.posRewards += reward\n        self.max_endTime = self.LBs.max()\n\n        return self.adj, fea, reward, self.done(), self.omega, self.mask\n\n    @override\n    def reset(self, data):\n\n        self.step_count = 0\n        self.m = data[-1]\n        self.dur = data[0].astype(np.single)\n        self.dur_cp = np.copy(self.dur)\n        # record action history\n        self.partial_sol_sequeence = []\n        self.flags = []\n        self.posRewards = 0\n\n        # initialize adj matrix\n        conj_nei_up_stream = np.eye(self.number_of_tasks, k=-1, dtype=np.single)\n        conj_nei_low_stream = np.eye(self.number_of_tasks, k=1, dtype=np.single)\n        # first column does not have upper stream conj_nei\n        conj_nei_up_stream[self.first_col] = 0\n        # last column does not have lower stream conj_nei\n        conj_nei_low_stream[self.last_col] = 0\n        self_as_nei = np.eye(self.number_of_tasks, dtype=np.single)\n        self.adj = self_as_nei + conj_nei_up_stream\n\n        # initialize features\n        self.LBs = np.cumsum(self.dur, axis=1, dtype=np.single)\n        self.initQuality = self.LBs.max() if not configs.init_quality_flag else 0\n        self.max_endTime = self.initQuality\n        self.finished_mark = np.zeros_like(self.m, dtype=np.single)\n\n        fea = np.concatenate((self.LBs.reshape(-1, 1)/configs.et_normalize_coef,\n                              # self.dur.reshape(-1, 1)/configs.high,\n                              # wkr.reshape(-1, 1)/configs.wkr_normalize_coef,\n                              self.finished_mark.reshape(-1, 1)), axis=1)\n        # initialize feasible omega\n        self.omega = self.first_col.astype(np.int64)\n\n        # initialize mask\n        self.mask = np.full(shape=self.number_of_jobs, fill_value=0, dtype=bool)\n\n        # start time of operations on machines\n        self.mchsStartTimes = -configs.high * np.ones_like(self.dur.transpose(), dtype=np.int32)\n        # Ops ID on machines\n        self.opIDsOnMchs = -self.number_of_jobs * np.ones_like(self.dur.transpose(), dtype=np.int32)\n\n        self.temp1 = np.zeros_like(self.dur, dtype=np.single)\n\n        return self.adj, fea, self.omega, self.mask\n\n# ==========================================\n# File: PPO_jssp_multiInstances.py\n# Function/Context: main\n# ==========================================\nfrom mb_agg import *\nfrom agent_utils import eval_actions\nfrom agent_utils import select_action\nfrom models.actor_critic import ActorCritic\nfrom copy import deepcopy\nimport torch\nimport time\nimport torch.nn as nn\nimport numpy as np\nfrom Params import configs\nfrom validation import validate\n\ndevice = torch.device(configs.device)\n\n\nclass Memory:\n    def __init__(self):\n        self.adj_mb = []\n        self.fea_mb = []\n        self.candidate_mb = []\n        self.mask_mb = []\n        self.a_mb = []\n        self.r_mb = []\n        self.done_mb = []\n        self.logprobs = []\n\n    def clear_memory(self):\n        del self.adj_mb[:]\n        del self.fea_mb[:]\n        del self.candidate_mb[:]\n        del self.mask_mb[:]\n        del self.a_mb[:]\n        del self.r_mb[:]\n        del self.done_mb[:]\n        del self.logprobs[:]\n\n\nclass PPO:\n    def __init__(self,\n                 lr,\n                 gamma,\n                 k_epochs,\n                 eps_clip,\n                 n_j,\n                 n_m,\n                 num_layers,\n                 neighbor_pooling_type,\n                 input_dim,\n                 hidden_dim,\n                 num_mlp_layers_feature_extract,\n                 num_mlp_layers_actor,\n                 hidden_dim_actor,\n                 num_mlp_layers_critic,\n                 hidden_dim_critic,\n                 ):\n        self.lr = lr\n        self.gamma = gamma\n        self.eps_clip = eps_clip\n        self.k_epochs = k_epochs\n\n        self.policy = ActorCritic(n_j=n_j,\n                                  n_m=n_m,\n                                  num_layers=num_layers,\n                                  learn_eps=False,\n                                  neighbor_pooling_type=neighbor_pooling_type,\n                                  input_dim=input_dim,\n                                  hidden_dim=hidden_dim,\n                                  num_mlp_layers_feature_extract=num_mlp_layers_feature_extract,\n                                  num_mlp_layers_actor=num_mlp_layers_actor,\n                                  hidden_dim_actor=hidden_dim_actor,\n                                  num_mlp_layers_critic=num_mlp_layers_critic,\n                                  hidden_dim_critic=hidden_dim_critic,\n                                  device=device)\n        self.policy_old = deepcopy(self.policy)\n\n        '''self.policy.load_state_dict(\n            torch.load(path='./{}.pth'.format(str(n_j) + '_' + str(n_m) + '_' + str(1) + '_' + str(99))))'''\n\n        self.policy_old.load_state_dict(self.policy.state_dict())\n        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer,\n                                                         step_size=configs.decay_step_size,\n                                                         gamma=configs.decay_ratio)\n\n        self.V_loss_2 = nn.MSELoss()\n\n    def update(self, memories, n_tasks, g_pool):\n\n        vloss_coef = configs.vloss_coef\n        ploss_coef = configs.ploss_coef\n        entloss_coef = configs.entloss_coef\n\n        rewards_all_env = []\n        adj_mb_t_all_env = []\n        fea_mb_t_all_env = []\n        candidate_mb_t_all_env = []\n        mask_mb_t_all_env = []\n        a_mb_t_all_env = []\n        old_logprobs_mb_t_all_env = []\n        # store data for all env\n        for i in range(len(memories)):\n            rewards = []\n            discounted_reward = 0\n            for reward, is_terminal in zip(reversed(memories[i].r_mb), reversed(memories[i].done_mb)):\n                if is_terminal:\n                    discounted_reward = 0\n                discounted_reward = reward + (self.gamma * discounted_reward)\n                rewards.insert(0, discounted_reward)\n            rewards = torch.tensor(rewards, dtype=torch.float).to(device)\n            rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n            rewards_all_env.append(rewards)\n            # process each env data\n            adj_mb_t_all_env.append(aggr_obs(torch.stack(memories[i].adj_mb).to(device), n_tasks))\n            fea_mb_t = torch.stack(memories[i].fea_mb).to(device)\n            fea_mb_t = fea_mb_t.reshape(-1, fea_mb_t.size(-1))\n            fea_mb_t_all_env.append(fea_mb_t)\n            candidate_mb_t_all_env.append(torch.stack(memories[i].candidate_mb).to(device).squeeze())\n            mask_mb_t_all_env.append(torch.stack(memories[i].mask_mb).to(device).squeeze())\n            a_mb_t_all_env.append(torch.stack(memories[i].a_mb).to(device).squeeze())\n            old_logprobs_mb_t_all_env.append(torch.stack(memories[i].logprobs).to(device).squeeze().detach())\n\n        # get batch argument for net forwarding: mb_g_pool is same for all env\n        mb_g_pool = g_pool_cal(g_pool, torch.stack(memories[0].adj_mb).to(device).shape, n_tasks, device)\n\n        # Optimize policy for K epochs:\n        for _ in range(self.k_epochs):\n            loss_sum = 0\n            vloss_sum = 0\n            for i in range(len(memories)):\n                pis, vals = self.policy(x=fea_mb_t_all_env[i],\n                                        graph_pool=mb_g_pool,\n                                        adj=adj_mb_t_all_env[i],\n                                        candidate=candidate_mb_t_all_env[i],\n                                        mask=mask_mb_t_all_env[i],\n                                        padded_nei=None)\n                logprobs, ent_loss = eval_actions(pis.squeeze(), a_mb_t_all_env[i])\n                ratios = torch.exp(logprobs - old_logprobs_mb_t_all_env[i].detach())\n                advantages = rewards_all_env[i] - vals.view(-1).detach()\n                surr1 = ratios * advantages\n                surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n                v_loss = self.V_loss_2(vals.squeeze(), rewards_all_env[i])\n                p_loss = - torch.min(surr1, surr2).mean()\n                ent_loss = - ent_loss.clone()\n                loss = vloss_coef * v_loss + ploss_coef * p_loss + entloss_coef * ent_loss\n                loss_sum += loss\n                vloss_sum += v_loss\n            self.optimizer.zero_grad()\n            loss_sum.mean().backward()\n            self.optimizer.step()\n\n        # Copy new weights into old policy:\n        self.policy_old.load_state_dict(self.policy.state_dict())\n        if configs.decayflag:\n            self.scheduler.step()\n        return loss_sum.mean().item(), vloss_sum.mean().item()\n\n\ndef main():\n\n    from JSSP_Env import SJSSP\n    envs = [SJSSP(n_j=configs.n_j, n_m=configs.n_m) for _ in range(configs.num_envs)]\n    \n    from uniform_instance_gen import uni_instance_gen\n    data_generator = uni_instance_gen\n\n    dataLoaded = np.load('./DataGen/generatedData' + str(configs.n_j) + '_' + str(configs.n_m) + '_Seed' + str(configs.np_seed_validation) + '.npy')\n    vali_data = []\n    for i in range(dataLoaded.shape[0]):\n        vali_data.append((dataLoaded[i][0], dataLoaded[i][1]))\n\n    torch.manual_seed(configs.torch_seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(configs.torch_seed)\n    np.random.seed(configs.np_seed_train)\n\n    memories = [Memory() for _ in range(configs.num_envs)]\n\n    ppo = PPO(configs.lr, configs.gamma, configs.k_epochs, configs.eps_clip,\n              n_j=configs.n_j,\n              n_m=configs.n_m,\n              num_layers=configs.num_layers,\n              neighbor_pooling_type=configs.neighbor_pooling_type,\n              input_dim=configs.input_dim,\n              hidden_dim=configs.hidden_dim,\n              num_mlp_layers_feature_extract=configs.num_mlp_layers_feature_extract,\n              num_mlp_layers_actor=configs.num_mlp_layers_actor,\n              hidden_dim_actor=configs.hidden_dim_actor,\n              num_mlp_layers_critic=configs.num_mlp_layers_critic,\n              hidden_dim_critic=configs.hidden_dim_critic)\n\n    g_pool_step = g_pool_cal(graph_pool_type=configs.graph_pool_type,\n                             batch_size=torch.Size([1, configs.n_j*configs.n_m, configs.n_j*configs.n_m]),\n                             n_nodes=configs.n_j*configs.n_m,\n                             device=device)\n    # training loop\n    log = []\n    validation_log = []\n    optimal_gaps = []\n    optimal_gap = 1\n    record = 100000\n    for i_update in range(configs.max_updates):\n\n        t3 = time.time()\n\n        ep_rewards = [0 for _ in range(configs.num_envs)]\n        adj_envs = []\n        fea_envs = []\n        candidate_envs = []\n        mask_envs = []\n        \n        for i, env in enumerate(envs):\n            adj, fea, candidate, mask = env.reset(data_generator(n_j=configs.n_j, n_m=configs.n_m, low=configs.low, high=configs.high))\n            adj_envs.append(adj)\n            fea_envs.append(fea)\n            candidate_envs.append(candidate)\n            mask_envs.append(mask)\n            ep_rewards[i] = - env.initQuality\n        # rollout the env\n        while True:\n            fea_tensor_envs = [torch.from_numpy(np.copy(fea)).to(device) for fea in fea_envs]\n            adj_tensor_envs = [torch.from_numpy(np.copy(adj)).to(device).to_sparse() for adj in adj_envs]\n            candidate_tensor_envs = [torch.from_numpy(np.copy(candidate)).to(device) for candidate in candidate_envs]\n            mask_tensor_envs = [torch.from_numpy(np.copy(mask)).to(device) for mask in mask_envs]\n            \n            with torch.no_grad():\n                action_envs = []\n                a_idx_envs = []\n                for i in range(configs.num_envs):\n                    pi, _ = ppo.policy_old(x=fea_tensor_envs[i],\n                                           graph_pool=g_pool_step,\n                                           padded_nei=None,\n                                           adj=adj_tensor_envs[i],\n                                           candidate=candidate_tensor_envs[i].unsqueeze(0),\n                                           mask=mask_tensor_envs[i].unsqueeze(0))\n                    action, a_idx = select_action(pi, candidate_envs[i], memories[i])\n                    action_envs.append(action)\n                    a_idx_envs.append(a_idx)\n            \n            adj_envs = []\n            fea_envs = []\n            candidate_envs = []\n            mask_envs = []\n            # Saving episode data\n            for i in range(configs.num_envs):\n                memories[i].adj_mb.append(adj_tensor_envs[i])\n                memories[i].fea_mb.append(fea_tensor_envs[i])\n                memories[i].candidate_mb.append(candidate_tensor_envs[i])\n                memories[i].mask_mb.append(mask_tensor_envs[i])\n                memories[i].a_mb.append(a_idx_envs[i])\n\n                adj, fea, reward, done, candidate, mask = envs[i].step(action_envs[i].item())\n                adj_envs.append(adj)\n                fea_envs.append(fea)\n                candidate_envs.append(candidate)\n                mask_envs.append(mask)\n                ep_rewards[i] += reward\n                memories[i].r_mb.append(reward)\n                memories[i].done_mb.append(done)\n            if envs[0].done():\n                break\n        for j in range(configs.num_envs):\n            ep_rewards[j] -= envs[j].posRewards\n\n        loss, v_loss = ppo.update(memories, configs.n_j*configs.n_m, configs.graph_pool_type)\n        for memory in memories:\n            memory.clear_memory()\n        mean_rewards_all_env = sum(ep_rewards) / len(ep_rewards)\n        log.append([i_update, mean_rewards_all_env])\n        if (i_update + 1) % 100 == 0:\n            file_writing_obj = open('./' + 'log_' + str(configs.n_j) + '_' + str(configs.n_m) + '_' + str(configs.low) + '_' + str(configs.high) + '.txt', 'w')\n            file_writing_obj.write(str(log))\n\n        # log results\n        print('Episode {}\\t Last reward: {:.2f}\\t Mean_Vloss: {:.8f}'.format(\n            i_update + 1, mean_rewards_all_env, v_loss))\n        \n        # validate and save use mean performance\n        t4 = time.time()\n        if (i_update + 1) % 100 == 0:\n            vali_result = - validate(vali_data, ppo.policy).mean()\n            validation_log.append(vali_result)\n            if vali_result < record:\n                torch.save(ppo.policy.state_dict(), './{}.pth'.format(\n                    str(configs.n_j) + '_' + str(configs.n_m) + '_' + str(configs.low) + '_' + str(configs.high)))\n                record = vali_result\n            print('The validation quality is:', vali_result)\n            file_writing_obj1 = open(\n                './' + 'vali_' + str(configs.n_j) + '_' + str(configs.n_m) + '_' + str(configs.low) + '_' + str(configs.high) + '.txt', 'w')\n            file_writing_obj1.write(str(validation_log))\n        t5 = time.time()\n\n        # print('Training:', t4 - t3)\n        # print('Validation:', t5 - t4)\n\n\nif __name__ == '__main__':\n    total1 = time.time()\n    main()\n    total2 = time.time()\n    # print(total2 - total1)\n\n# ==========================================\n# File: models/actor_critic.py\n# Function/Context: ActorCritic.forward\n# ==========================================\nimport torch.nn as nn\nfrom models.mlp import MLPActor\nfrom models.mlp import MLPCritic\nimport torch.nn.functional as F\nfrom models.graphcnn_congForSJSSP import GraphCNN\nimport torch\n\n\nclass ActorCritic(nn.Module):\n    def __init__(self,\n                 n_j,\n                 n_m,\n                 # feature extraction net unique attributes:\n                 num_layers,\n                 learn_eps,\n                 neighbor_pooling_type,\n                 input_dim,\n                 hidden_dim,\n                 # feature extraction net MLP attributes:\n                 num_mlp_layers_feature_extract,\n                 # actor net MLP attributes:\n                 num_mlp_layers_actor,\n                 hidden_dim_actor,\n                 # actor net MLP attributes:\n                 num_mlp_layers_critic,\n                 hidden_dim_critic,\n                 # actor/critic/feature_extraction shared attribute\n                 device\n                 ):\n        super(ActorCritic, self).__init__()\n        # job size for problems, no business with network\n        self.n_j = n_j\n        # machine size for problems, no business with network\n        self.n_m = n_m\n        self.n_ops_perjob = n_m\n        self.device = device\n\n        self.feature_extract = GraphCNN(num_layers=num_layers,\n                                        num_mlp_layers=num_mlp_layers_feature_extract,\n                                        input_dim=input_dim,\n                                        hidden_dim=hidden_dim,\n                                        learn_eps=learn_eps,\n                                        neighbor_pooling_type=neighbor_pooling_type,\n                                        device=device).to(device)\n        self.actor = MLPActor(num_mlp_layers_actor, hidden_dim*2, hidden_dim_actor, 1).to(device)\n        self.critic = MLPCritic(num_mlp_layers_critic, hidden_dim, hidden_dim_critic, 1).to(device)\n\n    def forward(self,\n                x,\n                graph_pool,\n                padded_nei,\n                adj,\n                candidate,\n                mask,\n                ):\n\n        h_pooled, h_nodes = self.feature_extract(x=x,\n                                                 graph_pool=graph_pool,\n                                                 padded_nei=padded_nei,\n                                                 adj=adj)\n        # prepare policy feature: concat omega feature with global feature\n        dummy = candidate.unsqueeze(-1).expand(-1, self.n_j, h_nodes.size(-1))\n        candidate_feature = torch.gather(h_nodes.reshape(dummy.size(0), -1, dummy.size(-1)), 1, dummy)\n        h_pooled_repeated = h_pooled.unsqueeze(1).expand_as(candidate_feature)\n\n        '''# prepare policy feature: concat row work remaining feature\n        durfea2mat = x[:, 1].reshape(shape=(-1, self.n_j, self.n_m))\n        mask_right_half = torch.zeros_like(durfea2mat)\n        mask_right_half.put_(omega, torch.ones_like(omega, dtype=torch.float))\n        mask_right_half = torch.cumsum(mask_right_half, dim=-1)\n        # calculate work remaining and normalize it with job size\n        wkr = (mask_right_half * durfea2mat).sum(dim=-1, keepdim=True)/self.n_ops_perjob'''\n\n        # concatenate feature\n        # concateFea = torch.cat((wkr, candidate_feature, h_pooled_repeated), dim=-1)\n        concateFea = torch.cat((candidate_feature, h_pooled_repeated), dim=-1)\n        candidate_scores = self.actor(concateFea)\n\n        # perform mask\n        mask_reshape = mask.reshape(candidate_scores.size())\n        candidate_scores[mask_reshape] = float('-inf')\n\n        pi = F.softmax(candidate_scores, dim=1)\n        v = self.critic(h_pooled)\n        return pi, v\n\n\nif __name__ == '__main__':\n    print('Go home')\n\n# ==========================================\n# File: models/graphcnn_congForSJSSP.py\n# Function/Context: GraphCNN\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom models.mlp import MLP\n\nclass GraphCNN(nn.Module):\n    def __init__(self,\n                 num_layers,\n                 num_mlp_layers,\n                 input_dim,\n                 hidden_dim,\n                 learn_eps,\n                 neighbor_pooling_type,\n                 device):\n        super(GraphCNN, self).__init__()\n        self.device = device\n        self.num_layers = num_layers\n        self.neighbor_pooling_type = neighbor_pooling_type\n        self.learn_eps = learn_eps\n        self.mlps = torch.nn.ModuleList()\n        self.batch_norms = torch.nn.ModuleList()\n        for layer in range(self.num_layers-1):\n            if layer == 0:\n                self.mlps.append(MLP(num_mlp_layers, input_dim, hidden_dim, hidden_dim))\n            else:\n                self.mlps.append(MLP(num_mlp_layers, hidden_dim, hidden_dim, hidden_dim))\n            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n\n    def next_layer_eps(self, h, layer, padded_neighbor_list = None, Adj_block = None):\n        if self.neighbor_pooling_type == \"max\":\n            pooled = self.maxpool(h, padded_neighbor_list)\n        else:\n            pooled = torch.mm(Adj_block, h)\n            if self.neighbor_pooling_type == \"average\":\n                degree = torch.mm(Adj_block, torch.ones((Adj_block.shape[0], 1)).to(self.device))\n                pooled = pooled/degree\n        pooled = pooled + (1 + self.eps[layer])*h\n        pooled_rep = self.mlps[layer](pooled)\n        h = self.batch_norms[layer](pooled_rep)\n        h = F.relu(h)\n        return h\n\n    def next_layer(self, h, layer, padded_neighbor_list = None, Adj_block = None):\n        if self.neighbor_pooling_type == \"max\":\n            pooled = self.maxpool(h, padded_neighbor_list)\n        else:\n            pooled = torch.mm(Adj_block, h)\n            if self.neighbor_pooling_type == \"average\":\n                degree = torch.mm(Adj_block, torch.ones((Adj_block.shape[0], 1)).to(self.device))\n                pooled = pooled/degree\n        pooled_rep = self.mlps[layer](pooled)\n        h = self.batch_norms[layer](pooled_rep)\n        h = F.relu(h)\n        return h\n\n    def forward(self,\n                x,\n                graph_pool,\n                padded_nei,\n                adj):\n        x_concat = x\n        graph_pool = graph_pool\n        if self.neighbor_pooling_type == \"max\":\n            padded_neighbor_list = padded_nei\n        else:\n            Adj_block = adj\n        h = x_concat\n        for layer in range(self.num_layers-1):\n            if self.neighbor_pooling_type == \"max\" and self.learn_eps:\n                h = self.next_layer_eps(h, layer, padded_neighbor_list=padded_neighbor_list)\n            elif not self.neighbor_pooling_type == \"max\" and self.learn_eps:\n                h = self.next_layer_eps(h, layer, Adj_block=Adj_block)\n            elif self.neighbor_pooling_type == \"max\" and not self.learn_eps:\n                h = self.next_layer(h, layer, padded_neighbor_list=padded_neighbor_list)\n            elif not self.neighbor_pooling_type == \"max\" and not self.learn_eps:\n                h = self.next_layer(h, layer, Adj_block=Adj_block)\n        h_nodes = h.clone()\n        pooled_h = torch.sparse.mm(graph_pool, h)\n        return pooled_h, h_nodes\n\n# ==========================================\n# File: permissibleLS.py\n# Function/Context: permissibleLeftShift\n# ==========================================\nfrom Params import configs\nimport numpy as np\n\ndef permissibleLeftShift(a, durMat, mchMat, mchsStartTimes, opIDsOnMchs):\n    jobRdyTime_a, mchRdyTime_a = calJobAndMchRdyTimeOfa(a, mchMat, durMat, mchsStartTimes, opIDsOnMchs)\n    dur_a = np.take(durMat, a)\n    mch_a = np.take(mchMat, a) - 1\n    startTimesForMchOfa = mchsStartTimes[mch_a]\n    opsIDsForMchOfa = opIDsOnMchs[mch_a]\n    flag = False\n\n    possiblePos = np.where(jobRdyTime_a < startTimesForMchOfa)[0]\n    # print('possiblePos:', possiblePos)\n    if len(possiblePos) == 0:\n        startTime_a = putInTheEnd(a, jobRdyTime_a, mchRdyTime_a, startTimesForMchOfa, opsIDsForMchOfa)\n    else:\n        idxLegalPos, legalPos, endTimesForPossiblePos = calLegalPos(dur_a, jobRdyTime_a, durMat, possiblePos, startTimesForMchOfa, opsIDsForMchOfa)\n        # print('legalPos:', legalPos)\n        if len(legalPos) == 0:\n            startTime_a = putInTheEnd(a, jobRdyTime_a, mchRdyTime_a, startTimesForMchOfa, opsIDsForMchOfa)\n        else:\n            flag = True\n            startTime_a = putInBetween(a, idxLegalPos, legalPos, endTimesForPossiblePos, startTimesForMchOfa, opsIDsForMchOfa)\n    return startTime_a, flag\n\ndef putInTheEnd(a, jobRdyTime_a, mchRdyTime_a, startTimesForMchOfa, opsIDsForMchOfa):\n    # index = first position of -config.high in startTimesForMchOfa\n    # print('Yes!OK!')\n    index = np.where(startTimesForMchOfa == -configs.high)[0][0]\n    startTime_a = max(jobRdyTime_a, mchRdyTime_a)\n    startTimesForMchOfa[index] = startTime_a\n    opsIDsForMchOfa[index] = a\n    return startTime_a\n\ndef calLegalPos(dur_a, jobRdyTime_a, durMat, possiblePos, startTimesForMchOfa, opsIDsForMchOfa):\n    startTimesOfPossiblePos = startTimesForMchOfa[possiblePos]\n    durOfPossiblePos = np.take(durMat, opsIDsForMchOfa[possiblePos])\n    startTimeEarlst = max(jobRdyTime_a, startTimesForMchOfa[possiblePos[0]-1] + np.take(durMat, [opsIDsForMchOfa[possiblePos[0]-1]]))\n    endTimesForPossiblePos = np.append(startTimeEarlst, (startTimesOfPossiblePos + durOfPossiblePos))[:-1]# end time for last ops don't care\n    possibleGaps = startTimesOfPossiblePos - endTimesForPossiblePos\n    idxLegalPos = np.where(dur_a <= possibleGaps)[0]\n    legalPos = np.take(possiblePos, idxLegalPos)\n    return idxLegalPos, legalPos, endTimesForPossiblePos\n\ndef putInBetween(a, idxLegalPos, legalPos, endTimesForPossiblePos, startTimesForMchOfa, opsIDsForMchOfa):\n    earlstIdx = idxLegalPos[0]\n    # print('idxLegalPos:', idxLegalPos)\n    earlstPos = legalPos[0]\n    startTime_a = endTimesForPossiblePos[earlstIdx]\n    # print('endTimesForPossiblePos:', endTimesForPossiblePos)\n    startTimesForMchOfa[:] = np.insert(startTimesForMchOfa, earlstPos, startTime_a)[:-1]\n    opsIDsForMchOfa[:] = np.insert(opsIDsForMchOfa, earlstPos, a)[:-1]\n    return startTime_a\n\ndef calJobAndMchRdyTimeOfa(a, mchMat, durMat, mchsStartTimes, opIDsOnMchs):\n    mch_a = np.take(mchMat, a) - 1\n    # cal jobRdyTime_a\n    jobPredecessor = a - 1 if a % mchMat.shape[1] != 0 else None\n    if jobPredecessor is not None:\n        durJobPredecessor = np.take(durMat, jobPredecessor)\n        mchJobPredecessor = np.take(mchMat, jobPredecessor) - 1\n        jobRdyTime_a = (mchsStartTimes[mchJobPredecessor][np.where(opIDsOnMchs[mchJobPredecessor] == jobPredecessor)] + durJobPredecessor).item()\n    else:\n        jobRdyTime_a = 0\n    # cal mchRdyTime_a\n    mchPredecessor = opIDsOnMchs[mch_a][np.where(opIDsOnMchs[mch_a] >= 0)][-1] if len(np.where(opIDsOnMchs[mch_a] >= 0)[0]) != 0 else None\n    if mchPredecessor is not None:\n        durMchPredecessor = np.take(durMat, mchPredecessor)\n        mchRdyTime_a = (mchsStartTimes[mch_a][np.where(mchsStartTimes[mch_a] >= 0)][-1] + durMchPredecessor).item()\n    else:\n        mchRdyTime_a = 0\n\n    return jobRdyTime_a, mchRdyTime_a\n\n# ==========================================\n# File: test_learned.py\n# Function/Context: test\n# ==========================================\nfrom mb_agg import *\nfrom agent_utils import *\nimport torch\nimport argparse\nfrom Params import configs\nimport time\nimport numpy as np\n\n\ndevice = torch.device(configs.device)\n\nparser = argparse.ArgumentParser(description='Arguments for ppo_jssp')\nparser.add_argument('--Pn_j', type=int, default=15, help='Number of jobs of instances to test')\nparser.add_argument('--Pn_m', type=int, default=15, help='Number of machines instances to test')\nparser.add_argument('--Nn_j', type=int, default=15, help='Number of jobs on which to be loaded net are trained')\nparser.add_argument('--Nn_m', type=int, default=15, help='Number of machines on which to be loaded net are trained')\nparser.add_argument('--low', type=int, default=1, help='LB of duration')\nparser.add_argument('--high', type=int, default=99, help='UB of duration')\nparser.add_argument('--seed', type=int, default=200, help='Seed for validate set generation')\nparams = parser.parse_args()\n\nN_JOBS_P = params.Pn_j\nN_MACHINES_P = params.Pn_m\nLOW = params.low\nHIGH = params.high\nSEED = params.seed\nN_JOBS_N = params.Nn_j\nN_MACHINES_N = params.Nn_m\n\n\nfrom JSSP_Env import SJSSP\nfrom PPO_jssp_multiInstances import PPO\nenv = SJSSP(n_j=N_JOBS_P, n_m=N_MACHINES_P)\n\nppo = PPO(configs.lr, configs.gamma, configs.k_epochs, configs.eps_clip,\n          n_j=N_JOBS_P,\n          n_m=N_MACHINES_P,\n          num_layers=configs.num_layers,\n          neighbor_pooling_type=configs.neighbor_pooling_type,\n          input_dim=configs.input_dim,\n          hidden_dim=configs.hidden_dim,\n          num_mlp_layers_feature_extract=configs.num_mlp_layers_feature_extract,\n          num_mlp_layers_actor=configs.num_mlp_layers_actor,\n          hidden_dim_actor=configs.hidden_dim_actor,\n          num_mlp_layers_critic=configs.num_mlp_layers_critic,\n          hidden_dim_critic=configs.hidden_dim_critic)\npath = './SavedNetwork/{}.pth'.format(str(N_JOBS_N) + '_' + str(N_MACHINES_N) + '_' + str(LOW) + '_' + str(HIGH))\nppo.policy.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\ng_pool_step = g_pool_cal(graph_pool_type=configs.graph_pool_type,\n                         batch_size=torch.Size([1, env.number_of_tasks, env.number_of_tasks]),\n                         n_nodes=env.number_of_tasks,\n                         device=device)\n\nfrom uniform_instance_gen import uni_instance_gen\nnp.random.seed(SEED)\n\ndataLoaded = np.load('./DataGen/generatedData' + str(N_JOBS_P) + '_' + str(N_MACHINES_P) + '_Seed' + str(SEED) + '.npy')\ndataset = []\n\nfor i in range(dataLoaded.shape[0]):\n    dataset.append((dataLoaded[i][0], dataLoaded[i][1]))\n\n\ndef test(dataset):\n    result = []\n    t1 = time.time()\n    for i, data in enumerate(dataset):\n        adj, fea, candidate, mask = env.reset(data)\n        ep_reward = - env.max_endTime\n        while True:\n            fea_tensor = torch.from_numpy(fea).to(device)\n            adj_tensor = torch.from_numpy(adj).to(device)\n            candidate_tensor = torch.from_numpy(candidate).to(device)\n            mask_tensor = torch.from_numpy(mask).to(device)\n\n            with torch.no_grad():\n                pi, _ = ppo.policy(x=fea_tensor,\n                                   graph_pool=g_pool_step,\n                                   padded_nei=None,\n                                   adj=adj_tensor,\n                                   candidate=candidate_tensor.unsqueeze(0),\n                                   mask=mask_tensor.unsqueeze(0))\n                action = greedy_select_action(pi, candidate)\n\n            adj, fea, reward, done, candidate, mask = env.step(action)\n            ep_reward += reward\n\n            if done:\n                break\n        print('Instance' + str(i + 1) + ' makespan:', -ep_reward + env.posRewards)\n        result.append(-ep_reward + env.posRewards)\n    t2 = time.time()\n    print(t2 - t1)\n    file_writing_obj = open('./' + 'drltime_' + str(N_JOBS_N) + 'x' + str(N_MACHINES_N) + '_' + str(N_JOBS_P) + 'x' + str(N_MACHINES_P) + '.txt', 'w')\n    file_writing_obj.write(str((t2 - t1)/len(dataset)))\n\n    np.save('drlResult_' + str(N_JOBS_N) + 'x' + str(N_MACHINES_N) + '_' + str(N_JOBS_P) + 'x' + str(N_MACHINES_P) + '_Seed' + str(SEED), np.array(result, dtype=np.single))\n\n\nif __name__ == '__main__':\n    import cProfile\n    cProfile.run('test(dataset)', filename='restats')\n\n# ==========================================\n# File: updateEntTimeLB.py\n# Function/Context: calEndTimeLB\n# ==========================================\nimport numpy as np\n\n\ndef lastNonZero(arr, axis, invalid_val=-1):\n    mask = arr != 0\n    val = arr.shape[axis] - np.flip(mask, axis=axis).argmax(axis=axis) - 1\n    yAxis = np.where(mask.any(axis=axis), val, invalid_val)\n    xAxis = np.arange(arr.shape[0], dtype=np.int64)\n    xRet = xAxis[yAxis >= 0]\n    yRet = yAxis[yAxis >= 0]\n    return xRet, yRet\n\n\ndef calEndTimeLB(temp1, dur_cp):\n    x, y = lastNonZero(temp1, 1, invalid_val=-1)\n    dur_cp[np.where(temp1 != 0)] = 0\n    dur_cp[x, y] = temp1[x, y]\n    temp2 = np.cumsum(dur_cp, axis=1)\n    temp2[np.where(temp1 != 0)] = 0\n    ret = temp1+temp2\n    return ret\n\n\nif __name__ == '__main__':\n    dur = np.array([[1, 2], [3, 4]])\n    temp1 = np.zeros_like(dur)\n\n    temp1[0, 0] = 1\n    temp1[1, 0] = 3\n    temp1[1, 1] = 5\n    print(temp1)\n\n    ret = calEndTimeLB(temp1, dur)\n\n# ==========================================\n# File: validation.py\n# Function/Context: validate\n# ==========================================\ndef validate(vali_set, model):\n    N_JOBS = vali_set[0][0].shape[0]\n    N_MACHINES = vali_set[0][0].shape[1]\n\n    from JSSP_Env import SJSSP\n    from mb_agg import g_pool_cal\n    from agent_utils import sample_select_action\n    from agent_utils import greedy_select_action\n    import numpy as np\n    import torch\n    from Params import configs\n    env = SJSSP(n_j=N_JOBS, n_m=N_MACHINES)\n    device = torch.device(configs.device)\n    g_pool_step = g_pool_cal(graph_pool_type=configs.graph_pool_type,\n                             batch_size=torch.Size([1, env.number_of_tasks, env.number_of_tasks]),\n                             n_nodes=env.number_of_tasks,\n                             device=device)\n    make_spans = []\n    # rollout using model\n    for data in vali_set:\n        adj, fea, candidate, mask = env.reset(data)\n        rewards = - env.initQuality\n        while True:\n            fea_tensor = torch.from_numpy(np.copy(fea)).to(device)\n            adj_tensor = torch.from_numpy(np.copy(adj)).to(device).to_sparse()\n            candidate_tensor = torch.from_numpy(np.copy(candidate)).to(device)\n            mask_tensor = torch.from_numpy(np.copy(mask)).to(device)\n            with torch.no_grad():\n                pi, _ = model(x=fea_tensor,\n                              graph_pool=g_pool_step,\n                              padded_nei=None,\n                              adj=adj_tensor,\n                              candidate=candidate_tensor.unsqueeze(0),\n                              mask=mask_tensor.unsqueeze(0))\n            # action = sample_select_action(pi, candidate)\n            action = greedy_select_action(pi, candidate)\n            adj, fea, reward, done, candidate, mask = env.step(action.item())\n            rewards += reward\n            if done:\n                break\n        make_spans.append(rewards - env.posRewards)\n        # print(rewards - env.posRewards)\n    return np.array(make_spans)\n\n\nif __name__ == '__main__':\n\n    from uniform_instance_gen import uni_instance_gen\n    import numpy as np\n    import time\n    import argparse\n    from Params import configs\n\n    parser = argparse.ArgumentParser(description='Arguments for ppo_jssp')\n    parser.add_argument('--Pn_j', type=int, default=20, help='Number of jobs of instances to test')\n    parser.add_argument('--Pn_m', type=int, default=15, help='Number of machines instances to test')\n    parser.add_argument('--Nn_j', type=int, default=20, help='Number of jobs on which to be loaded net are trained')\n    parser.add_argument('--Nn_m', type=int, default=15, help='Number of machines on which to be loaded net are trained')\n    parser.add_argument('--low', type=int, default=1, help='LB of duration')\n    parser.add_argument('--high', type=int, default=99, help='UB of duration')\n    parser.add_argument('--seed', type=int, default=200, help='Cap seed for validate set generation')\n    parser.add_argument('--n_vali', type=int, default=100, help='validation set size')\n    params = parser.parse_args()\n\n    N_JOBS_P = params.Pn_j\n    N_MACHINES_P = params.Pn_m\n    LOW = params.low\n    HIGH = params.high\n    N_JOBS_N = params.Nn_j\n    N_MACHINES_N = params.Nn_m\n\n    from PPO_jssp_multiInstances import PPO\n    import torch\n\n    ppo = PPO(configs.lr, configs.gamma, configs.k_epochs, configs.eps_clip,\n              n_j=N_JOBS_P,\n              n_m=N_MACHINES_P,\n              num_layers=configs.num_layers,\n              neighbor_pooling_type=configs.neighbor_pooling_type,\n              input_dim=configs.input_dim,\n              hidden_dim=configs.hidden_dim,\n              num_mlp_layers_feature_extract=configs.num_mlp_layers_feature_extract,\n              num_mlp_layers_actor=configs.num_mlp_layers_actor,\n              hidden_dim_actor=configs.hidden_dim_actor,\n              num_mlp_layers_critic=configs.num_mlp_layers_critic,\n              hidden_dim_critic=configs.hidden_dim_critic)\n\n    path = './{}.pth'.format(str(N_JOBS_N) + '_' + str(N_MACHINES_N) + '_' + str(LOW) + '_' + str(HIGH))\n    ppo.policy.load_state_dict(torch.load(path))\n\n    SEEDs = range(0, params.seed, 10)\n    result = []\n    for SEED in SEEDs:\n\n        np.random.seed(SEED)\n\n        vali_data = [uni_instance_gen(n_j=N_JOBS_P, n_m=N_MACHINES_P, low=LOW, high=HIGH) for _ in range(params.n_vali)]\n\n        makespan = - validate(vali_data, ppo.policy)\n        print(makespan.mean())\n\n\n    # print(min(result))",
  "description": "Combined Analysis:\n- [JSSP_Env.py]: This file implements the core MDP environment for JSSP as described in the paper. The SJSSP class extends gym.Env and implements: 1) State representation as disjunctive graph (adjacency matrix) with node features (lower bound end times and completion status), 2) Action space of selecting eligible operations (via omega/mask), 3) Transition dynamics through permissible left shift scheduling, 4) Reward based on reduction in makespan lower bound, 5) Constraint enforcement through precedence updates in adjacency matrix. The step() method exactly implements the scheduling process where actions dispatch operations to machines while respecting constraints, and reset() initializes the problem instance with proper graph structure.\n- [PPO_jssp_multiInstances.py]: This file implements the core Proximal Policy Optimization (PPO) training loop for the Job Shop Scheduling Problem (JSSP) as described in the paper. It matches the algorithm steps by: 1) Creating multiple JSSP environments (SJSSP instances) for parallel training, 2) Using a Graph Neural Network (ActorCritic) to embed disjunctive graph states and output action probabilities, 3) Implementing the PPO update with clipped surrogate objective and value function loss, 4) Following the MDP formulation where actions select eligible operations and rewards are based on makespan reduction. The code directly corresponds to the paper's DRL approach for learning priority dispatching rules.\n- [models/actor_critic.py]: This file implements the core neural network architecture for the DRL-based JSSP solver. The ActorCritic class contains the GraphCNN-based feature extractor that embeds the disjunctive graph state representation (aligning with the paper's MDP formulation). The forward method computes action probabilities (policy π) over eligible operations and state value (V) estimates. The actor network processes concatenated features of candidate operations and global graph embeddings, while the critic evaluates the global state. This directly implements the policy network component of the PPO algorithm described in the paper.\n- [models/graphcnn_congForSJSSP.py]: This file implements the GraphCNN component of the GNN-based policy network described in the paper. The GraphCNN processes the disjunctive graph representation of the JSSP state through multiple graph convolutional layers with configurable pooling operations (max, sum, average). It outputs both node-level embeddings (h_nodes) for operation features and a graph-level embedding (pooled_h) for global state representation, which are used to compute action probabilities for dispatching decisions. This directly corresponds to the state embedding step in the DRL algorithm's MDP formulation.\n- [permissibleLS.py]: This file implements the core scheduling logic for the Job Shop Scheduling Problem (JSSP) by performing permissible left shifts to minimize makespan. The main function 'permissibleLeftShift' computes the earliest feasible start time for a given operation 'a' while respecting job precedence constraints (S_{i,j} ≥ S_{i,j-1} + p_{i,j-1}) and machine capacity constraints (disjunctive constraints). It calculates job ready time (completion of previous operation in the same job) and machine ready time (completion of previous operation on the same machine), then attempts to insert the operation into existing gaps on the machine schedule to left-shift it as early as possible. This directly implements the constraint satisfaction and makespan minimization objective by compacting the schedule through left shifts.\n- [test_learned.py]: This file implements the core evaluation/testing logic of the learned dispatching policy for Job Shop Scheduling. It loads a pre-trained PPO model and evaluates it on generated JSSP instances. The key algorithm steps from the paper are present: (1) State representation via disjunctive graph (adjacency matrix and node features), (2) Action selection using the GNN-based policy network (with graph pooling), (3) Step-by-step environment interaction to build complete schedules, (4) Makespan computation as the objective metric. The code matches the paper's MDP formulation where states are graph representations, actions are selections from eligible operations, and the policy is evaluated greedily. The optimization model's constraints are enforced by the environment (JSSP_Env), while this file focuses on policy execution and performance measurement.\n- [updateEntTimeLB.py]: This file implements a key component of the makespan lower bound calculation used in the reward function of the DRL algorithm. The function calEndTimeLB computes a job-based lower bound for operation completion times by considering the cumulative processing time of remaining operations in each job. This lower bound is essential for evaluating the quality of scheduling decisions during training, as the reward is defined as the reduction in the makespan lower bound. The helper function lastNonZero identifies the last scheduled operation in each job, which is necessary for calculating the cumulative processing time of unscheduled operations.\n- [validation.py]: This file implements the evaluation phase of the learned dispatching policy for JSSP. The validate() function performs a rollout of the trained GNN policy on validation instances, following the MDP formulation from the paper. It uses the disjunctive graph state representation (adjacency matrix adj and node features fea), selects actions via the learned policy (using greedy selection), and computes makespan as the negative cumulative reward. The main block handles loading a pre-trained PPO model, generating validation instances, and reporting average makespan. This directly corresponds to the algorithm's inference step where the learned PDR is applied to minimize makespan.",
  "dependencies": [
    "uniform_instance_gen.override",
    "models.mlp.MLPActor",
    "putInBetween",
    "agent_utils.sample_select_action",
    "permissibleLS.permissibleLeftShift",
    "validation.validate",
    "calJobAndMchRdyTimeOfa",
    "time",
    "calLegalPos",
    "models.actor_critic",
    "copy.deepcopy",
    "cProfile",
    "torch.nn.functional",
    "PPO_jssp_multiInstances.PPO",
    "uniform_instance_gen.uni_instance_gen",
    "models.mlp.MLP",
    "models.mlp.MLPCritic",
    "putInTheEnd",
    "agent_utils.greedy_select_action",
    "gym",
    "models.graphcnn_congForSJSSP.GraphCNN",
    "Params.configs",
    "argparse",
    "JSSP_Env.SJSSP",
    "mb_agg",
    "updateEntTimeLB.calEndTimeLB",
    "numpy",
    "mb_agg.g_pool_cal",
    "updateAdjMat.getActionNbghs",
    "torch.nn",
    "agent_utils",
    "torch"
  ]
}