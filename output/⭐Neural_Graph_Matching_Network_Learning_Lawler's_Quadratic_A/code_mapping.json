{
  "file_path": "models/NGM/gnn.py, models/NGM/hypermodel.py, models/NGM/mgmmodel.py, models/NGM/model.py, models/NGM/model_v2.py, src/lap_solvers/sinkhorn.py",
  "function_name": "GNNLayer, HyperGNNLayer, Net.forward, Net.forward and Net.__ngm_forward, Net.forward, Net.forward, Sinkhorn.forward_log",
  "code_snippet": "\n\n# ==========================================\n# File: models/NGM/gnn.py\n# Function/Context: GNNLayer, HyperGNNLayer\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_sparse import spmm\nfrom torch_geometric.nn import MessagePassing, GCNConv, GINConv, SAGEConv\n\nfrom src.lap_solvers.sinkhorn import Sinkhorn\n\nfrom collections import Iterable\n\nclass GNNLayer(nn.Module):\n    def __init__(self, in_node_features, in_edge_features, out_node_features, out_edge_features,\n                 sk_channel=0, sk_iter=20, sk_tau=0.05, edge_emb=False):\n        super(GNNLayer, self).__init__()\n        self.in_nfeat = in_node_features\n        self.in_efeat = in_edge_features\n        self.out_efeat = out_edge_features\n        self.sk_channel = sk_channel\n        assert out_node_features == out_edge_features + self.sk_channel\n        if self.sk_channel > 0:\n            self.out_nfeat = out_node_features - self.sk_channel\n            self.sk = Sinkhorn(sk_iter, sk_tau)\n            self.classifier = nn.Linear(self.out_nfeat, self.sk_channel)\n        else:\n            self.out_nfeat = out_node_features\n            self.sk = self.classifier = None\n\n        if edge_emb:\n            self.e_func = nn.Sequential(\n                nn.Linear(self.in_efeat + self.in_nfeat, self.out_efeat),\n                nn.ReLU(),\n                nn.Linear(self.out_efeat, self.out_efeat),\n                nn.ReLU()\n            )\n        else:\n            self.e_func = None\n\n        self.n_func = nn.Sequential(\n            nn.Linear(self.in_nfeat, self.out_nfeat),\n            nn.ReLU(),\n            nn.Linear(self.out_nfeat, self.out_nfeat),\n            nn.ReLU(),\n        )\n\n        self.n_self_func = nn.Sequential(\n            nn.Linear(self.in_nfeat, self.out_nfeat),\n            nn.ReLU(),\n            nn.Linear(self.out_nfeat, self.out_nfeat),\n            nn.ReLU()\n        )\n\n    def forward(self, A, W, x, n1=None, n2=None, norm=True):\n        \"\"\"\n        :param A: adjacent matrix in 0/1 (b x n x n)\n        :param W: edge feature tensor (b x n x n x feat_dim)\n        :param x: node feature tensor (b x n x feat_dim)\n        \"\"\"\n        if self.e_func is not None:\n            W1 = torch.mul(A.unsqueeze(-1), x.unsqueeze(1))\n            W2 = torch.cat((W, W1), dim=-1)\n            W_new = self.e_func(W2)\n        else:\n            W_new = W\n\n        if norm is True:\n            A = F.normalize(A, p=1, dim=2)\n\n        x1 = self.n_func(x)\n        x2 = torch.matmul((A.unsqueeze(-1) * W_new).permute(0, 3, 1, 2), x1.unsqueeze(2).permute(0, 3, 1, 2)).squeeze(-1).transpose(1, 2)\n        x2 += self.n_self_func(x)\n\n        if self.classifier is not None:\n            assert n1.max() * n2.max() == x.shape[1]\n            x3 = self.classifier(x2)\n            n1_rep = torch.repeat_interleave(n1, self.sk_channel, dim=0)\n            n2_rep = torch.repeat_interleave(n2, self.sk_channel, dim=0)\n            x4 = x3.permute(0,2,1).reshape(x.shape[0] * self.sk_channel, n2.max(), n1.max()).transpose(1, 2)\n            x5 = self.sk(x4, n1_rep, n2_rep, dummy_row=True).transpose(2, 1).contiguous()\n\n            x6 = x5.reshape(x.shape[0], self.sk_channel, n1.max() * n2.max()).permute(0, 2, 1)\n            x_new = torch.cat((x2, x6), dim=-1)\n        else:\n            x_new = x2\n\n        return W_new, x_new\n\n\nclass HyperGNNLayer(nn.Module):\n    def __init__(self, in_node_features, in_edge_features, out_node_features, out_edge_features, orders=3, eps=1e-10,\n                 sk_channel=False, sk_iter=20, sk_tau=0.05):\n        super(HyperGNNLayer, self).__init__()\n        self.in_nfeat = in_node_features\n        self.in_efeat = in_edge_features\n        self.out_efeat = out_edge_features\n        self.eps = eps\n        self.sk_channel = sk_channel\n        assert out_node_features == out_edge_features + self.sk_channel\n        if self.sk_channel > 0:\n            self.out_nfeat = out_node_features - self.sk_channel\n            self.sk = Sinkhorn(sk_iter, sk_tau)\n            self.classifier = nn.Linear(self.out_nfeat, self.sk_channel)\n        else:\n            self.out_nfeat = out_node_features\n            self.sk = self.classifier = None\n\n        # used by forward_dense\n        self.n_func = nn.Sequential(\n            nn.Linear(self.in_nfeat, self.out_nfeat),\n            nn.ReLU(),\n            nn.Linear(self.out_nfeat, self.out_nfeat),\n            nn.ReLU(),\n        )\n\n        # used by forward_sparse\n        for i in range(2, orders + 1):\n            n_func = nn.Sequential(\n                nn.Linear(self.in_nfeat, self.out_nfeat),\n                nn.ReLU(),\n                nn.Linear(self.out_nfeat, self.out_nfeat),\n                nn.ReLU()\n            )\n            self.add_module('n_func_{}'.format(i), n_func)\n\n        self.n_self_func = nn.Sequential(\n            nn.Linear(self.in_nfeat, self.out_nfeat),\n            nn.ReLU(),\n            nn.Linear(self.out_nfeat, self.out_nfeat),\n            nn.ReLU()\n        )\n\n    def forward(self, A, W, x, n1=None, n2=None, weight=None, norm=True):\n        \"\"\"wrapper function of forward (support dense/sparse)\"\"\"\n        if not isinstance(A, Iterable):\n            A = [A]\n            W = [W]\n\n        W_new = []\n        if weight is None:\n            weight = [1.] * len(A)\n        assert len(weight) == len(A)\n        for i, (_A, _W, w) in enumerate(zip(A, W, weight)):\n            if type(_W) is tuple or (type(_W) is torch.Tensor and _W.is_sparse):\n                _W_new, _x = self.forward_sparse(_A, _W, x, norm)\n            else:\n                _W_new, _x = self.forward_dense(_A, _W, x, norm)\n            if i == 0:\n                x2 = _x * w\n            else:\n                x2 += _x * w\n            W_new.append(_W_new)\n\n        x2 += self.n_self_func(x)\n\n        if self.classifier is not None:\n            assert n1.max() * n2.max() == x.shape[1]\n            x3 = self.classifier(x2)\n            n1_rep = torch.repeat_interleave(n1, self.sk_channel, dim=0)\n            n2_rep = torch.repeat_interleave(n2, self.sk_channel, dim=0)\n            x4 = x3.permute(0,2,1).reshape(x.shape[0] * self.sk_channel, n2.max(), n1.max()).transpose(1, 2)\n            x5 = self.sk(x4, n1_rep, n2_rep, dummy_row=True).transpose(2, 1).contiguous()\n            x6 = x5.reshape(x.shape[0], self.sk_channel, n1.max() * n2.max()).permute(0, 2, 1)\n            x_new = torch.cat((x2, x6), dim=-1)\n        else:\n            x_new = x2\n\n        return W_new, x_new\n\n    def forward_sparse(self, A, W, x, norm=True):\n        \"\"\"\n        :param A: adjacent tensor in 0/1 (b x {n x ... x n})\n        :param W: edge feature tensor (b x {n x ... x n} x feat_dim)\n        :param x: node feature tensor (b x n x feat_dim)\n        \"\"\"\n        order = len(A.shape) - 1\n\n        if type(W) is tuple:\n            W_ind, W_val = W\n        elif type(W) is torch.Tensor and W.is_sparse:\n            W_ind = W._indices()\n            W_val = W._values()\n        else:\n            raise ValueError('Unknown datatype {}'.format(type(W)))\n\n        W_new_val = W_val\n        if norm is True:\n            assert not A.is_sparse, 'sparse normalization is currently not supported'\n            A_sum = torch.sum(A, dim=tuple(range(2, order + 1)), keepdim=True)\n            A = A / A_sum.expand_as(A)\n            A[torch.isnan(A)] = 0\n\n        if not A.is_sparse:\n            A = A.to_sparse()\n\n        assert A._values().shape[0] == W_new_val.shape[0]\n        assert len(W_new_val.shape) == 2\n\n        n_func = getattr(self, 'n_func_{}'.format(order))\n        x1 = n_func(x)\n\n        tp_val = torch.mul(A._values().unsqueeze(-1), W_new_val)\n        for i in range(order - 1):\n            tp_val = x1[W_ind[0, :], W_ind[-1-i, :], :] * tp_val\n\n        assert torch.all(W_ind == A._indices())\n\n        x_new = torch.zeros_like(x1)\n        x_new.index_put_((W_ind[0, :], W_ind[1, :]), tp_val, True)\n\n        return (W_ind, W_new_val), x_new\n\n\n    def forward_dense(self, A, W, x, norm=True):\n        \"\"\"\n        :param A: adjacent tensor in 0/1 (b x {n x ... x n})\n        :param W: edge feature tensor (b x {n x ... x n} x feat_dim)\n        :param x: node feature tensor (b x n x feat_dim)\n        \"\"\"\n        order = len(A.shape) - 1\n        W_new = W\n\n        if norm is True:\n            A_sum = torch.sum(A, dim=tuple(range(2, order + 1)), keepdim=True)\n            A = A / A_sum.expand_as(A)\n            A[torch.isnan(A)] = 0\n\n        x1 = self.n_func(x)\n\n        x_new = torch.mul(A.unsqueeze(-1), W_new)\n        for i in range(order - 1):\n            x1_shape = [x1.shape[0]] + [1] * (order - 1 - i) + list(x1.shape[1:])\n            x_new = torch.sum(torch.mul(x_new, x1.view(*x1_shape)), dim=-2)\n\n        return W_new, x_new\n\n# ==========================================\n# File: models/NGM/hypermodel.py\n# Function/Context: Net.forward\n# ==========================================\nimport torch.nn.functional as F\n\nfrom src.lap_solvers.sinkhorn import Sinkhorn\nfrom src.lap_solvers.hungarian import hungarian\nfrom src.build_graphs import reshape_edge_feature\nfrom src.feature_align import feature_align\nfrom src.factorize_graph_matching import construct_aff_mat\nfrom models.NGM.gnn import HyperGNNLayer\nfrom models.NGM.geo_edge_feature import geo_edge_feature\nfrom models.GMN.affinity_layer import GaussianAffinity, InnerpAffinity\n\nfrom src.utils.config import cfg\n\nfrom src.backbone import *\nCNN = eval(cfg.BACKBONE)\n\n\nclass Net(CNN):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.geo_affinity_layer = GaussianAffinity(1, cfg.NGM.GAUSSIAN_SIGMA)\n        self.feat_affinity_layer = InnerpAffinity(cfg.NGM.FEATURE_CHANNEL)\n        self.feat_affinity_layer3 = InnerpAffinity(cfg.NGM.FEATURE_CHANNEL)\n        self.tau = cfg.NGM.SK_TAU\n        self.bi_stochastic = Sinkhorn(max_iter=cfg.NGM.SK_ITER_NUM, tau=self.tau, epsilon=cfg.NGM.SK_EPSILON)\n        self.l2norm = nn.LocalResponseNorm(cfg.NGM.FEATURE_CHANNEL * 2, alpha=cfg.NGM.FEATURE_CHANNEL * 2, beta=0.5, k=0)\n\n        self.gnn_layer = cfg.NGM.GNN_LAYER\n        for i in range(self.gnn_layer):\n            if i == 0:\n                gnn_layer = HyperGNNLayer(\n                    1, 1, cfg.NGM.GNN_FEAT[i] + (1 if cfg.NGM.SK_EMB else 0), cfg.NGM.GNN_FEAT[i],\n                    sk_channel=cfg.NGM.SK_EMB, sk_tau=self.tau\n                )\n            else:\n                gnn_layer = HyperGNNLayer(\n                    cfg.NGM.GNN_FEAT[i - 1] + (1 if cfg.NGM.SK_EMB else 0), cfg.NGM.GNN_FEAT[i - 1],\n                    cfg.NGM.GNN_FEAT[i] + (1 if cfg.NGM.SK_EMB else 0), cfg.NGM.GNN_FEAT[i],\n                    sk_channel=cfg.NGM.SK_EMB, sk_tau=self.tau\n                )\n            self.add_module('gnn_layer_{}'.format(i), gnn_layer)\n\n        self.classifier = nn.Linear(cfg.NGM.GNN_FEAT[-1] + (1 if cfg.NGM.SK_EMB else 0), 1)\n\n        self.weight2 = cfg.NGM.WEIGHT2\n        self.weight3 = cfg.NGM.WEIGHT3\n        self.rescale = cfg.PROBLEM.RESCALE\n\n    def forward(self, data_dict):\n        if 'images' in data_dict:\n            # real image data\n            src, tgt = data_dict['images']\n            P_src, P_tgt = data_dict['Ps']\n            ns_src, ns_tgt = data_dict['ns']\n            G_src, G_tgt = data_dict['Gs']\n            H_src, H_tgt = data_dict['Hs']\n            K_G, K_H = data_dict['KGHs']\n\n            # extract feature\n            src_node = self.node_layers(src)\n            src_edge = self.edge_layers(src_node)\n            tgt_node = self.node_layers(tgt)\n            tgt_edge = self.edge_layers(tgt_node)\n\n            # feature normalization\n            src_node = self.l2norm(src_node)\n            src_edge = self.l2norm(src_edge)\n            tgt_node = self.l2norm(tgt_node)\n            tgt_edge = self.l2norm(tgt_edge)\n\n            # arrange features\n            U_src = feature_align(src_node, P_src, ns_src, self.rescale)\n            F_src = feature_align(src_edge, P_src, ns_src, self.rescale)\n            U_tgt = feature_align(tgt_node, P_tgt, ns_tgt, self.rescale)\n            F_tgt = feature_align(tgt_edge, P_tgt, ns_tgt, self.rescale)\n        elif 'features' in data_dict:\n            # synthetic data\n            src, tgt = data_dict['features']\n            P_src, P_tgt = data_dict['Ps']\n            ns_src, ns_tgt = data_dict['ns']\n            G_src, G_tgt = data_dict['Gs']\n            H_src, H_tgt = data_dict['Hs']\n            K_G, K_H = data_dict['KGHs']\n\n            U_src = src[:, :src.shape[1] // 2, :]\n            F_src = src[:, src.shape[1] // 2:, :]\n            U_tgt = tgt[:, :tgt.shape[1] // 2, :]\n            F_tgt = tgt[:, tgt.shape[1] // 2:, :]\n        else:\n            raise ValueError('unknown type string {}'.format(type))\n\n        X = reshape_edge_feature(F_src, G_src, H_src)\n        Y = reshape_edge_feature(F_tgt, G_tgt, H_tgt)\n        dx = geo_edge_feature(P_src, G_src, H_src)[:, :1, :]\n        dy = geo_edge_feature(P_tgt, G_tgt, H_tgt)[:, :1, :]\n\n        # affinity layer for 2-order affinity matrix\n        if cfg.NGM.EDGE_FEATURE == 'cat':\n            Ke, Kp = self.feat_affinity_layer(X, Y, U_src, U_tgt)\n        elif cfg.NGM.EDGE_FEATURE == 'geo':\n             Ke, Kp = self.geo_affinity_layer(dx, dy, U_src, U_tgt)\n        else:\n            raise ValueError('Unknown edge feature type {}'.format(cfg.NGM.EDGE_FEATURE))\n\n        K = construct_aff_mat(Ke, torch.zeros_like(Kp), K_G, K_H)\n        adj = (K > 0).to(K.dtype)\n\n        # build 3-order affinity tensor\n        hshape = list(adj.shape) + [adj.shape[-1]]\n        order3A = adj.unsqueeze(1).expand(hshape) * adj.unsqueeze(2).expand(hshape) * adj.unsqueeze(3).expand(hshape)\n        hyper_adj = order3A\n\n        if cfg.NGM.ORDER3_FEATURE == 'cat':\n            Ke_3, _ = self.feat_affinity_layer3(X, Y, torch.zeros(1, 1, 1), torch.zeros(1, 1, 1), w1=0.5, w2=1)\n            K_3 = construct_aff_mat(Ke_3, torch.zeros_like(Kp), K_G, K_H)\n            H = (K_3.unsqueeze(1).expand(hshape) + K_3.unsqueeze(2).expand(hshape) + K_3.unsqueeze(3).expand(hshape)) * F.relu(self.weight3)\n        elif cfg.NGM.ORDER3_FEATURE == 'geo':\n            Ke_d, _ = self.geo_affinity_layer(dx, dy, torch.zeros(1, 1, 1), torch.zeros(1, 1, 1))\n\n            m_d_src = construct_aff_mat(dx.squeeze().unsqueeze(-1).expand_as(Ke_d), torch.zeros_like(Kp), K_G, K_H).cpu()\n            m_d_tgt = construct_aff_mat(dy.squeeze().unsqueeze(-2).expand_as(Ke_d), torch.zeros_like(Kp), K_G, K_H).cpu()\n            order3A = order3A.cpu()\n\n            cum_sin = torch.zeros_like(order3A)\n            for i in range(3):\n                def calc_sin(t):\n                    a = t.unsqueeze(i % 3 + 1).expand(hshape)\n                    b = t.unsqueeze((i + 1) % 3 + 1).expand(hshape)\n                    c = t.unsqueeze((i + 2) % 3 + 1).expand(hshape)\n                    cos = torch.clamp((a.pow(2) + b.pow(2) - c.pow(2)) / (2 * a * b + 1e-15), -1, 1)\n                    cos *= order3A\n                    sin = torch.sqrt(1 - cos.pow(2)) * order3A\n                    assert torch.sum(torch.isnan(sin)) == 0\n                    return sin\n                sin_src = calc_sin(m_d_src)\n                sin_tgt = calc_sin(m_d_tgt)\n                cum_sin += torch.abs(sin_src - sin_tgt)\n\n            H = torch.exp(- 1 / cfg.NGM.SIGMA3 * cum_sin) * order3A\n            H = H.cuda()\n            order3A = order3A.cuda()\n        elif cfg.NGM.ORDER3_FEATURE == 'none':\n            H = torch.zeros_like(hyper_adj)\n        else:\n            raise ValueError('Unknown edge feature type {}'.format(cfg.NGM.ORDER3_FEATURE))\n\n        hyper_adj = hyper_adj.cpu()\n        hyper_adj_sum = torch.sum(hyper_adj, dim=tuple(range(2, 3 + 1)), keepdim=True) + 1e-10\n        hyper_adj = hyper_adj / hyper_adj_sum\n        hyper_adj = hyper_adj.to_sparse().coalesce().cuda()\n\n        H = H.sparse_mask(hyper_adj)\n        H = (H._indices(), H._values().unsqueeze(-1))\n\n        if cfg.NGM.FIRST_ORDER:\n            emb = Kp.transpose(1, 2).contiguous().view(Kp.shape[0], -1, 1)\n        else:\n            emb = torch.ones(K.shape[0], K.shape[1], 1, device=K.device)\n\n        adj_sum = torch.sum(adj, dim=2, keepdim=True) + 1e-10\n        adj = adj / adj_sum\n        pack_M = [K.unsqueeze(-1), H]\n        pack_A = [adj, hyper_adj]\n        for i in range(self.gnn_layer):\n            gnn_layer = getattr(self, 'gnn_layer_{}'.format(i))\n            pack_M, emb = gnn_layer(pack_A, pack_M, emb, ns_src, ns_tgt, norm=False)\n\n        v = self.classifier(emb)\n        s = v.view(v.shape[0], P_tgt.shape[1], -1).transpose(1, 2)\n\n        ss = self.bi_stochastic(s, ns_src, ns_tgt)\n        x = hungarian(ss, ns_src, ns_tgt)\n\n        data_dict.update({\n            'ds_mat': ss,\n            'perm_mat': x,\n            'aff_mat': K\n        })\n\n        return data_dict\n\n# ==========================================\n# File: models/NGM/mgmmodel.py\n# Function/Context: Net.forward and Net.__ngm_forward\n# ==========================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as functional\n\nfrom src.lap_solvers.sinkhorn import Sinkhorn\nfrom src.build_graphs import reshape_edge_feature\nfrom src.feature_align import feature_align\nfrom src.factorize_graph_matching import construct_aff_mat\nfrom models.NGM.gnn import GNNLayer\nfrom models.NGM.geo_edge_feature import geo_edge_feature\nfrom models.GMN.affinity_layer import InnerpAffinity, GaussianAffinity\nfrom src.lap_solvers.hungarian import hungarian\n\nfrom itertools import combinations\nimport numpy as np\n\nfrom src.utils.config import cfg\n\nfrom src.backbone import *\nCNN = eval(cfg.BACKBONE)\n\ndef pad_tensor(inp):\n    assert type(inp[0]) == torch.Tensor\n    it = iter(inp)\n    t = next(it)\n    max_shape = list(t.shape)\n    while True:\n        try:\n            t = next(it)\n            for i in range(len(max_shape)):\n                max_shape[i] = int(max(max_shape[i], t.shape[i]))\n        except StopIteration:\n            break\n    max_shape = np.array(max_shape)\n\n    padded_ts = []\n    for t in inp:\n        pad_pattern = np.zeros(2 * len(max_shape), dtype=np.int64)\n        pad_pattern[::-2] = max_shape - np.array(t.shape)\n        pad_pattern = tuple(pad_pattern.tolist())\n        padded_ts.append(functional.pad(t, pad_pattern, 'constant', 0))\n\n    return padded_ts\n\n\nclass Net(CNN):\n    def __init__(self, pretrained=True):\n        super(Net, self).__init__()\n        if cfg.NGM.EDGE_FEATURE == 'cat':\n            self.affinity_layer = InnerpAffinity(cfg.NGM.FEATURE_CHANNEL)\n        elif cfg.NGM.EDGE_FEATURE == 'geo':\n            self.affinity_layer = GaussianAffinity(1, cfg.NGM.GAUSSIAN_SIGMA)\n        else:\n            raise ValueError('Unknown edge feature type {}'.format(cfg.NGM.EDGE_FEATURE))\n        self.tau = cfg.NGM.SK_TAU\n        self.rescale = cfg.PROBLEM.RESCALE\n        self.sinkhorn = Sinkhorn(max_iter=cfg.NGM.SK_ITER_NUM, tau=self.tau, epsilon=cfg.NGM.SK_EPSILON)\n        self.l2norm = nn.LocalResponseNorm(cfg.NGM.FEATURE_CHANNEL * 2, alpha=cfg.NGM.FEATURE_CHANNEL * 2, beta=0.5, k=0)\n\n        self.gnn_layer = cfg.NGM.GNN_LAYER\n        for i in range(self.gnn_layer):\n            if i == 0:\n                gnn_layer = GNNLayer(1, 1, cfg.NGM.GNN_FEAT[i] + (1 if cfg.NGM.SK_EMB else 0), cfg.NGM.GNN_FEAT[i],\n                                     sk_channel=cfg.NGM.SK_EMB, sk_tau=self.tau, edge_emb=cfg.NGM.EDGE_EMB)\n            else:\n                gnn_layer = GNNLayer(cfg.NGM.GNN_FEAT[i - 1] + (1 if cfg.NGM.SK_EMB else 0), cfg.NGM.GNN_FEAT[i - 1],\n                                     cfg.NGM.GNN_FEAT[i] + (1 if cfg.NGM.SK_EMB else 0), cfg.NGM.GNN_FEAT[i],\n                                     sk_channel=cfg.NGM.SK_EMB, sk_tau=self.tau, edge_emb=cfg.NGM.EDGE_EMB)\n            self.add_module('gnn_layer_{}'.format(i), gnn_layer)\n\n        self.classifier = nn.Linear(cfg.NGM.GNN_FEAT[-1] + (1 if cfg.NGM.SK_EMB else 0), 1)\n\n        self.sinkhorn2 = Sinkhorn(max_iter=cfg.NGM.SK_ITER_NUM, epsilon=cfg.NGM.SK_EPSILON, tau=cfg.NGM.MGM_SK_TAU)\n\n    def forward(self, data_dict, **kwargs):\n        # extract graph feature\n        if 'images' in data_dict:\n            # extract data\n            data = data_dict['images']\n            Ps = data_dict['Ps']\n            ns = data_dict['ns']\n            Gs = data_dict['Gs']\n            Hs = data_dict['Hs']\n            Gs_tgt = data_dict['Gs_tgt']\n            Hs_tgt = data_dict['Hs_tgt']\n            KGs = {k: v[0] for k, v in  data_dict['KGHs'].items()}\n            KHs = {k: v[1] for k, v in  data_dict['KGHs'].items()}\n\n            batch_size = data[0].shape[0]\n            device = data[0].device\n\n            data_cat = torch.cat(data, dim=0)\n            P_cat = torch.cat(pad_tensor(Ps), dim=0)\n            n_cat = torch.cat(ns, dim=0)\n            node = self.node_layers(data_cat)\n            edge = self.edge_layers(node)\n            U = feature_align(node, P_cat, n_cat, self.rescale)\n            F = feature_align(edge, P_cat, n_cat, self.rescale)\n            feats = torch.cat((U, F), dim=1)\n            feats = self.l2norm(feats)\n            feats = torch.split(feats, batch_size, dim=0)\n        elif 'features' in data_dict:\n            # extract data\n            data = data_dict['features']\n            Ps = data_dict['Ps']\n            ns = data_dict['ns']\n            Gs = data_dict['Gs']\n            Hs = data_dict['Hs']\n            Gs_tgt = data_dict['Gs_tgt']\n            Hs_tgt = data_dict['Hs_tgt']\n            KGs = {k: v[0] for k, v in data_dict['KGHs'].items()}\n            KHs = {k: v[1] for k, v in data_dict['KGHs'].items()}\n\n            batch_size = data[0].shape[0]\n            device = data[0].device\n\n            feats = data\n        else:\n            raise ValueError('Unknown data type for this model.')\n\n        # extract reference graph feature\n        feat_list = []\n        joint_indices = [0]\n        iterator = zip(feats, Ps, Gs, Hs, Gs_tgt, Hs_tgt, ns)\n        for idx, (feat, P, G, H, G_tgt, H_tgt, n) in enumerate(iterator):\n            feat_list.append(\n                (\n                    idx,\n                    feat,\n                    P, G, H, G_tgt, H_tgt, n\n                )\n            )\n            joint_indices.append(joint_indices[-1] + P.shape[1])\n\n        joint_S = torch.zeros(batch_size, joint_indices[-1], joint_indices[-1], device=device)\n        joint_S_diag = torch.diagonal(joint_S, dim1=1, dim2=2)\n        joint_S_diag += 1\n\n        pred_s = []\n        pred_x = []\n        indices = []\n\n        for src, tgt in combinations(feat_list, 2):\n            # pca forward\n            src_idx, src_feat, P_src, G_src, H_src, _, __, n_src = src\n            tgt_idx, tgt_feat, P_tgt, _, __, G_tgt, H_tgt, n_tgt = tgt\n            K_G = KGs['{},{}'.format(src_idx, tgt_idx)]\n            K_H = KHs['{},{}'.format(src_idx, tgt_idx)]\n            s = self.__ngm_forward(src_feat, tgt_feat, P_src, P_tgt, G_src, G_tgt, H_src, H_tgt, K_G, K_H, n_src, n_tgt)\n\n            if src_idx > tgt_idx:\n                joint_S[:, joint_indices[tgt_idx]:joint_indices[tgt_idx+1], joint_indices[src_idx]:joint_indices[src_idx+1]] += s.transpose(1, 2)\n            else:\n                joint_S[:, joint_indices[src_idx]:joint_indices[src_idx+1], joint_indices[tgt_idx]:joint_indices[tgt_idx+1]] += s\n\n        matching_s = []\n        for b in range(batch_size):\n            e, v = torch.symeig(joint_S[b], eigenvectors=True)\n            topargs = torch.argsort(torch.abs(e), descending=True)[:joint_indices[1]]\n            diff = e[topargs[:-1]] - e[topargs[1:]]\n            if torch.min(torch.abs(diff)) > 1e-4:\n                matching_s.append(len(data) * torch.mm(v[:, topargs], v[:, topargs].transpose(0, 1)))\n            else:\n                matching_s.append(joint_S[b])\n\n        matching_s = torch.stack(matching_s, dim=0)\n\n        for idx1, idx2 in combinations(range(len(data)), 2):\n            s = matching_s[:, joint_indices[idx1]:joint_indices[idx1+1], joint_indices[idx2]:joint_indices[idx2+1]]\n            s = self.sinkhorn2(s)\n\n            pred_s.append(s)\n            pred_x.append(hungarian(s))\n            indices.append((idx1, idx2))\n\n        data_dict.update({\n            'ds_mat_list': pred_s,\n            'perm_mat_list': pred_x,\n            'graph_indices': indices,\n        })\n        return data_dict\n\n    def __ngm_forward(self, src, tgt, P_src, P_tgt, G_src, G_tgt, H_src, H_tgt, K_G, K_H, ns_src, ns_tgt):\n        U_src = src[:, :src.shape[1] // 2, :]\n        F_src = src[:, src.shape[1] // 2:, :]\n        U_tgt = tgt[:, :tgt.shape[1] // 2, :]\n        F_tgt = tgt[:, tgt.shape[1] // 2:, :]\n\n        if cfg.NGM.EDGE_FEATURE == 'cat':\n            X = reshape_edge_feature(F_src, G_src, H_src)\n            Y = reshape_edge_feature(F_tgt, G_tgt, H_tgt)\n        elif cfg.NGM.EDGE_FEATURE == 'geo':\n            X = geo_edge_feature(P_src, G_src, H_src)[:, :1, :]\n            Y = geo_edge_feature(P_tgt, G_tgt, H_tgt)[:, :1, :]\n        else:\n            raise ValueError('Unknown edge feature type {}'.format(cfg.NGM.EDGE_FEATURE))\n\n        # affinity layer\n        Ke, Kp = self.affinity_layer(X, Y, U_src, U_tgt)\n\n        K = construct_aff_mat(Ke, torch.zeros_like(Kp), K_G, K_H)\n\n        A = (K > 0).to(K.dtype)\n\n        if cfg.NGM.FIRST_ORDER:\n            emb = Kp.transpose(1, 2).contiguous().view(Kp.shape[0], -1, 1)\n        else:\n            emb = torch.ones(K.shape[0], K.shape[1], 1, device=K.device)\n\n        emb_K = K.unsqueeze(-1)\n\n        for i in range(self.gnn_layer):\n            gnn_layer = getattr(self, 'gnn_layer_{}'.format(i))\n            emb_K, emb = gnn_layer(A, emb_K, emb, ns_src, ns_tgt) #, norm=False)\n\n        v = self.classifier(emb)\n        s = v.view(v.shape[0], U_tgt.shape[2], -1).transpose(1, 2)\n\n        ss = self.sinkhorn(s, ns_src, ns_tgt, dummy_row=True)\n\n        return ss\n\n# ==========================================\n# File: models/NGM/model.py\n# Function/Context: Net.forward\n# ==========================================\nimport torch\nimport torch.nn as nn\n\nfrom src.lap_solvers.sinkhorn import Sinkhorn, GumbelSinkhorn\nfrom src.build_graphs import reshape_edge_feature\nfrom src.feature_align import feature_align\nfrom src.factorize_graph_matching import construct_aff_mat\nfrom models.NGM.gnn import GNNLayer\nfrom models.NGM.geo_edge_feature import geo_edge_feature\nfrom models.GMN.affinity_layer import InnerpAffinity, GaussianAffinity\nfrom src.evaluation_metric import objective_score\nfrom src.lap_solvers.hungarian import hungarian\nimport math\nfrom src.utils.gpu_memory import gpu_free_memory\n\nfrom src.utils.config import cfg\n\nfrom src.backbone import *\nCNN = eval(cfg.BACKBONE)\n\n\nclass Net(CNN):\n    def __init__(self):\n        super(Net, self).__init__()\n        if cfg.NGM.EDGE_FEATURE == 'cat':\n            self.affinity_layer = InnerpAffinity(cfg.NGM.FEATURE_CHANNEL)\n        elif cfg.NGM.EDGE_FEATURE == 'geo':\n            self.affinity_layer = GaussianAffinity(1, cfg.NGM.GAUSSIAN_SIGMA)\n        else:\n            raise ValueError('Unknown edge feature type {}'.format(cfg.NGM.EDGE_FEATURE))\n        self.tau = cfg.NGM.SK_TAU\n        self.rescale = cfg.PROBLEM.RESCALE\n        self.sinkhorn = Sinkhorn(max_iter=cfg.NGM.SK_ITER_NUM, tau=self.tau, epsilon=cfg.NGM.SK_EPSILON)\n        self.gumbel_sinkhorn = GumbelSinkhorn(max_iter=cfg.NGM.SK_ITER_NUM, tau=self.tau * 10, epsilon=cfg.NGM.SK_EPSILON, batched_operation=True)\n        self.l2norm = nn.LocalResponseNorm(cfg.NGM.FEATURE_CHANNEL * 2, alpha=cfg.NGM.FEATURE_CHANNEL * 2, beta=0.5, k=0)\n\n        self.gnn_layer = cfg.NGM.GNN_LAYER\n        for i in range(self.gnn_layer):\n            tau = cfg.NGM.SK_TAU\n            if i == 0:\n                gnn_layer = GNNLayer(1, 1, cfg.NGM.GNN_FEAT[i] + (1 if cfg.NGM.SK_EMB else 0), cfg.NGM.GNN_FEAT[i],\n                                     sk_channel=cfg.NGM.SK_EMB, sk_tau=tau, edge_emb=cfg.NGM.EDGE_EMB)\n            else:\n                gnn_layer = GNNLayer(cfg.NGM.GNN_FEAT[i - 1] + (1 if cfg.NGM.SK_EMB else 0), cfg.NGM.GNN_FEAT[i - 1],\n                                     cfg.NGM.GNN_FEAT[i] + (1 if cfg.NGM.SK_EMB else 0), cfg.NGM.GNN_FEAT[i],\n                                     sk_channel=cfg.NGM.SK_EMB, sk_tau=tau, edge_emb=cfg.NGM.EDGE_EMB)\n            self.add_module('gnn_layer_{}'.format(i), gnn_layer)\n\n        self.classifier = nn.Linear(cfg.NGM.GNN_FEAT[-1] + (1 if cfg.NGM.SK_EMB else 0), 1)\n\n    def forward(self, data_dict, **kwargs):\n        batch_size = data_dict['batch_size']\n        if 'images' in data_dict:\n            src, tgt = data_dict['images']\n            P_src, P_tgt = data_dict['Ps']\n            ns_src, ns_tgt = data_dict['ns']\n            G_src, G_tgt = data_dict['Gs']\n            H_src, H_tgt = data_dict['Hs']\n            K_G, K_H = data_dict['KGHs']\n\n            src_node = self.node_layers(src)\n            src_edge = self.edge_layers(src_node)\n            tgt_node = self.node_layers(tgt)\n            tgt_edge = self.edge_layers(tgt_node)\n\n            src_node = self.l2norm(src_node)\n            src_edge = self.l2norm(src_edge)\n            tgt_node = self.l2norm(tgt_node)\n            tgt_edge = self.l2norm(tgt_edge)\n\n            U_src = feature_align(src_node, P_src, ns_src, self.rescale)\n            F_src = feature_align(src_edge, P_src, ns_src, self.rescale)\n            U_tgt = feature_align(tgt_node, P_tgt, ns_tgt, self.rescale)\n            F_tgt = feature_align(tgt_edge, P_tgt, ns_tgt, self.rescale)\n        elif 'features' in data_dict:\n            src, tgt = data_dict['features']\n            P_src, P_tgt = data_dict['Ps']\n            ns_src, ns_tgt = data_dict['ns']\n            G_src, G_tgt = data_dict['Gs']\n            H_src, H_tgt = data_dict['Hs']\n            K_G, K_H = data_dict['KGHs']\n\n            U_src = src[:, :src.shape[1] // 2, :]\n            F_src = src[:, src.shape[1] // 2:, :]\n            U_tgt = tgt[:, :tgt.shape[1] // 2, :]\n            F_tgt = tgt[:, tgt.shape[1] // 2:, :]\n        elif 'aff_mat' in data_dict:\n            K = data_dict['aff_mat']\n            ns_src, ns_tgt = data_dict['ns']\n        else:\n            raise ValueError('Unknown data type for this model.')\n\n        if 'images' in data_dict or 'features' in data_dict:\n            tgt_len = P_tgt.shape[1]\n            if cfg.NGM.EDGE_FEATURE == 'cat':\n                X = reshape_edge_feature(F_src, G_src, H_src)\n                Y = reshape_edge_feature(F_tgt, G_tgt, H_tgt)\n            elif cfg.NGM.EDGE_FEATURE == 'geo':\n                X = geo_edge_feature(P_src, G_src, H_src)[:, :1, :]\n                Y = geo_edge_feature(P_tgt, G_tgt, H_tgt)[:, :1, :]\n            else:\n                raise ValueError('Unknown edge feature type {}'.format(cfg.NGM.EDGE_FEATURE))\n\n            Ke, Kp = self.affinity_layer(X, Y, U_src, U_tgt)\n\n            K = construct_aff_mat(Ke, torch.zeros_like(Kp), K_G, K_H)\n\n            A = (K > 0).to(K.dtype)\n\n            if cfg.NGM.FIRST_ORDER:\n                emb = Kp.transpose(1, 2).contiguous().view(Kp.shape[0], -1, 1)\n            else:\n                emb = torch.ones(K.shape[0], K.shape[1], 1, device=K.device)\n        else:\n            tgt_len = int(math.sqrt(K.shape[2]))\n            dmax = (torch.max(torch.sum(K, dim=2, keepdim=True), dim=1, keepdim=True).values + 1e-5)\n            K = K / dmax * 1000\n            A = (K > 0).to(K.dtype)\n            emb = torch.ones(K.shape[0], K.shape[1], 1, device=K.device)\n\n        emb_K = K.unsqueeze(-1)\n\n        for i in range(self.gnn_layer):\n            gnn_layer = getattr(self, 'gnn_layer_{}'.format(i))\n            emb_K, emb = gnn_layer(A, emb_K, emb, ns_src, ns_tgt)\n\n        v = self.classifier(emb)\n        s = v.view(v.shape[0], tgt_len, -1).transpose(1, 2)\n\n        if self.training or cfg.NGM.GUMBEL_SK <= 0:\n            ss = self.sinkhorn(s, ns_src, ns_tgt, dummy_row=True)\n            x = hungarian(ss, ns_src, ns_tgt)\n        else:\n            gumbel_sample_num = cfg.NGM.GUMBEL_SK\n            if self.training:\n                gumbel_sample_num //= 10\n            ss_gumbel = self.gumbel_sinkhorn(s, ns_src, ns_tgt, sample_num=gumbel_sample_num, dummy_row=True)\n\n            repeat = lambda x, rep_num=gumbel_sample_num: torch.repeat_interleave(x, rep_num, dim=0)\n            if not self.training:\n                ss_gumbel = hungarian(ss_gumbel, repeat(ns_src), repeat(ns_tgt))\n            ss_gumbel = ss_gumbel.reshape(batch_size, gumbel_sample_num, ss_gumbel.shape[-2], ss_gumbel.shape[-1])\n\n            if ss_gumbel.device.type == 'cuda':\n                dev_idx = ss_gumbel.device.index\n                free_mem = gpu_free_memory(dev_idx) - 100 * 1024 ** 2\n                K_mem_size = K.element_size() * K.nelement()\n                max_repeats = free_mem // K_mem_size\n                if max_repeats <= 0:\n                    print('Warning: GPU may not have enough memory')\n                    max_repeats = 1\n            else:\n                max_repeats = gumbel_sample_num\n\n            obj_score = []\n            for idx in range(0, gumbel_sample_num, max_repeats):\n                if idx + max_repeats > gumbel_sample_num:\n                    rep_num = gumbel_sample_num - idx\n                else:\n                    rep_num = max_repeats\n                obj_score.append(\n                    objective_score(\n                        ss_gumbel[:, idx:(idx+rep_num), :, :].reshape(-1, ss_gumbel.shape[-2], ss_gumbel.shape[-1]),\n                        repeat(K, rep_num)\n                    ).reshape(batch_size, -1)\n                )\n            obj_score = torch.cat(obj_score, dim=1)\n            min_obj_score = obj_score.min(dim=1)\n            ss = ss_gumbel[torch.arange(batch_size), min_obj_score.indices.cpu(), :, :]\n            x = hungarian(ss, repeat(ns_src), repeat(ns_tgt))\n\n        data_dict.update({\n            'ds_mat': ss,\n            'perm_mat': x,\n            'aff_mat': K\n        })\n        return data_dict\n\n# ==========================================\n# File: models/NGM/model_v2.py\n# Function/Context: Net.forward\n# ==========================================\nimport itertools\nfrom torch_sparse import spmm, SparseTensor\n\nfrom models.BBGM.affinity_layer import InnerProductWithWeightsAffinity\nfrom models.BBGM.sconv_archs import SiameseSConvOnNodes, SiameseNodeFeaturesToEdgeFeatures\nfrom src.feature_align import feature_align\nfrom src.factorize_graph_matching import construct_aff_mat, construct_sparse_aff_mat\nfrom src.utils.pad_tensor import pad_tensor\nfrom models.NGM.gnn import GNNLayer, PYGNNLayer, SPGNNLayer\nfrom src.lap_solvers.sinkhorn import Sinkhorn\nfrom models.AFAT.sinkhorn_topk import greedy_perm\nfrom src.lap_solvers.hungarian import hungarian\n\nfrom src.utils.config import cfg\n\nfrom src.backbone import *\nCNN = eval(cfg.BACKBONE)\n\n\ndef lexico_iter(lex):\n    return itertools.combinations(lex, 2)\n\n\ndef normalize_over_channels(x):\n    channel_norms = torch.norm(x, dim=1, keepdim=True)\n    return x / channel_norms\n\n\ndef concat_features(embeddings, num_vertices):\n    res = torch.cat([embedding[:, :num_v] for embedding, num_v in zip(embeddings, num_vertices)], dim=-1)\n    return res.transpose(0, 1)\n\n\nclass Net(CNN):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.message_pass_node_features = SiameseSConvOnNodes(input_node_dim=cfg.NGM.FEATURE_CHANNEL * 2)\n        self.build_edge_features_from_node_features = SiameseNodeFeaturesToEdgeFeatures(\n            total_num_nodes=self.message_pass_node_features.num_node_features\n        )\n        self.global_state_dim = cfg.NGM.FEATURE_CHANNEL * 2\n        self.vertex_affinity = InnerProductWithWeightsAffinity(\n            self.global_state_dim, self.message_pass_node_features.num_node_features)\n        self.edge_affinity = InnerProductWithWeightsAffinity(\n            self.global_state_dim,\n            self.build_edge_features_from_node_features.num_edge_features)\n\n        self.rescale = cfg.PROBLEM.RESCALE\n        self.tau = cfg.NGM.SK_TAU\n        self.mgm_tau = cfg.NGM.MGM_SK_TAU\n        self.univ_size = cfg.NGM.UNIV_SIZE\n        self.sparse = cfg.NGM.SPARSE_MODEL\n        self.thresholding = cfg.NGM.THRESHOLDING\n        self.gt_k = cfg.NGM.GT_K\n\n        self.sinkhorn = Sinkhorn(max_iter=cfg.NGM.SK_ITER_NUM, tau=self.tau, epsilon=cfg.NGM.SK_EPSILON)\n        self.sinkhorn_mgm = Sinkhorn(max_iter=cfg.NGM.SK_ITER_NUM, epsilon=cfg.NGM.SK_EPSILON, tau=self.mgm_tau)\n        self.gnn_layer = cfg.NGM.GNN_LAYER\n        if not self.sparse:\n            for i in range(self.gnn_layer):\n                tau = cfg.NGM.SK_TAU\n                if i == 0:\n                    gnn_layer = GNNLayer(1, 1,\n                                         cfg.NGM.GNN_FEAT[i] + cfg.NGM.SK_EMB, cfg.NGM.GNN_FEAT[i],\n                                         sk_channel=cfg.NGM.SK_EMB, sk_tau=tau, edge_emb=cfg.NGM.EDGE_EMB)\n                else:\n                    gnn_layer = GNNLayer(cfg.NGM.GNN_FEAT[i - 1] + cfg.NGM.SK_EMB, cfg.NGM.GNN_FEAT[i - 1],\n                                         cfg.NGM.GNN_FEAT[i] + cfg.NGM.SK_EMB, cfg.NGM.GNN_FEAT[i],\n                                         sk_channel=cfg.NGM.SK_EMB, sk_tau=tau, edge_emb=cfg.NGM.EDGE_EMB)\n                self.add_module('gnn_layer_{}'.format(i), gnn_layer)\n        else:\n            self.geometric = True\n            if self.geometric:\n                for i in range(self.gnn_layer):\n                    tau = cfg.NGM.SK_TAU\n                    if i == 0:\n                        gnn_layer = PYGNNLayer(1, 1,\n                                               cfg.NGM.GNN_FEAT[i] + cfg.NGM.SK_EMB, cfg.NGM.GNN_FEAT[i],\n                                               sk_channel=cfg.NGM.SK_EMB, sk_tau=tau, edge_emb=cfg.NGM.EDGE_EMB)\n                    else:\n                        gnn_layer = PYGNNLayer(cfg.NGM.GNN_FEAT[i - 1] + cfg.NGM.SK_EMB, cfg.NGM.GNN_FEAT[i - 1],\n                                               cfg.NGM.GNN_FEAT[i] + cfg.NGM.SK_EMB, cfg.NGM.GNN_FEAT[i],\n                                               sk_channel=cfg.NGM.SK_EMB, sk_tau=tau, edge_emb=cfg.NGM.EDGE_EMB)\n                    self.add_module('gnn_layer_{}'.format(i), gnn_layer)\n            else:\n                for i in range(self.gnn_layer):\n                    tau = cfg.NGM.SK_TAU\n                    if i == 0:\n                        gnn_layer = SPGNNLayer(1, 1,\n                                               cfg.NGM.GNN_FEAT[i] + cfg.NGM.SK_EMB, cfg.NGM.GNN_FEAT[i],\n                                               sk_channel=cfg.NGM.SK_EMB, sk_tau=tau, edge_emb=cfg.NGM.EDGE_EMB)\n                    else:\n                        gnn_layer = SPGNNLayer(cfg.NGM.GNN_FEAT[i - 1] + cfg.NGM.SK_EMB, cfg.NGM.GNN_FEAT[i - 1],\n                                               cfg.NGM.GNN_FEAT[i] + cfg.NGM.SK_EMB, cfg.NGM.GNN_FEAT[i],\n                                               sk_channel=cfg.NGM.SK_EMB, sk_tau=tau, edge_emb=cfg.NGM.EDGE_EMB)\n                    self.add_module('gnn_layer_{}'.format(i), gnn_layer)\n        self.classifier = nn.Linear(cfg.NGM.GNN_FEAT[-1] + cfg.NGM.SK_EMB, 1)\n\n    def forward(\n        self,\n        data_dict,\n    ):\n        images = data_dict['images']\n        points = data_dict['Ps']\n        n_points = data_dict['ns']\n        graphs = data_dict['pyg_graphs']\n        batch_size = data_dict['batch_size']\n        num_graphs = len(images)\n\n        global_list = []\n        orig_graph_list = []\n        for image, p, n_p, graph in zip(images, points, n_points, graphs):\n            # extract feature\n            nodes = self.node_layers(image)\n            edges = self.edge_layers(nodes)\n\n            global_list.append(self.final_layers(edges).reshape((nodes.shape[0], -1)))\n            nodes = normalize_over_channels(nodes)\n            edges = normalize_over_channels(edges)\n\n            # arrange features\n            U = concat_features(feature_align(nodes, p, n_p, self.rescale), n_p)\n            F = concat_features(feature_align(edges, p, n_p, self.rescale), n_p)\n            node_features = torch.cat((U, F), dim=1)\n            graph.x = node_features\n\n            graph = self.message_pass_node_features(graph)\n            orig_graph = self.build_edge_features_from_node_features(graph)\n            orig_graph_list.append(orig_graph)\n\n        global_weights_list = [\n            torch.cat([global_src, global_tgt], axis=-1) for global_src, global_tgt in lexico_iter(global_list)\n        ]\n\n        global_weights_list = [normalize_over_channels(g) for g in global_weights_list]\n\n        unary_affs_list = [\n            self.vertex_affinity([item.x for item in g_1], [item.x for item in g_2], global_weights)\n            for (g_1, g_2), global_weights in zip(lexico_iter(orig_graph_list), global_weights_list)\n        ]\n\n        quadratic_affs_list = [\n            self.edge_affinity([item.edge_attr for item in g_1], [item.edge_attr for item in g_2], global_weights)\n            for (g_1, g_2), global_weights in zip(lexico_iter(orig_graph_list), global_weights_list)\n        ]\n\n        quadratic_affs_list = [[0.5 * x for x in quadratic_affs] for quadratic_affs in quadratic_affs_list]\n\n        s_list, mgm_s_list, x_list, mgm_x_list, indices = [], [], [], [], []\n\n        for unary_affs, quadratic_affs, (idx1, idx2) in zip(unary_affs_list, quadratic_affs_list, lexico_iter(range(num_graphs))):\n            if not self.sparse:\n                kro_G, kro_H = data_dict['KGHs'] if num_graphs == 2 else data_dict['KGHs']['{},{}'.format(idx1, idx2)]\n                Kp = torch.stack(pad_tensor(unary_affs), dim=0)\n                Ke = torch.stack(pad_tensor(quadratic_affs), dim=0)\n                K = construct_aff_mat(Ke, Kp, kro_G, kro_H)\n                if num_graphs == 2: data_dict['aff_mat'] = K\n\n                if cfg.NGM.FIRST_ORDER:\n                    emb = Kp.transpose(1, 2).contiguous().view(Kp.shape[0], -1, 1)\n                else:\n                    emb = torch.ones(K.shape[0], K.shape[1], 1, device=K.device)\n\n                if cfg.NGM.POSITIVE_EDGES:\n                    A = (K > 0).to(K.dtype)\n                else:\n                    A = (K != 0).to(K.dtype)\n\n                emb_K = K.unsqueeze(-1)\n\n                # NGM qap solver\n                for i in range(self.gnn_layer):\n                    gnn_layer = getattr(self, 'gnn_layer_{}'.format(i))\n                    emb_K, emb = gnn_layer(A, emb_K, emb, n_points[idx1], n_points[idx2])\n            else:\n                kro_G, kro_H = data_dict['KGHs'] if num_graphs == 2 else data_dict['KGHs']['{},{}'.format(idx1, idx2)]\n                Kp = torch.stack(pad_tensor(unary_affs), dim=0)\n                Ke = torch.stack(pad_tensor(quadratic_affs), dim=0)\n                K_value, row_idx, col_idx = construct_sparse_aff_mat(Ke, Kp, kro_G, kro_H)\n\n                if cfg.NGM.FIRST_ORDER:\n                    emb = Kp.transpose(1, 2).contiguous().view(Kp.shape[0], -1, 1)\n                else:\n                    emb = torch.ones(cfg.BATCH_SIZE, Kp.shape[1] * Kp.shape[2], 1, device=K_value.device)\n\n                # NGM qap solver\n                if self.geometric:\n                    adj = SparseTensor(row=row_idx.long(), col=col_idx.long(), value=K_value,\n                                       sparse_sizes=(Kp.shape[1] * Kp.shape[2], Kp.shape[1] * Kp.shape[2]))\n                    for i in range(self.gnn_layer):\n                        gnn_layer = getattr(self, 'gnn_layer_{}'.format(i))\n                        emb = gnn_layer(adj, emb, n_points[idx1], n_points[idx2])\n                else:\n                    K_index = torch.cat((row_idx.unsqueeze(0), col_idx.unsqueeze(0)), dim=0).long()\n                    A_value = torch.ones(K_value.shape, device=K_value.device)\n                    tmp = torch.ones([Kp.shape[1] * Kp.shape[2]], device=K_value.device).unsqueeze(-1)\n                    normed_A_value = 1 / torch.flatten(\n                        spmm(K_index, A_value, Kp.shape[1] * Kp.shape[2], Kp.shape[1] * Kp.shape[2], tmp))\n                    A_index = torch.linspace(0, Kp.shape[1] * Kp.shape[2] - 1, Kp.shape[1] * Kp.shape[2]).unsqueeze(0)\n                    A_index = torch.repeat_interleave(A_index, 2, dim=0).long().to(K_value.device)\n\n                    for i in range(self.gnn_layer):\n                        gnn_layer = getattr(self, 'gnn_layer_{}'.format(i))\n                        emb = gnn_layer(K_value, K_index, normed_A_value, A_index, emb, n_points[idx1], n_points[idx2])\n\n            v = self.classifier(emb)\n            s = v.view(v.shape[0], points[idx2].shape[1], -1).transpose(1, 2)\n\n            ss = self.sinkhorn(s, n_points[idx1], n_points[idx2], dummy_row=True)\n            x = hungarian(ss, n_points[idx1], n_points[idx2])\n            if self.thresholding > 0 and self.gt_k:\n                raise AssertionError\n            if self.thresholding > 0:\n                valid_map = ss > self.thresholding\n                x = x * valid_map\n            if self.gt_k:\n                ks = torch.tensor([torch.sum(data_dict['gt_perm_mat'][i]) for i in range(data_dict['gt_perm_mat'].shape[0])], dtype=torch.float32, device=ss.device)\n                top_indices = torch.argsort(x.mul(ss).reshape(x.shape[0], -1), descending=True, dim=-1)\n                x = torch.zeros(ss.shape, device=ss.device)\n                x = greedy_perm(x, top_indices, ks.view(-1))\n            s_list.append(ss)\n            x_list.append(x)\n            indices.append((idx1, idx2))\n\n        if num_graphs > 2:\n            joint_indices = torch.cat((torch.cumsum(torch.stack([torch.max(np) for np in n_points]), dim=0), torch.zeros((1,), dtype=torch.long, device=K.device)))\n            joint_S = torch.zeros(batch_size, torch.max(joint_indices), torch.max(joint_indices), device=K.device)\n            for idx in range(num_graphs):\n                for b in range(batch_size):\n                    start = joint_indices[idx-1]\n                    joint_S[b, start:start+n_points[idx][b], start:start+n_points[idx][b]] += torch.eye(n_points[idx][b], device=K.device)\n\n            for (idx1, idx2), s in zip(indices, s_list):\n                if idx1 > idx2:\n                    joint_S[:, joint_indices[idx2-1]:joint_indices[idx2], joint_indices[idx1-1]:joint_indices[idx1]] += s.transpose(1, 2)\n                else:\n                    joint_S[:, joint_indices[idx1-1]:joint_indices[idx1], joint_indices[idx2-1]:joint_indices[idx2]] += s\n\n            matching_s = []\n            for b in range(batch_size):\n                e, v = torch.symeig(joint_S[b], eigenvectors=True)\n                diff = e[-self.univ_size:-1] - e[-self.univ_size+1:]\n                if self.training and torch.min(torch.abs(diff)) <= 1e-4:\n                    matching_s.append(joint_S[b])\n                else:\n                    matching_s.append(num_graphs * torch.mm(v[:, -self.univ_size:], v[:, -self.univ_size:].transpose(0, 1))\n\n            matching_s = torch.stack(matching_s, dim=0)\n\n            for idx1, idx2 in indices:\n                s = matching_s[:, joint_indices[idx1-1]:joint_indices[idx1], joint_indices[idx2-1]:joint_indices[idx2]]\n                s = self.sinkhorn_mgm(torch.log(torch.relu(s)), n_points[idx1], n_points[idx2]) # only perform row/col norm, do not perform exp\n                x = hungarian(s, n_points[idx1], n_points[idx2])\n\n                mgm_s_list.append(s)\n                mgm_x_list.append(x)\n\n        if cfg.PROBLEM.TYPE == '2GM':\n            data_dict.update({\n                'ds_mat': s_list[0],\n                'perm_mat': x_list[0]\n            })\n        elif cfg.PROBLEM.TYPE == 'MGM':\n            data_dict.update({\n                'ds_mat_list': mgm_s_list,\n                'perm_mat_list': mgm_x_list,\n                'graph_indices': indices,\n            })\n\n        return data_dict\n\n# ==========================================\n# File: src/lap_solvers/sinkhorn.py\n# Function/Context: Sinkhorn.forward_log\n# ==========================================\nimport torch\nimport torch.nn as nn\nfrom torch import Tensor\nimport pygmtools as pygm\n\nclass Sinkhorn(nn.Module):\n    r\"\"\"\n    Sinkhorn algorithm turns the input matrix into a bi-stochastic matrix.\n\n    Sinkhorn algorithm firstly applies an ``exp`` function with temperature :math:`\\tau`:\n\n    .. math::\n        \\mathbf{S}_{i,j} = \\exp \\left(\\frac{\\mathbf{s}_{i,j}}{\\tau}\\right)\n\n    And then turns the matrix into doubly-stochastic matrix by iterative row- and column-wise normalization:\n\n    .. math::\n        \\mathbf{S} &= \\mathbf{S} \\oslash (\\mathbf{1}_{n_2} \\mathbf{1}_{n_2}^\\top \\cdot \\mathbf{S}) \\\\\n        \\mathbf{S} &= \\mathbf{S} \\oslash (\\mathbf{S} \\cdot \\mathbf{1}_{n_2} \\mathbf{1}_{n_2}^\\top)\n\n    where :math:`\\oslash` means element-wise division, :math:`\\mathbf{1}_n` means a column-vector with length :math:`n`\n    whose elements are all :math:`1`\\ s.\n\n    :param max_iter: maximum iterations (default: ``10``)\n    :param tau: the hyper parameter :math:`\\tau` controlling the temperature (default: ``1``)\n    :param epsilon: a small number for numerical stability (default: ``1e-4``)\n    :param log_forward: apply log-scale computation for better numerical stability (default: ``True``)\n    :param batched_operation: apply batched_operation for better efficiency (but may cause issues for back-propagation,\n     default: ``False``)\n\n    .. note::\n        ``tau`` is an important hyper parameter to be set for Sinkhorn algorithm. ``tau`` controls the distance between\n        the predicted doubly-stochastic matrix, and the discrete permutation matrix computed by Hungarian algorithm (see\n        :func:`~src.lap_solvers.hungarian.hungarian`). Given a small ``tau``, Sinkhorn performs more closely to\n        Hungarian, at the cost of slower convergence speed and reduced numerical stability.\n\n    .. note::\n        We recommend setting ``log_forward=True`` because it is more numerically stable. It provides more precise\n        gradient in back propagation and helps the model to converge better and faster.\n\n    .. note::\n        Setting ``batched_operation=True`` may be preferred when you are doing inference with this module and do not\n        need the gradient.\n    \"\"\"\n    def __init__(self, max_iter: int=10, tau: float=1., epsilon: float=1e-4,\n                 log_forward: bool=True, batched_operation: bool=False):\n        super(Sinkhorn, self).__init__()\n        self.max_iter = max_iter\n        self.tau = tau\n        self.epsilon = epsilon\n        self.log_forward = log_forward\n        if not log_forward:\n            print('Warning: Sinkhorn algorithm without log forward is deprecated because log_forward is more stable.')\n        self.batched_operation = batched_operation # batched operation may cause instability in backward computation,\n                                                   # but will boost computation.\n\n    def forward(self, s: Tensor, nrows: Tensor=None, ncols: Tensor=None, dummy_row: bool=False) -> Tensor:\n        r\"\"\"\n        :param s: :math:`(b\\times n_1 \\times n_2)` input 3d tensor. :math:`b`: batch size\n        :param nrows: :math:`(b)` number of objects in dim1\n        :param ncols: :math:`(b)` number of objects in dim2\n        :param dummy_row: whether to add dummy rows (rows whose elements are all 0) to pad the matrix to square matrix.\n         default: ``False``\n        :return: :math:`(b\\times n_1 \\times n_2)` the computed doubly-stochastic matrix\n\n        .. note::\n            We support batched instances with different number of nodes, therefore ``nrows`` and ``ncols`` are\n            required to specify the exact number of objects of each dimension in the batch. If not specified, we assume\n            the batched matrices are not padded.\n\n        .. note::\n            The original Sinkhorn algorithm only works for square matrices. To handle cases where the graphs to be\n            matched have different number of nodes, it is a common practice to add dummy rows to construct a square\n            matrix. After the row and column normalizations, the padded rows are discarded.\n\n        .. note::\n            We assume row number <= column number. If not, the input matrix will be transposed.\n        \"\"\"\n        if self.log_forward:\n            return self.forward_log(s, nrows, ncols, dummy_row)\n        else:\n            return self.forward_ori(s, nrows, ncols, dummy_row) # deprecated\n\n    def forward_log(self, s, nrows=None, ncols=None, dummy_row=False):\n        \"\"\"Compute sinkhorn with row/column normalization in the log space.\"\"\"\n        return pygm.sinkhorn(s, n1=nrows, n2=ncols, dummy_row=dummy_row, max_iter=self.max_iter, tau=self.tau, batched_operation=self.batched_operation, backend='pytorch')",
  "description": "Combined Analysis:\n- [models/NGM/gnn.py]: This file implements the core neural network layers for the Neural Graph Matching (NGM) model and its hypergraph extension (NHGM). The GNNLayer class corresponds to the pairwise graph matching formulation, where the affinity matrix K is treated as an association graph. It performs vertex embedding via graph convolution (n_func and n_self_func) and optionally uses a Sinkhorn layer (when sk_channel>0) to enforce assignment constraints, directly aligning with the paper's algorithm steps. The HyperGNNLayer extends this to hypergraph matching by handling higher-order affinity tensors (orders>2) through sparse/dense forward passes, implementing the hypergraph matching extension. Both classes integrate the Sinkhorn normalization for constraint satisfaction, which is central to the paper's approach of transforming QAP into constrained vertex classification.\n- [models/NGM/hypermodel.py]: This file implements the core Neural Graph Matching (NGM) algorithm as described in the paper. The forward method constructs the affinity matrix K (Lawler's QAP formulation) from node and edge features, builds higher-order hypergraph adjacency tensors for 3rd-order matching, processes them through HyperGNN layers for vertex classification on the association graph, and finally applies Sinkhorn normalization and Hungarian algorithm to obtain a permutation matrix. The implementation directly corresponds to the paper's optimization model: it maximizes the quadratic form vec(X)^T K vec(X) by learning to classify vertices in the association graph, with extensions to hypergraph matching via 3rd-order affinity tensors.\n- [models/NGM/mgmmodel.py]: This file implements the Neural Graph Matching (NGM) network for multiple-graph matching as described in the paper. The core logic includes:\n1. Pairwise graph matching via Lawler's QAP formulation using an affinity matrix K constructed from node and edge features\n2. GNN-based vertex classification on the association graph (affinity matrix treated as adjacency)\n3. Sinkhorn normalization for assignment constraints (doubly-stochastic matrices)\n4. Extension to multiple-graph matching via spectral synchronization (eigendecomposition of joint matching matrix)\n5. Hungarian algorithm for final discrete permutation matrices\n\nThe implementation directly maps to the mathematical model: vec(X)^T K vec(X) optimization through neural network layers that operate on the affinity matrix, with constraints enforced via differentiable Sinkhorn layers. The multiple-graph matching extension uses spectral methods to achieve consistency across multiple graphs.\n- [models/NGM/model.py]: This file implements the core NGM algorithm as described in the paper. The forward method constructs the affinity matrix K (either from images/features or uses provided aff_mat), treats it as an association graph adjacency matrix, applies GNN layers for vertex classification, then uses Sinkhorn normalization and Hungarian algorithm to enforce assignment constraints. The optimization objective vec(X)^T K vec(X) is implicitly maximized through the GNN-based vertex classification and the subsequent discrete assignment via Hungarian algorithm. The code handles both first-order (node) and second-order (edge) affinity terms through the affinity_layer, and the GNN layers operate directly on the association graph built from K.\n- [models/NGM/model_v2.py]: This file implements the Neural Graph Matching (NGM) model exactly as described in the paper. The core logic includes: 1) Construction of the quadratic affinity matrix K from node and edge features, 2) Treating the matching problem as vertex classification on an association graph using GNN layers, 3) Using Sinkhorn normalization to enforce assignment constraints, 4) Hungarian algorithm for discretization, and 5) Extension to multiple-graph matching via spectral synchronization. The code directly corresponds to the mathematical formulation of Lawler's QAP: max vec(X)^T K vec(X) with permutation constraints, implemented through differentiable neural network layers.\n- [src/lap_solvers/sinkhorn.py]: This file implements the Sinkhorn algorithm, which is a core component of the Neural Graph Matching Network's optimization pipeline. The paper uses Sinkhorn normalization to enforce the assignment constraints (doubly-stochastic/bistochastic matrix constraints) in a differentiable manner. Specifically, after the neural network produces raw matching scores, the Sinkhorn layer projects these scores onto the space of doubly-stochastic matrices, approximating the discrete permutation matrix constraints required by Lawler's QAP. The implementation supports batched operations, variable graph sizes via dummy rows, and log-space computation for numerical stability - all essential for integrating with deep learning frameworks. The forward_log method delegates to pygmtools.sinkhorn, which implements the actual Sinkhorn iterations with temperature scaling and row/column normalization.",
  "dependencies": [
    "src.evaluation_metric",
    "src.utils.pad_tensor",
    "pygmtools",
    "src.utils.config.cfg",
    "src.feature_align.feature_align",
    "torch_sparse.spmm",
    "collections.Iterable",
    "torch_geometric.nn.GCNConv",
    "src.factorize_graph_matching",
    "models.BBGM.sconv_archs",
    "torch_geometric.nn.SAGEConv",
    "src.backbone",
    "models.AFAT.sinkhorn_topk",
    "models.GMN.affinity_layer.GaussianAffinity",
    "torch_sparse",
    "src.feature_align",
    "src.build_graphs",
    "itertools",
    "torch.nn.functional",
    "src.lap_solvers.hungarian",
    "models.NGM.gnn",
    "src.lap_solvers.sinkhorn",
    "src.utils.config",
    "math",
    "models.BBGM.affinity_layer",
    "pad_tensor (helper function)",
    "models.NGM.gnn.HyperGNNLayer",
    "models.GMN.affinity_layer.InnerpAffinity",
    "src.build_graphs.reshape_edge_feature",
    "models.NGM.geo_edge_feature",
    "torch_geometric.nn.MessagePassing",
    "src.lap_solvers.sinkhorn.Sinkhorn",
    "torch_geometric.nn.GINConv",
    "models.NGM.gnn.GNNLayer",
    "models.GMN.affinity_layer",
    "numpy",
    "src.utils.gpu_memory",
    "itertools.combinations",
    "torch.nn",
    "torch",
    "src.factorize_graph_matching.construct_aff_mat",
    "src.lap_solvers.hungarian.hungarian",
    "models.NGM.geo_edge_feature.geo_edge_feature"
  ]
}