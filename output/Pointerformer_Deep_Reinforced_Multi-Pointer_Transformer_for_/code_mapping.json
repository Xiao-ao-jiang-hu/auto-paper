{
  "file_path": "env.py, eval.py, models.py, train.py",
  "function_name": "MultiTrajectoryTSP, TSPModel.validate_all, RevMHAEncoder, DecoderForLarge, TSPModel",
  "code_snippet": "\n\n# ==========================================\n# File: env.py\n# Function/Context: MultiTrajectoryTSP\n# ==========================================\nimport torch\nimport numpy as np\nfrom torch.utils.data import DataLoader, Dataset\nimport os\n\n\nclass GroupState:\n    def __init__(self, group_size, x):\n        # x.shape = [B, N, 2]\n        self.batch_size = x.size(0)\n        self.group_size = group_size\n        self.device = x.device\n\n        self.selected_count = 0\n        # current_node.shape = [B, G]\n        self.current_node = None\n        # selected_node_list.shape = [B, G, selected_count]\n        self.selected_node_list = torch.zeros(\n            x.size(0), group_size, 0, device=x.device\n        ).long()\n        # ninf_mask.shape = [B, G, N]\n        self.ninf_mask = torch.zeros(x.size(0), group_size, x.size(1), device=x.device)\n\n    def move_to(self, selected_idx_mat):\n        # selected_idx_mat.shape = [B, G]\n        self.selected_count += 1\n        self.current_node = selected_idx_mat\n        self.selected_node_list = torch.cat(\n            (self.selected_node_list, selected_idx_mat[:, :, None]), dim=2\n        )\n        self.ninf_mask.scatter_(\n            dim=-1, index=selected_idx_mat[:, :, None], value=-torch.inf\n        )\n\n\nclass MultiTrajectoryTSP:\n    def __init__(self, x, x_raw=None, integer=False):\n        self.integer = integer\n        if x_raw is None:\n            self.x_raw = x.clone()\n        else:\n            self.x_raw = x_raw\n        self.x = x\n        self.batch_size = self.B = x.size(0)\n        self.graph_size = self.N = x.size(1)\n        self.node_dim = self.C = x.size(2)\n        self.group_size = self.G = None\n        self.group_state = None\n\n    def reset(self, group_size):\n        self.group_size = group_size\n        self.group_state = GroupState(group_size=group_size, x=self.x)\n        reward = None\n        done = False\n        return self.group_state, reward, done\n\n    def step(self, selected_idx_mat):\n        # move state\n        self.group_state.move_to(selected_idx_mat)\n\n        # returning values\n        done = self.group_state.selected_count == self.graph_size\n        if done:\n            reward = -self._get_group_travel_distance()  # note the minus sign!\n        else:\n            reward = None\n        return self.group_state, reward, done\n\n    def _get_group_travel_distance(self):\n        # ordered_seq.shape = [B, G, N, C]\n        shp = (self.B, self.group_size, self.N, self.C)\n        gathering_index = self.group_state.selected_node_list.unsqueeze(3).expand(*shp)\n        seq_expanded = self.x_raw[:, None, :, :].expand(*shp)\n        ordered_seq = seq_expanded.gather(dim=2, index=gathering_index)\n        rolled_seq = ordered_seq.roll(dims=2, shifts=-1)\n        # segment_lengths.size = [B, G, N]\n        segment_lengths = ((ordered_seq - rolled_seq) ** 2).sum(3).sqrt()\n        if self.integer:\n            group_travel_distances = segment_lengths.round().sum(2)\n        else:\n            group_travel_distances = segment_lengths.sum(2)\n        return group_travel_distances\n\n# ==========================================\n# File: eval.py\n# Function/Context: TSPModel.validate_all\n# ==========================================\n    def validate_all(\n        self, batch, val_type=\"None\", return_pi=False, real_data=False, sample=False\n    ):\n        batch_raw = batch\n        if real_data:\n            scale = (\n                (batch.max(1, keepdim=True).values - batch.min(1, keepdim=True).values)\n                .max(2, keepdim=True)\n                .values\n            )\n            batch_trans = (\n                (batch - batch.min(1, keepdim=True).values) / scale\n            ) * 0.9 + 0.05\n\n        if self.cfg.val_type == \"x8Aug_nTraj\":\n            if real_data:\n                batch_raw = batch_raw.repeat(8, 1, 1)\n                batch = utils.augment_xy_data_by_8_fold(batch_trans)\n            else:\n                batch_raw = batch_raw.repeat(8, 1, 1)\n                batch = utils.augment_xy_data_by_8_fold(batch)\n            B, N, _ = batch.shape\n            G = N\n        elif self.cfg.val_type == \"nTraj\":\n            B, N, _ = batch.shape\n            G = N\n        else:\n            B, N, _ = batch.shape\n            G = 1\n\n        env = MultiTrajectoryTSP(batch, x_raw=batch_raw, integer=self.cfg.integer)\n\n        s, r, d = env.reset(group_size=G)\n\n        if self.cfg.data_augment:\n            batch = utils.data_augment(batch)\n\n        embeddings = self.encoder(batch)\n        self.decoder.reset(batch, embeddings, G, trainging=False)\n\n        first_action = torch.randperm(N)[None, :G].expand(B, G).to(self.device)\n        pi = first_action[..., None]\n        s, r, d = env.step(first_action)\n\n        while not d:\n            action_probs = self.decoder(\n                s.current_node.to(self.device),\n                s.ninf_mask.to(self.device),\n                s.selected_count,\n            )\n\n            if sample:\n                m = Categorical(action_probs.reshape(B * G, -1))\n                action = m.sample().view(B, G)\n            else:\n                action = action_probs.argmax(dim=2)\n\n            # pi = torch.cat([pi, action[..., None]], dim=-1)\n            s, r, d = env.step(action)\n\n        if self.cfg.val_type == \"x8Aug_nTraj\":\n            B = round(B / 8)\n            reward = r.reshape(8, B, G)\n            # pi = pi.reshape(8, B, G, N)\n\n            reward_greedy = reward[0, :, 0]\n            reward_ntraj, idx_dim_ntraj = reward[0, :, :].max(dim=-1)\n            max_reward_aug_ntraj, idx_dim_2 = reward.max(dim=2)\n            max_reward_aug_ntraj, idx_dim_0 = max_reward_aug_ntraj.max(dim=0)\n\n            # best_pi_greedy = pi[0,:,0]\n            # idx_dim_ntraj = idx_dim_ntraj.reshape(B,1,1)\n            # best_pi_n_traj = pi[0,:,:].gather(1,idx_dim_ntraj.repeat(1,1,N)) #(B,G,N)\n            # idx_dim_0 = idx_dim_0.reshape(1, B, 1, 1)\n            # idx_dim_2 = idx_dim_2.reshape(8, B, 1, 1).gather(0, idx_dim_0)\n            # best_pi_aug_ntraj = pi.gather(0, idx_dim_0.repeat(1, 1, G, N))\n            # best_pi_aug_ntraj = best_pi_aug_ntraj.gather(2, idx_dim_2.repeat(1, 1, 1, N))\n            # print(torch.sort(best_pi_aug_ntraj[0][0][0]))\n\n            return {\n                \"max_reward_aug_ntraj\": -max_reward_aug_ntraj,\n                \"reward_greedy\": -reward_greedy,\n                \"reward_ntraj\": -reward_ntraj,\n            }\n        elif self.cfg.val_type == \"nTraj\":\n            reward = r\n            reward_greedy = reward[:, 0]\n            reward_ntraj = reward.max(dim=-1).values\n\n            return {\n                \"max_reward_aug_ntraj\": torch.zeros(B),\n                \"reward_greedy\": -reward_greedy,\n                \"reward_ntraj\": -reward_ntraj,\n            }\n        elif self.cfg.val_type == \"1Traj\":\n            reward = r\n            reward_greedy = reward[:, 0]\n            return {\n                \"max_reward_aug_ntraj\": torch.zeros(B),\n                \"reward_greedy\": -reward_greedy,\n                \"reward_ntraj\": torch.zeros(B),\n            }\n        else:\n            print(\"no {} eval type\".format(self.cfg.val_type))\n\n# ==========================================\n# File: models.py\n# Function/Context: RevMHAEncoder, DecoderForLarge\n# ==========================================\nimport math\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor, nn\nimport torch.utils.checkpoint\nimport utils\nimport revtorch as rv\n\nclass MHABlock(nn.Module):\n    def __init__(self, hidden_size: int, num_heads: int):\n        super().__init__()\n        self.mixing_layer_norm = nn.BatchNorm1d(hidden_size)\n        self.mha = nn.MultiheadAttention(hidden_size, num_heads, bias=False)\n\n    def forward(self, hidden_states: Tensor):\n        assert hidden_states.dim() == 3\n        hidden_states = self.mixing_layer_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n        hidden_states_t = hidden_states.transpose(0, 1)\n        mha_output = self.mha(hidden_states_t, hidden_states_t, hidden_states_t)[0].transpose(0, 1)\n        return mha_output\n\n\nclass FFBlock(nn.Module):\n    def __init__(self, hidden_size: int, intermediate_size: int):\n        super().__init__()\n        self.feed_forward = nn.Linear(hidden_size, intermediate_size)\n        self.output_dense = nn.Linear(intermediate_size, hidden_size)\n        self.output_layer_norm = nn.BatchNorm1d(hidden_size)\n        self.activation = nn.GELU()\n\n    def forward(self, hidden_states: Tensor):\n        hidden_states = self.output_layer_norm(hidden_states.transpose(1, 2)).transpose(1, 2).contiguous()\n        intermediate_output = self.feed_forward(hidden_states)\n        intermediate_output = self.activation(intermediate_output)\n        output = self.output_dense(intermediate_output)\n        return output\n\n\nclass RevMHAEncoder(nn.Module):\n    def __init__(\n        self,\n        n_layers: int,\n        n_heads: int,\n        embedding_dim: int,\n        input_dim: int,\n        intermediate_dim: int,\n        add_init_projection=True,\n    ):\n        super().__init__()\n        if add_init_projection or input_dim != embedding_dim:\n            self.init_projection_layer = torch.nn.Linear(input_dim, embedding_dim)\n        self.num_hidden_layers = n_layers\n        blocks = []\n        for _ in range(n_layers):\n            f_func = MHABlock(embedding_dim, n_heads)\n            g_func = FFBlock(embedding_dim, intermediate_dim)\n            blocks.append(rv.ReversibleBlock(f_func, g_func, split_along_dim=-1))\n        self.sequence = rv.ReversibleSequence(nn.ModuleList(blocks))\n\n    def forward(self, x: Tensor, mask=None):\n        if hasattr(self, \"init_projection_layer\"):\n            x = self.init_projection_layer(x)\n        x = torch.cat([x, x], dim=-1)\n        out = self.sequence(x)\n        return torch.stack(out.chunk(2, dim=-1))[-1]\n\n\nclass DecoderForLarge(torch.nn.Module):\n    def __init__(\n        self,\n        embedding_dim,\n        n_heads=8,\n        tanh_clipping=10.0,\n        multi_pointer=1,\n        multi_pointer_level=1,\n        add_more_query=True,\n    ):\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.n_heads = n_heads\n        self.tanh_clipping = tanh_clipping\n        self.Wq_graph = torch.nn.Linear(embedding_dim, embedding_dim, bias=False)\n        self.Wq_first = torch.nn.Linear(embedding_dim, embedding_dim, bias=False)\n        self.Wq_last = torch.nn.Linear(embedding_dim, embedding_dim, bias=False)\n        self.wq = torch.nn.Linear(embedding_dim, embedding_dim, bias=False)\n        self.W_visited = torch.nn.Linear(embedding_dim, embedding_dim, bias=False)\n        self.Wk = torch.nn.Linear(embedding_dim, embedding_dim, bias=False)\n        self.q_graph = None\n        self.q_first = None\n        self.glimpse_k = None\n        self.glimpse_v = None\n        self.logit_k = None\n        self.group_ninf_mask = None\n        self.multi_pointer = multi_pointer\n        self.multi_pointer_level = multi_pointer_level\n        self.add_more_query = add_more_query\n\n    def reset(self, coordinates, embeddings, G, trainging=True):\n        B, N, H = embeddings.shape\n        self.coordinates = coordinates\n        self.embeddings = embeddings\n        self.embeddings_group = self.embeddings.unsqueeze(1).expand(B, G, N, H)\n        graph_embedding = self.embeddings.mean(dim=1, keepdim=True)\n        self.q_graph = self.Wq_graph(graph_embedding)\n        self.q_first = None\n        self.logit_k = embeddings.transpose(1, 2)\n        if self.multi_pointer > 1:\n            self.logit_k = utils.make_heads(self.Wk(embeddings), self.multi_pointer).transpose(2, 3)\n\n    def forward(self, last_node, group_ninf_mask, S):\n        B, N, H = self.embeddings.shape\n        G = group_ninf_mask.size(1)\n        last_node_index = last_node.view(B, G, 1).expand(-1, -1, H)\n        last_node_embedding = self.embeddings.gather(1, last_node_index)\n        q_last = self.Wq_last(last_node_embedding)\n        if self.q_first is None:\n            self.q_first = self.Wq_first(last_node_embedding)\n        group_ninf_mask = group_ninf_mask.detach()\n        mask_visited = group_ninf_mask.clone()\n        mask_visited[mask_visited == -np.inf] = 1.0\n        q_visited = self.W_visited(torch.bmm(mask_visited, self.embeddings) / N)\n        D = self.coordinates.size(-1)\n        last_node_coordinate = self.coordinates.gather(dim=1, index=last_node.unsqueeze(-1).expand(B, G, D))\n        distances = torch.cdist(last_node_coordinate, self.coordinates)\n        if self.add_more_query:\n            final_q = q_last + self.q_first + self.q_graph + q_visited\n        else:\n            final_q = q_last + self.q_first + self.q_graph\n        if self.multi_pointer > 1:\n            final_q = utils.make_heads(self.wq(final_q), self.n_heads)\n            score = (torch.matmul(final_q, self.logit_k) / math.sqrt(H)) - (distances / math.sqrt(2)).unsqueeze(1)\n            if self.multi_pointer_level == 1:\n                score_clipped = self.tanh_clipping * torch.tanh(score.mean(1))\n            elif self.multi_pointer_level == 2:\n                score_clipped = (self.tanh_clipping * torch.tanh(score)).mean(1)\n            else:\n                score_clipped = self.tanh_clipping * torch.tanh(score)\n                mask_prob = group_ninf_mask.detach().clone()\n                mask_prob[mask_prob == -np.inf] = -1e8\n                score_masked = score_clipped + mask_prob.unsqueeze(1)\n                probs = F.softmax(score_masked, dim=-1).mean(1)\n                return probs\n        else:\n            score = torch.matmul(final_q, self.logit_k) / math.sqrt(H) - distances / math.sqrt(2)\n            score_clipped = self.tanh_clipping * torch.tanh(score)\n        mask_prob = group_ninf_mask.detach().clone()\n        mask_prob[mask_prob == -np.inf] = -1e8\n        score_masked = score_clipped + mask_prob\n        probs = F.softmax(score_masked, dim=2)\n        return probs\n\n# ==========================================\n# File: train.py\n# Function/Context: TSPModel\n# ==========================================\nimport math\nimport os\nimport time\n\nimport hydra\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom omegaconf import DictConfig, open_dict\nfrom pytorch_lightning.loggers import WandbLogger, TensorBoardLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom torch.utils.data import DataLoader, Dataset\nimport models\nimport utils\nfrom torch.distributions.categorical import Categorical\n\nfrom env import MultiTrajectoryTSP, TSPDataset\n\n\nclass TSPModel(pl.LightningModule):\n    def __init__(self, cfg: DictConfig):\n        super().__init__()\n        if cfg.node_dim > 2:\n            assert (\n                \"noAug\" in cfg.val_type\n            ), \"High-dimension TSP doesn't support augmentation\"\n\n        ## Encoder model\n        if cfg.encoder_type == \"mha\":\n            self.encoder = models.MHAEncoder(\n                n_layers=cfg.n_layers,\n                n_heads=cfg.n_heads,\n                embedding_dim=cfg.embedding_dim,\n                input_dim=24 if cfg.data_augment else 2,\n                add_init_projection=cfg.add_init_projection,\n            )\n        elif cfg.encoder_type == \"revmha\":\n            self.encoder = models.RevMHAEncoder(\n                n_layers=cfg.n_layers,\n                n_heads=cfg.n_heads,\n                embedding_dim=cfg.embedding_dim,\n                input_dim=24 if cfg.data_augment else 2,\n                intermediate_dim=cfg.embedding_dim * 4,\n                add_init_projection=cfg.add_init_projection,\n            )\n\n        ## Decoder model\n        if cfg.decoder_type == \"DecoderForLarge\":\n            self.decoder = models.DecoderForLarge(\n                embedding_dim=cfg.embedding_dim,\n                n_heads=cfg.n_heads,\n                tanh_clipping=cfg.tanh_clipping,\n                multi_pointer=cfg.multi_pointer,\n                multi_pointer_level=cfg.multi_pointer_level,\n                add_more_query=cfg.add_more_query,\n            )\n        else:\n            self.decoder = models.Decoder(\n                embedding_dim=cfg.embedding_dim,\n                n_heads=cfg.n_heads,\n                tanh_clipping=cfg.tanh_clipping,\n            )\n\n        self.cfg = cfg\n        self.save_hyperparameters(cfg)\n\n    def training_step(self, batch, _):\n        B, N, _ = batch.shape\n        G = self.group_size\n\n        batch_idx_range = torch.arange(B)[:, None].expand(B, G)\n        group_idx_range = torch.arange(G)[None, :].expand(B, G)\n\n        env = MultiTrajectoryTSP(batch.cpu())  # to cpu for env\n        s, r, d = env.reset(group_size=G)  # reset env\n\n        # Data argument\n        if self.cfg.data_augment:\n            batch = utils.data_augment(batch)\n\n        # Encode\n        embeddings = self.encoder(batch)\n        self.decoder.reset(batch, embeddings, G)  # decoder reset\n\n        entropy_list = []\n        log_prob = torch.zeros(B, G, device=self.device)\n        while not d:\n            if s.current_node is None:\n                first_action = torch.randperm(N)[None, :G].expand(B, G)\n                s, r, d = env.step(first_action)\n                continue\n            else:\n                last_node = s.current_node\n\n            action_probs = self.decoder(\n                last_node.to(self.device), s.ninf_mask.to(self.device), s.selected_count\n            )\n            m = Categorical(action_probs.reshape(B * G, -1))\n            entropy_list.append(m.entropy().mean().item())\n            action = m.sample().view(B, G)\n            chosen_action_prob = (\n                action_probs[batch_idx_range, group_idx_range, action].reshape(B, G)\n                + 1e-8\n            )\n            log_prob += chosen_action_prob.log()\n            s, r, d = env.step(action.cpu())\n\n        # Note that when G == 1, we can only use the PG without baseline so far\n        r_trans = r.to(self.device)  # -1/torch.exp(r)\n        if self.cfg.divide_std:\n            advantage = (\n                (r_trans - r_trans.mean(dim=1, keepdim=True))\n                / (r_trans.std(dim=1, unbiased=False, keepdim=True) + 1e-8)\n                if G != 1\n                else r_trans\n            )\n        else:\n            advantage = (\n                (r_trans - r_trans.mean(dim=1, keepdim=True)) if G != 1 else r_trans\n            )\n        loss = (-advantage * log_prob).mean()\n\n        length_max = -r.max(dim=1)[0].mean().clone().detach().item()\n        lenght_mean = -r.mean(1).mean().clone().detach().item()\n        entropy_mean = sum(entropy_list) / len(entropy_list)\n        adv = advantage.abs().mean().clone().detach().item()\n\n        self.log(\n            name=\"length_max\", value=length_max, prog_bar=True,\n        )  # sync_dist=True\n        self.log(\n            name=\"length_mean\", value=lenght_mean, prog_bar=True,\n        )\n        self.log(\n            name=\"entropy\", value=entropy_mean, prog_bar=True,\n        )\n        self.log(name=\"adv\", value=adv, prog_bar=True)\n\n        embed_max = embeddings.abs().max()\n        self.log(\n            name=\"embed_max\", value=embed_max, prog_bar=True,\n        )\n\n        assert torch.isnan(loss).sum() == 0, print(\"loss is nan!\")\n\n        return {\"loss\": loss, \"length\": length_max, \"entropy\": entropy_mean}\n\n    def validate_all(self, batch, val_type=None, return_pi=False):\n        batch = utils.augment_xy_data_by_8_fold(batch)\n        B, N, _ = batch.shape\n        G = N\n\n        env = MultiTrajectoryTSP(batch)\n        s, r, d = env.reset(group_size=G)\n\n        if self.cfg.data_augment:\n            batch = utils.data_augment(batch)\n\n        embeddings = self.encoder(batch)\n        self.decoder.reset(batch, embeddings, G, trainging=False)\n\n        first_action = torch.randperm(N)[None, :G].expand(B, G).to(self.device)\n        pi = first_action[..., None]\n        s, r, d = env.step(first_action)\n\n        while not d:\n            action_probs = self.decoder(\n                s.current_node.to(self.device),\n                s.ninf_mask.to(self.device),\n                s.selected_count,\n            )\n            action = action_probs.argmax(dim=2)\n            pi = torch.cat([pi, action[..., None]], dim=-1)\n            s, r, d = env.step(action)\n\n        B = round(B / 8)\n        reward = r.reshape(8, B, G)\n        pi = pi.reshape(8, B, G, N)\n\n        reward_greedy = reward[0, :, 0]\n        reward_ntraj, idx_dim_ntraj = reward[0, :, :].max(dim=-1)\n        max_reward_aug_ntraj, idx_dim_2 = reward.max(dim=2)\n        max_reward_aug_ntraj, idx_dim_0 = max_reward_aug_ntraj.max(dim=0)\n\n        if return_pi:\n            best_pi_greedy = pi[0, :, 0]\n            idx_dim_ntraj = idx_dim_ntraj.reshape(B, 1, 1)\n            best_pi_n_traj = pi[0, :, :].gather(\n                1, idx_dim_ntraj.repeat(1, 1, N)\n            )  # (B,G,N)\n            idx_dim_0 = idx_dim_0.reshape(1, B, 1, 1)\n            idx_dim_2 = idx_dim_2.reshape(8, B, 1, 1).gather(0, idx_dim_0)\n            best_pi_aug_ntraj = pi.gather(0, idx_dim_0.repeat(1, 1, G, N))\n            best_pi_aug_ntraj = best_pi_aug_ntraj.gather(\n                2, idx_dim_2.repeat(1, 1, 1, N)\n            )\n            return -max_reward_aug_ntraj, best_pi_aug_ntraj.squeeze()\n        return {\n            \"max_reward_aug_ntraj\": -max_reward_aug_ntraj,\n            \"reward_greedy\": -reward_greedy,\n            \"reward_ntraj\": -reward_ntraj,\n        }\n\n    def validation_step(self, batch, batch_idx):\n        with torch.no_grad():\n            outputs = self.validate_all(batch)\n        return outputs\n\n    def on_validation_epoch_start(self) -> None:\n        self.validation_start_time = time.time()\n\n    def validation_epoch_end(self, outputs):\n        max_reward_aug_ntraj = [item[\"max_reward_aug_ntraj\"] for item in outputs]\n        reward_greedy = [item[\"reward_greedy\"] for item in outputs]\n        reward_ntraj = [item[\"reward_ntraj\"] for item in outputs]\n\n        self.max_reward_aug_ntraj = torch.cat(max_reward_aug_ntraj).mean().item()\n        self.max_reward_aug_ntraj_std = torch.cat(max_reward_aug_ntraj).std().item()\n        self.reward_greedy = torch.cat(reward_greedy).mean().item()\n        self.reward_greedy_std = torch.cat(reward_greedy).std().item()\n        self.reward_ntraj = torch.cat(reward_ntraj).mean().item()\n        self.reward_ntraj_std = torch.cat(reward_ntraj).std().item()\n\n    def on_validation_epoch_end(self):\n        validation_time = time.time() - self.validation_start_time\n\n        self.log_dict(\n            {\n                \"max_reward_aug_ntraj\": self.max_reward_aug_ntraj,\n                \"reward_greedy\": self.reward_greedy,\n                \"reward_ntraj\": self.reward_ntraj,\n            },\n            on_epoch=True,\n            on_step=False,\n        )\n\n        self.print(\n            f\"\\nEpoch {self.current_epoch}: \",\n            \"max_reward_aug_ntraj={:.03f}±{:.03f}, \".format(\n                self.max_reward_aug_ntraj, self.max_reward_aug_ntraj_std\n            ),\n            \"reward_greedy={:.03f}±{:.3f}, \".format(\n                self.reward_greedy, self.reward_greedy_std\n            ),\n            \"reward_ntraj={:.03f}±{:.03f}, \".format(\n                self.reward_ntraj, self.reward_ntraj_std\n            ),\n            \"validation time={:.03f}\".format(validation_time),\n        )\n\n\n@hydra.main(config_name=\"config\")\ndef run(cfg: DictConfig) -> None:\n    pl.seed_everything(cfg.seed)\n    cfg.run_name = cfg.run_name or cfg.default_run_name\n    if cfg.save_dir is None:\n        root_dir = (os.getcwd(),)\n    elif os.path.isabs(cfg.save_dir):\n        root_dir = cfg.save_dir\n    else:\n        root_dir = os.path.join(hydra.utils.get_original_cwd(), cfg.save_dir)\n    root_dir = os.path.join(root_dir, f\"{cfg.run_name}\")\n    with open_dict(cfg):\n        cfg.root_dir = root_dir\n\n    cfg.val_data_path = os.path.join(hydra.utils.get_original_cwd(), cfg.val_data_path)\n\n    # build  TSPModel\n    tsp_model = TSPModel(cfg)\n    checkpoint_callback = ModelCheckpoint(\n        monitor=\"reward_ntraj\",\n        dirpath=os.path.join(root_dir, \"checkpoints\"),\n        filename=cfg.encoder_type + \"{epoch:02d}-{val_length:.2f}\",\n        save_top_k=3,\n        save_last=True,\n        mode=\"min\",\n        every_n_val_epochs=2,\n    )\n\n    # set up loggers\n    loggers = []\n    if cfg.tensorboard:\n        tb_logger = TensorBoardLogger(\"logs\")\n        loggers.append(tb_logger)\n    # wandb logger\n    if cfg.wandb:\n        os.makedirs(os.path.join(os.path.abspath(root_dir), \"wandb\"))\n        wandb_logger = WandbLogger(\n            name=cfg.run_name,\n            save_dir=root_dir,\n            project=cfg.wandb_project,\n            log_model=True,\n            save_code=True,\n            group=time.strftime(\"%Y%m%d\", time.localtime()),\n            tags=cfg.default_run_name.split(\"-\")[:-1],\n        )\n        wandb_logger.log_hyperparams(cfg)\n        wandb_logger.watch(tsp_model)\n        loggers.append(wandb_logger)\n\n    # auto resumed training\n    last_ckpt_path = os.path.join(checkpoint_callback.dirpath, \"last.ckpt\")\n    if os.path.exists(last_ckpt_path):\n        resume = last_ckpt_path\n    elif cfg.load_path:\n        resume = cfg.load_path\n    else:\n        resume = None\n\n    # build trainer\n    trainer = pl.Trainer(\n        default_root_dir=root_dir,\n        gpus=cfg.gpus,\n        accelerator=\"dp\",\n        precision=cfg.precision,\n        max_epochs=cfg.total_epoch,\n        reload_dataloaders_every_epoch=True,\n        num_sanity_val_steps=0,\n        resume_from_checkpoint=resume,\n        logger=loggers,\n        callbacks=[checkpoint_callback],\n    )\n\n    # training and save ckpt\n    trainer.fit(tsp_model)\n    trainer.save_checkpoint(os.path.join(root_dir, \"checkpoint.ckpt\"))",
  "description": "Combined Analysis:\n- [env.py]: This file implements the core Markov Decision Process (MDP) environment for the Traveling Salesman Problem (TSP) as described in the Pointerformer paper. The MultiTrajectoryTSP class defines the sequential decision-making process where an agent selects cities one by one to form a tour. Key implementations include: 1) State management through GroupState tracking visited cities via ninf_mask and selected_node_list; 2) Step function that updates state and computes reward as negative tour length upon completion; 3) Tour length calculation (_get_group_travel_distance) matching the paper's objective function L(τ) using Euclidean distances between consecutive cities (including return to start via torch.roll). The environment supports batched multi-trajectory generation for reinforcement learning training.\n- [eval.py]: This file implements the core evaluation logic of Pointerformer's sequential decision-making process for TSP. The validate_all method executes the key algorithm steps: (1) Environment initialization with MultiTrajectoryTSP, (2) Encoder processing of node embeddings, (3) Decoder's sequential action selection (either greedy argmax or sampling), (4) Step-by-step tour construction until termination. It directly corresponds to the paper's MDP formulation where the policy π sequentially selects unvisited cities until a complete tour is formed. The method supports multiple evaluation strategies (x8Aug_nTraj, nTraj, 1Traj) that align with the multi-pointer transformer architecture's ability to generate multiple candidate tours.\n- [models.py]: This file implements the core neural architecture of Pointerformer as described in the paper. The RevMHAEncoder class uses reversible residual blocks (via revtorch) to reduce memory usage during training, aligning with the reversible residual network in the encoder. The DecoderForLarge class implements the multi-pointer network that computes probability distributions over unvisited cities using multiple attention heads (controlled by multi_pointer and multi_pointer_level parameters). The forward method sequentially computes scores combining embeddings, distances, and masking to produce action probabilities, matching the sequential decision process of the TSP Markov decision process. Key mathematical operations include multi-head attention, Euclidean distance calculations, tanh clipping, and softmax normalization for policy output.\n- [train.py]: This file implements the core training and validation logic of Pointerformer. The TSPModel class encapsulates the encoder-decoder architecture (reversible residual network encoder and multi-pointer decoder) and the REINFORCE-based training algorithm. The training_step method performs the sequential decision process: encoding city coordinates, using the decoder to generate action probabilities (next city selections), sampling actions, and computing the policy gradient loss with a baseline. The validate_all method implements greedy rollouts with 8-fold augmentation for evaluation. The run function sets up the training pipeline with PyTorch Lightning, including checkpointing and logging. The code directly corresponds to the algorithm steps described in the paper: sequential tour construction via a policy network, training with a modified REINFORCE algorithm, and using a multi-pointer transformer architecture.",
  "dependencies": [
    "torch.utils.data",
    "torch.utils.data.Dataset",
    "torch.optim",
    "torch.distributions.categorical.Categorical",
    "torch.utils.data.DataLoader",
    "torch.nn.functional",
    "math",
    "hydra",
    "torch.distributions.categorical",
    "os",
    "env.MultiTrajectoryTSP",
    "revtorch",
    "env",
    "numpy",
    "utils",
    "omegaconf",
    "torch.nn",
    "torch",
    "pytorch_lightning",
    "models"
  ]
}