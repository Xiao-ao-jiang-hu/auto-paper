{
  "paper_id": "End-to-End_Stochastic_Optimization_with_Energy-Based_Model",
  "title": "End-to-End Stochastic Optimization with Energy-Based Model",
  "abstract": "Decision-focused learning (DFL) was recently proposed for stochastic optimization problems that involve unknown parameters. By integrating predictive modeling with an implicitly differentiable optimization layer, DFL has shown superior performance to the standard two-stage predict-then-optimize pipeline. However, most existing DFL methods are only applicable to convex problems or a subset of non-convex problems that can be easily relaxed to convex ones. Further, they can be inefficient in training due to the requirement of solving and differentiating through the optimization problem in every training iteration. We propose SO-EBM, a general and efficient DFL method for stochastic optimization using energy-based models. Instead of relying on KKT conditions to induce an implicit optimization layer, SO-EBM explicitly parameterizes the original optimization problem using a differentiable optimization layer based on energy functions. To better approximate the optimization landscape, we propose a coupled training objective that uses a maximum likelihood loss to capture the optimum location and a distribution-based regularizer to capture the overall energy landscape. Finally, we propose an efficient training procedure for SO-EBM with a self-normalized importance sampler based on a Gaussian mixture proposal. We evaluate SO-EBM in three applications: power scheduling, COVID-19 resource allocation, and non-convex adversarial security game, demonstrating the effectiveness and efficiency of SO-EBM.",
  "problem_description_natural": "The paper addresses stochastic optimization problems where decisions must be made to minimize an expected cost function that depends on unknown, context-dependent stochastic parameters (e.g., future demand, disease spread, or attacker behavior). These parameters must be predicted from observed features. The goal is to learn a model that maps input features directly to optimal decisions, integrating prediction and optimization in an end-to-end fashion. Unlike traditional two-stage approaches (predict-then-optimize), the proposed method tailors the predictive model specifically to minimize decision regret, even when the underlying optimization problem is non-convex.",
  "problem_type": "Stochastic optimization (general, including non-convex and combinatorial instances)",
  "datasets": [
    "Generated ER Graphs",
    "SEIR+HD ODE model simulations",
    "Power system load and temperature data"
  ],
  "performance_metrics": [
    "Task loss",
    "Training time per epoch"
  ],
  "lp_model": {
    "objective": "$\\min_{a \\in \\mathbb{R}^{24}} \\sum_{i=1}^{24} \\mathbb{E}_{y \\sim p(y|x;\\theta)} \\left[ \\gamma_s [y_i - a_i]_+ + \\gamma_e [a_i - y_i]_+ + \\frac{1}{2}(a_i - y_i)^2 \\right]$",
    "constraints": [
      "$|a_i - a_{i-1}| \\leq c_r \\quad \\forall i$"
    ],
    "variables": [
      "$a_i \\in \\mathbb{R}$ for $i=1,\\dots,24$: decision variable for electricity generation scheduled at hour i"
    ]
  },
  "raw_latex_model": "$$\\begin{aligned} & \\min_{a \\in \\mathbb{R}^{24}} \\sum_{i=1}^{24} \\mathbb{E}_{y \\sim p(y|x;\\theta)} \\left[ \\gamma_s [y_i - a_i]_+ + \\gamma_e [a_i - y_i]_+ + \\frac{1}{2}(a_i - y_i)^2 \\right] \\\\ & \\text{subject to } |a_i - a_{i-1}| \\leq c_r \\quad \\forall i \\end{aligned}$$",
  "algorithm_description": "SO-EBM (Stochastic Optimization with Energy-Based Model) is an end-to-end learning method that replaces the implicit optimization layer with an energy-based surrogate function. It uses a coupled training objective with maximum likelihood for optimal decisions and KL-divergence regularization for the energy landscape, and employs efficient self-normalized importance sampling with a Gaussian mixture proposal for training."
}