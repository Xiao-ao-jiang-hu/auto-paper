{
  "file_path": "annotation-cli.py, imle/noise.py, imle/target.py, imle/wrapper.py, solvers/dijkstra.py, sum-of-gamma-cli.py",
  "function_name": "learning, SumOfGammaNoiseDistribution, TargetDistribution.params, imle, ShortestPath, sample_sum_of_gammas, sample_sum_of_gammas_torch, main",
  "code_snippet": "\n\n# ==========================================\n# File: annotation-cli.py\n# Function/Context: learning\n# ==========================================\nimport sys\nimport numpy as np\nimport torch\nfrom torch import nn, Tensor\nfrom imle.wrapper import imle\nfrom imle.target import TargetDistribution\nfrom imle.noise import SumOfGammaNoiseDistribution\nfrom solvers.dijkstra import get_solver\nimport multiprocessing\nimport ray\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation\n\ndef set_seed(seed: int, is_deterministic: bool = True):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        if is_deterministic is True:\n            torch.backends.cudnn.deterministic = True\n            torch.backends.cudnn.benchmark = False\n\ndef maybe_parallelize(function, arg_list):\n    if ray.is_initialized():\n        ray_fn = ray.remote(function)\n        return ray.get([ray_fn.remote(arg) for arg in arg_list])\n    else:\n        return [function(arg) for arg in arg_list]\n\nclass HammingLoss(torch.nn.Module):\n    def forward(self, suggested, target):\n        errors = suggested * (1.0 - target) + (1.0 - suggested) * target\n        return errors.mean(dim=0).sum()\n\ndef learning(argv):\n    set_seed(0)\n    neighbourhood_fn = \"8-grid\"\n    solver = get_solver(neighbourhood_fn)\n    grid_size = [16, 16]\n    def torch_solver(weights_batch: Tensor) -> Tensor:\n        weights_batch = - 1.0 * weights_batch.detach().cpu().numpy()\n        y_batch = np.asarray(maybe_parallelize(solver, arg_list=list(weights_batch)))\n        return torch.tensor(y_batch, requires_grad=False)\n    for input_noise_temperature in [0.0, 1.0, 2.0, 5.0]:\n        for target_noise_temperature in [0.0, 1.0, 2.0, 5.0]:\n            for nb_samples in [1, 10, 100]:\n                true_weights = np.empty(shape=[1] + grid_size, dtype=float)\n                true_weights.fill(-1)\n                true_weights[0, 1:6, 0:12] = -100\n                true_weights[0, 8:12, 1:] = -100\n                true_weights[0, 14:, 6:10] = -100\n                true_y = torch_solver(torch.tensor(true_weights)).detach()\n                weights = np.random.uniform(low=-0.1, high=0.1, size=[1] + grid_size)\n                weights_tensor = torch.tensor(weights, dtype=torch.float)\n                weights_params = nn.Parameter(weights_tensor, requires_grad=True)\n                optimizer = torch.optim.Adam([weights_params], lr=0.005)\n                evolving_weights_lst = []\n                evolving_paths_lst = []\n                loss_fn = HammingLoss()\n                for t in range(1100):\n                    target_distribution = TargetDistribution(alpha=1.0, beta=10.0)\n                    noise_distribution = SumOfGammaNoiseDistribution(k=grid_size[0] * 1.3, nb_iterations=100)\n                    @imle(target_distribution=target_distribution,\n                          noise_distribution=noise_distribution,\n                          input_noise_temperature=input_noise_temperature,\n                          target_noise_temperature=target_noise_temperature,\n                          nb_samples=nb_samples)\n                    def imle_solver(weights_batch: Tensor) -> Tensor:\n                        return torch_solver(weights_batch)\n                    imle_y_tensor = imle_solver(weights_params)\n                    evolving_weights_lst += [np.copy(weights_params[0].detach().cpu().numpy())]\n                    evolving_paths_lst += [np.copy(imle_y_tensor[0].detach().cpu().numpy())]\n                    loss = loss_fn(imle_y_tensor, true_y)\n                    if t % 10:\n                        print(f\"Iteration: {t}\\tLoss: {loss.item():.2f}\")\n                    optimizer.zero_grad()\n                    loss.backward()\n                    optimizer.step()\n\n# ==========================================\n# File: imle/noise.py\n# Function/Context: SumOfGammaNoiseDistribution\n# ==========================================\n# -*- coding: utf-8 -*-\n\nimport math\n\nimport torch\nfrom torch import Tensor, Size\nfrom torch.distributions.gamma import Gamma\n\nfrom abc import ABC, abstractmethod\n\nfrom typing import Optional\n\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseNoiseDistribution(ABC):\n    def __init__(self):\n        super().__init__()\n\n    @abstractmethod\n    def sample(self,\n               shape: Size) -> Tensor:\n        raise NotImplementedError\n\n\nclass SumOfGammaNoiseDistribution(BaseNoiseDistribution):\n    r\"\"\"\n    Creates a generator of samples for the Sum-of-Gamma distribution [1], parameterized\n    by :attr:`k`, :attr:`nb_iterations`, and :attr:`device`.\n\n    [1] Mathias Niepert, Pasquale Minervini, Luca Franceschi - Implicit MLE: Backpropagating Through Discrete\n    Exponential Family Distributions. NeurIPS 2021 (https://arxiv.org/abs/2106.01798)\n\n    Example::\n\n        >>> import torch\n        >>> noise_distribution = SumOfGammaNoiseDistribution(k=5, nb_iterations=100)\n        >>> noise_distribution.sample(torch.Size([5]))\n        tensor([ 0.2504,  0.0112,  0.5466,  0.0051, -0.1497])\n\n    Args:\n        k (float): k parameter -- see [1] for more details.\n        nb_iterations (int): number of iterations for estimating the sample.\n        device (torch.devicde): device where to store samples.\n    \"\"\"\n    def __init__(self,\n                 k: float,\n                 nb_iterations: int = 10,\n                 device: Optional[torch.device] = None):\n        super().__init__()\n        self.k = k\n        self.nb_iterations = nb_iterations\n        self.device = device\n\n    def sample(self,\n               shape: Size) -> Tensor:\n        samples = torch.zeros(size=shape, device=self.device)\n        for i in range(1, self.nb_iterations + 1):\n            concentration = torch.tensor(1. / self.k, device=self.device)\n            rate = torch.tensor(i / self.k, device=self.device)\n\n            gamma = Gamma(concentration=concentration, rate=rate)\n            samples = samples + gamma.sample(sample_shape=shape).to(self.device)\n        samples = (samples - math.log(self.nb_iterations)) / self.k\n        return samples.to(self.device)\n\n# ==========================================\n# File: imle/target.py\n# Function/Context: TargetDistribution.params\n# ==========================================\n# -*- coding: utf-8 -*-\n\nfrom torch import Tensor\nfrom abc import ABC, abstractmethod\n\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseTargetDistribution(ABC):\n    def __init__(self):\n        super().__init__()\n\n    @abstractmethod\n    def params(self,\n               theta: Tensor,\n               dy: Tensor) -> Tensor:\n        raise NotImplementedError\n\n\nclass TargetDistribution(BaseTargetDistribution):\n    r\"\"\"\n    Creates a generator of target distributions parameterized by :attr:`alpha` and :attr:`beta`.\n\n    Example::\n\n        >>> import torch\n        >>> target_distribution = TargetDistribution(alpha=1.0, beta=1.0)\n        >>> target_distribution.params(theta=torch.tensor([1.0]), dy=torch.tensor([1.0]))\n        tensor([2.])\n\n    Args:\n        alpha (float): weight of the initial distribution parameters theta\n        beta (float): weight of the downstream gradient dy\n    \"\"\"\n    def __init__(self,\n                 alpha: float = 1.0,\n                 beta: float = 1.0):\n        super().__init__()\n        self.alpha = alpha\n        self.beta = beta\n\n    def params(self,\n               theta: Tensor,\n               dy: Tensor) -> Tensor:\n        theta_prime = self.alpha * theta - self.beta * dy\n        return theta_prime\n\n# ==========================================\n# File: imle/wrapper.py\n# Function/Context: imle\n# ==========================================\nimport functools\n\nimport torch\nfrom torch import Tensor\n\nfrom imle.noise import BaseNoiseDistribution\nfrom imle.target import BaseTargetDistribution, TargetDistribution\n\nfrom typing import Callable, Optional\n\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\ndef imle(function: Callable[[Tensor], Tensor] = None,\n         target_distribution: Optional[BaseTargetDistribution] = None,\n         noise_distribution: Optional[BaseNoiseDistribution] = None,\n         nb_samples: int = 1,\n         input_noise_temperature: float = 1.0,\n         target_noise_temperature: float = 1.0):\n    r\"\"\"Turns a black-box combinatorial solver in an Exponential Family distribution via Perturb-and-MAP and I-MLE [1].\n\n    The input function (solver) needs to return the solution to the problem of finding a MAP state for a constrained\n    exponential family distribution -- this is the case for most black-box combinatorial solvers [2]. If this condition\n    is violated though, the result would not hold and there is no guarantee on the validity of the obtained gradients.\n\n    This function can be used directly or as a decorator.\n\n    [1] Mathias Niepert, Pasquale Minervini, Luca Franceschi - Implicit MLE: Backpropagating Through Discrete\n    Exponential Family Distributions. NeurIPS 2021 (https://arxiv.org/abs/2106.01798)\n    [2] Marin Vlastelica, Anselm Paulus, Vít Musil, Georg Martius, Michal Rolínek - Differentiation of Blackbox\n    Combinatorial Solvers. ICLR 2020 (https://arxiv.org/abs/1912.02175)\n\n    Example::\n\n        >>> from imle.wrapper import imle\n        >>> from imle.target import TargetDistribution\n        >>> from imle.noise import SumOfGammaNoiseDistribution\n        >>> target_distribution = TargetDistribution(alpha=0.0, beta=10.0)\n        >>> noise_distribution = SumOfGammaNoiseDistribution(k=21, nb_iterations=100)\n        >>> @imle(target_distribution=target_distribution, noise_distribution=noise_distribution, nb_samples=100,\n        >>>       input_noise_temperature=input_noise_temperature, target_noise_temperature=5.0)\n        >>> def imle_solver(weights_batch: Tensor) -> Tensor:\n        >>>     return torch_solver(weights_batch)\n\n    Args:\n        function (Callable[[Tensor], Tensor]): black-box combinatorial solver\n        target_distribution (Optional[BaseTargetDistribution]): factory for target distributions\n        noise_distribution (Optional[BaseNoiseDistribution]): noise distribution\n        nb_samples (int): number of noise sammples\n        input_noise_temperature (float): noise temperature for the input distribution\n        target_noise_temperature (float): noise temperature for the target distribution\n    \"\"\"\n    if target_distribution is None:\n        target_distribution = TargetDistribution(alpha=1.0, beta=1.0)\n\n    if function is None:\n        return functools.partial(imle,\n                                 target_distribution=target_distribution,\n                                 noise_distribution=noise_distribution,\n                                 nb_samples=nb_samples,\n                                 input_noise_temperature=input_noise_temperature,\n                                 target_noise_temperature=target_noise_temperature)\n\n    @functools.wraps(function)\n    def wrapper(input: Tensor, *args):\n        class WrappedFunc(torch.autograd.Function):\n\n            @staticmethod\n            def forward(ctx, input: Tensor, *args):\n                # [BATCH_SIZE, ...]\n                input_shape = input.shape\n\n                batch_size = input_shape[0]\n                instance_shape = input_shape[1:]\n\n                # [BATCH_SIZE, N_SAMPLES, ...]\n                perturbed_input_shape = [batch_size, nb_samples] + list(instance_shape)\n\n                if noise_distribution is None:\n                    noise = torch.zeros(size=perturbed_input_shape)\n                else:\n                    noise = noise_distribution.sample(shape=torch.Size(perturbed_input_shape))\n\n                input_noise = noise * input_noise_temperature\n\n                # [BATCH_SIZE, N_SAMPLES, ...]\n                perturbed_input_3d = input.view(batch_size, 1, -1).repeat(1, nb_samples, 1).view(perturbed_input_shape)\n                perturbed_input_3d = perturbed_input_3d + input_noise\n\n                # [BATCH_SIZE * N_SAMPLES, ...]\n                perturbed_input_2d = perturbed_input_3d.view([-1] + perturbed_input_shape[2:])\n                perturbed_input_2d_shape = perturbed_input_2d.shape\n\n                # [BATCH_SIZE * N_SAMPLES, ...]\n                perturbed_output = function(perturbed_input_2d)\n                # [BATCH_SIZE, N_SAMPLES, ...]\n                perturbed_output = perturbed_output.view(perturbed_input_shape)\n\n                ctx.save_for_backward(input, noise, perturbed_output)\n\n                # [BATCH_SIZE * N_SAMPLES, ...]\n                res = perturbed_output.view(perturbed_input_2d_shape)\n                return res\n\n            @staticmethod\n            def backward(ctx, dy):\n                # input: [BATCH_SIZE, ...]\n                # noise: [BATCH_SIZE, N_SAMPLES, ...]\n                # perturbed_output_3d: # [BATCH_SIZE, N_SAMPLES, ...]\n                input, noise, perturbed_output_3d = ctx.saved_variables\n\n                input_shape = input.shape\n                batch_size = input_shape[0]\n\n                # dy is [BATCH_SIZE * N_SAMPLES, ...]\n                dy_shape = dy.shape\n                # noise is [BATCH_SIZE, N_SAMPLES, ...]\n                noise_shape = noise.shape\n\n                # [BATCH_SIZE * NB_SAMPLES, ...]\n                input_2d = input.view(batch_size, 1, -1).repeat(1, nb_samples, 1).view(dy_shape)\n                target_input_2d = target_distribution.params(input_2d, dy)\n\n                # [BATCH_SIZE, NB_SAMPLES, ...]\n                target_input_3d = target_input_2d.view(noise_shape)\n\n                # [BATCH_SIZE, NB_SAMPLES, ...]\n                target_noise = noise * target_noise_temperature\n\n                # [BATCH_SIZE, N_SAMPLES, ...]\n                perturbed_target_input_3d = target_input_3d + target_noise\n\n                # [BATCH_SIZE * N_SAMPLES, ...]\n                perturbed_target_input_2d = perturbed_target_input_3d.view(dy_shape)\n\n                # [BATCH_SIZE * N_SAMPLES, ...]\n                target_output_2d = function(perturbed_target_input_2d)\n\n                # [BATCH_SIZE, N_SAMPLES, ...]\n                target_output_3d = target_output_2d.view(noise_shape)\n\n                # [BATCH_SIZE, ...]\n                gradient = (perturbed_output_3d - target_output_3d)\n                gradient = gradient.mean(axis=1)\n                return gradient\n\n        return WrappedFunc.apply(input, *args)\n    return wrapper\n\n# ==========================================\n# File: solvers/dijkstra.py\n# Function/Context: ShortestPath\n# ==========================================\n# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport heapq\nimport torch\n\nfrom functools import partial\n\nfrom solvers.utils import get_neighbourhood_func, maybe_parallelize\nfrom collections import namedtuple\n\nDijkstraOutput = namedtuple(\"DijkstraOutput\",\n                            [\n                                \"shortest_path\",\n                                \"is_unique\",\n                                \"transitions\"\n                            ])\n\n\ndef dijkstra(matrix, neighbourhood_fn=\"8-grid\", request_transitions=False):\n    x_max, y_max = matrix.shape\n    neighbors_func = partial(get_neighbourhood_func(neighbourhood_fn), x_max=x_max, y_max=y_max)\n\n    costs = np.full_like(matrix, 1.0e10)\n    costs[0][0] = matrix[0][0]\n    num_path = np.zeros_like(matrix)\n    num_path[0][0] = 1\n    priority_queue = [(matrix[0][0], (0, 0))]\n    certain = set()\n    transitions = dict()\n\n    while priority_queue:\n        cur_cost, (cur_x, cur_y) = heapq.heappop(priority_queue)\n        if (cur_x, cur_y) in certain:\n            pass\n\n        for x, y in neighbors_func(cur_x, cur_y):\n            if (x, y) not in certain:\n                if matrix[x][y] + costs[cur_x][cur_y] < costs[x][y]:\n                    costs[x][y] = matrix[x][y] + costs[cur_x][cur_y]\n                    heapq.heappush(priority_queue, (costs[x][y], (x, y)))\n                    transitions[(x, y)] = (cur_x, cur_y)\n                    num_path[x, y] = num_path[cur_x, cur_y]\n                elif matrix[x][y] + costs[cur_x][cur_y] == costs[x][y]:\n                    num_path[x, y] += 1\n\n        certain.add((cur_x, cur_y))\n    # retrieve the path\n    cur_x, cur_y = x_max - 1, y_max - 1\n    on_path = np.zeros_like(matrix)\n    on_path[-1][-1] = 1\n    while (cur_x, cur_y) != (0, 0):\n        cur_x, cur_y = transitions[(cur_x, cur_y)]\n        on_path[cur_x, cur_y] = 1.0\n\n    is_unique = num_path[-1, -1] == 1\n\n    if request_transitions:\n        return DijkstraOutput(shortest_path=on_path, is_unique=is_unique, transitions=transitions)\n    else:\n        return DijkstraOutput(shortest_path=on_path, is_unique=is_unique, transitions=None)\n\n\ndef get_solver(neighbourhood_fn):\n    def solver(matrix):\n        return dijkstra(matrix, neighbourhood_fn).shortest_path\n\n    return solver\n\n\nclass ShortestPath(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, weights, lambda_val, neighbourhood_fn=\"8-grid\"):\n        ctx.lambda_val = lambda_val\n        ctx.neighbourhood_fn = neighbourhood_fn\n        ctx.solver = get_solver(neighbourhood_fn)\n\n        ctx.weights = weights.detach().cpu().numpy()\n        ctx.suggested_tours = np.asarray(maybe_parallelize(ctx.solver, arg_list=list(ctx.weights)))\n        return torch.from_numpy(ctx.suggested_tours).float().to(weights.device)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        assert grad_output.shape == ctx.suggested_tours.shape\n        grad_output_numpy = grad_output.detach().cpu().numpy()\n        weights_prime = np.maximum(ctx.weights + ctx.lambda_val * grad_output_numpy, 0.0)\n        better_paths = np.asarray(maybe_parallelize(ctx.solver, arg_list=list(weights_prime)))\n        gradient = -(ctx.suggested_tours - better_paths) / ctx.lambda_val\n        return torch.from_numpy(gradient).to(grad_output.device), None, None\n\n# ==========================================\n# File: sum-of-gamma-cli.py\n# Function/Context: sample_sum_of_gammas, sample_sum_of_gammas_torch, main\n# ==========================================\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport sys\n\nimport math\nimport numpy as np\nimport torch\n\nfrom torch import Tensor\nfrom torch.distributions.gamma import Gamma\n\nimport matplotlib.pyplot as plt\n\n\ndef sample_sum_of_gammas(nb_samples: int,\n                         k: np.ndarray,\n                         nb_iterations: int = 10):\n    \"\"\"These should be the epsilons from Th, 1, \"\"\"\n    k_size = k.shape[0]\n    samples = np.zeros((nb_samples, k_size), dtype='float')\n    for i in range(1, nb_iterations + 1):\n        print('XXX', 1. / k, k / i)\n        gs = np.random.gamma(1. / k, k / i, size=[nb_samples, k_size])\n        samples = samples + gs\n    samples = ((samples - math.log(nb_iterations)) / k)\n    return samples\n\n\ndef sample_sum_of_gammas_torch(batch_size: int,\n                               k: Tensor,\n                               nb_iterations: int = 10):\n    nb_samples = k.shape[0]\n    samples = torch.zeros((batch_size, nb_samples))\n    for i in range(1, nb_iterations + 1):\n        gamma = Gamma(1. / k, i / k)\n        samples = samples + gamma.sample(sample_shape=torch.Size([batch_size]))\n    samples = (samples - math.log(nb_iterations)) / k\n    return samples\n\n\ndef main(argv):\n    # samples = sample_sum_of_gammas(8192, k=np.ones(20) * 20, nb_iterations=100)\n    # print(samples)\n\n    # with torch.inference_mode():\n    #     samples = sample_sum_of_gammas_torch(1024, k=torch.ones(20) * 20, nb_iterations=100).cpu().numpy()\n\n    from imle.noise import SumOfGammaNoiseDistribution\n    distribution = SumOfGammaNoiseDistribution(k=20.0, nb_iterations=1000)\n    with torch.inference_mode():\n        samples = distribution.sample(shape=torch.Size([8192, 20])).cpu().numpy()\n\n    count, bins, ignored = plt.hist(np.sum(samples, axis=1), 32, density=True)\n\n    mu, beta = 0.0, 1.0\n    y = (1 / beta) * np.exp(-(bins - mu) / beta) * np.exp(-np.exp(-(bins - mu) / beta))\n\n    plt.plot(bins, y, linewidth=2, color='r')\n    plt.show()\n\n\nif __name__ == '__main__':\n    main(sys.argv[1:])",
  "description": "Combined Analysis:\n- [annotation-cli.py]: This file implements the core I-MLE algorithm for a shortest path optimization problem. The key components include: 1) A black-box Dijkstra solver (torch_solver) that solves the combinatorial optimization problem of finding minimal-cost paths. 2) The I-MLE wrapper (@imle decorator) that makes the solver differentiable by applying Perturb-and-MAP sampling with SumOfGammaNoiseDistribution and target distribution adjustments. 3) A training loop that uses I-MLE gradients to update weight parameters via backpropagation, minimizing Hamming loss between sampled and target paths. This directly corresponds to the paper's I-MLE framework: using perturb-and-MAP sampling (noise_distribution) and surrogate target distributions (TargetDistribution) to estimate gradients through discrete combinatorial solutions.\n- [imle/noise.py]: This file implements the Sum-of-Gamma noise distribution, which is a key component of the Perturb-and-MAP sampling mechanism in I-MLE. The SumOfGammaNoiseDistribution class generates structured noise samples that are added to model parameters (θ) before calling the MAP solver, enabling gradient estimation through discrete distributions. This corresponds to the first ingredient of I-MLE mentioned in the paper: a method to approximately sample from the induced distribution over solutions using tailored noise perturbations. The implementation follows the mathematical formulation from the NeurIPS 2021 paper, where the noise distribution is designed to work well with combinatorial constraints like k-subset selection.\n- [imle/target.py]: This file implements the target distribution construction, a core component of the I-MLE algorithm. The TargetDistribution class computes the parameters θ' = αθ - β∇_zL for the target distribution p_target(z) ∝ exp(⟨z, θ'⟩), which is used to compute the implicit MLE gradient. This directly corresponds to Equation 6 in the paper, where the target distribution is defined based on the downstream loss gradient to enable gradient estimation through discrete structures without explicit relaxations.\n- [imle/wrapper.py]: This file implements the core I-MLE gradient estimation algorithm as described in the NeurIPS 2021 paper. The 'imle' function wraps any black-box combinatorial solver to make it differentiable. It uses Perturb-and-MAP sampling (forward pass adds noise to inputs and calls solver) and constructs target distributions based on downstream gradients (backward pass computes gradient as difference between perturbed outputs and target outputs). The implementation matches the paper's algorithm: 1) Uses noise perturbations for sampling, 2) Constructs surrogate target distribution via target_distribution.params(), 3) Estimates gradients via difference of MAP solutions under shared perturbations. The code handles batched inputs and multiple noise samples, implementing the exact mathematical procedure from the paper.\n- [solvers/dijkstra.py]: This file implements the I-MLE gradient estimator for Dijkstra's shortest path algorithm. The core logic matches the paper's description: 1) It uses a combinatorial black-box solver (Dijkstra's algorithm) to find MAP solutions. 2) The ShortestPath class implements the perturb-and-MAP approach: forward pass computes the MAP solution (shortest path), backward pass perturbs weights using gradient information (weights_prime = weights + λ*grad_output) and computes gradient via difference between original and perturbed MAP solutions. This exactly follows I-MLE's gradient estimation scheme without requiring smooth relaxations. The implementation handles batched inputs via parallelization and provides differentiable shortest path computation.\n- [sum-of-gamma-cli.py]: This file implements the Sum-of-Gamma noise distribution, which is a key component of the Perturb-and-MAP sampling method used in I-MLE. The two functions sample_sum_of_gammas and sample_sum_of_gammas_torch generate noise samples following Theorem 1 from the paper, where the sum of gamma-distributed variables approximates Gumbel noise. This noise is essential for the perturb-and-MAP step, enabling gradient estimation through discrete combinatorial solvers without requiring explicit relaxations. The main function demonstrates usage by sampling from this distribution and comparing the histogram of sums to a Gumbel distribution, validating the theoretical properties.",
  "dependencies": [
    "ray",
    "abc.ABC",
    "multiprocessing",
    "sys",
    "matplotlib.pyplot",
    "seaborn",
    "solvers.utils.maybe_parallelize",
    "logging",
    "heapq",
    "imle.noise.BaseNoiseDistribution",
    "imle.noise",
    "solvers.utils.get_neighbourhood_func",
    "typing.Optional",
    "imle.noise.SumOfGammaNoiseDistribution",
    "imle.wrapper",
    "math",
    "imle.target.TargetDistribution",
    "imle.target.BaseTargetDistribution",
    "solvers.dijkstra",
    "matplotlib.animation",
    "functools",
    "functools.partial",
    "random",
    "numpy",
    "torch.distributions.gamma.Gamma",
    "collections.namedtuple",
    "torch",
    "abc.abstractmethod",
    "imle.target"
  ]
}