{
  "file_path": "blackbox_backprop/mAP.py, blackbox_backprop/perfect_matching.py, blackbox_backprop/ranking.py, blackbox_backprop/recall.py, blackbox_backprop/shortest_path.py, blackbox_backprop/travelling_salesman.py, solvers_to_visualize.py",
  "function_name": "MapLoss.raw_map_computation, MinCostPerfectMatchingSolver, TrueRanker, RecallLoss.forward, ShortestPath, TspSolver, ShortestPathBBSolver",
  "code_snippet": "\n\n# ==========================================\n# File: blackbox_backprop/mAP.py\n# Function/Context: MapLoss.raw_map_computation\n# ==========================================\nfrom collections import deque\n\nimport torch\n\nfrom blackbox_backprop.ranking import TrueRanker, rank_normalised\n\n\nclass MapLoss(torch.nn.Module):\n    \"\"\" Torch module for computing recall-based loss as in 'Blackbox differentiation of Ranking-based Metrics' \"\"\"\n    def __init__(self,\n                 lambda_val,\n                 margin,\n                 interclass_coef,\n                 batch_memory,\n                 ):\n        \"\"\"\n        :param lambda_val:  hyperparameter of black-box backprop\n        :param margin: margin to be enforced between positives and negatives (alpha in the paper)\n        :param interclass_coef: coefficient for interclass loss (beta in paper)\n        :param batch_memory: how many batches should be in memory\n        \"\"\"\n        super().__init__()\n        self.batch_memory = batch_memory\n        self.margin = margin\n        self.lambda_val = lambda_val\n        self.interclass_coef = interclass_coef\n\n        self.storage = deque()\n\n    def raw_map_computation(self, scores, targets):\n        \"\"\"\n                :param scores: [batch_size, num_classes] predicted relevance scores\n                :param targets: [batch_size, num_classes] ground truth relevances\n        \"\"\"\n        # Compute map\n        HIGH_CONSTANT = 2.0\n        epsilon = 1e-5\n        transposed_scores = scores.transpose(0, 1)\n        transposed_targets = targets.transpose(0, 1)\n        deviations = torch.abs(torch.randn_like(transposed_targets)) * (transposed_targets - 0.5)\n\n        transposed_scores = transposed_scores - self.margin * deviations\n        ranks_of_positive = TrueRanker.apply(transposed_scores, self.lambda_val)\n        scores_for_ranking_positives = -ranks_of_positive + HIGH_CONSTANT * transposed_targets\n        ranks_within_positive = rank_normalised(scores_for_ranking_positives)\n        ranks_within_positive.requires_grad = False\n        assert torch.all(ranks_within_positive * transposed_targets < ranks_of_positive * transposed_targets + epsilon)\n\n        sum_of_precisions_at_j_per_class = ((ranks_within_positive / ranks_of_positive) * transposed_targets).sum(dim=1)\n        precisions_per_class = sum_of_precisions_at_j_per_class / (transposed_targets.sum(dim=1) + epsilon)\n\n        present_class_mask = targets.sum(axis=0) != 0\n        return 1.0 - precisions_per_class[present_class_mask].mean()\n\n\n    def forward(self, output, target):\n\n        current_storage = list(self.storage)\n        long_output = torch.cat([output] + [x[0] for x in current_storage], dim=0)\n        long_target = torch.cat([target] + [x[1] for x in current_storage], dim=0)\n\n        assert long_output.shape[0] == long_target.shape[0]  # even in multi-gpu setups\n        cross_batch_loss = self.raw_map_computation(long_output, long_target)\n\n        output_flat = output.reshape((-1, 1))\n        target_flat = target.reshape((-1, 1))\n        interclass_loss = self.raw_map_computation(output_flat, target_flat)\n\n        while len(self.storage) >= self.batch_memory:\n            self.storage.popleft()\n\n        self.storage.append([output.detach(), target.detach()])\n\n        loss = (1.0 - self.interclass_coef) * cross_batch_loss + self.interclass_coef * interclass_loss\n        return loss\n\n# ==========================================\n# File: blackbox_backprop/perfect_matching.py\n# Function/Context: MinCostPerfectMatchingSolver\n# ==========================================\nfrom functools import partial\nimport itertools\n\nimport blossom_v\nimport numpy as np\nimport torch\n\nfrom .utils import maybe_parallelize\n\n\nclass MinCostPerfectMatchingSolver(torch.autograd.Function):\n    \"\"\"\n    Torch module implementing Blossom V (Kolmogorov, 2009) algorithm to find the min-cost perfect matching\n    between nodes on a graph given.\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, weights, lambda_val, num_vertices, edges):\n        \"\"\"\n        :param ctx: context for backpropagation\n        :param weights: torch.Tensor [batch_size, len(edges)] - suggested edge weights\n        :param lambda_val (float): hyperparameter lambda\n        :param num_vertices (int): number of vertices os the graph\n        :param edges (list of pairs): list of graph edges\n        :return solution torch.Tensor [batch_size, len(edges)] - indicator vectors of selected edges\n        \"\"\"\n        ctx.weights = weights.detach().cpu().numpy()\n        ctx.lambda_val = lambda_val\n        ctx.solver = partial(min_cost_perfect_matching, edges=edges, num_vertices=num_vertices)\n        ctx.perfect_matchings = np.array(maybe_parallelize(ctx.solver, list(ctx.weights)))\n\n        return torch.from_numpy(ctx.perfect_matchings).float().to(weights.device)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        assert grad_output.shape == ctx.perfect_matchings.shape\n        device = grad_output.device\n        grad_output = grad_output.cpu().numpy()\n\n        weights_prime = np.maximum(ctx.weights + ctx.lambda_val * grad_output, 0.0)\n        better_matchings = np.array(maybe_parallelize(ctx.solver, list(weights_prime)))\n\n        gradient = -(ctx.perfect_matchings - better_matchings) / ctx.lambda_val\n        return torch.from_numpy(gradient).to(device), None\n\n\ndef min_cost_perfect_matching(edges, edge_weights, num_vertices):\n    \"\"\"\n    Blossom V (Kolmogorov, 2009) algorithm to find the min-cost perfect matching between nodes on a graph\n    given as a matrix of edge weights.\n    :param edges (list of pairs): edges of the graph\n    :param edge_weights (np.ndarray shape: [len(edges)]): vector where the i-th element is the edge weight of edge i\n    :param num_vertices (int): total number of vertices in the graph\n    :return: solution (np.ndarray shape: [len(edges)]) as an indicator vector of selected edges\n    \"\"\"\n\n    edges = tuple(map(tuple, edges))  # Make hashable\n    pm = blossom_v.PerfectMatching(num_vertices, len(edges))\n\n    for (v1, v2), w in zip(edges, edge_weights):\n        pm.AddEdge(int(v1), int(v2), float(w))\n\n    pm.Solve()\n\n    edge_to_index_dict = dict(zip(edges, itertools.count()))\n    unique_matched_edges = [(v, pm.GetMatch(v)) for v in range(num_vertices) if v < pm.GetMatch(v)]\n    indices = [edge_to_index_dict[edge] for edge in unique_matched_edges]\n    solution = np.zeros(len(edges)).astype(np.float32)\n    solution[indices] = 1\n    return solution\n\n# ==========================================\n# File: blackbox_backprop/ranking.py\n# Function/Context: TrueRanker\n# ==========================================\nimport torch\n\n\ndef rank(seq):\n    return torch.argsort(torch.argsort(seq).flip(1))\n\n\ndef rank_normalised(seq):\n    return (rank(seq) + 1).float() / seq.size()[1]\n\n\nclass TrueRanker(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, sequence, lambda_val):\n        rank = rank_normalised(sequence)\n        ctx.lambda_val = lambda_val\n        ctx.save_for_backward(sequence, rank)\n        return rank\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        sequence, rank = ctx.saved_tensors\n        assert grad_output.shape == rank.shape\n        sequence_prime = sequence + ctx.lambda_val * grad_output\n        rank_prime = rank_normalised(sequence_prime)\n        gradient = -(rank - rank_prime) / (ctx.lambda_val + 1e-8)\n        return gradient, None\n\n# ==========================================\n# File: blackbox_backprop/recall.py\n# Function/Context: RecallLoss.forward\n# ==========================================\nfrom collections import deque\n\nimport torch\n\nfrom .ranking import TrueRanker, rank_normalised\n\n\nclass RecallLoss(torch.nn.Module):\n    def __init__(self, lambda_val, margin, weight_fn):\n        \"\"\"\n        Torch module for computing recall-based loss as in \"Blackbox differentiation of Ranking-based Metrics\"\n        :param lambda_val:  hyperparameter of black-box backprop\n        :param margin: margin to be enforced between positives and negatives (alpha in the paper)\n        :param weight_fn: callable torch.Tensor -> torch.Tensor (such as log(1+x) or log(1+log(1+x)) etc.)\n        \"\"\"\n        super().__init__()\n        self.sorter = TrueRanker()\n        self.margin = margin\n        self.lambda_val = lambda_val\n        self.weight_fn = weight_fn\n\n    def forward(self, score_sequences, gt_relevance_sequences):\n        \"\"\"\n        :param score_sequences: [num_sequences, len_of_sequence] scores of images (floats in [-1,1])\n        :param gt_relevance_sequences: [num_sequences, len_of_sequence] of booleans relevant/irrelevant\n        \"\"\"\n        HIGH_CONSTANT = 2.0  # This is actually high enough as normalised ranks live in [0,1].\n        TINY_CONSTANT = 1e-5\n        length = score_sequences.shape[0]\n        device = score_sequences.device\n\n        deviations = (gt_relevance_sequences - 0.5).to(device)\n        score_sequences = score_sequences - self.margin * deviations\n\n        ranks_among_all = TrueRanker.apply(score_sequences, self.lambda_val)\n        scores_among_positive = -ranks_among_all + HIGH_CONSTANT * gt_relevance_sequences\n        scores_among_positive = scores_among_positive.to(device)\n        ranks_among_positive = rank_normalised(scores_among_positive)\n        ranks_among_positive.require_grad = False\n\n        ranks_for_queries = (ranks_among_all - ranks_among_positive) * gt_relevance_sequences\n\n        assert torch.all(ranks_for_queries > -TINY_CONSTANT)\n\n        # denormalize ranks\n        ranks_for_queries = ranks_for_queries * length\n        recall = self.weight_fn(ranks_for_queries * gt_relevance_sequences).sum() / gt_relevance_sequences.sum()\n        return recall\n\n\nclass BatchMemoryRecallLoss(torch.nn.Module):\n    \"\"\"\n    A wrapper around a rank-based loss that allows batch memory.\n    \"\"\"\n    def __init__(self, batch_memory, **kwargs):\n        \"\"\"\n        :param batch_memory: How many batches should be in memory\n        :param kwargs: arguments of the underlying loss\n        \"\"\"\n        super().__init__()\n        self.batch_memory = batch_memory\n        self.loss = RecallLoss(**kwargs)\n\n        self.batch_storage = deque()\n        self.labels_storage = deque()\n\n    def reset(self):\n        self.batch_storage.clear()\n        self.labels_storage.clear()\n\n\n    def forward(self, score_sequences, gt_relevance_sequences):\n        if self.batch_memory > 0:\n            all_score_sequences = torch.cat((score_sequences,) + tuple(self.batch_storage), dim=0)\n            all_relevance_sequences = torch.cat((gt_relevance_sequences,) + tuple(self.labels_storage), dim=0)\n            result = self.loss(all_score_sequences, all_relevance_sequences)\n\n            if len(self.batch_storage) == self.batch_memory:\n                self.batch_storage.popleft()\n\n            self.batch_storage.append(score_sequences.detach())\n\n            if len(self.labels_storage) == self.batch_memory:\n                self.labels_storage.popleft()\n\n            self.labels_storage.append(gt_relevance_sequences.detach())\n        else:\n            result = self.loss(score_sequences, gt_relevance_sequences)\n        return result\n\n# ==========================================\n# File: blackbox_backprop/shortest_path.py\n# Function/Context: ShortestPath\n# ==========================================\nimport heapq\nimport itertools\nfrom functools import partial\n\nimport numpy as np\nimport torch\n\nfrom .utils import maybe_parallelize\n\n\ndef neighbours_fn(x, y, x_max, y_max):\n    \"\"\"\n    Returns all 8 neighbours of a given coordinate on a grid.\n    \"\"\"\n    deltas_x = (-1, 0, 1)\n    deltas_y = (-1, 0, 1)\n    for (dx, dy) in itertools.product(deltas_x, deltas_y):\n        x_new, y_new = x + dx, y + dy\n        if 0 <= x_new < x_max and 0 <= y_new < y_max and (dx, dy) != (0, 0):\n            yield x_new, y_new\n\n\ndef dijkstra(matrix):\n    \"\"\"\n    Implementation of Dijkstra algorithm to find the (s,t)-shortest path between top-left and bottom-right nodes\n    on a nxn grid graph (with 8-neighbourhood).\n    NOTE: This is an vertex variant of the problem, i.e. nodes carry weights, not edges.\n    :param matrix (np.ndarray [grid_dim, grid_dim]): Matrix of node-costs.\n    :return: matrix (np.ndarray [grid_dim, grid_dim]), indicator matrix of nodes on the shortest path.\n    \"\"\"\n\n    x_max, y_max = matrix.shape\n    neighbors_func = partial(neighbours_fn, x_max=x_max, y_max=y_max)\n\n    costs = np.full_like(matrix, 1.0e10)\n    costs[0][0] = matrix[0][0]\n    num_path = np.zeros_like(matrix)\n    num_path[0][0] = 1\n    priority_queue = [(matrix[0][0], (0, 0))]\n    certain = set()\n    transitions = dict()\n\n    while priority_queue:\n        cur_cost, (cur_x, cur_y) = heapq.heappop(priority_queue)\n        if (cur_x, cur_y) in certain:\n            pass\n\n        for x, y in neighbors_func(cur_x, cur_y):\n            if (x, y) not in certain:\n                if matrix[x][y] + costs[cur_x][cur_y] < costs[x][y]:\n                    costs[x][y] = matrix[x][y] + costs[cur_x][cur_y]\n                    heapq.heappush(priority_queue, (costs[x][y], (x, y)))\n                    transitions[(x, y)] = (cur_x, cur_y)\n                    num_path[x, y] = num_path[cur_x, cur_y]\n                elif matrix[x][y] + costs[cur_x][cur_y] == costs[x][y]:\n                    num_path[x, y] += 1\n\n        certain.add((cur_x, cur_y))\n    # retrieve the path\n    cur_x, cur_y = x_max - 1, y_max - 1\n    on_path = np.zeros_like(matrix)\n    on_path[-1][-1] = 1\n    while (cur_x, cur_y) != (0, 0):\n        cur_x, cur_y = transitions[(cur_x, cur_y)]\n        on_path[cur_x, cur_y] = 1.0\n    return on_path\n\n\nclass ShortestPath(torch.autograd.Function):\n    \"\"\"\n    torch module calculating the solution of the shortest path problem from top-left to bottom-right\n    on a given grid graph.\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, weights, lambda_val):\n        \"\"\"\n        :param ctx: context for backpropagation\n        :param weights (torch.Tensor of shape [batch_size, grid_dim, grid_dim]): vertex weights\n        :param lambda_val: hyperparameter lambda\n        :return: shortest paths (torch.Tensor of shape [batch_size, grid_dim, grid_dim]): indicator matrices\n        of taken paths\n        \"\"\"\n        ctx.weights = weights.detach().cpu().numpy()\n        ctx.lambda_val = lambda_val\n        ctx.suggested_tours = np.asarray(maybe_parallelize(dijkstra, arg_list=list(ctx.weights)))\n        return torch.from_numpy(ctx.suggested_tours).float().to(weights.device)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        assert grad_output.shape == ctx.suggested_tours.shape\n        grad_output_numpy = grad_output.detach().cpu().numpy()\n        weights_prime = np.maximum(ctx.weights + ctx.lambda_val * grad_output_numpy, 0.0)\n        better_paths = np.asarray(maybe_parallelize(dijkstra, arg_list=list(weights_prime)))\n        gradient = -(ctx.suggested_tours - better_paths) / ctx.lambda_val\n        return torch.from_numpy(gradient).to(grad_output.device), None\n\n# ==========================================\n# File: blackbox_backprop/travelling_salesman.py\n# Function/Context: TspSolver\n# ==========================================\nimport numpy as np\nimport torch\n\ntry:\n    from gurobipy import GRB, Model, quicksum\nexcept ImportError:\n    print(\"GurobiPy missing, TSP module not available\")\n\nfrom .utils import maybe_parallelize\n\n\ndef subtour(n, edges):\n    \"\"\"\n    Given a list of edges, finds the shortest subtour.\n    \"\"\"\n    visited = [False] * n\n    cycles = []\n    lengths = []\n    selected = [[] for _ in range(n)]\n    for x, y in edges:\n        selected[x].append(y)\n    while True:\n        current = visited.index(False)\n        thiscycle = [current]\n        while True:\n            visited[current] = True\n            neighbors = [x for x in selected[current] if not visited[x]]\n            if len(neighbors) == 0:\n                break\n            current = neighbors[0]\n            thiscycle.append(current)\n        cycles.append(thiscycle)\n        lengths.append(len(thiscycle))\n        if sum(lengths) == n:\n            break\n    return cycles[lengths.index(min(lengths))]\n\n\ndef subtourelim(n, model, where):\n    \"\"\"\n    Callback - use lazy constraints to eliminate sub-tours.\n    \"\"\"\n    if where == GRB.callback.MIPSOL:\n        selected = []\n\n        # make a list of edges selected in the solution\n        for i in range(n):\n            sol = model.cbGetSolution([model._vars[i, j] for j in range(n)])\n            selected += [(i, j) for j in range(n) if sol[j] > 0.5]\n\n        # find the shortest cycle in the selected edge list\n        tour = subtour(n, selected)\n        if len(tour) < n:\n            # add a subtour elimination constraint\n            expr = 0\n            for i in range(len(tour)):\n                for j in range(i + 1, len(tour)):\n                    expr += model._vars[tour[i], tour[j]]\n            model.cbLazy(expr <= len(tour) - 1)\n\n\ndef gurobi_tsp(distance_matrix):\n    \"\"\"\n    Solves tsp problem.\n    :param distance_matrix: symmetric matrix of distances, where the i,j element is the distance between object i and j\n    :return: matrix containing {0, 1}, 1 for each transition that is included in the tsp solution\n    \"\"\"\n    n = len(distance_matrix)\n    m = Model()\n    m.setParam(\"OutputFlag\", False)\n    m.setParam(\"Threads\", 1)\n\n    # Create variables\n    vars = {}\n    for i in range(n):\n        for j in range(i + 1):\n            vars[i, j] = m.addVar(\n                obj=0.0 if i == j else distance_matrix[i][j], vtype=GRB.BINARY, name=\"e\" + str(i) + \"_\" + str(j)\n            )\n            vars[j, i] = vars[i, j]\n        m.update()\n\n    # Add degree-2 constraint, and forbid loops\n    for i in range(n):\n        m.addConstr(quicksum(vars[i, j] for j in range(n)) == 2)\n        vars[i, i].ub = 0\n    m.update()\n\n    # Optimize model\n    m._vars = vars\n    m.params.LazyConstraints = 1\n\n    def subtour_fn(model, where):\n        return subtourelim(n, model, where)\n\n    m.optimize(subtour_fn)\n    solution = m.getAttr(\"x\", vars)\n    selected = [(i, j) for i in range(n) for j in range(n) if solution[i, j] > 0.5]\n    result = np.zeros_like(distance_matrix)\n    for (i, j) in selected:\n        result[i][j] = 1\n\n    return result\n\n\nclass TspSolver(torch.autograd.Function):\n    \"\"\"\n    Torch module calculating the solution of the travelling salesman problem on a given distance matrix\n    using a Gurobi implementation of a cutting plane algorithm.\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, distance_matrices, lambda_val):\n        \"\"\"\n        distance_matrices: torch.Tensor of shape [batch_size, num_vertices, num_vertices]\n        return: torch.Tenspr of shape [batch_size, num_vertices, num_vertices] 0-1 indicator matrices of the solution\n        \"\"\"\n        ctx.distance_matrices = distance_matrices.detach().cpu().numpy()\n        ctx.lambda_val = lambda_val\n        suggested_tours = np.asarray(maybe_parallelize(gurobi_tsp, arg_list=list(ctx.distance_matrices)))\n        return torch.from_numpy(suggested_tours).float().to(distance_matrices.device)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        assert grad_output.shape == ctx.suggested_tours.shape\n        grad_output_numpy = grad_output.detach().cpu().numpy()\n        distances_prime = ctx.distance_matrices + ctx.lambda_val * grad_output_numpy\n        better_tours = np.array(maybe_parallelize(gurobi_tsp, arg_list=list(distances_prime)))\n        gradient = -(ctx.suggested_tours - better_tours) / ctx.lambda_val\n        return torch.from_numpy(gradient.astype(np.float32)).to(grad_output.device), None\n\n# ==========================================\n# File: solvers_to_visualize.py\n# Function/Context: ShortestPathBBSolver\n# ==========================================\nfrom visualization_utils import BlackboxSolverAbstract, gen_w_and_y_grad, gen_edges\n\ntry:\n    from lpmp_py import gm_solver\n    from lpmp_py import mgm_solver\nexcept ImportError:\n    print(\"lpmp_py missing. Install it separately for using (multi)graph matching solvers\")\n\nfrom blackbox_backprop.travelling_salesman import gurobi_tsp\nfrom blackbox_backprop.shortest_path import dijkstra\n\nimport numpy as np\nimport itertools as it\n\nclass ShortestPathBBSolver(BlackboxSolverAbstract):\n    \"\"\"\n    Shortest path solver.\n    \"\"\"\n\n    @staticmethod\n    def solver(inputs):\n        matrix = inputs[0]\n        m = dijkstra(matrix)\n        return [m]\n\n    @staticmethod\n    def gen_input(num_nodes, seed, w_normal_factor=1, w_normal_addend=0, w_shift_factor=1, w_shift_addend=0, y_factor=1, \n                  y_addend=0):\n        w_slice_l, y_grad_l = gen_w_and_y_grad(\n            seed=seed,\n            params=[dict(shape=(num_nodes, num_nodes),\n                         w_slice_par=dict(mode='slice_random', normal_factor=w_normal_factor, normal_addend=w_normal_addend, \n                                          shift_factor=w_shift_factor, shift_addend=w_shift_addend, pos=True),\n                         y_grad_par=dict(mode='hamming_random', factor=y_factor, addend=y_addend))])\n\n        solver_config = {}\n        return w_slice_l, y_grad_l, solver_config",
  "description": "Combined Analysis:\n- [blackbox_backprop/mAP.py]: This file implements the blackbox backpropagation method for ranking-based metrics (specifically mAP loss) as described in the paper. The core logic is in the raw_map_computation method, which uses TrueRanker.apply() - a blackbox combinatorial solver for ranking that follows the paper's differentiation technique. The solver is called with transposed scores and lambda_val (hyperparameter), and the backward pass uses the perturbed problem approach. The implementation matches the paper's extension to ranking problems, where the combinatorial solver optimizes a linear objective over permutations.\n- [blackbox_backprop/perfect_matching.py]: This file implements the core algorithm from the paper for min-cost perfect matching. The MinCostPerfectMatchingSolver class is a PyTorch autograd Function that wraps the Blossom V algorithm. It implements the exact gradient computation method described in the paper: forward pass solves the optimization problem with original weights, backward pass solves a perturbed problem with weights adjusted by λ * dL/dy to compute informative gradients. The optimization model matches the paper's formulation: minimize Σ w_e * x_e subject to perfect matching constraints (x_e ∈ {0,1}, each vertex incident to exactly one edge).\n- [blackbox_backprop/ranking.py]: This file implements the core blackbox differentiation algorithm from the paper for the ranking problem. The TrueRanker class is a PyTorch autograd Function that wraps the combinatorial ranking solver. The forward pass computes normalized ranks via argsort operations. The backward pass implements the paper's gradient estimation method: it perturbs the input sequence by adding λ * grad_output, recomputes ranks on the perturbed input, and returns the gradient as -(rank - rank_prime)/λ. This exactly matches the paper's algorithm where the blackbox solver (ranking) is called twice: once in forward pass and once in backward pass with perturbed weights.\n- [blackbox_backprop/recall.py]: This file implements the core blackbox backpropagation algorithm for ranking-based combinatorial optimization. The RecallLoss class directly uses TrueRanker.apply() - which implements the paper's key algorithm: during forward pass it solves the ranking problem (combinatorial solver), and during backward pass it computes gradients using the perturbed weights method (w' = w + λ * dL/dy). The lambda_val parameter controls the perturbation strength, matching the paper's hyperparameter λ. The ranking problem is formulated as optimizing a linear objective over permutation matrices, making it a direct instance of the paper's framework.\n- [blackbox_backprop/shortest_path.py]: This file implements the core algorithm from the paper for the shortest path problem. The ShortestPath class is a PyTorch autograd Function that wraps Dijkstra's algorithm as a blackbox solver. In the forward pass, it computes the optimal path (indicator matrix) given vertex weights. In the backward pass, it applies the paper's gradient estimation method: it perturbs the weights by adding λ * grad_output, re-solves the problem, and computes the gradient as -(original_solution - perturbed_solution)/λ. This matches the paper's proposed method for differentiating through blackbox combinatorial solvers.\n- [blackbox_backprop/travelling_salesman.py]: This file implements the core algorithm from the paper for the Traveling Salesman Problem (TSP). The TspSolver class is a PyTorch autograd Function that wraps a blackbox Gurobi TSP solver. In the forward pass, it solves the TSP with the given distance matrix (objective weights). In the backward pass, it computes gradients by solving a perturbed TSP with adjusted weights: w' = w + λ * dL/dy, where λ is a hyperparameter. This matches the paper's proposed method for differentiating through blackbox combinatorial solvers. The optimization model is TSP-specific: minimize sum of edge distances with binary variables, degree-2 constraints, and subtour elimination constraints (implemented via Gurobi callbacks). The code directly implements the key algorithm step of solving a perturbed problem during backpropagation to obtain informative gradients.\n- [solvers_to_visualize.py]: This file implements the core forward pass of blackbox combinatorial solvers as described in the paper. The ShortestPathBBSolver class specifically implements the shortest path problem with linear objective minimization over binary edge selections. The solver() method calls dijkstra() which solves the optimization problem: min Σ w_e x_e subject to x_e ∈ {0,1} and path constraints. The gen_input() method generates weight matrices w (edge costs) and ground truth gradients for the backward pass. This matches the paper's framework where neural networks predict edge weights w, the blackbox solver computes optimal solution y(w), and during backpropagation, perturbed weights w' = w + λ dL/dy are used.",
  "dependencies": [
    "gurobipy",
    "gurobi_tsp",
    "heapq",
    ".ranking.TrueRanker",
    "collections.deque",
    ".ranking.rank_normalised",
    "blackbox_backprop.shortest_path.dijkstra",
    "subtourelim",
    "blackbox_backprop.ranking.TrueRanker",
    "visualization_utils.gen_w_and_y_grad",
    "blossom_v",
    "neighbours_fn",
    ".utils.maybe_parallelize",
    "functools.partial",
    "dijkstra",
    "numpy",
    "subtour",
    "visualization_utils.BlackboxSolverAbstract",
    "visualization_utils.gen_edges",
    "blackbox_backprop.ranking.rank_normalised",
    "torch",
    "blackbox_backprop.utils.maybe_parallelize",
    "itertools"
  ]
}