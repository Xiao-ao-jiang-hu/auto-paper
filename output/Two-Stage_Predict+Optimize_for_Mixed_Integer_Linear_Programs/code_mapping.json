{
  "file_path": "code/0-1 knapsack/ip_model_whole.py, code/0-1 knapsack/test.py, code/0-1 knapsack/train.py, code/Alloy production/ip_model_whole.py, code/Alloy production/test.py, code/Alloy production/train.py, code/NSP/ip_model_whole.py, code/NSP/test.py, code/NSP/train.py",
  "function_name": "ip_model_whole, correction_single_obj, Intopt.fit, This file contains multiple helper functions but appears to be part of a larger optimization framework. The main logic is not contained in a single function but distributed across the file., correction_single_obj, Intopt.fit, correction_single_obj, Intopt",
  "code_snippet": "\n\n# ==========================================\n# File: code/0-1 knapsack/ip_model_whole.py\n# Function/Context: ip_model_whole\n# ==========================================\nimport numpy as np\nimport scipy as sp\nimport time\nimport scipy.sparse as sps\nimport numbers\nfrom warnings import warn\nfrom scipy.linalg import LinAlgError\nimport torch\nfrom torch import nn, optim\nfrom torch.autograd import Variable,Function\nimport sys\nimport logging\nimport time, datetime\nfrom scipy.optimize import OptimizeWarning\nfrom remove_redundancy import _remove_redundancy, _remove_redundancy_sparse, _remove_redundancy_dense\nnp.set_printoptions(threshold=np.inf)\nimport gurobipy as gp\nfrom gurobipy import GRB\n\n#purchase_fee = 0.2\n#compensation_fee = 0.25\nvarNum = 10\nx_s1 = torch.zeros(varNum,dtype=torch.float64)\n\ndef _format_A_constraints(A, n_x, sparse_lhs=False):\n    \"\"\"Format the left hand side of the constraints to a 2D array\n\n    Parameters\n    ----------\n    A : 2D array\n        2D array such that ``A @ x`` gives the values of the upper-bound\n        (in)equality constraints at ``x``.\n    n_x : int\n        The number of variables in the linear programming problem.\n    sparse_lhs : bool\n        Whether either of `A_ub` or `A_eq` are sparse. If true return a\n        coo_matrix instead of a numpy array.\n\n    Returns\n    -------\n    np.ndarray or sparse.coo_matrix\n        2D array such that ``A @ x`` gives the values of the upper-bound\n        (in)equality constraints at ``x``.\n\n    \"\"\"\n    if sparse_lhs:\n        return sps.coo_matrix(\n            (0, n_x) if A is None else A, dtype=np.float, copy=True\n        )\n    elif A is None:\n        return np.zeros((0, n_x), dtype=np.float)\n    else:\n        return np.array(A, dtype=np.float, copy=True)\n\ndef _format_b_constraints(b):\n    \"\"\"Format the upper bounds of the constraints to a 1D array\n\n    Parameters\n    ----------\n    b : 1D array\n        1D array of values representing the upper-bound of each (in)equality\n        constraint (row) in ``A``.\n\n    Returns\n    -------\n    1D np.array\n        1D array of values representing the upper-bound of each (in)equality\n        constraint (row) in ``A``.\n\n    \"\"\"\n    if b is None:\n        return np.array([], dtype=np.float)\n    b = np.array(b, dtype=np.float, copy=True).squeeze()\n    return b if b.size != 1 else b.reshape((-1))\n\ndef _clean_inputs(c, A_ub=None, b_ub=None, A_eq=None, b_eq=None, bounds=None,\n                  x0=None):\n    \"\"\"\n    Given user inputs for a linear programming problem, return the\n    objective vector, upper bound constraints, equality constraints,\n    and simple bounds in a preferred format.\n\n    Parameters\n    ----------\n    c : 1D array\n        Coefficients of the linear objective function to be minimized.\n    A_ub : 2D array, optional\n        2D array such that ``A_ub @ x`` gives the values of the upper-bound\n        inequality constraints at ``x``.\n    b_ub : 1D array, optional\n        1D array of values representing the upper-bound of each inequality\n        constraint (row) in ``A_ub``.\n    A_eq : 2D array, optional\n        2D array such that ``A_eq @ x`` gives the values of the equality\n        constraints at ``x``.\n    b_eq : 1D array, optional\n        1D array of values representing the RHS of each equality constraint\n        (row) in ``A_eq``.\n    bounds : sequence, optional\n        ``(min, max)`` pairs for each element in ``x``, defining\n        the bounds on that parameter. Use None for one of ``min`` or\n        ``max`` when there is no bound in that direction. By default\n        bounds are ``(0, None)`` (non-negative).\n        If a sequence containing a single tuple is provided, then ``min`` and\n        ``max`` will be applied to all variables in the problem.\n    x0 : 1D array, optional\n        Starting values of the independent variables, which will be refined by\n        the optimization algorithm.\n\n    Returns\n    -------\n    c : 1D array\n        Coefficients of the linear objective function to be minimized.\n    A_ub : 2D array, optional\n        2D array such that ``A_ub @ x`` gives the values of the upper-bound\n        inequality constraints at ``x``.\n    b_ub : 1D array, optional\n        1D array of values representing the upper-bound of each inequality\n        constraint (row) in ``A_ub``.\n    A_eq : 2D array, optional\n        2D array such that ``A_eq @ x`` gives the values of the equality\n        constraints at ``x``.\n    b_eq : 1D array, optional\n        1D array of values representing the RHS of each equality constraint\n        (row) in ``A_eq``.\n    bounds : sequence of tuples\n        ``(min, max)`` pairs for each element in ``x``, defining\n        the bounds on that parameter. Use None for each of ``min`` or\n        ``max`` when there is no bound in that direction. By default\n        bounds are ``(0, None)`` (non-negative).\n    x0 : 1D array, optional\n        Starting values of the independent variables, which will be refined by\n        the optimization algorithm.\n    \"\"\"\n    if c is None:\n        raise TypeError\n\n    try:\n        c = np.array(c, dtype=np.float, copy=True).squeeze()\n    except ValueError:\n        raise TypeError(\n            \"Invalid input for linprog: c must be a 1D array of numerical \"\n            \"coefficients\")\n    else:\n        # If c is a single value, convert it to a 1D array.\n        if c.size == 1:\n            c = c.reshape((-1))\n\n        n_x = len(c)\n        if n_x == 0 or len(c.shape) != 1:\n            raise ValueError(\n                \"Invalid input for linprog: c must be a 1D array and must \"\n                \"not have more than one non-singleton dimension\")\n        if not(np.isfinite(c).all()):\n            raise ValueError(\n                \"Invalid input for linprog: c must not contain values \"\n                \"inf, nan, or None\")\n\n    sparse_lhs = sps.issparse(A_eq) or sps.issparse(A_ub)\n    try:\n        A_ub = _format_A_constraints(A_ub, n_x, sparse_lhs=sparse_lhs)\n    except ValueError:\n        raise TypeError(\n            \"Invalid input for linprog: A_ub must be a 2D array \"\n            \"of numerical values\")\n    else:\n        n_ub = A_ub.shape[0]\n        if len(A_ub.shape) != 2 or A_ub.shape[1] != n_x:\n            raise ValueError(\n                \"Invalid input for linprog: A_ub must have exactly two \"\n                \"dimensions, and the number of columns in A_ub must be \"\n                \"equal to the size of c\")\n        if (sps.issparse(A_ub) and not np.isfinite(A_ub.data).all()\n                or not sps.issparse(A_ub) and not np.isfinite(A_ub).all()):\n            raise ValueError(\n                \"Invalid input for linprog: A_ub must not contain values \"\n                \"inf, nan, or None\")\n\n    try:\n        b_ub = _format_b_constraints(b_ub)\n    except ValueError:\n        raise TypeError(\n            \"Invalid input for linprog: b_ub must be a 1D array of \"\n            \"numerical values, each representing the upper bound of an \"\n            \"inequality constraint (row) in A_ub\")\n    else:\n        if b_ub.shape != (n_ub,):\n            raise ValueError(\n                \"Invalid input for linprog: b_ub must be a 1D array; b_ub \"\n                \"must not have more than one non-singleton dimension and \"\n                \"the number of rows in A_ub must equal the number of values \"\n                \"in b_ub\")\n        if not(np.isfinite(b_ub).all()):\n            raise ValueError(\n                \"Invalid input for linprog: b_ub must not contain values \"\n                \"inf, nan, or None\")\n\n    try:\n        A_eq = _format_A_constraints(A_eq, n_x, sparse_lhs=sparse_lhs)\n    except ValueError:\n        raise TypeError(\n            \"Invalid input for linprog: A_eq must be a 2D array \"\n            \"of numerical values\")\n    else:\n        n_eq = A_eq.shape[0]\n        if len(A_eq.shape) != 2 or A_eq.shape[1] != n_x:\n            raise ValueError(\n                \"Invalid input for linprog: A_eq must have exactly two \"\n                \"dimensions, and the number of columns in A_eq must be \"\n                \"equal to the size of c\")\n\n        if (sps.issparse(A_eq) and not np.isfinite(A_eq.data).all()\n                or not sps.issparse(A_eq) and not np.isfinite(A_eq).all()):\n            raise ValueError(\n                \"Invalid input for linprog: A_eq must not contain values \"\n                \"inf, nan, or None\")\n    try:\n        b_eq = _format_b_constraints(b_eq)\n\n    except ValueError:\n        raise TypeError(\n            \"Invalid input for linprog: b_eq must be a 1D array of \"\n            \"numerical values, each representing the upper bound of an \"\n            \"inequality constraint (row) in A_eq\")\n    else:\n        if b_eq.shape != (n_eq,):\n            raise ValueError(\n                \"Invalid input for linprog: b_eq must be a 1D array; b_eq \"\n                \"must not have more than one non-singleton dimension and \"\n                \"the number of rows in A_eq must equal the number of values \"\n                \"in b_eq\")\n        if not(np.isfinite(b_eq).all()):\n            raise ValueError(\n                \"Invalid input for linprog: b_eq must not contain values \"\n                \"inf, nan, or None\")\n\n    # x0 gives a (optional) starting solution to the solver. If x0 is None,\n    # skip the checks. Initial solution will be generated automatically.\n    if x0 is not None:\n        try:\n            x0 = np.array(x0, dtype=float, copy=True).squeeze()\n        except ValueError:\n            raise TypeError(\n                \"Invalid input for linprog: x0 must be a 1D array of \"\n                \"numerical coefficients\")\n        if x0.ndim == 0:\n            x0 = x0.reshape((-1))\n        if len(x0) == 0 or x0.ndim != 1:\n            raise ValueError(\n                \"Invalid input for linprog: x0 should be a 1D array; it \"\n                \"must not have more than one non-singleton dimension\")\n        if not x0.size == c.size:\n            raise ValueError(\n                \"Invalid input for linprog: x0 and c should contain the \"\n                \"same number of elements\")\n        if not np.isfinite(x0).all():\n            raise ValueError(\n            \"Invalid input for linprog: x0 must not contain values \"\n            \"inf, nan, or None\")\n\n    # \"If a sequence containing a single tuple is provided, then min and max\n    # will be applied to all variables in the problem.\"\n    # linprog doesn't treat this right: it didn't accept a list with one tuple\n    # in it\n    try:\n        if isinstance(bounds, str):\n            raise TypeError\n        if bounds is None or len(bounds) == 0:\n            bounds = [(0, None)] * n_x\n        elif len(bounds) == 1:\n            b = bounds[0]\n            if len(b) != 2:\n                raise ValueError(\n                    \"Invalid input for linprog: exactly one lower bound and \"\n                    \"one upper bound must be specified for each element of x\")\n            bounds = [b] * n_x\n        elif len(bounds) == n_x:\n            try:\n                len(bounds[0])\n            except BaseException:\n                bounds = [(bounds[0], bounds[1])] * n_x\n            for i, b in enumerate(bounds):\n                if len(b) != 2:\n                    raise ValueError(\n                        \"Invalid input for linprog, bound \" +\n                        str(i) +\n                        \" \" +\n                        str(b) +\n                        \": exactly one lower bound and one upper bound must \"\n                        \"be specified for each element of x\")\n        elif (len(bounds) == 2 and np.isreal(bounds[0])\n                and np.isreal(bounds[1])):\n            bounds = [(bounds[0], bounds[1])] * n_x\n        else:\n            raise ValueError(\n                \"Invalid input for linprog: exactly one lower bound and one \"\n                \"upper bound must be specified for each element of x\")\n\n        clean_bounds = []  # also creates a copy so user's object isn't changed\n        for i, b in enumerate(bounds):\n            if b[0] is not None and b[1] is not None and b[0] > b[1]:\n                raise ValueError(\n                    \"Invalid input for linprog, bound \" +\n                    str(i) +\n                    \" \" +\n                    str(b) +\n                    \": a lower bound must be less than or equal to the \"\n                    \"corresponding upper bound\")\n            if b[0] == np.inf:\n                raise ValueError(\n                    \"Invalid input for linprog, bound \" +\n                    str(i) +\n                    \" \" +\n                    str(b) +\n                    \": infinity is not a valid lower bound\")\n            if b[1] == -np.inf:\n                raise ValueError(\n                    \"Invalid input for linprog, bound \" +\n                    str(i) +\n                    \" \" +\n                    str(b) +\n                    \": negative infinity is not a valid upper bound\")\n            lb = float(b[0]) if b[0] is not None and b[0] != -np.inf else None\n            ub = float(b[1]) if b[1] is not None and b[1] != np.inf else None\n            clean_bounds.append((lb, ub))\n        bounds = clean_bounds\n    except ValueError as e:\n        if \"could not convert string to float\" in e.args[0]:\n            raise TypeError\n        else:\n            raise e\n    except TypeError as e:\n        print(e)\n        raise TypeError(\n            \"Invalid input for linprog: bounds must be a sequence of \"\n            \"(min,max) pairs, each defining bounds on an element of x \")\n\n    return c, A_ub, b_ub, A_eq, b_eq, bounds, x0\n\n\ndef _get_Abc(c,A_ub=None, b_ub=None, A_eq=None, b_eq=None, bounds=None,\n             x0=None, undo=[],c0=0):\n    \"\"\"\n    Given a linear programming problem of the form:\n    Minimize::\n        c @ x\n    Subject to::\n        A_ub @ x <= b_ub\n        A_eq @ x == b_eq\n         lb <= x <= ub\n    where ``lb = 0`` and ``ub = None`` unless set in ``bounds``.\n    Return the problem in standard form:\n    Minimize::\n        c @ x\n    Subject to::\n        A @ x == b\n            x >= 0\n    by adding slack variables and making variable substitutions as necessary.\n    Parameters\n    ----------\n    c : 1D array\n        Coefficients of the linear objective function to be minimized.\n        Components corresponding with fixed variables have been eliminated.\n    c0 : float\n        Constant term in objective function due to fixed (and eliminated)\n        variables.\n    A_ub : 2D array, optional\n        2D array such that ``A_ub @ x`` gives the values of the upper-bound\n        inequality constraints at ``x``.\n    b_ub : 1D array, optional\n        1D array of values representing the upper-bound of each inequality\n        constraint (row) in ``A_ub``.\n    A_eq : 2D array, optional\n        2D array such that ``A_eq @ x`` gives the values of the equality\n        constraints at ``x``.\n    b_eq : 1D array, optional\n        1D array of values representing the RHS of each equality constraint\n        (row) in ``A_eq``.\n    bounds : sequence of tuples\n        ``(min, max)`` pairs for each element in ``x``, defining\n        the bounds on that parameter. Use None for each of ``min`` or\n        ``max\"\"\"\n\n# ==========================================\n# File: code/0-1 knapsack/test.py\n# Function/Context: correction_single_obj\n# ==========================================\nimport sys\nfrom collections import defaultdict\nimport numpy as np\nimport gurobipy as gp\nfrom gurobipy import GRB\nfrom sklearn.metrics import mean_squared_error\nfrom numpy import inf\n\npurchase_fee = 0.2\ncompensation_fee = 0.21\ncapacity_list = [100, 150, 200, 250]\n\ndef actual_obj(valueTemp, cap, weightTemp, n_instance):\n    obj_list = []\n    selectedNum_list = []\n    for num in range(n_instance):\n        weight = np.zeros(itemNum)\n        value = np.zeros(itemNum)\n        cnt = num * itemNum\n        for i in range(itemNum):\n            weight[i] = weightTemp[cnt]\n            value[i] = valueTemp[cnt]\n            cnt = cnt + 1\n        weight = weight.tolist()\n        value = value.tolist()\n        \n        m = gp.Model()\n        m.setParam('OutputFlag', 0)\n        x = m.addVars(itemNum, vtype=GRB.BINARY, name='x')\n        m.setObjective(purchase_fee * x.prod(value), GRB.MAXIMIZE)\n        m.addConstr((x.prod(weight)) <= cap)\n\n        m.optimize()\n        sol = []\n        selectedItemNum = 0\n        for i in range(itemNum):\n            sol.append(x[i].x)\n            if x[i].x == 1:\n              selectedItemNum = selectedItemNum + 1\n        objective = m.objVal\n        obj_list.append(objective)\n        selectedNum_list.append(selectedItemNum)\n        \n    return np.array(obj_list), np.array(selectedNum_list)\n\n\ndef correction_single_obj(realPrice, predPrice, cap, realWeightTemp, predWeightTemp):\n    realWeight = np.zeros(itemNum)\n    predWeight = np.zeros(itemNum)\n    realPriceNumpy = np.zeros(itemNum)\n    for i in range(itemNum):\n        realWeight[i] = realWeightTemp[i]\n        predWeight[i] = predWeightTemp[i]\n        realPriceNumpy[i] = realPrice[i]\n        \n    if min(predWeight) >= 0:\n        predWeight = predWeight.tolist()\n        m = gp.Model()\n        m.setParam('OutputFlag', 0)\n        x = m.addVars(itemNum, vtype=GRB.BINARY, name='x')\n        m.setObjective(purchase_fee * x.prod(predPrice), GRB.MAXIMIZE)\n        m.addConstr((x.prod(predWeight)) <= cap)\n\n        m.optimize()\n        predSol = []\n        x1_selectedItemNum = 0\n        for i in range(itemNum):\n            predSol.append(x[i].x)\n            if x[i].x == 1:\n              x1_selectedItemNum = x1_selectedItemNum + 1\n        objective1 = m.objVal\n\n        # Stage 2:\n        realWeight = realWeight.tolist()\n        m2 = gp.Model()\n        m2.setParam('OutputFlag', 0)\n        x = m2.addVars(itemNum, vtype=GRB.BINARY, name='x')\n        sigma = m2.addVars(itemNum, vtype=GRB.BINARY, name='sigma')\n\n        OBJ = purchase_fee * x.prod(realPrice)\n        for i in range(itemNum):\n            OBJ = OBJ - compensation_fee * realPrice[i] * sigma[i]\n        m2.setObjective(OBJ, GRB.MAXIMIZE)\n\n        m2.addConstr((x.prod(realWeight) - sigma.prod(realWeight)) <= cap)\n        for i in range(itemNum):\n            m2.addConstr(x[i] == predSol[i])\n            m2.addConstr(x[i] >= sigma[i])\n\n        m2.optimize()\n        objective = m2.objVal\n        sol = []\n        x2_selectedItemNum = 0\n        for i in range(itemNum):\n            sol.append(x[i].x - sigma[i].x)\n            if x[i].x - sigma[i].x == 1:\n              x2_selectedItemNum = x2_selectedItemNum + 1\n\n    return objective, x1_selectedItemNum, x2_selectedItemNum\n\n\ntestmarkNum = 300\nitemNum = 10\n\nstartmark = 0\nendmark = startmark + 1\n\nprint('2S')\nfor capacity in capacity_list:\n    print(capacity)\n    for testmark in range(startmark, endmark):\n        priceT = np.loadtxt('./data/2S_prices/2S_prices_cap' + str(capacity) + '_compensation' + str(compensation_fee) + '(' + str(testmark) + ').txt')\n        weightT = np.loadtxt('./data/2S_weights/2S_weights_cap' + str(capacity) + '_compensation' + str(compensation_fee) + '(' + str(testmark) + ').txt')\n        realPriceT = priceT[:, 0]\n        predPriceT = priceT[:, 1]\n        realWeightT = weightT[:, 0]\n        predWeightT = weightT[:, 1]\n        realPriceWeight = np.vstack((realPriceT, realWeightT))\n        predPriceWeight = np.vstack((predPriceT, predWeightT))\n        meanPriceVal = np.mean(realPriceT)\n        meanWeightVal = np.mean(realWeightT)\n\n        real_obj, real_selected_num = actual_obj(realPriceT, capacity, realWeightT, n_instance=testmarkNum)\n        corr_obj_list = []\n        x1_selected_num_list = []\n        x2_selected_num_list = []\n        feasibleNum = 0\n        for testNum in range(testmarkNum):\n           realWT = {}\n           predWT = {}\n           realValue = {}\n           predValue = {}\n           for i in range(itemNum):\n               realWT[i] = realWeightT[i+testNum*itemNum]\n               predWT[i] = predWeightT[i+testNum*itemNum]\n               realValue[i] = realPriceT[i+testNum*itemNum]\n               predValue[i] = predPriceT[i+testNum*itemNum]\n\n           corrrlst, x1_selected_num, x2_selected_num = correction_single_obj(realValue, predValue, capacity, realWT, predWT)\n           corr_obj_list.append(corrrlst)\n\n        print(\"MSE: \", mean_squared_error(realPriceWeight, predPriceWeight), \"avgCorrReg: \", sum(abs(real_obj - np.array(corr_obj_list)))/testmarkNum, \"avgTOV: \", sum(real_obj)/testmarkNum)\n\n    print(\"\\n\")\n\n# ==========================================\n# File: code/0-1 knapsack/train.py\n# Function/Context: Intopt.fit\n# ==========================================\nimport sys\nimport numpy as np\nimport random\nimport pandas as pd\nimport math, time\nimport itertools\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\nimport datetime\nimport torch\nfrom torch import nn, optim\nfrom torch.autograd import Variable\nimport torch.utils.data as data_utils\nfrom torch.utils.data.dataset import Dataset\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport gurobipy as gp\nimport logging\nimport copy\nfrom collections import defaultdict\nimport joblib\nimport gurobipy as gp\nfrom gurobipy import GRB\n\ncapacity = 100\npurchase_fee = 0.2\ncompensation_fee = 0.21\n\nitemNum = 10\nfeatureNum = 4096\ntrainSize = 700\ntargetNum = 2\n\ndef get_Xs1Xs2(realPrice, predPrice, cap, realWeightTemp, predWeightTemp):\n    realWeight = np.zeros(itemNum)\n    predWeight = np.zeros(itemNum)\n    realPriceNumpy = np.zeros(itemNum)\n    for i in range(itemNum):\n        realWeight[i] = realWeightTemp[i]\n        predWeight[i] = predWeightTemp[i]\n        realPriceNumpy[i] = realPrice[i]\n        \n    if min(predWeight) >= 0:\n        predWeight = predWeight.tolist()\n        m = gp.Model()\n        m.setParam('OutputFlag', 0)\n        x = m.addVars(itemNum, vtype=GRB.BINARY, name='x')\n        m.setObjective(purchase_fee * x.prod(predPrice), GRB.MAXIMIZE)\n        m.addConstr((x.prod(predWeight)) <= cap)\n\n        m.optimize()\n        predSol = np.zeros(itemNum,dtype='i')\n        for i in range(itemNum):\n            predSol[i] = x[i].x\n            \n        objective1 = m.objVal\n\n        # Stage 2:\n        realWeight = realWeight.tolist()\n        m2 = gp.Model()\n        m2.setParam('OutputFlag', 0)\n        x = m2.addVars(itemNum, vtype=GRB.BINARY, name='x')\n        sigma = m2.addVars(itemNum, vtype=GRB.BINARY, name='sigma')\n\n        OBJ = purchase_fee * x.prod(realPrice)\n        for i in range(itemNum):\n            OBJ = OBJ - compensation_fee * realPrice[i] * sigma[i]\n        m2.setObjective(OBJ, GRB.MAXIMIZE)\n\n        m2.addConstr((x.prod(realWeight) - sigma.prod(realWeight)) <= cap)\n        for i in range(itemNum):\n            m2.addConstr(x[i] == predSol[i])\n            m2.addConstr(x[i] >= sigma[i])\n        try:\n            m2.optimize()\n            objective = m2.objVal\n            sol = np.zeros(itemNum)\n            for i in range(itemNum):\n                sol[i] = x[i].x - sigma[i].x\n        except:\n            print(predPrice, predWeight, realPrice, realWeight, predSol)\n\n    return predSol,sol\n    \n\ndef correction_single_obj(realPrice, predPrice, cap, realWeightTemp, predWeightTemp):\n    realWeight = np.zeros(itemNum)\n    predWeight = np.zeros(itemNum)\n    realPriceNumpy = np.zeros(itemNum)\n    for i in range(itemNum):\n        realWeight[i] = realWeightTemp[i]\n        predWeight[i] = predWeightTemp[i]\n        realPriceNumpy[i] = realPrice[i]\n        \n    if min(predWeight) >= 0:\n        predWeight = predWeight.tolist()\n        m = gp.Model()\n        m.setParam('OutputFlag', 0)\n        x = m.addVars(itemNum, vtype=GRB.BINARY, name='x')\n        m.setObjective(purchase_fee * x.prod(predPrice), GRB.MAXIMIZE)\n        m.addConstr((x.prod(predWeight)) <= cap)\n\n        m.optimize()\n        predSol = np.zeros(itemNum,dtype='i')\n        x1_selectedItemNum = 0\n        for i in range(itemNum):\n            predSol[i] = x[i].x\n            if x[i].x == 1:\n              x1_selectedItemNum = x1_selectedItemNum + 1\n        objective1 = m.objVal\n\n        # Stage 2:\n        realWeight = realWeight.tolist()\n        m2 = gp.Model()\n        m2.setParam('OutputFlag', 0)\n        x = m2.addVars(itemNum, vtype=GRB.BINARY, name='x')\n        sigma = m2.addVars(itemNum, vtype=GRB.BINARY, name='sigma')\n\n        OBJ = purchase_fee * x.prod(realPrice)\n        for i in range(itemNum):\n            OBJ = OBJ - compensation_fee * realPrice[i] * sigma[i]\n        m2.setObjective(OBJ, GRB.MAXIMIZE)\n\n        m2.addConstr((x.prod(realWeight) - sigma.prod(realWeight)) <= cap)\n        for i in range(itemNum):\n            m2.addConstr(x[i] == predSol[i])\n            m2.addConstr(x[i] >= sigma[i])\n        \n        try:\n            m2.optimize()\n            objective = m2.objVal\n            sol = []\n            x2_selectedItemNum = 0\n            for i in range(itemNum):\n                sol.append(x[i].x - sigma[i].x)\n                if x[i].x - sigma[i].x == 1:\n                  x2_selectedItemNum = x2_selectedItemNum + 1\n        except:\n            print(predPrice, predWeight, realPrice, realWeight, predSol)\n\n    return objective\n\n\ndef make_fc(num_layers, num_features, num_targets=targetNum,\n            activation_fn = nn.ReLU,intermediate_size=512, regularizers = True):\n    net_layers = [nn.Linear(num_features, intermediate_size),activation_fn()]\n    for hidden in range(num_layers-2):\n        net_layers.append(nn.Linear(intermediate_size, intermediate_size))\n        net_layers.append(activation_fn())\n    net_layers.append(nn.Linear(intermediate_size, num_targets))\n    net_layers.append(activation_fn())\n    return nn.Sequential(*net_layers)\n        \n\nclass MyCustomDataset():\n    def __init__(self, feature, value):\n        self.feature = feature\n        self.value = value\n\n    def __len__(self):\n        return len(self.value)\n\n    def __getitem__(self, idx):\n        return self.feature[idx], self.value[idx]\n\n\nimport sys\nimport ip_model_whole as ip_model_wholeFile\nfrom ip_model_whole import IPOfunc\n\nclass Intopt:\n    def __init__(self, c, h, A, b, purchase_fee, compensation_fee, n_features, num_layers=5, smoothing=False, thr=0.1, max_iter=None, method=1, mu0=None,\n                 damping=0.5, target_size=targetNum, epochs=8, optimizer=optim.Adam,\n                 batch_size=itemNum, **hyperparams):\n        self.c = c\n        self.h = h\n        self.A = A\n        self.b = b\n        self.target_size = target_size\n        self.n_features = n_features\n        self.damping = damping\n        self.num_layers = num_layers\n        self.purchase_fee = purchase_fee\n        self.compensation_fee = compensation_fee\n\n        self.smoothing = smoothing\n        self.thr = thr\n        self.max_iter = max_iter\n        self.method = method\n        self.mu0 = mu0\n\n        self.optimizer = optimizer\n        self.batch_size = batch_size\n        self.hyperparams = hyperparams\n        self.epochs = epochs\n\n        self.model = make_fc(num_layers=self.num_layers,num_features=n_features)\n        self.optimizer = optimizer(self.model.parameters(), **hyperparams)\n\n    def fit(self, feature, value):\n        logging.info(\"Intopt\")\n        train_df = MyCustomDataset(feature, value)\n\n        criterion = nn.L1Loss(reduction='mean')\n        grad_list = np.zeros(self.epochs)\n        for e in range(self.epochs):\n          total_loss = 0\n          if e < 5:\n            train_dl = data_utils.DataLoader(train_df, batch_size=self.batch_size, shuffle=False)\n            for feature, value in train_dl:\n                self.optimizer.zero_grad()\n                op = self.model(feature).squeeze()\n                loss = criterion(op, value)\n                total_loss += loss.item()\n                loss.backward()\n                self.optimizer.step()\n            grad_list[e] = total_loss\n            print(\"Epoch{} ::loss {} ->\".format(e,total_loss))\n                \n          else:\n            train_dl = data_utils.DataLoader(train_df, batch_size=self.batch_size, shuffle=False)\n            \n            num = 0\n            batchCnt = 0\n            loss = Variable(torch.tensor(0.0, dtype=torch.double), requires_grad=True)\n            for feature, value in train_dl:\n                self.optimizer.zero_grad()\n                op = self.model(feature).squeeze()\n                while torch.min(op) <= 0 or torch.isnan(op).any() or torch.isinf(op).any():\n                    self.optimizer.zero_grad()\n                    self.model = make_fc(num_layers=self.num_layers,num_features=self.n_features)\n                    op = self.model(feature).squeeze()\n  \n                price = np.zeros(itemNum)\n                for i in range(itemNum):\n                    price[i] = self.c[i+num*itemNum]\n                    op[i] = op[i]\n                    \n                c_torch = torch.from_numpy(price).float()\n                h_torch = torch.from_numpy(self.h).float()\n                A_torch = torch.from_numpy(self.A).float()\n                b_torch = torch.from_numpy(self.b).float()\n                \n                G_torch = torch.zeros((itemNum+1, itemNum))\n                for i in range(itemNum):\n                    G_torch[i][i] = 1\n                G_torch[itemNum] = value[:, 1]\n                trueWeight = value[:, 1]\n                \n                x_s2 = IPOfunc(A=A_torch, b=b_torch, h=h_torch, cTrue=-c_torch, GTrue=G_torch, purchase_fee=self.purchase_fee, compensation_fee=self.compensation_fee, max_iter=self.max_iter, thr=self.thr, damping=self.damping,\n                            smoothing=self.smoothing)(op)\n                x_s1 = ip_model_wholeFile.x_s1\n                \n                newLoss = - (purchase_fee * (x_s2 * c_torch).sum() - (compensation_fee - purchase_fee) * torch.dot(c_torch, abs(x_s2-x_s1).float()))\n                loss = loss + newLoss\n                batchCnt = batchCnt + 1\n                total_loss += newLoss.item()\n                newLoss.backward()\n                self.optimizer.step()\n                \n                if batchCnt % 70 == 0:\n                    pass\n\n# ==========================================\n# File: code/Alloy production/ip_model_whole.py\n# Function/Context: This file contains multiple helper functions but appears to be part of a larger optimization framework. The main logic is not contained in a single function but distributed across the file.\n# ==========================================\nimport numpy as np\nimport scipy as sp\nimport time\nimport scipy.sparse as sps\nimport numbers\nfrom warnings import warn\nfrom scipy.linalg import LinAlgError\nimport torch\nfrom torch import nn, optim\nfrom torch.autograd import Variable,Function\nimport sys\nimport logging\nimport time, datetime\nfrom scipy.optimize import OptimizeWarning\nfrom remove_redundancy import _remove_redundancy, _remove_redundancy_sparse, _remove_redundancy_dense\nnp.set_printoptions(threshold=np.inf)\nimport gurobipy as gp\nfrom gurobipy import GRB\n\nvarNum = 10\nx_s1 = torch.zeros(varNum,dtype=torch.float64)\n\ndef _format_A_constraints(A, n_x, sparse_lhs=False):\n    \"\"\"Format the left hand side of the constraints to a 2D array\n\n    Parameters\n    ----------\n    A : 2D array\n        2D array such that ``A @ x`` gives the values of the upper-bound\n        (in)equality constraints at ``x``.\n    n_x : int\n        The number of variables in the linear programming problem.\n    sparse_lhs : bool\n        Whether either of `A_ub` or `A_eq` are sparse. If true return a\n        coo_matrix instead of a numpy array.\n\n    Returns\n    -------\n    np.ndarray or sparse.coo_matrix\n        2D array such that ``A @ x`` gives the values of the upper-bound\n        (in)equality constraints at ``x``.\n\n    \"\"\"\n    if sparse_lhs:\n        return sps.coo_matrix(\n            (0, n_x) if A is None else A, dtype=np.float, copy=True\n        )\n    elif A is None:\n        return np.zeros((0, n_x), dtype=np.float)\n    else:\n        return np.array(A, dtype=np.float, copy=True)\n\ndef _format_b_constraints(b):\n    \"\"\"Format the upper bounds of the constraints to a 1D array\n\n    Parameters\n    ----------\n    b : 1D array\n        1D array of values representing the upper-bound of each (in)equality\n        constraint (row) in ``A``.\n\n    Returns\n    -------\n    1D np.array\n        1D array of values representing the upper-bound of each (in)equality\n        constraint (row) in ``A``.\n\n    \"\"\"\n    if b is None:\n        return np.array([], dtype=np.float)\n    b = np.array(b, dtype=np.float, copy=True).squeeze()\n    return b if b.size != 1 else b.reshape((-1))\n\ndef _clean_inputs(c, A_ub=None, b_ub=None, A_eq=None, b_eq=None, bounds=None,\n                  x0=None):\n    \"\"\"\n    Given user inputs for a linear programming problem, return the\n    objective vector, upper bound constraints, equality constraints,\n    and simple bounds in a preferred format.\n\n    Parameters\n    ----------\n    c : 1D array\n        Coefficients of the linear objective function to be minimized.\n    A_ub : 2D array, optional\n        2D array such that ``A_ub @ x`` gives the values of the upper-bound\n        inequality constraints at ``x``.\n    b_ub : 1D array, optional\n        1D array of values representing the upper-bound of each inequality\n        constraint (row) in ``A_ub``.\n    A_eq : 2D array, optional\n        2D array such that ``A_eq @ x`` gives the values of the equality\n        constraints at ``x``.\n    b_eq : 1D array, optional\n        1D array of values representing the RHS of each equality constraint\n        (row) in ``A_eq``.\n    bounds : sequence, optional\n        ``(min, max)`` pairs for each element in ``x``, defining\n        the bounds on that parameter. Use None for one of ``min`` or\n        ``max`` when there is no bound in that direction. By default\n        bounds are ``(0, None)`` (non-negative).\n        If a sequence containing a single tuple is provided, then ``min`` and\n        ``max`` will be applied to all variables in the problem.\n    x0 : 1D array, optional\n        Starting values of the independent variables, which will be refined by\n        the optimization algorithm.\n\n    Returns\n    -------\n    c : 1D array\n        Coefficients of the linear objective function to be minimized.\n    A_ub : 2D array, optional\n        2D array such that ``A_ub @ x`` gives the values of the upper-bound\n        inequality constraints at ``x``.\n    b_ub : 1D array, optional\n        1D array of values representing the upper-bound of each inequality\n        constraint (row) in ``A_ub``.\n    A_eq : 2D array, optional\n        2D array such that ``A_eq @ x`` gives the values of the equality\n        constraints at ``x``.\n    b_eq : 1D array, optional\n        1D array of values representing the RHS of each equality constraint\n        (row) in ``A_eq``.\n    bounds : sequence of tuples\n        ``(min, max)`` pairs for each element in ``x``, defining\n        the bounds on that parameter. Use None for each of ``min`` or\n        ``max`` when there is no bound in that direction. By default\n        bounds are ``(0, None)`` (non-negative).\n    x0 : 1D array, optional\n        Starting values of the independent variables, which will be refined by\n        the optimization algorithm.\n    \"\"\"\n    if c is None:\n        raise TypeError\n\n    try:\n        c = np.array(c, dtype=np.float, copy=True).squeeze()\n    except ValueError:\n        raise TypeError(\n            \"Invalid input for linprog: c must be a 1D array of numerical \"\n            \"coefficients\")\n    else:\n        # If c is a single value, convert it to a 1D array.\n        if c.size == 1:\n            c = c.reshape((-1))\n\n        n_x = len(c)\n        if n_x == 0 or len(c.shape) != 1:\n            raise ValueError(\n                \"Invalid input for linprog: c must be a 1D array and must \"\n                \"not have more than one non-singleton dimension\")\n        if not(np.isfinite(c).all()):\n            raise ValueError(\n                \"Invalid input for linprog: c must not contain values \"\n                \"inf, nan, or None\")\n\n    sparse_lhs = sps.issparse(A_eq) or sps.issparse(A_ub)\n    try:\n        A_ub = _format_A_constraints(A_ub, n_x, sparse_lhs=sparse_lhs)\n    except ValueError:\n        raise TypeError(\n            \"Invalid input for linprog: A_ub must be a 2D array \"\n            \"of numerical values\")\n    else:\n        n_ub = A_ub.shape[0]\n        if len(A_ub.shape) != 2 or A_ub.shape[1] != n_x:\n            raise ValueError(\n                \"Invalid input for linprog: A_ub must have exactly two \"\n                \"dimensions, and the number of columns in A_ub must be \"\n                \"equal to the size of c\")\n        if (sps.issparse(A_ub) and not np.isfinite(A_ub.data).all()\n                or not sps.issparse(A_ub) and not np.isfinite(A_ub).all()):\n            raise ValueError(\n                \"Invalid input for linprog: A_ub must not contain values \"\n                \"inf, nan, or None\")\n\n    try:\n        b_ub = _format_b_constraints(b_ub)\n    except ValueError:\n        raise TypeError(\n            \"Invalid input for linprog: b_ub must be a 1D array of \"\n            \"numerical values, each representing the upper bound of an \"\n            \"inequality constraint (row) in A_ub\")\n    else:\n        if b_ub.shape != (n_ub,):\n            raise ValueError(\n                \"Invalid input for linprog: b_ub must be a 1D array; b_ub \"\n                \"must not have more than one non-singleton dimension and \"\n                \"the number of rows in A_ub must equal the number of values \"\n                \"in b_ub\")\n        if not(np.isfinite(b_ub).all()):\n            raise ValueError(\n                \"Invalid input for linprog: b_ub must not contain values \"\n                \"inf, nan, or None\")\n\n    try:\n        A_eq = _format_A_constraints(A_eq, n_x, sparse_lhs=sparse_lhs)\n    except ValueError:\n        raise TypeError(\n            \"Invalid input for linprog: A_eq must be a 2D array \"\n            \"of numerical values\")\n    else:\n        n_eq = A_eq.shape[0]\n        if len(A_eq.shape) != 2 or A_eq.shape[1] != n_x:\n            raise ValueError(\n                \"Invalid input for linprog: A_eq must have exactly two \"\n                \"dimensions, and the number of columns in A_eq must be \"\n                \"equal to the size of c\")\n\n        if (sps.issparse(A_eq) and not np.isfinite(A_eq.data).all()\n                or not sps.issparse(A_eq) and not np.isfinite(A_eq).all()):\n            raise ValueError(\n                \"Invalid input for linprog: A_eq must not contain values \"\n                \"inf, nan, or None\")\n    try:\n        b_eq = _format_b_constraints(b_eq)\n\n    except ValueError:\n        raise TypeError(\n            \"Invalid input for linprog: b_eq must be a 1D array of \"\n            \"numerical values, each representing the upper bound of an \"\n            \"inequality constraint (row) in A_eq\")\n    else:\n        if b_eq.shape != (n_eq,):\n            raise ValueError(\n                \"Invalid input for linprog: b_eq must be a 1D array; b_eq \"\n                \"must not have more than one non-singleton dimension and \"\n                \"the number of rows in A_eq must equal the number of values \"\n                \"in b_eq\")\n        if not(np.isfinite(b_eq).all()):\n            raise ValueError(\n                \"Invalid input for linprog: b_eq must not contain values \"\n                \"inf, nan, or None\")\n\n    # x0 gives a (optional) starting solution to the solver. If x0 is None,\n    # skip the checks. Initial solution will be generated automatically.\n    if x0 is not None:\n        try:\n            x0 = np.array(x0, dtype=float, copy=True).squeeze()\n        except ValueError:\n            raise TypeError(\n                \"Invalid input for linprog: x0 must be a 1D array of \"\n                \"numerical coefficients\")\n        if x0.ndim == 0:\n            x0 = x0.reshape((-1))\n        if len(x0) == 0 or x0.ndim != 1:\n            raise ValueError(\n                \"Invalid input for linprog: x0 should be a 1D array; it \"\n                \"must not have more than one non-singleton dimension\")\n        if not x0.size == c.size:\n            raise ValueError(\n                \"Invalid input for linprog: x0 and c should contain the \"\n                \"same number of elements\")\n        if not np.isfinite(x0).all():\n            raise ValueError(\n            \"Invalid input for linprog: x0 must not contain values \"\n            \"inf, nan, or None\")\n\n    # \"If a sequence containing a single tuple is provided, then min and max\n    # will be applied to all variables in the problem.\"\n    # linprog doesn't treat this right: it didn't accept a list with one tuple\n    # in it\n    try:\n        if isinstance(bounds, str):\n            raise TypeError\n        if bounds is None or len(bounds) == 0:\n            bounds = [(0, None)] * n_x\n        elif len(bounds) == 1:\n            b = bounds[0]\n            if len(b) != 2:\n                raise ValueError(\n                    \"Invalid input for linprog: exactly one lower bound and \"\n                    \"one upper bound must be specified for each element of x\")\n            bounds = [b] * n_x\n        elif len(bounds) == n_x:\n            try:\n                len(bounds[0])\n            except BaseException:\n                bounds = [(bounds[0], bounds[1])] * n_x\n            for i, b in enumerate(bounds):\n                if len(b) != 2:\n                    raise ValueError(\n                        \"Invalid input for linprog, bound \" +\n                        str(i) +\n                        \" \" +\n                        str(b) +\n                        \": exactly one lower bound and one upper bound must \"\n                        \"be specified for each element of x\")\n        elif (len(bounds) == 2 and np.isreal(bounds[0])\n                and np.isreal(bounds[1])):\n            bounds = [(bounds[0], bounds[1])] * n_x\n        else:\n            raise ValueError(\n                \"Invalid input for linprog: exactly one lower bound and one \"\n                \"upper bound must be specified for each element of x\")\n\n        clean_bounds = []  # also creates a copy so user's object isn't changed\n        for i, b in enumerate(bounds):\n            if b[0] is not None and b[1] is not None and b[0] > b[1]:\n                raise ValueError(\n                    \"Invalid input for linprog, bound \" +\n                    str(i) +\n                    \" \" +\n                    str(b) +\n                    \": a lower bound must be less than or equal to the \"\n                    \"corresponding upper bound\")\n            if b[0] == np.inf:\n                raise ValueError(\n                    \"Invalid input for linprog, bound \" +\n                    str(i) +\n                    \" \" +\n                    str(b) +\n                    \": infinity is not a valid lower bound\")\n            if b[1] == -np.inf:\n                raise ValueError(\n                    \"Invalid input for linprog, bound \" +\n                    str(i) +\n                    \" \" +\n                    str(b) +\n                    \": negative infinity is not a valid upper bound\")\n            lb = float(b[0]) if b[0] is not None and b[0] != -np.inf else None\n            ub = float(b[1]) if b[1] is not None and b[1] != np.inf else None\n            clean_bounds.append((lb, ub))\n        bounds = clean_bounds\n    except ValueError as e:\n        if \"could not convert string to float\" in e.args[0]:\n            raise TypeError\n        else:\n            raise e\n    except TypeError as e:\n        print(e)\n        raise TypeError(\n            \"Invalid input for linprog: bounds must be a sequence of \"\n            \"(min,max) pairs, each defining bounds on an element of x \")\n\n    return c, A_ub, b_ub, A_eq, b_eq, bounds, x0\n\n# ==========================================\n# File: code/Alloy production/test.py\n# Function/Context: correction_single_obj\n# ==========================================\nimport sys\nfrom collections import defaultdict\nimport numpy as np\nimport gurobipy as gp\nfrom gurobipy import GRB\nfrom sklearn.metrics import mean_squared_error\nfrom numpy import inf\n\npenaltyTerm_list = [0.25, 0.5, 1, 2, 4, 8]\nrowSizeG = 4\ncolSizeG = 10\nvarNum = colSizeG\ncap = [0.8, 60, 40, 2.5]\n\ndef actual_obj(cTemp, GTemp, hTemp, n_instance):\n    obj_list = []\n    for num in range(n_instance):\n        c = np.zeros((colSizeG))\n        cntC = num * colSizeG\n        for i in range(colSizeG):\n            c[i] = cTemp[cntC]\n            cntC = cntC + 1\n        c = c.tolist()\n        h = np.zeros((rowSizeG))\n        cntH = num * rowSizeG\n        for i in range(rowSizeG):\n            h[i] = hTemp[cntH]\n            cntH = cntH + 1\n        h = h.tolist()\n        \n        G = np.zeros((rowSizeG, colSizeG))\n        cnt = num * rowSizeG * colSizeG\n        for i in range(rowSizeG):\n            for j in range(colSizeG):\n                G[i][j] = GTemp[cnt]\n                cnt = cnt + 1\n        G = G.tolist()\n        \n        m = gp.Model()\n        m.setParam('OutputFlag', 0)\n        x = m.addVars(varNum, vtype=GRB.CONTINUOUS, name='x')\n        m.setObjective(x.prod(c), GRB.MINIMIZE)\n        for i in range(rowSizeG):\n            m.addConstr((x.prod(G[i])) >= h[i])\n\n        m.optimize()\n        sol = []\n        for i in range(varNum):\n            sol.append(x[i].x)\n        \n        objective = m.objVal\n        obj_list.append(objective)\n        \n    return np.array(obj_list)\n    \ndef correction_single_obj(c, realG, preG, h, penalty):\n    rowSizeG = realG.shape[0]\n    if preG.all() >= 0:\n        realG = realG.tolist()\n        preG = preG.tolist()\n        m = gp.Model()\n        m.setParam('OutputFlag', 0)\n        x = m.addVars(varNum, vtype=GRB.CONTINUOUS, name='x')\n        m.setObjective(x.prod(c), GRB.MINIMIZE)\n        for i in range(rowSizeG):\n            m.addConstr((x.prod(preG[i])) >= h[i])\n\n        m.optimize()\n        predSol = []\n        for i in range(varNum):\n            predSol.append(x[i].x)\n        objective = m.objVal\n        \n        # Stage 2:\n        m2 = gp.Model()\n        m2.setParam('OutputFlag', 0)\n        x = m2.addVars(varNum, vtype=GRB.CONTINUOUS, name='x')\n        sigma = m2.addVars(varNum, vtype=GRB.CONTINUOUS, name='sigma')\n\n        OBJ = objective\n        for i in range(varNum):\n            OBJ = OBJ + (1 + penalty[i]) * c[i] * sigma[i]\n        m2.setObjective(OBJ, GRB.MINIMIZE)\n\n        for i in range(rowSizeG):\n            m2.addConstr((x.prod(realG[i]) + sigma.prod(realG[i])) >= h[i])\n        for i in range(varNum):\n            m2.addConstr(x[i] == predSol[i])\n\n        m2.optimize()\n        objective = m2.objVal\n        sol = []\n        for i in range(varNum):\n            sol.append(sigma[i].x)\n        \n    return objective\n\n# ==========================================\n# File: code/Alloy production/train.py\n# Function/Context: Intopt.fit\n# ==========================================\nimport sys\nimport numpy as np\nimport random\nimport pandas as pd\nimport math, time\nimport itertools\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\nimport datetime\nimport torch\nfrom torch import nn, optim\nfrom torch.autograd import Variable\nimport torch.utils.data as data_utils\nfrom torch.utils.data.dataset import Dataset\nfrom sklearn.preprocessing import StandardScaler\nimport gurobipy as gp\nimport logging\nimport copy\nfrom collections import defaultdict\nimport joblib\nimport gurobipy as gp\nfrom gurobipy import GRB\nimport random\n\nrowSizeG = 2\ncolSizeG = 10\nvarNum = colSizeG\nfeatureNum = 4096\ntrainSize = 350\ncap = [627.54, 369.72]\npenaltyTerm = 0.25\n\ndef actual_obj(cTemp, GTemp, hTemp, n_instance):\n    obj_list = []\n    for num in range(n_instance):\n        c = np.zeros((colSizeG))\n        cntC = num * colSizeG\n        for i in range(colSizeG):\n            c[i] = cTemp[cntC]\n            cntC = cntC + 1\n        c = c.tolist()\n        h = np.zeros((rowSizeG))\n        cntH = num * rowSizeG\n        for i in range(rowSizeG):\n            h[i] = hTemp[cntH]\n            cntH = cntH + 1\n        h = h.tolist()\n        \n        G = np.zeros((rowSizeG, colSizeG))\n        cnt = num * rowSizeG * colSizeG\n        for i in range(rowSizeG):\n            for j in range(colSizeG):\n                G[i][j] = GTemp[cnt]\n                cnt = cnt + 1\n        G = G.tolist()\n        \n        m = gp.Model()\n        m.setParam('OutputFlag', 0)\n        x = m.addVars(varNum, vtype=GRB.CONTINUOUS, name='x')\n        m.setObjective(x.prod(c), GRB.MINIMIZE)\n        for i in range(rowSizeG):\n            m.addConstr((x.prod(G[i])) >= h[i])\n\n        m.optimize()\n        sol = []\n        for i in range(varNum):\n            sol.append(x[i].x)\n        \n        objective = m.objVal\n        obj_list.append(objective)\n        \n    return np.array(obj_list)\n    \ndef correction_single_obj(c, realG, preG, h, penalty):\n    rowSizeG = realG.shape[0]\n    if preG.all() >= 0:\n        realG = realG.tolist()\n        preG = preG.tolist()\n        m = gp.Model()\n        m.setParam('OutputFlag', 0)\n        x = m.addVars(varNum, vtype=GRB.CONTINUOUS, name='x')\n        m.setObjective(x.prod(c), GRB.MINIMIZE)\n        for i in range(rowSizeG):\n            m.addConstr((x.prod(preG[i])) >= h[i])\n\n        m.optimize()\n        predSol = []\n        try:\n            for i in range(varNum):\n                predSol.append(x[i].x)\n            objective = m.objVal\n        except:\n            for i in range(varNum):\n                predSol.append(0)\n            objective = 0\n        \n        m2 = gp.Model()\n        m2.setParam('OutputFlag', 0)\n        x = m2.addVars(varNum, vtype=GRB.CONTINUOUS, name='x')\n        sigma = m2.addVars(varNum, vtype=GRB.CONTINUOUS, name='sigma')\n\n        OBJ = objective\n        for i in range(varNum):\n            OBJ = OBJ + (1 + penalty[i]) * c[i] * sigma[i]\n        m2.setObjective(OBJ, GRB.MINIMIZE)\n\n        for i in range(rowSizeG):\n            m2.addConstr((x.prod(realG[i]) + sigma.prod(realG[i])) >= h[i])\n        for i in range(varNum):\n            m2.addConstr(x[i] == predSol[i])\n\n        m2.optimize()\n        objective = m2.objVal\n        sol = []\n        for i in range(varNum):\n            sol.append(sigma[i].x)\n        \n    return objective\n\nimport sys\nimport ip_model_whole as ip_model_wholeFile\nfrom ip_model_whole import IPOfunc\n\nclass Intopt:\n    def __init__(self, c, h, A, b, penalty, n_features, num_layers=5, smoothing=False, thr=0.1, max_iter=None, method=1, mu0=None,\n                 damping=0.5, target_size=1, epochs=8, optimizer=optim.Adam,\n                 batch_size=rowSizeG*colSizeG, **hyperparams):\n        self.c = c\n        self.h = h\n        self.A = A\n        self.b = b\n        self.penalty = penalty\n        self.target_size = target_size\n        self.n_features = n_features\n        self.damping = damping\n        self.num_layers = num_layers\n\n        self.smoothing = smoothing\n        self.thr = thr\n        self.max_iter = max_iter\n        self.method = method\n        self.mu0 = mu0\n\n        self.optimizer = optimizer\n        self.batch_size = batch_size\n        self.hyperparams = hyperparams\n        self.epochs = epochs\n\n        self.model = make_fc(num_layers=self.num_layers,num_features=n_features)\n        self.optimizer = optimizer(self.model.parameters(), **hyperparams)\n\n    def fit(self, feature, value):\n        logging.info(\"Intopt\")\n        train_df = MyCustomDataset(feature, value)\n\n        criterion = nn.L1Loss(reduction='mean')\n        grad_list = np.zeros(self.epochs)\n        for e in range(self.epochs):\n          total_loss = 0\n          if e < 10:\n            lr = 1e-1\n            train_dl = data_utils.DataLoader(train_df, batch_size=self.batch_size, shuffle=False)\n            for feature, value in train_dl:\n                self.optimizer.zero_grad()\n                op = self.model(feature).squeeze()\n                loss = criterion(op, value)\n                total_loss += loss.item()\n                loss.backward()\n                self.optimizer.step()\n          else:\n            train_dl = data_utils.DataLoader(train_df, batch_size=self.batch_size, shuffle=False)\n            \n            corr_obj_list = []\n            num = 0\n            batchCnt = 0\n            loss = Variable(torch.tensor(0.0, dtype=torch.double), requires_grad=True)\n            for feature, value in train_dl:\n                self.optimizer.zero_grad()\n                op = self.model(feature).squeeze()\n                while torch.min(op) <= 0 or torch.isnan(op).any() or torch.isinf(op).any():\n                    self.optimizer.zero_grad()\n                    self.model = make_fc(num_layers=self.num_layers,num_features=self.n_features)\n                    op = self.model(feature).squeeze()\n  \n                price = np.zeros(colSizeG)\n                cap = np.zeros(rowSizeG)\n                penaltyVector = np.zeros(colSizeG)\n                for i in range(colSizeG):\n                    price[i] = self.c[i+num*colSizeG]\n                    penaltyVector[i] = self.penalty[i+num*colSizeG]\n                for j in range(rowSizeG):\n                    cap[j] = self.h[j+num*rowSizeG]\n                \n                c_torch = torch.from_numpy(price).float()\n                h_torch = torch.from_numpy(cap).float()\n                A_torch = torch.from_numpy(self.A).float()\n                b_torch = torch.from_numpy(self.b).float()\n                penalty_torch = torch.from_numpy(penaltyVector).float()\n                \n                cntG = 0\n                G_torch = torch.zeros((rowSizeG, colSizeG))\n                for i in range(rowSizeG):\n                    for j in range(colSizeG):\n                        G_torch[i][j] = value[cntG]\n                        cntG = cntG + 1\n                \n                cntOp = 0\n                op_torch = torch.zeros((rowSizeG, colSizeG))\n                for i in range(rowSizeG):\n                    for j in range(colSizeG):\n                        op_torch[i][j] = op[cntOp]\n                        cntOp = cntOp + 1\n                \n                x_s2 = IPOfunc(A=A_torch, b=b_torch, h=-h_torch, c=c_torch, GTrue=-G_torch, penalty=penalty_torch, max_iter=self.max_iter, thr=self.thr, damping=self.damping,\n                            smoothing=self.smoothing)(-op_torch)\n                x_s1 = ip_model_wholeFile.x_s1\n                G_real_numpy = value.numpy()\n                newLoss = torch.dot(penalty_torch * c_torch, abs(x_s2-x_s1).float()) + (x_s2 * c_torch).sum()\n                corr_obj_list.append(newLoss.item())\n                loss = loss + newLoss\n                batchCnt = batchCnt + 1\n                total_loss += newLoss.item()\n                newLoss.backward()\n                self.optimizer.step()\n                num = num + 1\n            total_loss = total_loss/trainSize\n          \n          logging.info(\"EPOCH Ends\")\n          print(\"Epoch{} ::loss {} ->\".format(e,total_loss))\n          grad_list[e] = total_loss\n          if e > 0 and grad_list[e] >= grad_list[e-1]:\n            break\n\n    def val_loss(self, feature, value):\n        valueTemp = value.numpy()\n        test_instance = len(valueTemp) / self.batch_size\n        real_obj = actual_obj(self.c, value, self.h, n_instance=int(test_instance))\n        self.model.eval()\n        criterion = nn.L1Loss(reduction='mean')\n        valid_df = MyCustomDataset(feature, value)\n        valid_dl = data_utils.DataLoader(valid_df, batch_size=self.batch_size, shuffle=False)\n        obj_list = []\n        corr_obj_list = []\n        predVal = torch.zeros(len(valueTemp))\n        num = 0\n        for feature, value in valid_dl:\n            op = self.model(feature).squeeze()\n            for i in range(rowSizeG*colSizeG):\n                predVal[i+num*rowSizeG*colSizeG] = op[i]\n            loss = criterion(op, value)\n            price = np.zeros(colSizeG)\n            cap = np.zeros(rowSizeG)\n            penaltyVector = np.zeros(colSizeG)\n            for i in range(colSizeG):\n                price[i] = self.c[i+num*colSizeG]\n\n# ==========================================\n# File: code/NSP/ip_model_whole.py\n# Function/Context: \n# ==========================================\nimport numpy as np\nimport scipy as sp\nimport scipy.sparse as sps\nimport numbers\nfrom warnings import warn\nfrom scipy.linalg import LinAlgError\nimport torch\nfrom torch import nn, optim\nfrom torch.autograd import Variable,Function\nimport sys\nimport logging\nimport time, datetime\nfrom scipy.optimize import OptimizeWarning\nfrom remove_redundancy import _remove_redundancy, _remove_redundancy_sparse, _remove_redundancy_dense\nnp.set_printoptions(threshold=np.inf)\nimport gurobipy as gp\nfrom gurobipy import GRB\n\nnurse_num = 15\nday_num = 7\nshift_num = 3\nserve_patient_num = 20\ndecision_num = nurse_num * day_num * shift_num\nday_shift_num = day_num * shift_num\nx_s1 = torch.zeros(decision_num,dtype=torch.float64)\n\ndef _format_A_constraints(A, n_x, sparse_lhs=False):\n    \"\"\"Format the left hand side of the constraints to a 2D array\n\n    Parameters\n    ----------\n    A : 2D array\n        2D array such that ``A @ x`` gives the values of the upper-bound\n        (in)equality constraints at ``x``.\n    n_x : int\n        The number of variables in the linear programming problem.\n    sparse_lhs : bool\n        Whether either of `A_ub` or `A_eq` are sparse. If true return a\n        coo_matrix instead of a numpy array.\n\n    Returns\n    -------\n    np.ndarray or sparse.coo_matrix\n        2D array such that ``A @ x`` gives the values of the upper-bound\n        (in)equality constraints at ``x``.\n\n    \"\"\"\n    if sparse_lhs:\n        return sps.coo_matrix(\n            (0, n_x) if A is None else A, dtype=np.float, copy=True\n        )\n    elif A is None:\n        return np.zeros((0, n_x), dtype=np.float)\n    else:\n        return np.array(A, dtype=np.float, copy=True)\n\ndef _format_b_constraints(b):\n    \"\"\"Format the upper bounds of the constraints to a 1D array\n\n    Parameters\n    ----------\n    b : 1D array\n        1D array of values representing the upper-bound of each (in)equality\n        constraint (row) in ``A``.\n\n    Returns\n    -------\n    1D np.array\n        1D array of values representing the upper-bound of each (in)equality\n        constraint (row) in ``A``.\n\n    \"\"\"\n    if b is None:\n        return np.array([], dtype=np.float)\n    b = np.array(b, dtype=np.float, copy=True).squeeze()\n    return b if b.size != 1 else b.reshape((-1))\n\ndef _clean_inputs(c, A_ub=None, b_ub=None, A_eq=None, b_eq=None, bounds=None,\n                  x0=None):\n    \"\"\"\n    Given user inputs for a linear programming problem, return the\n    objective vector, upper bound constraints, equality constraints,\n    and simple bounds in a preferred format.\n\n    Parameters\n    ----------\n    c : 1D array\n        Coefficients of the linear objective function to be minimized.\n    A_ub : 2D array, optional\n        2D array such that ``A_ub @ x`` gives the values of the upper-bound\n        inequality constraints at ``x``.\n    b_ub : 1D array, optional\n        1D array of values representing the upper-bound of each inequality\n        constraint (row) in ``A_ub``.\n    A_eq : 2D array, optional\n        2D array such that ``A_eq @ x`` gives the values of the equality\n        constraints at ``x``.\n    b_eq : 1D array, optional\n        1D array of values representing the RHS of each equality constraint\n        (row) in ``A_eq``.\n    bounds : sequence, optional\n        ``(min, max)`` pairs for each element in ``x``, defining\n        the bounds on that parameter. Use None for one of ``min`` or\n        ``max`` when there is no bound in that direction. By default\n        bounds are ``(0, None)`` (non-negative).\n        If a sequence containing a single tuple is provided, then ``min`` and\n        ``max`` will be applied to all variables in the problem.\n    x0 : 1D array, optional\n        Starting values of the independent variables, which will be refined by\n        the optimization algorithm.\n\n    Returns\n    -------\n    c : 1D array\n        Coefficients of the linear objective function to be minimized.\n    A_ub : 2D array, optional\n        2D array such that ``A_ub @ x`` gives the values of the upper-bound\n        inequality constraints at ``x``.\n    b_ub : 1D array, optional\n        1D array of values representing the upper-bound of each inequality\n        constraint (row) in ``A_ub``.\n    A_eq : 2D array, optional\n        2D array such that ``A_eq @ x`` gives the values of the equality\n        constraints at ``x``.\n    b_eq : 1D array, optional\n        1D array of values representing the RHS of each equality constraint\n        (row) in ``A_eq``.\n    bounds : sequence of tuples\n        ``(min, max)`` pairs for each element in ``x``, defining\n        the bounds on that parameter. Use None for each of ``min`` or\n        ``max`` when there is no bound in that direction. By default\n        bounds are ``(0, None)`` (non-negative).\n    x0 : 1D array, optional\n        Starting values of the independent variables, which will be refined by\n        the optimization algorithm.\n    \"\"\"\n    if c is None:\n        raise TypeError\n\n    try:\n        c = np.array(c, dtype=np.float, copy=True).squeeze()\n    except ValueError:\n        raise TypeError(\n            \"Invalid input for linprog: c must be a 1D array of numerical \"\n            \"coefficients\")\n    else:\n        # If c is a single value, convert it to a 1D array.\n        if c.size == 1:\n            c = c.reshape((-1))\n\n        n_x = len(c)\n        if n_x == 0 or len(c.shape) != 1:\n            raise ValueError(\n                \"Invalid input for linprog: c must be a 1D array and must \"\n                \"not have more than one non-singleton dimension\")\n        if not(np.isfinite(c).all()):\n            raise ValueError(\n                \"Invalid input for linprog: c must not contain values \"\n                \"inf, nan, or None\")\n\n    sparse_lhs = sps.issparse(A_eq) or sps.issparse(A_ub)\n    try:\n        A_ub = _format_A_constraints(A_ub, n_x, sparse_lhs=sparse_lhs)\n    except ValueError:\n        raise TypeError(\n            \"Invalid input for linprog: A_ub must be a 2D array \"\n            \"of numerical values\")\n    else:\n        n_ub = A_ub.shape[0]\n        if len(A_ub.shape) != 2 or A_ub.shape[1] != n_x:\n            raise ValueError(\n                \"Invalid input for linprog: A_ub must have exactly two \"\n                \"dimensions, and the number of columns in A_ub must be \"\n                \"equal to the size of c\")\n        if (sps.issparse(A_ub) and not np.isfinite(A_ub.data).all()\n                or not sps.issparse(A_ub) and not np.isfinite(A_ub).all()):\n            raise ValueError(\n                \"Invalid input for linprog: A_ub must not contain values \"\n                \"inf, nan, or None\")\n\n    try:\n        b_ub = _format_b_constraints(b_ub)\n    except ValueError:\n        raise TypeError(\n            \"Invalid input for linprog: b_ub must be a 1D array of \"\n            \"numerical values, each representing the upper bound of an \"\n            \"inequality constraint (row) in A_ub\")\n    else:\n        if b_ub.shape != (n_ub,):\n            raise ValueError(\n                \"Invalid input for linprog: b_ub must be a 1D array; b_ub \"\n                \"must not have more than one non-singleton dimension and \"\n                \"the number of rows in A_ub must equal the number of values \"\n                \"in b_ub\")\n        if not(np.isfinite(b_ub).all()):\n            raise ValueError(\n                \"Invalid input for linprog: b_ub must not contain values \"\n                \"inf, nan, or None\")\n\n    try:\n        A_eq = _format_A_constraints(A_eq, n_x, sparse_lhs=sparse_lhs)\n    except ValueError:\n        raise TypeError(\n            \"Invalid input for linprog: A_eq must be a 2D array \"\n            \"of numerical values\")\n    else:\n        n_eq = A_eq.shape[0]\n        if len(A_eq.shape) != 2 or A_eq.shape[1] != n_x:\n            raise ValueError(\n                \"Invalid input for linprog: A_eq must have exactly two \"\n                \"dimensions, and the number of columns in A_eq must be \"\n                \"equal to the size of c\")\n\n        if (sps.issparse(A_eq) and not np.isfinite(A_eq.data).all()\n                or not sps.issparse(A_eq) and not np.isfinite(A_eq).all()):\n            raise ValueError(\n                \"Invalid input for linprog: A_eq must not contain values \"\n                \"inf, nan, or None\")\n    try:\n        b_eq = _format_b_constraints(b_eq)\n\n    except ValueError:\n        raise TypeError(\n            \"Invalid input for linprog: b_eq must be a 1D array of \"\n            \"numerical values, each representing the upper bound of an \"\n            \"inequality constraint (row) in A_eq\")\n    else:\n        if b_eq.shape != (n_eq,):\n            raise ValueError(\n                \"Invalid input for linprog: b_eq must be a 1D array; b_eq \"\n                \"must not have more than one non-singleton dimension and \"\n                \"the number of rows in A_eq must equal the number of values \"\n                \"in b_eq\")\n        if not(np.isfinite(b_eq).all()):\n            raise ValueError(\n                \"Invalid input for linprog: b_eq must not contain values \"\n                \"inf, nan, or None\")\n\n    # x0 gives a (optional) starting solution to the solver. If x0 is None,\n    # skip the checks. Initial solution will be generated automatically.\n    if x0 is not None:\n        try:\n            x0 = np.array(x0, dtype=float, copy=True).squeeze()\n        except ValueError:\n            raise TypeError(\n                \"Invalid input for linprog: x0 must be a 1D array of \"\n                \"numerical coefficients\")\n        if x0.ndim == 0:\n            x0 = x0.reshape((-1))\n        if len(x0) == 0 or x0.ndim != 1:\n            raise ValueError(\n                \"Invalid input for linprog: x0 should be a 1D array; it \"\n                \"must not have more than one non-singleton dimension\")\n        if not x0.size == c.size:\n            raise ValueError(\n                \"Invalid input for linprog: x0 and c should contain the \"\n                \"same number of elements\")\n        if not np.isfinite(x0).all():\n            raise ValueError(\n            \"Invalid input for linprog: x0 must not contain values \"\n            \"inf, nan, or None\")\n\n    # \"If a sequence containing a single tuple is provided, then min and max\n    # will be applied to all variables in the problem.\"\n    # linprog doesn't treat this right: it didn't accept a list with one tuple\n    # in it\n    try:\n        if isinstance(bounds, str):\n            raise TypeError\n        if bounds is None or len(bounds) == 0:\n            bounds = [(0, None)] * n_x\n        elif len(bounds) == 1:\n            b = bounds[0]\n            if len(b) != 2:\n                raise ValueError(\n                    \"Invalid input for linprog: exactly one lower bound and \"\n                    \"one upper bound must be specified for each element of x\")\n            bounds = [b] * n_x\n        elif len(bounds) == n_x:\n            try:\n                len(bounds[0])\n            except BaseException:\n                bounds = [(bounds[0], bounds[1])] * n_x\n            for i, b in enumerate(bounds):\n                if len(b) != 2:\n                    raise ValueError(\n                        \"Invalid input for linprog, bound \" +\n                        str(i) +\n                        \" \" +\n                        str(b) +\n                        \": exactly one lower bound and one upper bound must \"\n                        \"be specified for each element of x\")\n        elif (len(bounds) == 2 and np.isreal(bounds[0])\n                and np.isreal(bounds[1])):\n            bounds = [(bounds[0], bounds[1])] * n_x\n        else:\n            raise ValueError(\n                \"Invalid input for linprog: exactly one lower bound and one \"\n                \"upper bound must be specified for each element of x\")\n\n        clean_bounds = []  # also creates a copy so user's object isn't changed\n        for i, b in enumerate(bounds):\n            if b[0] is not None and b[1] is not None and b[0] > b[1]:\n                raise ValueError(\n                    \"Invalid input for linprog, bound \" +\n                    str(i) +\n                    \" \" +\n                    str(b) +\n                    \": a lower bound must be less than or equal to the \"\n                    \"corresponding upper bound\")\n            if b[0] == np.inf:\n                raise ValueError(\n                    \"Invalid input for linprog, bound \" +\n                    str(i) +\n                    \" \" +\n                    str(b) +\n                    \": infinity is not a valid lower bound\")\n            if b[1] == -np.inf:\n                raise ValueError(\n                    \"Invalid input for linprog, bound \" +\n                    str(i) +\n                    \" \" +\n                    str(b) +\n                    \": negative infinity is not a valid upper bound\")\n            lb = float(b[0]) if b[0] is not None and b[0] != -np.inf else None\n            ub = float(b[1]) if b[1] is not None and b[1] != np.inf else None\n            clean_bounds.append((lb, ub))\n        bounds = clean_bounds\n    except ValueError as e:\n        if \"could not convert string to float\" in e.args[0]:\n            raise TypeError\n        else:\n            raise e\n    except TypeError as e:\n        print(e)\n        raise TypeError(\n            \"Invalid input for linprog: bounds must be a sequence of \"\n            \"(min,max) pairs, each defining bounds on an element of x \")\n\n    return c, A_ub, b_ub, A_eq, b_eq, bounds, x0\n\n\ndef _get_Abc(c,A_ub=None, b_ub=None, A_eq=None, b_eq=None, bounds=None,\n             x0=None, undo=[],c0=0):\n    \"\"\"\n    Given a linear programming problem of the form:\n    Minimize::\n        c @ x\n    Subject to::\n        A_ub @ x <= b_ub\n        A_eq @ x == b_eq\n         lb <= x <= ub\n    where ``lb = 0`` and ``ub = None`` unless set in ``bounds``.\n    Return the problem in standard form:\n    Minimize::\n        c @ x\n    Subject to::\n        A @ x == b\n            x >= 0\n    by adding slack variables and making variable substitutions as necessary.\n    Parameters\n    ----------\n    c : 1D array\n        Coefficients of the linear objective function to be minimized.\n        Components corresponding with fixed variables have been eliminated.\n    c0 : float\n        Constant term in objective function due to fixed (and eliminated)\n        variables.\n    A_ub : 2D array, optional\n        2D array such that ``A_ub @ x`` gives the values of the upper-bound\n        inequality constraints at ``x``.\n    b_ub : 1D array, optional\n        1D array of values representing the upper-bound of each inequality\n        constraint (row) in ``A_ub``.\n    A_eq : 2D array, optional\n        2D array such that ``A_eq @ x`` gives the values of the equality\n        constraints at ``x``.\n    b_eq : 1D array, optional\n        1D array of values representing the RHS of each equality constraint\n        (row) in ``A_eq``.\n    bounds : sequence of tuples\n        ``(min, max)`` pairs for each element in ``x``, defining\n        the bounds on that parameter. Use None for each of ``min`` or\n        \"\"\"\n\n# ==========================================\n# File: code/NSP/test.py\n# Function/Context: correction_single_obj\n# ==========================================\nimport sys\nfrom collections import defaultdict\nimport numpy as np\nimport gurobipy as gp\nfrom gurobipy import GRB\nfrom sklearn.metrics import mean_squared_error\nfrom numpy import inf\nfrom ip_model_whole import solve_LP\nimport torch\n\nnurse_num = 15\nday_num = 7\nshift_num = 3\ndecision_num = nurse_num * day_num * shift_num\nday_shift_num = day_num * shift_num\ntestmarkNum = 90\npenaltyTerm = 4\n\ndef gen_matrix(nurse_num,day_num,shift_num,serve_patient_num,decision_num,day_shift_num):\n    A = np.zeros((nurse_num*day_num, decision_num))\n    for i in range(nurse_num):\n        for j in range(day_num):\n            for q in range(shift_num):\n                A[i*day_num+j][i*day_shift_num+shift_num*j+q] = 1\n    b = np.ones(nurse_num*day_num)\n\n    G1 = np.zeros((day_shift_num, decision_num))\n    for j in range(day_shift_num):\n        for i in range(nurse_num):\n            G1[j][i*day_shift_num+j] = -serve_patient_num[i]\n    G2 = np.zeros((nurse_num*(day_num-1), decision_num))\n    for i in range(nurse_num):\n        for j in range(day_num-1):\n            G2[i*(day_num-1)+j][i*day_shift_num+shift_num*(j+1)-1] = 1\n            G2[i*(day_num-1)+j][i*day_shift_num+shift_num*(j+1)] = 1\n    h2 = np.ones(nurse_num*(day_num-1))\n    G3 = np.identity(decision_num)\n    h3 = np.ones(decision_num)\n    \n    G = np.concatenate([G1, G2, G3], axis=0)\n    \n    return A,b,G,h2,h3\n\ndef actual_obj(c, A, b, G, real_patient_num, h2, h3, n_instance):\n    obj_list = []\n    rowSizeA = A.shape[0]\n    rowSizeG = G.shape[0]\n    c = c.tolist()\n    A = A.tolist()\n    b = b.tolist()\n    G = G.tolist()\n    \n    for num in range(n_instance):\n        h1 = np.zeros(day_shift_num)\n        cnt = num * day_shift_num\n        for i in range(day_shift_num):\n            h1[i] = -real_patient_num[cnt]\n            cnt = cnt + 1\n        h = np.concatenate([h1, h2, h3], axis=0)\n        h = h.tolist()\n\n        m = gp.Model()\n        m.setParam('OutputFlag', 0)\n        x = m.addVars(decision_num, vtype=GRB.BINARY, name='x')\n        \n        m.setObjective(x.prod(c), GRB.MAXIMIZE)\n        for i in range(rowSizeA):\n            m.addConstr(x.prod(A[i]) == b[i])\n        for j in range(rowSizeG):\n            m.addConstr(x.prod(G[j]) <= h[j])\n\n        m.optimize()\n        sol = []\n        try:\n            for i in range(decision_num):\n                sol.append(x[i].x)\n            objective = m.objVal\n        except:\n            for i in range(decision_num):\n                sol.append(0)\n            objective = 0\n\n        obj_list.append(objective)\n        \n    return np.array(obj_list)\n    \n    \ndef correction_single_obj(c, A, b, G, real_patient_num, pre_patient_num, h2, h3, penalty):\n    rowSizeA = A.shape[0]\n    rowSizeG = G.shape[0]\n    c = c.tolist()\n    A = A.tolist()\n    b = b.tolist()\n    G = G.tolist()\n    \n    pre_h1 = np.zeros(day_shift_num)\n    real_h1 = np.zeros(day_shift_num)\n    for i in range(day_shift_num):\n        pre_h1[i] = -pre_patient_num[i]\n        real_h1[i] = -real_patient_num[i]\n    pre_h = np.concatenate([pre_h1, h2, h3], axis=0)\n    real_h = np.concatenate([real_h1, h2, h3], axis=0)\n    pre_h = pre_h.tolist()\n    real_h1 = real_h1.tolist()\n\n    m = gp.Model()\n    m.setParam('OutputFlag', 0)\n    x = m.addVars(decision_num, vtype=GRB.BINARY, name='x')\n    \n    m.setObjective(x.prod(c), GRB.MAXIMIZE)\n    for i in range(rowSizeA):\n        m.addConstr(x.prod(A[i]) == b[i])\n    for j in range(rowSizeG):\n        m.addConstr(x.prod(G[j]) <= pre_h[j])\n\n    m.optimize()\n    \n    try:\n        predSol = []\n        for i in range(decision_num):\n            predSol.append(x[i].x)\n    except:\n        predSol = []\n        for i in range(decision_num):\n            predSol.append(0)\n\n    m2 = gp.Model()\n    m2.setParam('OutputFlag', 0)\n    x = m2.addVars(decision_num, vtype=GRB.BINARY, name='x')\n    sigma = m2.addVars(decision_num, vtype=GRB.BINARY, name='sigma')\n\n    OBJ = x.prod(c)\n    for i in range(decision_num):\n        OBJ = OBJ - penalty[i] * (5 - c[i]) * (5 - c[i]) * sigma[i]\n    m2.setObjective(OBJ, GRB.MAXIMIZE)\n\n    for i in range(rowSizeA):\n        m2.addConstr(x.prod(A[i]) == b[i])\n    for j in range(rowSizeG):\n        m2.addConstr(x.prod(G[j]) <= real_h[j])\n    for i in range(decision_num):\n        m2.addConstr(sigma[i] >= x[i] - predSol[i])\n    \n\n    m2.optimize()\n    sol = []\n    try:\n        for i in range(decision_num):\n            sol.append(x[i].x)\n        objective = m2.objVal\n    except:\n        for i in range(decision_num):\n            sol.append(0)\n        objective = 0\n    \n    return objective\n      \n\nstartmark = int(sys.argv[1])\nendmark = startmark + 10\nmethodList = ['2S']\ntestList = [5,8]\n\nfor methodName in methodList:\n    print(methodName)\n    for testmark in testList:\n        perference = np.loadtxt('./data/preference/preference(' + str(testmark) + ').txt')\n        c = np.zeros(decision_num)\n        for i in range(nurse_num):\n            for j in range(day_shift_num):\n                c[i*day_shift_num+j] = perference[i][j]\n        serve_patient_num = np.loadtxt('./data/serve_patient_num/serve_patient_num(' + str(testmark) + ').txt')\n        patient_num_temp = np.loadtxt('./data/' + methodName + '/' + methodName + '_penalty' + str(penaltyTerm) + '(' + str(testmark) + ').txt')\n        penalty_test = np.loadtxt('./data/penalty' + str(penaltyTerm) + '/test_penalty' + str(penaltyTerm) + '/test_penalty(' + str(testmark) + ').txt')\n        real_patient_num = patient_num_temp[:,1]\n        pre_patient_num = patient_num_temp[:,2]\n\n        A,b,G,h2,h3 = gen_matrix(nurse_num,day_num,shift_num,serve_patient_num,decision_num,day_shift_num)\n        real_obj = actual_obj(c, A, b, G, real_patient_num, h2, h3, n_instance=testmarkNum)\n        corr_obj_list = []\n        for testNum in range(testmarkNum):\n            real_patient = {}\n            pre_patient = {}\n            for i in range(day_shift_num):\n                real_patient[i] = real_patient_num[i+testNum*day_shift_num]\n                pre_patient[i] = pre_patient_num[i+testNum*day_shift_num]\n\n            penalty = np.zeros(decision_num)\n            for j in range(decision_num):\n                penalty[j] = penalty_test[j+testNum*decision_num]\n\n            corrrlst = correction_single_obj(c, A, b, G, real_patient, pre_patient, h2, h3, penalty)\n            corr_obj_list.append(corrrlst)\n\n        print(\"corrReg: \", sum(abs(real_obj - np.array(corr_obj_list)))/testmarkNum, \"TOV: \", sum(real_obj)/testmarkNum)\n\n# ==========================================\n# File: code/NSP/train.py\n# Function/Context: Intopt\n# ==========================================\nimport sys\nimport numpy as np\nimport random\nimport pandas as pd\nimport math, time\nimport itertools\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\nimport datetime\nimport torch\nfrom torch import nn, optim\nfrom torch.autograd import Variable\nimport torch.utils.data as data_utils\nfrom torch.utils.data.dataset import Dataset\nfrom sklearn.preprocessing import StandardScaler\nimport gurobipy as gp\nimport logging\nimport copy\nfrom collections import defaultdict\nimport joblib\nimport gurobipy as gp\nfrom gurobipy import GRB\n\nnurse_num = 15\nday_num = 7\nshift_num = 3\ndecision_num = nurse_num * day_num * shift_num\nday_shift_num = day_num * shift_num\npenaltyTerm = 0.25\nfeatureNum = 8\ntrain_loss = 0\n\ndef gen_matrix(nurse_num,day_num,shift_num,serve_patient_num,decision_num,day_shift_num):\n    # Each nurse must be scheduled for exactly one shift per day\n    A = np.zeros((nurse_num*day_num, decision_num))\n    for i in range(nurse_num):\n        for j in range(day_num):\n            for q in range(shift_num):\n                A[i*day_num+j][i*day_shift_num+shift_num*j+q] = 1\n    #print(A)\n    b = np.ones(nurse_num*day_num)\n\n    # Each schedule must satisfy the patients' need\n    G1 = np.zeros((day_shift_num, decision_num))\n    for j in range(day_shift_num):\n        for i in range(nurse_num):\n            G1[j][i*day_shift_num+j] = -serve_patient_num[i]\n#    G1 = serve_patient_num * G1\n    #print(G1)\n    # No nurse may be scheduled to work a night shift followed immendiately by a morning shift\n    G2 = np.zeros((nurse_num*(day_num-1), decision_num))\n    for i in range(nurse_num):\n        for j in range(day_num-1):\n            G2[i*(day_num-1)+j][i*day_shift_num+shift_num*(j+1)-1] = 1\n            G2[i*(day_num-1)+j][i*day_shift_num+shift_num*(j+1)] = 1\n    #print(G2)\n    h2 = np.ones(nurse_num*(day_num-1))\n    G3 = np.identity(decision_num)\n    h3 = np.ones(decision_num)\n    \n    G = np.concatenate([G1, G2, G3], axis=0)\n    \n    return A,b,G,h2,h3\n\ndef get_Xs1Xs2(c, A, b, G, real_patient_num, pre_patient_num, h2, h3, penalty):\n    rowSizeA = A.shape[0]\n    rowSizeG = G.shape[0]\n    c = c.tolist()\n    A = A.tolist()\n    b = b.tolist()\n    G = G.tolist()\n    \n    pre_h1 = np.zeros(day_shift_num)\n    real_h1 = np.zeros(day_shift_num)\n    for i in range(day_shift_num):\n        pre_h1[i] = -pre_patient_num[i]\n        real_h1[i] = -real_patient_num[i]\n    pre_h = np.concatenate([pre_h1, h2, h3], axis=0)\n    real_h = np.concatenate([real_h1, h2, h3], axis=0)\n    pre_h = pre_h.tolist()\n    real_h1 = real_h1.tolist()\n\n    m = gp.Model()\n    m.setParam('OutputFlag', 0)\n    x = m.addVars(decision_num, vtype=GRB.BINARY, name='x')\n    \n    m.setObjective(x.prod(c), GRB.MAXIMIZE)\n    for i in range(rowSizeA):\n        m.addConstr(x.prod(A[i]) == b[i])\n    for j in range(rowSizeG):\n        m.addConstr(x.prod(G[j]) <= pre_h[j])\n\n    m.optimize()\n    predSol = np.zeros(decision_num)\n    try:\n        for i in range(decision_num):\n            predSol[i] = x[i].x\n    except:\n        for i in range(decision_num):\n            predSol[i] = 0\n\n    # Stage 2:\n    m2 = gp.Model()\n    m2.setParam('OutputFlag', 0)\n    x = m2.addVars(decision_num, vtype=GRB.BINARY, name='x')\n    sigma = m2.addVars(decision_num, vtype=GRB.BINARY, name='sigma')\n\n    OBJ = x.prod(c)\n    for i in range(decision_num):\n        OBJ = OBJ - penalty[i] * (5 - c[i]) * (5 - c[i]) * sigma[i]\n    m2.setObjective(OBJ, GRB.MAXIMIZE)\n\n    for i in range(rowSizeA):\n        m2.addConstr(x.prod(A[i]) == b[i])\n    for j in range(rowSizeG):\n        m2.addConstr(x.prod(G[j]) <= real_h[j])\n    for i in range(decision_num):\n        m2.addConstr(sigma[i] >= x[i] - predSol[i])\n    \n\n    m2.optimize()\n    sol = np.zeros(decision_num)\n    try:\n        for i in range(decision_num):\n            sol[i] = x[i].x\n    except:\n        for i in range(decision_num):\n            sol[i] = 0\n        \n    return predSol, sol\n\n\ndef correction_single_obj(c, A, b, G, real_patient_num, pre_patient_num, h2, h3, penalty):\n    rowSizeA = A.shape[0]\n    rowSizeG = G.shape[0]\n    c = c.tolist()\n    A = A.tolist()\n    b = b.tolist()\n    G = G.tolist()\n    \n    pre_h1 = np.zeros(day_shift_num)\n    real_h1 = np.zeros(day_shift_num)\n    for i in range(day_shift_num):\n        pre_h1[i] = -pre_patient_num[i]\n        real_h1[i] = -real_patient_num[i]\n    pre_h = np.concatenate([pre_h1, h2, h3], axis=0)\n    real_h = np.concatenate([real_h1, h2, h3], axis=0)\n    pre_h = pre_h.tolist()\n    real_h1 = real_h1.tolist()\n\n    m = gp.Model()\n    m.setParam('OutputFlag', 0)\n    x = m.addVars(decision_num, vtype=GRB.BINARY, name='x')\n    \n    m.setObjective(x.prod(c), GRB.MAXIMIZE)\n    for i in range(rowSizeA):\n        m.addConstr(x.prod(A[i]) == b[i])\n    for j in range(rowSizeG):\n        m.addConstr(x.prod(G[j]) <= pre_h[j])\n\n    m.optimize()\n    \n    try:\n        predSol = []\n        for i in range(decision_num):\n            predSol.append(x[i].x)\n    except:\n        predSol = []\n        for i in range(decision_num):\n            predSol.append(0)\n\n    # Stage 2:\n    m2 = gp.Model()\n    m2.setParam('OutputFlag', 0)\n    x = m2.addVars(decision_num, vtype=GRB.BINARY, name='x')\n    sigma = m2.addVars(decision_num, vtype=GRB.BINARY, name='sigma')\n\n    OBJ = x.prod(c)\n    for i in range(decision_num):\n        OBJ = OBJ - penalty[i] * (5 - c[i]) * (5 - c[i]) * sigma[i]\n    m2.setObjective(OBJ, GRB.MAXIMIZE)\n\n    for i in range(rowSizeA):\n        m2.addConstr(x.prod(A[i]) == b[i])\n    for j in range(rowSizeG):\n        m2.addConstr(x.prod(G[j]) <= real_h[j])\n    for i in range(decision_num):\n        m2.addConstr(sigma[i] >= x[i] - predSol[i])\n    \n\n    m2.optimize()\n    sol = []\n    try:\n        for i in range(decision_num):\n            sol.append(x[i].x)\n        objective = m2.objVal\n    except:\n        for i in range(decision_num):\n            sol.append(0)\n        objective = 0\n    \n    return objective\n      \n\ndef weight_init(m):\n    if isinstance(m, nn.Linear):\n        nn.init.xavier_normal_(m.weight)\n        nn.init.constant_(m.bias, 0)\n\n    elif isinstance(m, nn.Conv2d):\n        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n\n    elif isinstance(m, nn.BatchNorm2d):\n        nn.init.constant_(m.weight, 1)\n        nn.init.constant_(m.bias, 0)\n\ndef make_fc(num_layers, num_features, num_targets=1,\n            activation_fn = nn.ReLU,intermediate_size=2*featureNum, regularizers = True):\n    net_layers = [nn.Linear(num_features, intermediate_size),\n         activation_fn()]\n    for hidden in range(num_layers-2):\n        net_layers.append(nn.Linear(intermediate_size, intermediate_size))\n        net_layers.append(activation_fn())\n    net_layers.append(nn.Linear(intermediate_size, num_targets))\n    net_layers.append(nn.ReLU())\n    return nn.Sequential(*net_layers)\n\nclass MyCustomDataset():\n    def __init__(self, feature, value):\n        self.feature = feature\n        self.value = value\n\n    def __len__(self):\n        return len(self.value)\n\n    def __getitem__(self, idx):\n        return self.feature[idx], self.value[idx]\n        \nimport sys\nimport ip_model_whole as ip_model_wholeFile\nfrom ip_model_whole import IPOfunc\nfrom ip_model_whole import solve_LP\n\nclass Intopt:\n    def __init__(self, c, G, A, b, h2, h3, penalty, n_features, num_layers=4, smoothing=False, thr=0.1, max_iter=None, method=1, mu0=None,\n                 damping=1e-7, target_size=1, epochs=8, optimizer=optim.Adam,\n                 batch_size=day_shift_num, **hyperparams):\n        self.c = c\n        self.G = G\n        self.A = A\n        self.b = b\n        self.h2 = h2\n        self.h3 = h3\n        self.penalty = penalty\n        self.target_size = target_size\n        self.n_features = n_features\n        self.damping = damping\n        self.num_layers = num_layers\n\n        self.smoothing = smoothing\n        self.thr = thr\n        self.max_iter = max_iter\n        self.method = method\n        self.mu0 = mu0\n\n        self.optimizer = optimizer\n        self.batch_size = batch_size\n        self.hyperparams = hyperparams\n        self.epochs = epochs\n        # print(\"embedding size {} n_features {}\".format(embedding_size, n_features))\n\n#        self.model = Net(n_features=n_features, target_size=target_size)\n        self.model = make_fc(num_layers=self.num_layers,num_features=n_features)\n        #self.model.apply(weight_init)\n#        w1 = self.model[0].weight\n#        print(w1)\n\n        self.optimizer = optimizer(self.model.parameters(), **hyperparams)\n\n    def fit(self, feature, value):\n        logging.info(\"Intopt\")\n        train_df = MyCustomDataset(feature, value)\n\n        criterion = nn.L1Loss(reduction='mean')  # nn.MSELoss(reduction='mean')\n        grad_list = np.zeros(self.epochs)\n        for e in range(self.epochs):\n          total_loss = 0\n#          for parameters in self.model.parameters():\n#            print(parameters)\n          if e < 0:\n            lr = 1e-5\n            self.batch_size=2*day_shift_num\n            #print('stage 1')\n            train_dl = data_utils.DataLoader(train_df, batch_size=self.batch_size, shuffle=False)\n            for feature, value in train_dl:\n                self.optimizer.zero_grad()\n                op = self.model(feature).squeeze()\n                while torch.min(op) <= 0 or torch.isnan(op).any() or torch.isinf(op).any():\n                    self.optimizer.zero_grad()\n#                    self.model.__init__(self.n_features, self.target_size)\n                    self.model = make_fc(num_layers=self.num_layers,num_features=self.n_features)\n                    op = self.model(feature).squeeze()\n                #print(op)\n                \n                loss = criterion(op, value)\n                total_loss += loss.item()\n                grad_list[e] = total_loss\n                loss.backward()\n                self.optimizer.step()\n            print(\"Epoch{} ::loss {} ->\".format(e,total_loss))\n                \n          else:\n            self.batch_size=day_shift_num\n            #print('stage 2')\n            train_dl = data_utils.DataLoader(train_df, batch_size=self.batch_size, shuffle=False)\n            instance_num = 0\n            batchCnt = 0\n#            train_loss = np.zeros(1)\n            \n            for feature, value in train_dl:\n                self.optimizer.zero_grad()\n                op = self.model(feature).squeeze()\n                while torch.min(op) <= 0 or torch.isnan(op).any() or torch.isinf(op).any():\n                    self.optimizer.zero_grad()\n#                    self.model.__init__(self.n_features, self.target_size)\n                    self.model = make_fc(num_layers=self.num_layers,num_features=self.n_features)\n                    op = self.model(feature).squeeze()\n    \n                penaltyVector = np.zeros(decision_num)\n                for i in range(decision_num):\n                    penaltyVector[i] = self.penalty[i+instance_num*decision_num]\n                \n                c_torch = torch.from_numpy(self.c).float()\n                G_torch = torch.from_numpy(self.G).float()\n                A_torch = torch.from_numpy(self.A).float()\n                b_torch = torch.from_numpy(self.b).float()\n                h2_torch = torch.from_numpy",
  "description": "Combined Analysis:\n- [code/0-1 knapsack/ip_model_whole.py]: This file implements the core two-stage optimization logic for the 0-1 knapsack problem. The code sets up the integer programming model using Gurobi and includes helper functions for constraint formatting and input cleaning. The main function (not fully shown in provided snippet) computes gradients of the objective coefficients (c) and constraint matrix (G) with respect to the two-stage solution, which is essential for the end-to-end training of the prediction model. The implementation aligns with the paper's two-stage framework: Stage 1 solves the MILP with predicted parameters, and Stage 2 re-optimizes with true parameters and a penalty for deviation from Stage 1. The gradient computation enables backpropagation through the optimization layer to train the predictor.\n- [code/0-1 knapsack/test.py]: This file implements the two-stage predict+optimize framework for the 0-1 knapsack problem. The core logic is in the 'correction_single_obj' function which exactly follows the paper's two-stage approach: Stage 1 solves the knapsack with predicted parameters to get a soft-commitment solution; Stage 2 re-optimizes with true parameters while penalizing deviations from Stage 1 via compensation fees. The mathematical model uses binary variables x (selection) and sigma (removal), with constraints linking stages and a penalty term in the objective. The test code evaluates the two-stage method by comparing corrected objective values with true optimal values, computing post-hoc regret metrics.\n- [code/0-1 knapsack/train.py]: This file implements the two-stage predict+optimize framework for the 0-1 knapsack problem as described in the paper. The core logic includes: 1) A neural network (make_fc) predicting unknown parameters (weights and values) from features. 2) Two-stage optimization: Stage 1 solves the knapsack with predicted parameters to obtain a soft-commitment solution (x_s1). Stage 2 re-optimizes with true parameters, penalizing deviations from x_s1 via a compensation fee. 3) End-to-end training: The Intopt class trains the predictor by minimizing post-hoc regret using gradient-based optimization through a differentiable relaxation of the MILP (IPOfunc). The loss function combines the corrected objective and penalty costs, aligning with the paper's mathematical model.\n- [code/Alloy production/ip_model_whole.py]: This file implements core preprocessing and constraint formatting functions for linear programming problems, which are essential components of the two-stage predict+optimize framework. The code includes functions for cleaning and standardizing optimization problem inputs (_clean_inputs, _format_A_constraints, _format_b_constraints) similar to those used in scipy.optimize.linprog. The presence of torch tensors (x_s1), gurobipy imports, and gradient-related imports (Variable, Function) suggests this is part of a differentiable optimization pipeline. The file appears to be setting up the infrastructure for solving MILPs with unknown parameters, though the complete two-stage optimization logic (Stage 1 with predicted parameters, Stage 2 with true parameters and penalty) is not fully visible in the provided snippet. The helper functions are crucial for the constraint handling described in the paper's optimization model.\n- [code/Alloy production/test.py]: This file implements the core two-stage Predict+Optimize framework for the alloy production problem. The key function 'correction_single_obj' exactly mirrors the paper's algorithm: Stage 1 solves the LP with predicted constraint parameters (preG) to obtain a soft-commitment solution (predSol); Stage 2 re-optimizes with true parameters (realG) while penalizing deviations from Stage 1 via correction variables (sigma). The penalty term (1 + penalty[i]) * c[i] * sigma[i] encodes modification costs. The 'actual_obj' function computes the true optimal objective for regret calculation. The code assumes continuous variables (GRB.CONTINUOUS) and uses Gurobi as the solver, aligning with the paper's MILP framework (though integer variables are not used here).\n- [code/Alloy production/train.py]: This file implements the two-stage predict+optimize framework for the alloy production problem (a continuous linear program variant). The core logic includes: 1) Neural network predictor for unknown constraint matrix G, 2) Stage 1 optimization using predicted G, 3) Stage 2 correction optimization with true G and deviation penalties, 4) End-to-end training using the post-hoc regret (Stage 2 total cost) as loss. The Intopt.fit method orchestrates the two-stage training, using IPOfunc (differentiable optimizer) for gradient propagation. The actual_obj and correction_single_obj functions implement the baseline and two-stage optimization respectively using Gurobi.\n- [code/NSP/ip_model_whole.py]: This file implements the core optimization model setup and preprocessing for the nurse scheduling problem (NSP) within the two-stage predict+optimize framework. It includes:\n1. Problem-specific constants (nurse_num, day_num, shift_num, etc.) for the NSP instance.\n2. A tensor x_s1 representing the Stage 1 solution (soft commitment).\n3. Helper functions (_format_A_constraints, _format_b_constraints, _clean_inputs, _get_Abc) that format and validate linear programming constraints, aligning with the paper's mathematical model structure (Ax=b, Gxh, x0).\n4. These functions prepare the MILP inputs for the two-stage optimization: Stage 1 uses predicted parameters, Stage 2 uses true parameters with a penalty for deviation from x_s1.\n5. The code integrates with PyTorch for gradient-based training and Gurobi for MILP solving, supporting the end-to-end learning of the predictor to minimize post-hoc regret.\n6. The preprocessing steps ensure the MILP is in standard form, which is critical for the KKT-based differentiation method described in the paper.\n- [code/NSP/test.py]: This file implements the core two-stage predict+optimize algorithm for the Nurse Scheduling Problem (NSP). The function 'correction_single_obj' exactly matches the paper's two-stage framework: Stage 1 solves the MILP with predicted parameters (pre_patient_num) to obtain a soft-commitment solution (predSol). Stage 2 re-solves the MILP with true parameters (real_patient_num) while adding a penalty term in the objective for deviations from the Stage 1 solution. The penalty is applied via auxiliary variables sigma and a quadratic penalty term. The main script evaluates this two-stage method by comparing the corrected objective (corr_obj_list) against the true optimal objective (real_obj).\n- [code/NSP/train.py]: This file implements the core two-stage predict+optimize framework for the Nurse Scheduling Problem (NSP). It contains: 1) Problem formulation functions (gen_matrix) that generate constraint matrices A, G and vectors b, h for the MILP; 2) Two-stage optimization functions (get_Xs1Xs2, correction_single_obj) that solve Stage 1 with predicted parameters and Stage 2 with true parameters plus deviation penalties; 3) The Intopt class that trains a neural network predictor end-to-end using gradient-based optimization through a relaxed MILP (via ip_model_whole). The implementation matches the paper's mathematical model: maximize c^T x subject to Ax=b, Gxh, x binary, with unknown parameters in h (patient demands). The two-stage approach uses predicted h in Stage 1, then true h in Stage 2 with penalty terms for schedule changes.",
  "dependencies": [
    "sklearn",
    "gurobipy",
    "collections.defaultdict",
    "copy",
    "sys",
    "_get_Abc",
    "time",
    "logging",
    "MyCustomDataset",
    "scipy.sparse",
    "correction_single_obj",
    "warnings",
    "numpy.inf",
    "_clean_inputs",
    "scipy",
    "math",
    "_format_A_constraints",
    "get_Xs1Xs2",
    "datetime",
    "ip_model_whole.solve_LP",
    "ip_model_whole",
    "sklearn.metrics.mean_squared_error",
    "random",
    "scipy.linalg",
    "remove_redundancy",
    "numpy",
    "joblib",
    "_format_b_constraints",
    "IPOfunc",
    "collections",
    "scipy.optimize",
    "pandas",
    "torch",
    "make_fc",
    "itertools"
  ]
}